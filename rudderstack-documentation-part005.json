[
  {
    "url": "https://www.rudderstack.com/docs/dashboard-guides/user-management/",
    "markdown": "# Members (User Management) | RudderStack Docs\n\nManage users across your organization and workspaces and assign them relevant roles.\n\n* * *\n\n*     6 minute read  \n    \n\nRudderStack’s user management feature lets you easily collaborate with other members of your organization. It provides different options to manage users and their permissions at the [workspace](https://www.rudderstack.com/docs/dashboard-guides/workspaces/#workspaces) level.\n\n## Manage users in organization\n\nRudderStack lets you invite users and assign them relevant user roles within your [organization](https://www.rudderstack.com/docs/dashboard-guides/workspaces/#organization).\n\nOnly the [Org OwnerAn Org Owner owns the RudderStack organization.](https://www.rudderstack.com/docs/resources/glossary/#org-owner) and [Org AdminAn Org Admin is an invited user with full access to the organization.](https://www.rudderstack.com/docs/resources/glossary/#org-admin) can invite/remove other members and edit their access policies. The **Org Admins** can also invite/remove and edit the access policies of other **Org Admins**. However, no one can edit the access policy or remove an **Org Owner**.\n\n### Invite users\n\nYou can invite users to an organization either as **Org Admin** or **Org Member**. The following sections detail the steps to invite users with these permissions.\n\n#### Org Admin\n\nTo invite a user with **Org Admin** permissions:\n\n1.  Go to **Settings** > **Organization**. Go to the **Members** tab and click the **Invite member** button:\n\n[![Invite member](https://www.rudderstack.com/docs/images/dashboard-guides/user-management/invite-user.webp)](https://www.rudderstack.com/docs/images/dashboard-guides/user-management/invite-user.webp)\n\n2.  Enter the member’s **Email address** and select [**Organization Role**](#organization-roles) as **admin** from the dropdown. You can send multiple invites at once by clicking **Add another member**. Finally, click **Send invite(s)** to proceed.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> The **Admin** user gets access to all the workspaces within that organization by default.\n\n[![Set email address and org role](https://www.rudderstack.com/docs/images/dashboard-guides/user-management/invite-user-role.webp)](https://www.rudderstack.com/docs/images/dashboard-guides/user-management/invite-user-role.webp)\n\n3.  Enter your password to send the invite(s). The user will be automatically added to the organization once they accept the invitation.\n\n#### Org Member\n\nTo invite a user with **Org Member** permissions:\n\n1.  Go to **Settings** > **Organization**. Go to the **Members** tab and click the **Invite member** button:\n\n[![Invite member](https://www.rudderstack.com/docs/images/dashboard-guides/user-management/invite-user.webp)](https://www.rudderstack.com/docs/images/dashboard-guides/user-management/invite-user.webp)\n\n2.  Enter the member’s **Email address** and select [**Organization Role**](#organization-roles) as **member** from the dropdown. You can send multiple invites at once by clicking **Add another member**. Finally, click **Send invite(s)** to proceed.\n\n[![Set email address and org role](https://www.rudderstack.com/docs/images/dashboard-guides/user-management/invite-user-member.webp)](https://www.rudderstack.com/docs/images/dashboard-guides/user-management/invite-user-member.webp)\n\n2.  Click **Add workspaces** next to the invited user.\n\n[![Invite member](https://www.rudderstack.com/docs/images/dashboard-guides/user-management/org-member-workspace.webp)](https://www.rudderstack.com/docs/images/dashboard-guides/user-management/org-member-workspace.webp)\n\n3.  Select the specific workspace(s) you want to add the user to and click **Add workspace(s)**.\n\n[![Invite member](https://www.rudderstack.com/docs/images/dashboard-guides/user-management/member-add-workspace.webp)](https://www.rudderstack.com/docs/images/dashboard-guides/user-management/member-add-workspace.webp)\n\nThe user will be automatically added to the specified workspace(s) once they accept the invitation.\n\n### Remove users\n\nTo remove a user from the organization, click the meatballs menu next to the user and select **Remove member**. Enter your password to confirm.\n\n[![Edit access policy](https://www.rudderstack.com/docs/images/dashboard-guides/user-management/edit-access-policy.webp)](https://www.rudderstack.com/docs/images/dashboard-guides/user-management/edit-access-policy.webp)\n\n### Edit access policy\n\n1.  In the **Members** tab, click the meatballs menu next to the user and select **Edit access policy**:\n\n[![Edit access policy](https://www.rudderstack.com/docs/images/dashboard-guides/user-management/edit-access-policy-1.webp)](https://www.rudderstack.com/docs/images/dashboard-guides/user-management/edit-access-policy-1.webp)\n\n2.  Click the edit icon corresponding to the Organization role or the specific workspace for which you want to change the user’s permissions.\n\n[![Edit access policy](https://www.rudderstack.com/docs/images/dashboard-guides/user-management/edit-access-policy-3.webp)](https://www.rudderstack.com/docs/images/dashboard-guides/user-management/edit-access-policy-3.webp)\n\n3.  Make the necessary changes, click **Save**, and enter your password to confirm.\n\n[![Edit user role to Member](https://www.rudderstack.com/docs/images/dashboard-guides/user-management/downgrade-user.webp)](https://www.rudderstack.com/docs/images/dashboard-guides/user-management/downgrade-user.webp)\n\nNote that:\n\n*   When you upgrade a user with **Member** role to **Admin**, they will automatically get full edit access to all the resources.\n    \n*   When you downgrade a user with **Admin** role to **Member**, RudderStack resets all access permissions to read-only. You need to manually set the permissions for each resource and click **Save** to update the access policy.\n    \n\n## Manage users in workspaces\n\nRudderStack lets you add/remove users and assign them relevant user roles within your [workspaces](https://www.rudderstack.com/docs/dashboard-guides/workspaces/#workspaces).\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> Only the RudderStack Cloud Starter, Growth, and Enterprise plans have access to [multiple workspaces](https://www.rudderstack.com/docs/dashboard-guides/workspaces/#available-workspaces-by-plan) within an organization.\n\nAs the [Org OwnerAn Org Owner owns the RudderStack organization.](https://www.rudderstack.com/docs/resources/glossary/#org-owner) and [Org AdminAn Org Admin is an invited user with full access to the organization.](https://www.rudderstack.com/docs/resources/glossary/#org-admin) have admin access over all the workspaces within an organization, only they can add/remove **Org members** and edit their access policies for different workspaces.\n\n### Add users\n\nYou can choose to add an existing **Org Member** to a [specific workspace](https://www.rudderstack.com/docs/dashboard-guides/workspaces/#workspace-types):\n\n1.  Click the meatballs menu next to the user and select **Add workspace(s)**.\n\n[![Set email address and org role](https://www.rudderstack.com/docs/images/dashboard-guides/user-management/add-workspace.webp)](https://www.rudderstack.com/docs/images/dashboard-guides/user-management/add-workspace.webp)\n\n2.  Select the workspace(s) you want to add the user to and click **Add workspace(s)**.\n\n[![Set email address and org role](https://www.rudderstack.com/docs/images/dashboard-guides/user-management/user-add-workspace.webp)](https://www.rudderstack.com/docs/images/dashboard-guides/user-management/user-add-workspace.webp)\n\n### Remove users\n\nYou can remove an **Org member** from a specific workspace by clicking the workspace next to the user and clicking **Remove from workspace**, as shown:\n\n[![Remove user from workspace](https://www.rudderstack.com/docs/images/dashboard-guides/user-management/remove-user-workspace.webp)](https://www.rudderstack.com/docs/images/dashboard-guides/user-management/remove-user-workspace.webp)\n\n### Edit access policy\n\n1.  In the **Members** tab, click the workspace next to the user and select **Edit access policy**:\n\n[![Edit access policy](https://www.rudderstack.com/docs/images/dashboard-guides/user-management/edit-access-policy-5.webp)](https://www.rudderstack.com/docs/images/dashboard-guides/user-management/edit-access-policy-5.webp)\n\n2.  Click the edit icon corresponding to the Organization role or the workspace for which you want to change the user’s permissions.\n\n[![Edit access policy](https://www.rudderstack.com/docs/images/dashboard-guides/user-management/edit-user-role-permissions.webp)](https://www.rudderstack.com/docs/images/dashboard-guides/user-management/edit-user-role-permissions.webp)\n\n3.  Make the necessary changes, click **Save**, and enter your password to confirm.\n\n## Role definitions\n\n| Term | Level | Definition |\n| --- | --- | --- |\n| [Organization roles](#organization-roles) | Organization | Highest abstraction level that determines if a user has a full access or customized access to specific resources.<br><br>**Example**: Org Owner, Org Admin, Org Member |\n| [Resource roles](#resource-roles) | Workspace | Predefined access to specific resources. A user can have multiple resource roles that grant access to all or some resources.<br><br>**Example**: Connections Admin/Editor/Viewer, Transformations Admin |\n\n## Role permissions\n\nThe following sections detail the access policies set for organization-level and resource-level roles.\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> All role permissions are packaged differently in RudderStack Cloud plans. See [Plan-wise features](#plan-wise-features) for more information.\n\n### Organization roles\n\n| Name | Access policy |\n| --- | --- |\n| Org Admin | Full organization access. |\n| Org Member | Dependent on [resource roles](#resource-roles). |\n\n### Resource roles\n\n#### Connections\n\n| Name | What user can do | What user cannot do |\n| --- | --- | --- |\n| Admin | *   Create and delete sources and destinations.<br>*   Create and delete models.<br>*   Connect and disconnect sources from destinations.<br>*   Edit source and destination configurations.<br>*   Connect or disconnect transformations.<br>*   Connect or disconnect models.<br>*   Link or unlink tracking plans.<br>*   CRUD operations on any resource for which there is no specific role (for example, Audience, Profiles). | \\-  |\n| Editor | *   Connect and disconnect sources from destinations.<br>*   Edit source and destination configurations.<br>*   Connect or disconnect transformations.<br>*   Connect or disconnect models.<br>*   Link or unlink tracking plans. | Create and delete sources and destinations. |\n| Viewer | Read-only access. | Create, edit, or delete anything. |\n\n#### Transformations and Library\n\n| Name | What user can do | What user cannot do |\n| --- | --- | --- |\n| Grant edit access | Create, edit, or delete transformations/libraries. | Connect and disconnect transformations/libraries. |\n\n## Plan-wise features\n\n| Feature | Free | Starter/Growth | Enterprise |\n| --- | --- | --- | --- |\n| [Organization roles](#organization-roles) | *   Org Admin<br>*   Org Member | *   Org Admin<br>*   Org Member | *   Org Admin<br>*   Org Member |\n| [Resource roles](#resource-roles) | None | *   [Connections](#connections)<br>*   [Transformations](#transformations) | *   [Connections](#connections)<br>*   [Transformations](#transformations) |\n| [Assign user access to PII](https://www.rudderstack.com/docs/dashboard-guides/data-management/#limiting-access-to-pii-related-features) | No  | No  | Yes |\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> **Summary**:\n> \n> *   All [RudderStack Cloud plans](https://rudderstack.com/pricing/) have these organization roles - **Org Owner**, **Org Admin**, and **Org Member**.\n> *   RudderStack Cloud’s **Free** plan does not have any granular, resource-level roles. It only has following roles:\n>     *   **Org Owner**/**Org Admin**: Full organization access.\n>     *   **Org Member**: View-only access.\n> *   Resource roles are available in paid plans and they expand by tier.\n> *   The [PII controls](https://www.rudderstack.com/docs/dashboard-guides/data-management/#limiting-access-to-pii-related-features) feature is available only in the [Enterprise plan](https://www.rudderstack.com/enterprise-quote/).\n\n### Member limit\n\n| Plan | Member limit |\n| --- | --- |\n| [Free](https://app.rudderstack.com/signup) | 10  |\n| [Starter](https://www.rudderstack.com/pricing/) | 10  |\n| [Growth](https://www.rudderstack.com/pricing/) | Unlimited |\n| [Enterprise](https://www.rudderstack.com/enterprise-quote/) | Unlimited |\n\n## Migration and mapping\n\nIf you have added users to your organization with the [previous role permissions](https://www.rudderstack.com/docs/dashboard-guides/members/#role-permissions), RudderStack will automatically migrate them into the new roles and map the granular resource-level permissions as applicable.\n\n| Previous organization role | New organization role | New resource roles |\n| --- | --- | --- |\n| Admin | Org Admin | Not applicable, full access already. |\n| Read-Write | *   **Free plan**: Org Member<br>*   **Paid plans**: Org Member | *   **Free plan**: None<br>*   **Paid plans**:<br>    *   [Connections Admin](#connections)<br>    *   [Transformations Admin](#transformations)<br><br>See [Plan-wise features](#plan-wise-features) for more information. |\n| Read-Only | Org Member | None – view-only access by default. |\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Members (User Management) | RudderStack Docs",
    "description": "Manage users across your organization and workspaces and assign them relevant roles.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/dashboard-guides/usage/",
    "markdown": "# Events Usage | RudderStack Docs\n\nView your monthly event usage across Event Stream, ETL, and Reverse ETL pipelines.\n\n* * *\n\n*     2 minute read  \n    \n\nRudderStack provides a comprehensive reporting feature that lets you view the monthly event volume across your Event Stream, ETL, and Reverse ETL pipelines. You can also view your event usage across time and break it down by source.\n\nThis guide walks you through the reporting feature in detail.\n\n## View events usage\n\nTo use the reporting feature, log in to your [RudderStack dashboard](https://app.rudderstack.com/) and navigate to **Settings** > **Organization** > **Usage**. Here, you can view the event usage by product and further break it down by source.\n\n[![Usage tab in RudderStack dashboard](https://www.rudderstack.com/docs/images/dashboard-guides/usage/usage-tab-latest.webp)](https://www.rudderstack.com/docs/images/dashboard-guides/usage/usage-tab-latest.webp)\n\n### Usage and monthly event limit\n\nIn this section, you will see your current RudderStack Cloud plan and the monthly event limit.\n\n[![RudderStack Cloud plan and monthly event limit](https://www.rudderstack.com/docs/images/dashboard-guides/usage/plan-monthly-limit.webp)](https://www.rudderstack.com/docs/images/dashboard-guides/usage/plan-monthly-limit.webp)\n\nIf your plan has support for multiple workspaces, you will also see a **All workspaces** tab. Here, you can filter your monthly event usage data by workspace.\n\n[![Filter metrics by workspace](https://www.rudderstack.com/docs/images/dashboard-guides/usage/workspaces.webp)](https://www.rudderstack.com/docs/images/dashboard-guides/usage/workspaces.webp)\n\n### Monthly event usage\n\nThis section gives you a quick overview of the event volume for the **current month** by product type (Event Stream, ETL, and Reverse ETL).\n\n[![Monthly event usage overview](https://www.rudderstack.com/docs/images/dashboard-guides/usage/monthly-event-usage-latest.webp)](https://www.rudderstack.com/docs/images/dashboard-guides/usage/monthly-event-usage-latest.webp)\n\nYou can also see the details on event volume used till date and the option to upgrade your plan (applicable for [Free and Starter](https://rudderstack.com/pricing/) plans).\n\n[![Monthly event usage overview for self-serve](https://www.rudderstack.com/docs/images/dashboard-guides/usage/monthly-event-usage-free-latest.webp)](https://www.rudderstack.com/docs/images/dashboard-guides/usage/monthly-event-usage-free-latest.webp)\n\n### Usage by source type\n\nIn the **Usage overview** section, you get tabbed charts that reflect your events usage by product type (Event Stream, Reverse ETL, and ETL). You also get details on the top five sources by event volume. Note that all the remaining sources are grouped into **All other sources**.\n\n[![Usage overview by source](https://www.rudderstack.com/docs/images/dashboard-guides/usage/usage-overview-latest.webp)](https://www.rudderstack.com/docs/images/dashboard-guides/usage/usage-overview-latest.webp)\n\nYou can also filter the event usage data by source and time period (day, week, or month). Hover on the chart to see the following metrics:\n\n*   Source name\n*   Number of ingested events\n*   Percentage of event volume\n*   Workspace associated with the source\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> You can filter data for up to 10 sources.\n\n[![Usage overview by source type](https://www.rudderstack.com/docs/images/dashboard-guides/usage/filter-metrics-latest.webp)](https://www.rudderstack.com/docs/images/dashboard-guides/usage/filter-metrics-latest.webp)\n\n### Usage breakdown by source\n\nTo view the events usage breakdown by source for the selected time period, click the **View Breakdown** option:\n\n[![Usage overview by source type](https://www.rudderstack.com/docs/images/dashboard-guides/usage/filter-metrics-latest-1.webp)](https://www.rudderstack.com/docs/images/dashboard-guides/usage/filter-metrics-latest-1.webp)\n\nHere, you will see a detailed event usage breakdown for all the sources in your workspace for the selected time period, including number of ingested events (not events sent to the destination).\n\n[![Usage breakdown by source](https://www.rudderstack.com/docs/images/dashboard-guides/usage/view-breakdown-source.webp)](https://www.rudderstack.com/docs/images/dashboard-guides/usage/view-breakdown-source.webp)\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Events Usage | RudderStack Docs",
    "description": "View your monthly event usage across Event Stream, ETL, and Reverse ETL pipelines.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/user-guides/how-to-guides/dynamic-destination-configuration/",
    "markdown": "# How to configure a destination dynamically\n\nConfigure your RudderStack destination dynamically via event payload properties.\n\n* * *\n\n*     4 minute read  \n    \n\nThe dynamic settings configuration feature is supported in the RudderStack destinations that allow sending data via [cloud mode](https://www.rudderstack.com/docs/destinations/rudderstack-connection-modes/#cloud-mode). This way, you can configure the destinations based on data in an event payload.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> Note that:\n> \n> *   This feature is only applicable for [destinations](https://www.rudderstack.com/docs/destinations/streaming-destinations/) that support [cloud mode](https://www.rudderstack.com/docs/destinations/rudderstack-connection-modes/#cloud-mode).\n> *   Use this feature to configure multiple instances of the same destination programmatically with different configurations for specific fields.\n\n## Create the configuration setting template\n\nTo use this feature, enter the path of your desired configuration value from the event payload using object notation in a [handlebars.js](https://handlebarsjs.com/guide/expressions.html) expression, as well as a default value for the setting in case the value is not present:\n\nFor example: `{{ event.path.to.key || \"default_config_value\" }}`, where\n\n*   `event` is the top level object for the event payload\n*   `path.to.key` is the location of the property you want to make configurable for your destination, e.g., `properties.url`\n*   `\"default_config_value\"` is the default value for the property\n\nIf `event.path.to.key` exists, its value will be passed as a configuration setting to your destination. If not present, RudderStack will send the default value that you supply in the expression.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> *   To set a number as the default value, set it as a string.\n> *   In case of a multi-word string, we recommend setting the value within double quotes, e.g. `\"default value\"`.\n\nThe next section walks through an example of dynamic configuration using multiple Google Analytics tracking IDs.\n\n## Use Case: Dynamically configure your Google Analytics destination using Tracking ID\n\nSuppose you have implemented two web properties with `rudderanalytics.js`, each with a unique Universal Analytics (UA) tracking ID. You don’t want to set up different RudderStack destinations for each property; rather, you would like to pass the event data to the appropriate tracking ID in your Google Analytics instance.\n\nTo achieve this you will:\n\n1.  Create a transformation that:\n    \n    *   adds a `trackingId` property to your event\n    *   sets the appropriate `trackingId` value based on the RudderStack `sourceId` for the given web property.\n2.  Configure your Google Analytics destination in the RudderStack dashboard with a template that dynamically updates the `trackingId` property for each event, routing the data to the appropriate GA property.\n    \n\n### Step 1: Add a transformation\n\nTo populate the configuration values from the event payload, add a [transformation function](https://www.rudderstack.com/docs/transformations/create/#adding-a-transformation).\n\nThe following sample code uses the transformation’s `metadata()` function to determine the `sourceId` from the metadata object. If the `sourceId` matches one of your provided IDs, then the associated `trackingId` is sent to Google Analytics.\n\nIf there is no match for the `sourceId`, the default value that you define in the next step will be passed to Google Analytics.\n\n```\nexport function transformEvent(event, metadata) {\n  let updatedEvent = event;\n  const met = metadata(event);\n  if (\n    met &&\n    met.sourceId === \"example_source_id_1\" &&\n    event &&\n    event.properties\n  ) {\n    updatedEvent.properties.trackingId = \"UA-Custom-TrackingID_1\";\n  } else if (\n    met &&\n    met.sourceId === \"example_source_id_2\" &&\n    event &&\n    event.properties\n  ) {\n    updatedEvent.properties.trackingId = \"UA-Custom-TrackingID_2\";\n  }\n  return updatedEvent;\n}\n```\n\n### Step 2: Specify the setting path in destination settings\n\nOnce you have added a transformation, the next step is to [add a Google Analytics destination](https://www.rudderstack.com/docs/dashboard-guides/destinations/#adding-a-destination) in the RudderStack dashboard.\n\nIn the **Connection Settings**, configure the **Tracking ID** field:\n\n[![](https://www.rudderstack.com/docs/images/user-guides/dynamic-destination-configuration.webp)](https://www.rudderstack.com/docs/images/user-guides/dynamic-destination-configuration.webp)\n\nWith the transformation in place and the dynamic tracking ID configured in the connection settings, RudderStack will route the event data to Google Analytics based on the tracking ID in the payload.\n\n## FAQ\n\n#### How should I use this feature? What if I don’t want to use it?\n\nIn the text field of a particular connection setting that you want to configure, enter the path of the payload along with a default value in the following format:\n\n```\n{{ event.path.config.name || \"default_config_value\" }}\n```\n\nIf you don’t want to use this feature, you can enter the configuration setting in the dashboard as you normally would.\n\n#### Can I specify only the payload path without a default value?\n\nWe highly recommend setting a payload path along with a default value. This is because if, for some reason, the payload path does not have any value, the default value is passed as the connection setting. If both the values are absent, you will get an error.\n\n#### My default value is a number starting with 0. What happens if I don’t set it as a string?\n\nIf your default value is a number starting with 0, you need to set it as a string. Otherwise, the value will not be accepted and you may encounter an error.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "How to configure a destination dynamically | RudderStack Docs",
    "description": "Configure your RudderStack destination dynamically via event payload properties.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/user-guides/administrators-guide/software-releases/",
    "markdown": "# Software releases | RudderStack Docs\n\nPrivacy Overview\n\nThis site uses cookies to improve your experience while you navigate through the website. Out of these cookies, the cookies that are categorized as necessary are stored on your browser as they are as essential for the working of basic functionalities of the website. We also use third-party cookies that help us analyze and understand how you use this website. These cookies will be stored in your browser only with your consent. You also have the option to opt-out of these cookies. But opting out of some of these cookies may have an effect on your browsing experience.\n\nNecessary cookies are absolutely essential for the website to function properly. This category only includes cookies that ensures basic functionalities and security features of the website. These cookies do not store any personal information.",
    "title": "Software releases | RudderStack Docs",
    "description": "Technical description of the RudderStack releases.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/dashboard-guides/permissions-management/",
    "markdown": "# Permissions Management | RudderStack Docs\n\nManage your team’s permissions and access controls in RudderStack.\n\nAvailable Plans\n\n*   enterprise\n\n* * *\n\n*     3 minute read  \n    \n\nRudderStack’s permissions management feature gives you the ability to:\n\n*   [Restrict edit permissions](#restricting-edit-permissions-for-individual-objects) for business-critical objects to selected users in your organization.\n*   [Limit access](#limiting-access-to-pii-related-features) to the product features where PII is exposed (for example, Live Events, debug logs, etc.) for compliance purposes.\n\n## Set granular access controls\n\nWhen you [invite a member to your organization](https://www.rudderstack.com/docs/dashboard-guides/user-management/#invite-users), RudderStack lets you assign any of the two default [organization roles](https://www.rudderstack.com/docs/dashboard-guides/user-management/#role-permissions) to them - **Org Admin** or **Org Member**.\n\nWith RudderStack’s granular access control features, admins can lock down business-critical objects to a select list of people. They can also restrict PII(Personally Identifiable Information) access to certain users.\n\n> ![success](https://www.rudderstack.com/docs/images/tick.svg)\n> \n> With these features, you can allow certain data pipelines to be edited **only** by the users with the required access. Also, you can ensure your access controls are in compliance with the major data regulations like SOC2, GDPR, CCPA, HIPAA, etc.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> All the access-related changes are recorded in the [audit logs](https://www.rudderstack.com/docs/dashboard-guides/audit-logs/).\n\n## Set permissions for individual resources\n\nGo to the **Permissions** tab to set permissions for a particular resource (source, destination, or model) in the workspace. You can also specify members who can make changes to these resources.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> Only users with [admin](https://www.rudderstack.com/docs/dashboard-guides/user-management/#organization-roles) permissions can see the **Permissions** tab.\n\n[![Permissions tab](https://www.rudderstack.com/docs/images/rs-cloud/permissions-management-2.webp)](https://www.rudderstack.com/docs/images/rs-cloud/permissions-management-2.webp)\n\n#### **Permissions scope**\n\nThe permissions set for a particular resource let you:\n\n*   Connect/disconnect a resource with another resource. For example, source to destination, source to tracking plan, transformation to destination, model to reverse ETL source, etc.\n*   Enable, disable, or delete a resource.\n*   Edit or change the resource-specific configuration.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> Any action involving setting up a connection between two resources or linking/de-linking a resource with another resource requires edit permissions for both the resources. The only exception is the [SQL Models](https://www.rudderstack.com/docs/data-pipelines/reverse-etl/features/models/) which can used without explicitly setting any edit permissions.\n\n#### **Specify members with edit permissions**\n\nTo specify members who can make changes to a given resource, follow these steps:\n\n1.  Go to the resource and click the **Permissions** tab:\n\n[![Permissions tab](https://www.rudderstack.com/docs/images/rs-cloud/permissions-management-2.webp)](https://www.rudderstack.com/docs/images/rs-cloud/permissions-management-2.webp)\n\n2.  Under **Who can make changes?**, select any of the following two options:\n    \n    *   **Anyone with write access**: All the members with the [admin permissions](https://www.rudderstack.com/docs/dashboard-guides/user-management/#organization-roles) can make changes to the resource.\n    *   **Only people you select**: With this option, only specific members can make changes to the resource.\n3.  To allow specific members of your team to edit the resource, click **Only people you select**, followed by **Add member**.\n    \n4.  Finally, select the team members from the drop-down and click **Add Members**:\n    \n\n[![Add members option](https://www.rudderstack.com/docs/images/rs-cloud/permissions-management-3.webp)](https://www.rudderstack.com/docs/images/rs-cloud/permissions-management-3.webp)\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> Members with view-only permissions cannot be added as they do not have permissions to modify a resource, by default.\n\nRudderStack’s data privacy options let you safeguard your customers’ privacy by controlling who has access to the raw event data containing PII(Personally Identifiable Information). You can allow anyone on your team to access the PII or restrict access only to a select list of members.\n\nOnly members with PII permissions can view the customers’ PII in the [Live Events](https://www.rudderstack.com/docs/dashboard-guides/live-events/) and errors logs in your [destination’s](https://www.rudderstack.com/docs/dashboard-guides/destinations/#destination-details) **Events** tab:\n\n[![Error logs](https://www.rudderstack.com/docs/images/rs-cloud/permissions-management-4.webp)](https://www.rudderstack.com/docs/images/rs-cloud/permissions-management-4.webp)\n\nTo set the PII permissions, follow these steps:\n\n1.  In your RudderStack dashboard, go to **Settings** > **Workspace** > **Data Management** and scroll down to the **Data Privacy** section.\n    \n2.  Under **Who can view restricted data?**, select the appropriate option:\n    \n    *   **Anyone on your team**: All the members in your workspace can view the raw event data containing PII.\n    *   **Only people you select**: Only specific members with access can view the raw data.\n3.  To allow specific members of your team to edit the object, click **Only people you select**, followed by **Add member**.\n    \n\n[![Add members](https://www.rudderstack.com/docs/images/rs-cloud/permissions-management-5.webp)](https://www.rudderstack.com/docs/images/rs-cloud/permissions-management-5.webp)\n\n4.  Finally, select the workspace members from the drop-down and click **Add Members**:\n\n[![Add members option](https://www.rudderstack.com/docs/images/rs-cloud/permissions-management-6.webp)](https://www.rudderstack.com/docs/images/rs-cloud/permissions-management-6.webp)\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> If the admins are removed from the access list, RudderStack will restrict them from viewing the PII.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Permissions Management | RudderStack Docs",
    "description": "Manage your team's permissions and access controls in RudderStack.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/user-guides/administrators-guide/sso-setup/",
    "markdown": "# Single Sign-On (SSO) setup guides\n\nPrivacy Overview\n\nThis site uses cookies to improve your experience while you navigate through the website. Out of these cookies, the cookies that are categorized as necessary are stored on your browser as they are as essential for the working of basic functionalities of the website. We also use third-party cookies that help us analyze and understand how you use this website. These cookies will be stored in your browser only with your consent. You also have the option to opt-out of these cookies. But opting out of some of these cookies may have an effect on your browsing experience.\n\nNecessary cookies are absolutely essential for the website to function properly. This category only includes cookies that ensures basic functionalities and security features of the website. These cookies do not store any personal information.",
    "title": "Single Sign-On (SSO) setup guides | RudderStack Docs",
    "description": "Set up the RudderStack Single Sign-On feature with various providers.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/user-guides/how-to-guides/debugging-destination-live-events/",
    "markdown": "# How to debug destination live events\n\nDebug and test event failures.\n\n* * *\n\n*     2 minute read  \n    \n\nWhen routing events to a destination via RudderStack, there may be times when the events do not show up in your destination. This guide walks you through the steps to debug your destination live events using RudderStack’s [Live Events](https://www.rudderstack.com/docs/dashboard-guides/live-events/) feature.\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> To view and debug your destination live events, your destination must be configured to send events via [cloud mode](https://www.rudderstack.com/docs/destinations/rudderstack-connection-modes/#cloud-mode).\n\n## Why are events sent to the destination failing?\n\nRouting events to a destination via RudderStack can fail for various reasons. Some of the common ones include:\n\n*   Incorrect destination configuration.\n*   Bad event structure, that is, event is not in a destination-specific format.\n*   Rate limiting enforced by the destination.\n*   Destination downtime.\n\nYou can use RudderStack’s **Live Events** feature to view the events sent to your destination in real-time, and debug any delivery failure.\n\n## Viewing destination live events\n\nTo view the events sent to your destination in real-time, follow these steps:\n\n1.  [Set up your destination](https://www.rudderstack.com/docs/dashboard-guides/destinations/#adding-a-destination) in RudderStack.\n2.  Click the **LIve Events** button to view the events sent from your source:\n\n[![Destination Live Events button](https://www.rudderstack.com/docs/images/user-guides/destination-live-events.webp)](https://www.rudderstack.com/docs/images/user-guides/destination-live-events.webp)\n\nThe resulting window highlights the following information:\n\n*   **Name** of the event.\n*   **Error message** of the event in case of any event failure. Upon clicking **See full error**, you get the specific details like the error response and the time of the first attempt made to send the event.\n\n[![Destination Live Events error details](https://www.rudderstack.com/docs/images/user-guides/destination-live-events-error-new.webp)](https://www.rudderstack.com/docs/images/user-guides/destination-live-events-error-new.webp)\n\n*   Clicking the event also lets you view the **payload** sent to the destination.\n\n[![Destination Live Events window](https://www.rudderstack.com/docs/images/user-guides/destination-live-events-details.webp)](https://www.rudderstack.com/docs/images/user-guides/destination-live-events-details.webp)\n\n## Use case\n\nSuppose that you send some events to the [Facebook Custom Audience](https://www.rudderstack.com/docs/destinations/reverse-etl-destinations/fb-custom-audience/) destination but they are not delivered.\n\nUpon checking the **Live Events** tab for the **Facebook Custom Audience** destination, you observe the following error:\n\n[![Custom Audience destination error](https://www.rudderstack.com/docs/images/rs-cloud/custom-audience-error.webp)](https://www.rudderstack.com/docs/images/rs-cloud/custom-audience-error.webp)\n\nOn clicking the **See full error** option, you can see the following error response:\n\n[![Custom Audience full destination error](https://www.rudderstack.com/docs/images/rs-cloud/custom-audience-full-error.webp)](https://www.rudderstack.com/docs/images/rs-cloud/custom-audience-full-error.webp)\n\nFrom the error response, it is clear that an [`identify`](https://www.rudderstack.com/docs/event-spec/standard-events/identify/) event was sent to the destination. According to the [Facebook Custom Audience documentation](https://www.rudderstack.com/docs/destinations/reverse-etl-destinations/fb-custom-audience/), only [`track`](https://www.rudderstack.com/docs/event-spec/standard-events/track/) events are supported. As a result, when RudderStack tries sending the `identify` event, the destination gives an error. RudderStack retries sending this event several times before marking it as aborted.\n\nThis way, you can use the **Live Events** feature to better understand the responses received from the destination in case of any delivery failures and resolve them faster.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "How to debug destination live events | RudderStack Docs",
    "description": "Debug and test event failures.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/user-guides/how-to-guides/self-hosting-js-sdk/",
    "markdown": "# Self-host JavaScript SDK in your CDN\n\nPrivacy Overview\n\nThis site uses cookies to improve your experience while you navigate through the website. Out of these cookies, the cookies that are categorized as necessary are stored on your browser as they are as essential for the working of basic functionalities of the website. We also use third-party cookies that help us analyze and understand how you use this website. These cookies will be stored in your browser only with your consent. You also have the option to opt-out of these cookies. But opting out of some of these cookies may have an effect on your browsing experience.\n\nNecessary cookies are absolutely essential for the website to function properly. This category only includes cookies that ensures basic functionalities and security features of the website. These cookies do not store any personal information.",
    "title": "Self-host JavaScript SDK in your CDN | RudderStack Docs",
    "description": "Steps on self-hosting and setting up the JavaScript SDK in your CDN.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/user-guides/administrators-guide/sso-setup/azure/",
    "markdown": "# Microsoft Azure Entra ID (formerly Azure AD) SSO Setup\n\nSet up the RudderStack SSO (Single Sign-On) feature with Microsoft Azure Entra ID.\n\nAvailable Plans\n\n*   enterprise\n\n* * *\n\n*     4 minute read  \n    \n\nThis guide lists the steps to set up your Azure Entra ID SAML integration with RudderStack. This integration supports the following features:\n\n*   SP-initiated SSO\n*   JIT(Just In Time) Provisioning\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> RudderStack does not support some SCIM features like importing users and groups, removing users, and syncing passwords. See [Known issues](#known-issues) before you set up SSO for your organization.\n\n## Setup\n\n### Step 1: Create new application\n\n1.  Sign in to [Microsoft Entra ID Admin Center](https://entra.microsoft.com/).\n2.  From the left sidebar, go to **Applications** > **Enterprise applications**.\n3.  Under **Manage**, click **All applications** followed by **New application**.\n\n[![New application option](https://www.rudderstack.com/docs/images/user-guides/azure-sso/rudderstack-azure-sso-1.webp)](https://www.rudderstack.com/docs/images/user-guides/azure-sso/rudderstack-azure-sso-1.webp)\n\n4.  In the **Microsoft Entra App Gallery**, click **Create your own application**.\n\n[![Create your own application option](https://www.rudderstack.com/docs/images/user-guides/azure-sso/rudderstack-azure-sso-2.webp)](https://www.rudderstack.com/docs/images/user-guides/azure-sso/rudderstack-azure-sso-2.webp)\n\n5.  In the expanded right sidebar, enter the name of your app. Under **What are you looking to do with your application?**, select **Integrate any other application you don’t find in the gallery (Non-gallery)**.\n\n[![Initial configuration](https://www.rudderstack.com/docs/images/user-guides/azure-sso/rudderstack-azure-sso-3.webp)](https://www.rudderstack.com/docs/images/user-guides/azure-sso/rudderstack-azure-sso-3.webp)\n\n6.  Click the **Create** button at the bottom and wait for a few seconds for Azure to provision the app. You will then be redirected to the admin view of the app.\n\n### Step 2: Set up SAML\n\n1.  In the left sidebar of the newly provisioned app, click **Single sign-on** under **Manage**. Then, click **SAML**.\n\n[![SAML SSO method](https://www.rudderstack.com/docs/images/user-guides/azure-sso/rudderstack-azure-sso-5.webp)](https://www.rudderstack.com/docs/images/user-guides/azure-sso/rudderstack-azure-sso-5.webp)\n\n2.  Click the meatballs menu (`...`) to the right of **Basic SAML Configuration**. In the expanded right sidebar, fill in the following information:\n\n| Field | Value |\n| --- | --- |\n| Identifier (Entity ID)  <br>Required | `urn:amazon:cognito:sp:us-east-1_ABZiTjXia` |\n| Reply URL (Assertion Consumer Service URL)  <br>Required | `https://auth2.rudderstack.com/saml2/idpresponse` |\n| Sign on URL  <br>Required | `https://auth2.rudderstack.com/saml2/idpresponse` |\n| Relay State | \\-  |\n\n3.  Click the meatballs menu (`...`) to the right of **Attributes & Claims** and remove any **Additional claims**. Then, click **Add new claim** and enter the following information:\n\n| Field | Value | Notes |\n| --- | --- | --- |\n| Email | `user.mail` | \\-  |\n| LastName | `user.displayname` | Choose your preferred name, for example, display name or surname. |\n| Unique User Identifier | `user.userprincipalname` | \\-  |\n\n4.  Copy the **App Federation Metadata URL** and share it with the [RudderStack team](mailto:support@rudderstack.com).\n\n[![Metadata URL](https://www.rudderstack.com/docs/images/user-guides/azure-sso/rudderstack-azure-sso-8.webp)](https://www.rudderstack.com/docs/images/user-guides/azure-sso/rudderstack-azure-sso-8.webp)\n\n### Step 3: Set up SCIM\n\nThis section lists the steps to set up SCIM provisioning in Azure Entra ID.\n\n#### **Prerequisites**\n\nBefore you configure the SCIM app, you need to generate a [personal access token](https://www.rudderstack.com/docs/dashboard-guides/personal-access-token/) with [admin](https://www.rudderstack.com/docs/dashboard-guides/user-management/#admin) privileges. Follow these steps:\n\n1.  Log in to the RudderStack workspace you want to enable SCIM for. Note that your role in the organization must be of [Org Admin](https://www.rudderstack.com/docs/dashboard-guides/user-management/#admin) type.\n2.  Go to **Settings** > **Your Profile** > **Account** tab and scroll down to **Personal access tokens**. Then, click **Generate new token**:\n\n[![New personal access token in RudderStack dashboard](https://www.rudderstack.com/docs/images/rudderstack-api/personal-access-token-1.webp)](https://www.rudderstack.com/docs/images/rudderstack-api/personal-access-token-1.webp)\n\n3.  Set an appropriate name for the token.\n4.  Select **Admin** from the **Role** dropdown.\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> Make sure your user role and personal access token has [admin](https://www.rudderstack.com/docs/dashboard-guides/user-management/#admin) privileges, otherwise your SCIM provisioning tasks will fail.\n\n5.  Click **Generate** and save the token securely. It will **not** be visible again once you close this window.\n\n#### **SCIM configuration**\n\n1.  In the left sidebar of your app, go to **Manage** > **Provisioning** > **Get started**.\n\n[![Provisioning](https://www.rudderstack.com/docs/images/user-guides/azure-sso/rudderstack-azure-sso-9.webp)](https://www.rudderstack.com/docs/images/user-guides/azure-sso/rudderstack-azure-sso-9.webp)\n\n2.  Under **Provisioning Mode**, choose **Automatic** and enter the following credentials:\n\n| Field | Value |\n| --- | --- |\n| Tenant URL  <br>Required | `https://api.rudderstack.com/scim/v2` |\n| Secret Token | Your personal access token obtained in the [Prerequisites](#prerequisites) section. |\n\n[![Provisioning](https://www.rudderstack.com/docs/images/user-guides/azure-sso/rudderstack-azure-sso-10.webp)](https://www.rudderstack.com/docs/images/user-guides/azure-sso/rudderstack-azure-sso-10.webp)\n\n3.  Click **Test Connection** - it should be successful.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> If you see a `403 - Forbidden` error, contact the [RudderStack team](mailto:support@rudderstack.com) to enable SCIM for your organization.\n\n## Performing SSO login\n\nRudderStack does not support IdP-initiated authentication. Make sure the users log in through `https://app.rudderstack.com/sso`.\n\n## Debugging\n\nThere are times when an SSO login might fail for some users due to some reason. In such cases, the RudderStack team requires a HAR (HTTP Archive) file to inspect the requests and identify any SSO-related issues.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> A HAR file is a log of exported network requests from the user’s browser. See the [HAR Analyzer](https://toolbox.googleapps.com/apps/har_analyzer/) guide for steps on generating this file depending on your browser.\n\nOnce you generate the HAR file, share it with the [RudderStack team](mailto:support@rudderstack.com) to troubleshoot the issue.\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> Note the following before capturing your HAR file:\n> \n> *   Start from `https://app.rudderstack.com/sso` with a clean session, preferably in incognito mode of your browser.\n> *   Complete the SSO flow until the step where you face an error.\n> *   Your HAR file might contain sensitive data - make sure to redact it using a text editor before sharing it with the team.\n\n## Known issues\n\nRudderStack does not support the following SCIM features currently:\n\n*   Import users\n*   Import groups\n*   Push groups (coming soon)\n*   Remove users\n*   Sync password\n*   Enhanced group push\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> RudderStack does not support removing users - this is because it uses SCIM with SAML, where removing a user from Azure Entra ID implies that they also lose the ability to authenticate to RudderStack completely (logins via passwords, Google, etc. are completely blocked).\n> \n> Instead, RudderStack supports deactivating the user which means they only lose access to the organization.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Microsoft Azure Entra ID (formerly Azure AD) SSO Setup | RudderStack Docs",
    "description": "Set up the RudderStack SSO (Single Sign-On) feature with Microsoft Azure Entra ID.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/user-guides/how-to-guides/rudderstack-jamstack-integration/",
    "markdown": "# How to integrate RudderStack with your Jamstack site\n\nPrivacy Overview\n\nThis site uses cookies to improve your experience while you navigate through the website. Out of these cookies, the cookies that are categorized as necessary are stored on your browser as they are as essential for the working of basic functionalities of the website. We also use third-party cookies that help us analyze and understand how you use this website. These cookies will be stored in your browser only with your consent. You also have the option to opt-out of these cookies. But opting out of some of these cookies may have an effect on your browsing experience.\n\nNecessary cookies are absolutely essential for the website to function properly. This category only includes cookies that ensures basic functionalities and security features of the website. These cookies do not store any personal information.",
    "title": "How to integrate RudderStack with your Jamstack site | RudderStack Docs",
    "description": "Add RudderStack to your Jamstack site.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/user-guides/how-to-guides/rudderstack-jamstack-integration/v1.1/angular/",
    "markdown": "# Integrate JavaScript SDK v1.1 with your Angular app\n\n* * *\n\n*     4 minute read  \n    \n\nThis guide will help you integrate RudderStack with your Angular app using the [JavaScript SDK](https://www.rudderstack.com/docs/sources/event-streams/sdks/rudderstack-javascript-sdk/). On successful integration, you can track and send real-time user events to your preferred destinations via RudderStack.\n\nTo set up the event stream on your Angular app, you need to perform the following steps:\n\n1.  [Integrate the JavaScript SDK with your Angular app and set up the tracking code](#integrating-the-javascript-sdk-with-your-angular-app)\n2.  [Configure a destination in RudderStack](#configuring-a-destination-in-rudderstack)\n3.  [Deploy your Angular app and verify the events](#deploying-your-angular-app-and-verifying-the-event-stream)\n\n## Prerequisites\n\nThis guide assumes you have installed and set up your Angular app. Refer to the [Angular documentation](https://angular.io/guide/setup-local) for more information.\n\n## Integrating the JavaScript SDK with your Angular app\n\nIntegrating the JavaScript SDK with your Angular app involves the following steps:\n\n1.  [Creating a JavaScript source in RudderStack](#creating-a-javascript-source-in-rudderstack)\n2.  [Installing and configuring the JavaScript SDK in your Angular app](#installing-and-configuring-the-javascript-sdk-in-your-angular-app)\n\n### Creating a JavaScript source in RudderStack\n\nThe RudderStack JavaScript source is required track the events from your Angular app. Follow these steps to set it up in your [RudderStack dashboard](https://app.rudderstack.com/):\n\n1.  Note the data plane URL in your RudderStack dashboard. This is required to set up the JavaScript SDK in your Angular app.\n\n[![Data plane URL](https://www.rudderstack.com/docs/images/user-guides/jamstack-guides/data-plane-url.webp)](https://www.rudderstack.com/docs/images/user-guides/jamstack-guides/data-plane-url.webp)\n\n2.  Click the **Sources** button in the left navigation bar and select **New Source** to create a source. Under **Sources**, select **Event Streams** > **JavaScript**.\n3.  Assign a name to your source and click **Continue**.\n4.  Your JavaScript source is now configured. Note down the write key for this source:\n\n[![JavaScript source write key](https://www.rudderstack.com/docs/images/user-guides/jamstack-guides/write-key.webp)](https://www.rudderstack.com/docs/images/user-guides/jamstack-guides/write-key.webp)\n\n### Installing and configuring the JavaScript SDK in your Angular app\n\nTo integrate RudderStack with your Angular app and set up the tracking code, follow the steps below:\n\n1.  In your Angular project, navigate to the `src` folder and open `index.html`.\n2.  Place the following code within the `<head>` section:\n\n```\n<script> \n    rudderanalytics=window.rudderanalytics=[];\n    var  methods = [\n            \"load\",\n            \"page\",\n            \"track\",\n            \"identify\",\n            \"alias\",\n            \"group\",\n            \"ready\",\n            \"reset\",\n            \"getAnonymousId\",\n            \"setAnonymousId\"\n        ];\n        for (var i = 0; i < methods.length; i++) {\n              var method = methods[i];\n              rudderanalytics[method] = function (methodName) {\n                    return function () {\n                          rudderanalytics.push([methodName].concat(Array.prototype.slice.call(arguments)));\n                    };\n                  }(method);\n        }\n    rudderanalytics.load(\"WRITE_KEY\",\"DATA_PLANE_URL\");\n    //rudderanalytics.page();\n  </script>\n  <script src=\"https://cdn.rudderlabs.com/v1.1/rudder-analytics.min.js\"></script>\n```\n\n3.  Replace `WRITE_KEY` and `DATA_PLANE_URL` with your JavaScript source write key and the data plane URL obtained in the [Creating a JavaScript source in RudderStack](#creating-a-javascript-source-in-rudderstack) section above.\n\n## Configuring a destination in RudderStack\n\nThis section details the steps required to set up a destination in RudderStack, where you can send all events tracked by the JavaScript SDK you set up above.\n\n1.  In your RudderStack dashboard, click **Destinations** > **New destination**.\n2.  Choose your preferred destination from the list.\n3.  Assign a name to the destination and click **Continue**.\n4.  Select the JavaScript source configured in the above section and click **Continue**.\n5.  Configure the destination with the required settings.\n\n## Deploying your Angular app and verifying the event stream\n\nTo verify if your event stream is working correctly, deploy your Angular app and test if the events are tracked and delivered correctly. To do so, follow these steps:\n\n1.  From your terminal, navigate to your Angular project’s root folder and run the following command:\n\n2.  Open the local server URL(generally `http://localhost:4200/`) in your browser to view the app.\n3.  Go to your browser’s developer tools and check the **Network** tab to verify if the RudderStack JavaScript SDK (`rudder-analytics.js`) is loaded correctly. The following image highlights this option for the Google Chrome browser:\n\n[![Chrome Network tab](https://www.rudderstack.com/docs/images/user-guides/browser-network-tab.webp)](https://www.rudderstack.com/docs/images/user-guides/browser-network-tab.webp)\n\n4.  Click the various links or pages in your app for RudderStack to track these actions.\n5.  Go to the **Live Events** tab of your JavaScript source in the RudderStack dashboard to check if the events are tracked. Note that you may face a minor delay before the events start showing up in your dashboard.\n6.  Go to your destination to verify if the events are received successfully.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Integrate JavaScript SDK v1.1 with your Angular app | RudderStack Docs",
    "description": "This guide will help you integrate RudderStack with your Angular app using the JavaScript SDK. On successful integration, you can track and send real-time user events to your preferred destinations via RudderStack.\nTo set up the event stream on your Angular app, you need to perform the following steps:\nIntegrate the JavaScript SDK with your Angular app and set up the tracking code Configure a destination in RudderStack Deploy your Angular app and verify the events PrerequisitesThis guide assumes you have installed and set up your Angular app.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/user-guides/how-to-guides/rudderstack-jamstack-integration/v1.1/astrojs/",
    "markdown": "# Integrate JavaScript SDK v1.1 with your Astro site\n\n* * *\n\n*     3 minute read  \n    \n\nThis guide will help you integrate RudderStack with your Astro site using the [JavaScript SDK](https://www.rudderstack.com/docs/sources/event-streams/sdks/rudderstack-javascript-sdk/). On successful integration, you can track and send real-time user events to your preferred destinations via RudderStack.\n\nTo set up the event stream on your Astro site, you need to perform the following steps:\n\n1.  [Integrate the JavaScript SDK with your Astro site and set up the tracking code](#integrating-the-javascript-sdk-with-your-astro-site)\n2.  [Configure a destination in RudderStack](#configuring-a-destination-in-rudderstack)\n3.  [Deploy your Astro site and verify the events](#deploying-your-astro-site-and-verifying-the-event-stream)\n\n## Prerequisites\n\nThis guide assumes you have installed and set up your Astro site. Refer to the [Astro documentation](https://docs.astro.build/en/install/auto/) for more information.\n\n## Integrating the JavaScript SDK with your Astro site\n\nIntegrating the JavaScript SDK with your Astro site involves the following steps:\n\n1.  [Creating a JavaScript source in RudderStack](#creating-a-javascript-source-in-rudderstack)\n2.  [Installing and configuring the JavaScript SDK in your app](#installing-and-configuring-the-javascript-sdk-in-your-astro-site)\n\n### Creating a JavaScript source in RudderStack\n\nThe RudderStack JavaScript source is required track the events from your Astro site. Follow these steps to set it up in your [RudderStack dashboard](https://app.rudderstack.com/):\n\n1.  Note the data plane URL in your RudderStack dashboard. This is required to set up the JavaScript SDK in your Astro app.\n\n[![Data plane URL](https://www.rudderstack.com/docs/images/user-guides/jamstack-guides/data-plane-url.webp)](https://www.rudderstack.com/docs/images/user-guides/jamstack-guides/data-plane-url.webp)\n\n2.  Click the **Sources** button in the left navigation bar and select **New Source** to create a source. Under **Sources**, select **Event Streams** > **JavaScript**.\n3.  Assign a name to your source and click **Continue**.\n4.  Your JavaScript source is now configured. Note down the write key for this source:\n\n[![JavaScript source write key](https://www.rudderstack.com/docs/images/user-guides/jamstack-guides/write-key.webp)](https://www.rudderstack.com/docs/images/user-guides/jamstack-guides/write-key.webp)\n\n### Installing and configuring the JavaScript SDK in your Astro site\n\nTo integrate RudderStack with your Astro site and set up the tracking code, follow the steps below:\n\n1.  In your Astro project folder, navigate to `src` > `pages` and open `index.astro`.\n2.  Enter the following script in the `<head>` section of the page:\n\n```\n<script>\n\t\trudderanalytics = window.rudderanalytics = [];\n\t\tvar methods = [\n\t\t  \"load\",\n\t\t  \"page\",\n\t\t  \"track\",\n\t\t  \"identify\",\n\t\t  \"alias\",\n\t\t  \"group\",\n\t\t  \"ready\",\n\t\t  \"reset\",\n\t\t  \"getAnonymousId\",\n\t\t  \"setAnonymousId\",\n\t\t];\n\t\tfor (var i = 0; i < methods.length; i++) {\n\t\t  var method = methods[i];\n\t\t  rudderanalytics[method] = (function (methodName) {\n\t\t\treturn function () {\n\t\t\t  rudderanalytics.push(\n\t\t\t\t[methodName].concat(Array.prototype.slice.call(arguments))\n\t\t\t  );\n\t\t\t};\n\t\t  })(method);\n\t\t}\n\t  \n\t\trudderanalytics.load(\"WRITE_KEY\",\"DATA_PLANE_URL\");\n\t\t//rudderanalytics.page();\n\t  </script>\n\t  \n\t  <script src=\"https://cdn.rudderlabs.com/v1.1/rudder-analytics.min.js\"></script>\n```\n\n3.  Replace `WRITE_KEY` and `DATA_PLANE_URL` with your JavaScript source write key and the data plane URL obtained in the [Creating a JavaScript source in RudderStack](#creating-a-javascript-source-in-rudderstack) section above.\n\n## Configuring a destination in RudderStack\n\nThis section details the steps required to set up a destination in RudderStack, where you can send all events tracked by the JavaScript SDK you set up above.\n\n1.  In your RudderStack dashboard, click **Destinations** > **New destination**.\n2.  Choose your preferred destination from the list.\n3.  Assign a name to the destination and click **Continue**.\n4.  Select the JavaScript source configured in the above section and click **Continue**.\n5.  Configure the destination with the required settings.\n\n## Deploying your Astro site and verifying the event stream\n\nTo verify if your event stream is working correctly, deploy your Astro site and test if the events are tracked and delivered correctly. To do so, follow these steps:\n\n1.  From your terminal, navigate to your Astro project’s root folder and run the following command:\n\n2.  Open the local server URL(generally `http://localhost:3000/`) in your browser to view the app.\n3.  Go to your browser’s developer tools and check the **Network** tab to verify if the RudderStack JavaScript SDK (`rudder-analytics.js`) is loaded correctly. The following image highlights this option for the Google Chrome browser:\n\n[![Chrome Network tab](https://www.rudderstack.com/docs/images/user-guides/browser-network-tab.webp)](https://www.rudderstack.com/docs/images/user-guides/browser-network-tab.webp)\n\n3.  Click the various links or pages in your site for RudderStack to track these actions.\n4.  Go to the **Live Events** tab of your JavaScript source in the RudderStack dashboard to check if the events are tracked. Note that you may face a minor delay before the events start showing up in your dashboard.\n5.  Go to your destination to verify if the events are received successfully.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Integrate JavaScript SDK v1.1 with your Astro site | RudderStack Docs",
    "description": "This guide will help you integrate RudderStack with your Astro site using the JavaScript SDK. On successful integration, you can track and send real-time user events to your preferred destinations via RudderStack.\nTo set up the event stream on your Astro site, you need to perform the following steps:\nIntegrate the JavaScript SDK with your Astro site and set up the tracking code Configure a destination in RudderStack Deploy your Astro site and verify the events PrerequisitesThis guide assumes you have installed and set up your Astro site.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/user-guides/how-to-guides/rudderstack-jamstack-integration/v1.1/",
    "markdown": "# Integrate JavaScript legacy SDK (v1.1) with your Jamstack site\n\nPrivacy Overview\n\nThis site uses cookies to improve your experience while you navigate through the website. Out of these cookies, the cookies that are categorized as necessary are stored on your browser as they are as essential for the working of basic functionalities of the website. We also use third-party cookies that help us analyze and understand how you use this website. These cookies will be stored in your browser only with your consent. You also have the option to opt-out of these cookies. But opting out of some of these cookies may have an effect on your browsing experience.\n\nNecessary cookies are absolutely essential for the website to function properly. This category only includes cookies that ensures basic functionalities and security features of the website. These cookies do not store any personal information.",
    "title": "Integrate JavaScript legacy SDK (v1.1) with your Jamstack site | RudderStack Docs",
    "description": "The most popular HTML, CSS, and JS library in the world.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/user-guides/how-to-guides/rudderstack-jamstack-integration/v1.1/eleventy/",
    "markdown": "# Integrate JavaScript SDK v1.1 with your Eleventy site\n\n* * *\n\n*     4 minute read  \n    \n\nThis guide will help you integrate RudderStack with your Eleventy site using the [JavaScript SDK](https://www.rudderstack.com/docs/sources/event-streams/sdks/rudderstack-javascript-sdk/). On successful integration, you can track and send real-time user events to your preferred destinations via RudderStack.\n\nTo set up the event streams on your Eleventy site, you need to perform the following steps:\n\n1.  [Integrate the JavaScript SDK with your Eleventy site and set up the tracking code](#integrating-the-javascript-sdk-with-your-eleventy-site)\n2.  [Configure a destination in RudderStack](#configuring-a-destination-in-rudderstack)\n3.  [Deploy your Eleventy site and verify the events](#deploying-your-eleventy-site-and-verifying-the-event-stream)\n\n## Prerequisites\n\nThis guide assumes you have installed and set up your Eleventy site. Refer to the [Eleventy documentation](https://www.11ty.dev/docs/getting-started/) for more information.\n\n## Integrating the JavaScript SDK with your Eleventy site\n\nIntegrating the JavaScript SDK with your Eleventy site involves the following steps:\n\n1.  [Creating a JavaScript source in RudderStack](#creating-a-javascript-source-in-rudderstack)\n2.  [Installing and configuring the JavaScript SDK in your Eleventy site](#installing-and-configuring-the-javascript-sdk-in-your-eleventy-site)\n\n### Creating a JavaScript source in RudderStack\n\nThe RudderStack JavaScript source is required track the events from your Eleventy app. Follow these steps to set it up in your [RudderStack dashboard](https://app.rudderstack.com/):\n\n1.  Note the data plane URL in your RudderStack dashboard. This is required to set up the JavaScript SDK in your Eleventy app.\n\n[![Data plane URL](https://www.rudderstack.com/docs/images/user-guides/jamstack-guides/data-plane-url.webp)](https://www.rudderstack.com/docs/images/user-guides/jamstack-guides/data-plane-url.webp)\n\n2.  Click the **Sources** button in the left navigation bar and select **New Source** to create a source. Under **Sources**, select **Event Streams** > **JavaScript**.\n3.  Assign a name to your source and click **Continue**.\n4.  Your JavaScript source is now configured. Note down the write key for this source:\n\n[![JavaScript source write key](https://www.rudderstack.com/docs/images/user-guides/jamstack-guides/write-key.webp)](https://www.rudderstack.com/docs/images/user-guides/jamstack-guides/write-key.webp)\n\n### Installing and configuring the JavaScript SDK in your Eleventy site\n\nTo integrate RudderStack with your Eleventy site and set up the tracking code, follow the steps below:\n\n1.  In your Eleventy project, navigate to the `_site` folder and open `index.html`.\n2.  Add the following script within the `head` tag of the page:\n\n```\n<script>\nrudderanalytics = window.rudderanalytics = [];\nfor (var methods = [\"load\", \"page\", \"track\", \"identify\", \"alias\", \"group\", \"ready\", \"reset\", \"getAnonymousId\", \"setAnonymousId\"], i = 0; i < methods.length; i++) {\n  var method = methods[i];\n  rudderanalytics[method] = function(a) {\n    return function() {\n      rudderanalytics.push([a].concat(Array.prototype.slice.call(arguments)))\n    }\n  }(method)\n}\nrudderanalytics.load(\"<WRITE_KEY>\", \"<DATA_PLANE_URL\"),\n  rudderanalytics.page();\nrudderanalytics.track(\n  \"Test Event in 11ty\", {\n    revenue: 30,\n    currency: 'INR',\n    user_actual_id: 12345\n  },\n  () => {\n    console.log(\"Track call\");\n  }\n);\n</script>\n<script src=\"https://cdn.rudderlabs.com/rudder-analytics.min.js\"></script>\n```\n\n3.  Replace `<WRITE_KEY>` and \\`<DATA\\_PLANE\\_URL> with your JavaScript source write key and the data plane URL obtained in the [Creating a JavaScript source in RudderStack](#creating-a-javascript-source-in-rudderstack) section above.\n\n## Configuring a destination in RudderStack\n\nThis section details the steps required to set up a destination in RudderStack, where you can send all events tracked by the JavaScript SDK you set up above.\n\n1.  In your RudderStack dashboard, click **Destinations** > **New destination**.\n2.  Choose your preferred destination from the list.\n3.  Assign a name to the destination and click **Continue**.\n4.  Select the JavaScript source configured in the above section and click **Continue**.\n5.  Configure the destination with the required settings.\n\n## Deploying your Eleventy site and verifying the event stream\n\nTo verify if your event stream is working correctly, deploy your Eleventy site and test if the events are tracked and delivered correctly. Follow these steps:\n\n1.  From your terminal, navigate to the folder containing your Eleventy site and run the following command:\n\n```\nnpx @11ty/eleventy --serve\n```\n\n2.  Open the local server URL(generally `http://localhost:8080/`) in your browser to view the site.\n3.  Go to your browser’s developer tools and check the **Network** tab to verify if the RudderStack JavaScript SDK (`rudder-analytics.js`) is loaded correctly. The following image highlights this option for Google Chrome:\n\n[![Chrome Network tab](https://www.rudderstack.com/docs/images/user-guides/browser-network-tab.webp)](https://www.rudderstack.com/docs/images/user-guides/browser-network-tab.webp)\n\n5.  Click the various links or pages in your site for RudderStack to track these actions.\n6.  Go to the **Live Events** tab of your JavaScript source in the RudderStack dashboard to check if the events are tracked. Note that you may face a minor delay before the events start showing up in your dashboard.\n7.  Go to your destination to verify if the events are received successfully.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Integrate JavaScript SDK v1.1 with your Eleventy site | RudderStack Docs",
    "description": "This guide will help you integrate RudderStack with your Eleventy site using the JavaScript SDK. On successful integration, you can track and send real-time user events to your preferred destinations via RudderStack.\nTo set up the event streams on your Eleventy site, you need to perform the following steps:\nIntegrate the JavaScript SDK with your Eleventy site and set up the tracking code Configure a destination in RudderStack Deploy your Eleventy site and verify the events PrerequisitesThis guide assumes you have installed and set up your Eleventy site.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/user-guides/how-to-guides/rudderstack-jamstack-integration/v1.1/jekyll/",
    "markdown": "# Integrate JavaScript SDK v1.1 with your Jekyll site\n\n* * *\n\n*     4 minute read  \n    \n\nThis guide will help you integrate RudderStack with your Jekyll site using the [RudderStack JavaScript SDK](https://www.rudderstack.com/docs/sources/event-streams/sdks/rudderstack-javascript-sdk/). On successful integration, you can track and send real-time user events to your preferred destinations via RudderStack.\n\nTo set up the event stream on your Jekyll site, you need to perform the following steps:\n\n1.  [Integrate the JavaScript SDK with your Jekyll site and set up the tracking code](#integrating-the-javascript-sdk-with-your-jekyll-site)\n2.  [Configure a destination in RudderStack](#configuring-a-destination-in-rudderstack)\n3.  [Deploy your Jekyll site and verify the events](#deploying-your-jekyll-site-and-verifying-the-event-stream)\n\n## Prerequisites\n\nThis guide assumes you have installed and set up your Jekyll site. Refer to the [Jekyll documentation](https://jekyllrb.com/docs/installation/) for more information.\n\n## Integrating the JavaScript SDK with your Jekyll site\n\nIntegrating the JavaScript SDK with your Jekyll site involves the following steps:\n\n1.  [Creating a JavaScript source in RudderStack](#creating-a-javascript-source-in-rudderstack)\n2.  [Installing and configuring the JavaScript SDK in your Jekyll site](#installing-and-configuring-the-javascript-sdk-in-your-jekyll-site)\n\n### Creating a JavaScript source in RudderStack\n\nThe RudderStack JavaScript source is required track the events from your Jekyll app. Follow these steps to set it up in your [RudderStack dashboard](https://app.rudderstack.com/):\n\n1.  Note the data plane URL in your RudderStack dashboard. This is required to set up the JavaScript SDK in your Jekyll app.\n\n[![Data plane URL](https://www.rudderstack.com/docs/images/user-guides/jamstack-guides/data-plane-url.webp)](https://www.rudderstack.com/docs/images/user-guides/jamstack-guides/data-plane-url.webp)\n\n2.  Click the **Sources** button in the left navigation bar and select **New Source** to create a source. Under **Sources**, select **Event Streams** > **JavaScript**.\n3.  Assign a name to your source and click **Continue**.\n4.  Your JavaScript source is now configured. Note down the write key for this source:\n\n[![JavaScript source write key](https://www.rudderstack.com/docs/images/user-guides/jamstack-guides/write-key.webp)](https://www.rudderstack.com/docs/images/user-guides/jamstack-guides/write-key.webp)\n\n### Installing and configuring the JavaScript SDK in your Jekyll site\n\nJekyll has an extensive theme system that helps customize your site’s presentation. **Minima** is Jekyll’s default theme. You can check your default Jekyll theme using the following command:\n\nThe following command opens an explorer window showing the theme files and directories:\n\n```\nopen $(bundle info --path)\n```\n\nTo integrate RudderStack with your Jekyll site and set up the tracking code, follow the steps below:\n\n1.  Create a `rudder.html` inside the `_includes` folder in your Jekyll theme’s folder and include the following script:\n\n```\n<script>\n  rudderanalytics = window.rudderanalytics = [];\n  var methods = [\n    \"load\",\n    \"page\",\n    \"track\",\n    \"identify\",\n    \"alias\",\n    \"group\",\n    \"ready\",\n    \"reset\",\n    \"setAnonymousId\",\n    \"getAnonymousId\",\n  ];\n  for (var i = 0; i < methods.length; i++) {\n    var method = methods[i];\n    rudderanalytics[method] = (function (methodName) {\n      return function () {\n        rudderanalytics.push(\n          [methodName].concat(Array.prototype.slice.call(arguments))\n        );\n      };\n    })(method);\n  }\n  rudderanalytics.load(\n    \"WRITE_KEY\",\n    \"DATA_PLANE_URL\"\n  );\n  rudderanalytics.ready(function () {\n    console.log(\"Application is ready with JS\");\n  });\n  rudderanalytics.track(\"Simple track call\");\n</script>\n<script src=\"https://cdn.rudderlabs.com/v1.1/rudder-analytics.min.js\"></script>\n```\n\n2.  Replace `<WRITE_KEY>` and `<DATA_PLANE_URL>` with your JavaScript source write key and the data plane URL obtained in the [Creating a JavaScript source in RudderStack](#creating-a-javascript-source-in-rudderstack) section above.\n3.  Go to the `_includes/head.html` file of your theme folder and run the following command:\n\n```\n{%- include rudder.html -%}\n```\n\n## Configuring a destination in RudderStack\n\nThis section details the steps required to set up a destination in RudderStack, where you can send all events tracked by the JavaScript SDK you set up above.\n\n1.  In your RudderStack dashboard, click **Destinations** > **New destination**.\n2.  Choose your preferred destination from the list.\n3.  Assign a name to the destination and click **Continue**.\n4.  Select the JavaScript source configured in the above section and click **Continue**.\n5.  Configure the destination with the required settings.\n\n## Deploying your Jekyll site and verifying the event stream\n\nTo verify if your event stream is working correctly, deploy your Jekyll site and test if the events are tracked and delivered correctly. Follow these steps:\n\n1.  From your terminal, navigate to the folder containing your Jekyll site and run following command:\n\n2.  Open the local server URL(generally `http://localhost:4000/`) in your browser to view the site.\n3.  Go to your browser’s developer tools and check the **Network** tab to verify if the RudderStack JavaScript SDK (`rudder-analytics.js`) is loaded correctly. The following image highlights this option for the Google Chrome browser:\n\n[![Chrome Network tab](https://www.rudderstack.com/docs/images/user-guides/browser-network-tab.webp)](https://www.rudderstack.com/docs/images/user-guides/browser-network-tab.webp)\n\n4.  Click the various links or pages in your site for RudderStack to track these actions.\n5.  Go to the **Live Events** tab of your JavaScript source in the RudderStack dashboard to check if the events are tracked. Note that you may face a minor delay before the events start showing up in your dashboard.\n6.  Go to your destination to verify if the events are received successfully.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Integrate JavaScript SDK v1.1 with your Jekyll site | RudderStack Docs",
    "description": "This guide will help you integrate RudderStack with your Jekyll site using the RudderStack JavaScript SDK. On successful integration, you can track and send real-time user events to your preferred destinations via RudderStack.\nTo set up the event stream on your Jekyll site, you need to perform the following steps:\nIntegrate the JavaScript SDK with your Jekyll site and set up the tracking code Configure a destination in RudderStack Deploy your Jekyll site and verify the events PrerequisitesThis guide assumes you have installed and set up your Jekyll site.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/user-guides/how-to-guides/rudderstack-jamstack-integration/v1.1/emberjs/",
    "markdown": "# Integrate JavaScript SDK v1.1 with your Ember.js app\n\n* * *\n\n*     3 minute read  \n    \n\nThis guide will help you integrate RudderStack with your Ember.js app using the [JavaScript SDK](https://www.rudderstack.com/docs/sources/event-streams/sdks/rudderstack-javascript-sdk/). On successful integration, you can track and send real-time user events to your preferred destinations via RudderStack.\n\nTo set up the event stream on your Ember.js app, you need to perform the following steps:\n\n1.  [Integrate the JavaScript SDK with your Ember.js app and set up the tracking code](#integrating-the-javascript-sdk-with-your-emberjs-app)\n2.  [Configure a destination in RudderStack](#configuring-a-destination-in-rudderstack)\n3.  [Deploy your Ember.js app and verify the events](#deploying-your-emberjs-app-and-verifying-the-event-stream)\n\n## Prerequisites\n\nThis guide assumes you have installed and set up your Ember.js app. Refer to the [Ember.js documentation](https://guides.emberjs.com/release/getting-started/quick-start/) for more information.\n\n## Integrating the JavaScript SDK with your Ember.js app\n\nIntegrating the JavaScript SDK with your Ember.js app involves the following steps:\n\n1.  [Creating a JavaScript source in RudderStack](#creating-a-javascript-source-in-rudderstack)\n2.  [Installing and configuring the JavaScript SDK in your app](#installing-and-configuring-the-javascript-sdk-in-your-app)\n\n### Creating a JavaScript source in RudderStack\n\nThe RudderStack JavaScript source is required track the events from your Ember.js app. Follow these steps to set it up in your [RudderStack dashboard](https://app.rudderstack.com/):\n\n1.  Note the data plane URL in your RudderStack dashboard. This is required to set up the JavaScript SDK in your Ember.js app.\n\n[![Data plane URL](https://www.rudderstack.com/docs/images/user-guides/jamstack-guides/data-plane-url.webp)](https://www.rudderstack.com/docs/images/user-guides/jamstack-guides/data-plane-url.webp)\n\n2.  Click the **Sources** button in the left navigation bar and select **New Source** to create a source. Under **Sources**, select **Event Streams** > **JavaScript**.\n3.  Assign a name to your source and click **Continue**.\n4.  Your JavaScript source is now configured. Note down the write key for this source:\n\n[![JavaScript source write key](https://www.rudderstack.com/docs/images/user-guides/jamstack-guides/write-key.webp)](https://www.rudderstack.com/docs/images/user-guides/jamstack-guides/write-key.webp)\n\n### Installing and configuring the JavaScript SDK in your app\n\nTo integrate RudderStack with your Ember.js app and set up the tracking code, follow the steps below:\n\n1.  In your Ember.js project folder, navigate to the `app` folder and open `index.html`.\n2.  Within the page’s `<body>` section, include the following code snippet:\n\n```\n<script>\n\n      rudderanalytics = window.rudderanalytics = [];\n      var methods = [\n        \"load\",\n        \"page\",\n        \"track\",\n        \"identify\",\n        \"alias\",\n        \"group\",\n        \"ready\",\n        \"reset\",\n        \"getAnonymousId\",\n        \"setAnonymousId\",\n      ];\n      for (var i = 0; i < methods.length; i++) {\n        var method = methods[i];\n        rudderanalytics[method] = (function (methodName) {\n          return function () {\n            rudderanalytics.push(\n              [methodName].concat(Array.prototype.slice.call(arguments))\n            );\n          };\n        })(method);\n      }\n    \n      rudderanalytics.load(\"WRITE_KEY\",\"DATA_PLANE_URL\");\n      //rudderanalytics.page();\n  </script> \n  <script src=\"https://cdn.rudderlabs.com/v1.1/rudder-analytics.min.js\"></script>\n```\n\n3.  Replace `WRITE_KEY` and `DATA_PLANE_URL` with your JavaScript source write key and the data plane URL obtained in the [Creating a JavaScript source in RudderStack](#creating-a-javascript-source-in-rudderstack) section above.\n\n## Configuring a destination in RudderStack\n\nThis section details the steps required to set up a destination in RudderStack, where you can send all events tracked by the JavaScript SDK you set up above.\n\n1.  In your RudderStack dashboard, click **Destinations** > **New destination**.\n2.  Choose your preferred destination from the list.\n3.  Assign a name to the destination and click **Continue**.\n4.  Select the JavaScript source configured in the above section and click **Continue**.\n5.  Configure the destination with the required settings.\n\n## Deploying your Ember.js app and verifying the event stream\n\nTo verify if your event stream is working correctly, deploy your Ember.js app and test if the events are tracked and delivered correctly. To do so, follow these steps:\n\n1.  From your terminal, navigate to your Ember.js project’s root folder and run the following command:\n\n2.  Open the local server URL(generally `http://localhost:4200/`) in your browser to view the app.\n3.  Go to your browser’s developer tools and check the **Network** tab to verify if the RudderStack JavaScript SDK (`rudder-analytics.js`) is loaded correctly. The following image highlights this option for the Google Chrome browser:\n\n[![Chrome Network tab](https://www.rudderstack.com/docs/images/user-guides/browser-network-tab.webp)](https://www.rudderstack.com/docs/images/user-guides/browser-network-tab.webp)\n\n4.  Click the various links or pages in your app for RudderStack to track these actions.\n5.  Go to the **Live Events** tab of your JavaScript source in the RudderStack dashboard to check if the events are tracked. Note that you may face a minor delay before the events start showing up in your dashboard.\n6.  Go to your destination to verify if the events are received successfully.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Integrate JavaScript SDK v1.1 with your Ember.js app | RudderStack Docs",
    "description": "This guide will help you integrate RudderStack with your Ember.js app using the JavaScript SDK. On successful integration, you can track and send real-time user events to your preferred destinations via RudderStack.\nTo set up the event stream on your Ember.js app, you need to perform the following steps:\nIntegrate the JavaScript SDK with your Ember.js app and set up the tracking code Configure a destination in RudderStack Deploy your Ember.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/user-guides/how-to-guides/rudderstack-jamstack-integration/v1.1/gatsby/",
    "markdown": "# How to integrate RudderStack with a Gatsby website\n\n* * *\n\n*     8 minute read  \n    \n\nYou can quickly and easily get RudderStack up and running in your Gatsby application by using RudderStack’s Gatsby plugin. It makes it easy to integrate your Gatsby website with the RudderStack API and easily track events.\n\n## Key features of this plugin\n\n*   Use multiple write keys (one for production environment, another optional one for development)\n*   Disable page view tracking (just in case you want to add it later manually)\n*   Always up-to-date and maintained by the RudderStack team\n\n## Installing the plugin\n\nTo install the plugin, run the following command with the package manager of your choice:\n\n*   NPM: `$ npm install --save gatsby-plugin-rudderstack`\n*   YARN: `$ yarn add gatsby-plugin-rudderstack`\n\n## Setup\n\n### Step 1: Configure your Gatsby config file\n\nIn your `gatsby-config.js` file you first need to add your write key from your account. You can find this in your RudderStack dashboard.\n\n[![](https://www.rudderstack.com/docs/images/source%20%281%29.webp)](https://www.rudderstack.com/docs/images/source%20%281%29.webp)\n\n#### Configuration options\n\n```\nplugins: [\n  {\n    resolve: `gatsby-plugin-rudderstack`,\n    options: {\n      // your rudderstack write key for your production environment\n      // when process.env.NODE_ENV === 'production'\n      // required; non-empty string\n      //NOTE: Do not commit this to git. Process from env.\n      prodKey: `RUDDERSTACK_PRODUCTION_WRITE_KEY`,\n      // if you have a development env for your rudderstack account, paste that key here\n      // when process.env.NODE_ENV === 'development'\n      // optional; non-empty string\n      //NOTE: Do not commit this to git. Process from env.\n      devKey: `RUDDERSTACK_DEV_WRITE_KEY`,\n      // boolean (defaults to false) on whether you want\n      // to include analytics.page() automatically\n      // if false, see below on how to track pageviews manually\n      trackPage: false,\n      // number (defaults to 50); time to wait after a route update before it should\n      // track the page change, to implement this, make sure your `trackPage` property is set to `true`\n      trackPageDelay: 50,\n      // If you need to proxy events through a custom data plane,\n      // add a `dataPlaneUrl` property (defaults to https://hosted.rudderlabs.com )\n      // RudderStack docs:\n      //   - https://rudderstack.com/docs/sources/event-streams/sdks/rudderstack-javascript-sdk/#31-load\n      dataPlaneUrl: `https://override-rudderstack-endpoint`,\n      // Add a `controlPlaneUrl` property if you are self-hosting the Control Plane\n      // RudderStack docs:\n      //   - https://rudderstack.com/docs/sources/event-streams/sdks/rudderstack-javascript-sdk/#311-self-hosted-control-plane\n      controlPlaneUrl: `https://override-control-plane-url`,\n      // boolean (defaults to false); whether to delay loading (add SDK script and load the SDK) of RudderStack JS SDK.\n      // This feature will force RudderStack to load _after_ either a page routing change\n      // or user scroll, whichever comes first. This delay time is controlled by\n      // `delayLoadTime` setting. This feature is used to help improve your website's\n      // TTI (for SEO, UX, etc).  See links below for more info.\n      // NOTE: But if you are using server-side routing and enable this feature,\n      // RudderStack will never load (because although client-side routing does not do\n      // a full page refresh, server-side routing does, thereby preventing RudderStack\n      // from ever loading).\n      // See here for more context:\n      // GIF: https://github.com/benjaminhoffman/gatsby-plugin-segment/pull/19#issuecomment-559569483\n      // TTI: https://github.com/GoogleChrome/lighthouse/blob/master/docs/scoring.md#performance\n      // Problem/solution: https://marketingexamples.com/seo/performance\n      delayLoad: false,\n      // number (default to 1000); time to wait after scroll or route change\n      // To be used when `delayLoad` is set to `true`\n      delayLoadTime: 1000,\n      // Whether to completely skip calling `analytics.load()`.\n      // ADVANCED FEATURE: only use if you are calling `analytics.load()` manually\n      // elsewhere in your code or are using a library\n      // that will call it for you.\n      // Useful for only loading the tracking script once a user has opted in to being tracked, for example.\n      // *Another use case is if you want to add callbacks to the methods at load time.\n      manualLoad: false,\n      // Can be used to override where the SDK should be loaded from. This is useful\n      // if you want to serve the SDK from a custom domain other than RudderStack to tackle ad-blockers\n      // By default, the plugin will use the latest JS SDK from RudderStack's CDN\n      sdkURL: `https://subdomain.yourdomain.com/v1.1/rudder-analytics.min.js`,\n      // string ('async' or 'defer'); whether to load the RudderStack SDK async or defer. Anything else\n      // will load normally.\n      // 'async' will load the SDK as <script async></script>\n      // 'defer' will load the SDK as <script defer></script>\n      loadType: 'default',\n      // Options to the `load` API\n      // Note: The above `controlPlaneUrl` overrides the `configUrl` field in this object\n      // You can find the loadOptions for JS SDK here:\n      // https://www.rudderstack.com/docs/sources/event-streams/sdks/rudderstack-javascript-sdk/load-js-sdk/#loading-options\n      loadOptions: {\n        ...\n      }\n    }\n  }\n];\n```\n\n### Step 2: Identify your users with the `identify()` method:\n\nThe `identify()` method allows you to link users and their actions to a specific user ID.\n\nA sample example of how the `identify()` method works in Gatsby is as shown:\n\n```\nclass CallToAction extends React.Component {\n    _handleCallToAction() {\n        window.rudderanalytics.identify(\n          \"12345\", {\n            email: \"name@domain.com\"\n          }, {\n            page: {\n              path: \"/post\",\n              referrer: \"internal\",\n              search: \"\",\n              title: \"Post Page\",\n              url: \"\",\n            },\n          }\n        }\n        render() {\n            return (\n                <Link onClick={ this._handleCallToAction } to=\"/write-post\">Write a Post</Link>\n              )\n        }\n}\n```\n\nIn the above example, information such as the user ID, email along with contextual information such as IP address, `anonymousId`, etc. will be captured.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> There is no need to call `identify()` for anonymous visitors to your website. Such visitors are automatically assigned an `anonymousId`.\n\n### Step 3: Track your users’ actions with the `track()` method\n\nThe `track()` method allows you to track any actions that your users might perform. A sample example of how the `track()` method works is as shown:\n\n```\nwindow.rudderanalytics.track(\n  \"test track event GA3\",\n  {\n    revenue: 30,\n    currency: \"USD\",\n    user_actual_id: 12345,\n  },\n  () => {\n    console.log(\"in track call\")\n  }\n)\n```\n\nIn the above example, the method tracks the event ‘**test track event GA3**’, and information such as the revenue, currency, and anonymousId. You can use this method to track various other success metrics for your website, such as user signups, item purchases, article bookmarks, and much more.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> To override contextual information, for ex: anonymizing IP and other contextual fields like `page` properties, the following template can be used. Similarly, you can override the auto-generated `anonymousId` with the provided ID. For this, follow the snippet below:\n\n```\nwindow.rudderanalytics.track(\n  \"test track event GA3\",\n  {\n    revenue: 30,\n    currency: \"USD\",\n    user_actual_id: 12345,\n  },\n  () => {\n    console.log(\"in track call\")\n  }\n)\n```\n\n#### Tracking pageviews\n\nIf you want to track pageviews automatically, set `trackPage` to `true` in your `gatsby-config.js` file. What we mean by _**automatically**_ is that whenever there is a route change, RudderStack leverages Gatsby’s `onRouteUpdate` API in the `gatsby-browser.js` file ([link](https://www.gatsbyjs.org/docs/browser-apis/#onRouteUpdate)) to invoke `window.rudderanalytics.page()` on each route change.\n\nHowever, if you want to pass in properties along with the pageview call (it is common to see some users pass in some user or account data with each `page` call), then you’ll have to set `trackPage: false` and call it yourself in your `gatsby-browser.js` file, as shown below:\n\n```\n// gatsby-browser.js\nexports.onRouteUpdate = () => {\n  window.rudderanalytics && window.rudderanalytics.page()\n}\n```\n\nYou’ve now successfully installed `rudder-analytics.js` tracking. You can enable and use any event destination to send your event data via RudderStack.\n\n### Step 4: Check ready state\n\nThere are cases when you may want to tap into the features provided by end destination SDKs to enhance tracking and other functionalities.\n\nRudderStack’s JavaScript SDK exposes a `ready` API with a `callback` parameter, that fires when the SDK is done initializing itself and other third-party native SDK destinations. For example:\n\n```\nwindow.rudderanalytics.ready(() => {\n  console.log(\"we are all set!!!\")\n})\n```\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> For detailed technical documentation and troubleshooting guide on the RudderStack’s JavaScript SDK, see [JavaScript SDK](https://www.rudderstack.com/docs/sources/event-streams/sdks/rudderstack-javascript-sdk/).\n\n## Using the Querystring API\n\nRudderStack’s Querystring API allows you to trigger `track`, `identify` calls using query parameters.\n\nIf you pass the following parameters in the URL, then it will trigger the corresponding SDK API call. For example:\n\n```\nhttp://hostname.com/?ajs_uid=12345&ajs_event=test%20event&ajs_aid=abcde&ajs_prop_testProp=prop1&ajs_trait_name=Firstname+Lastname\n```\n\nFor the above URL, the below SDK calls will be triggered:\n\n```\nrudderanalytics.identify(\"12345\", { name: \"Firstname Lastname\" })\nrudderanalytics.track(\"test event\", { testProp: \"prop1\" })\nrudderanalytics.setAnonymousId(\"abcde\")\n```\n\nYou may use the below parameters as a querystring parameter and trigger the corresponding call:\n\n| Parameter | Description |\n| --- | --- |\n| `ajs_uid` | Makes a `rudderanalytics.identify()` call with userId having the value of the parameter value. |\n| `ajs_aid` | Makes a `rudderanalytics.setAnonymousId()` call with anonymousId having the value of the parameter value. |\n| `ajs_event` | Makes a `rudderanalytics.track()` call with event name as parameter value. |\n| `ajs_prop_` | If `ajs_event` is passed as querystring, value of this parameter will populate the properties of the corresponding event in the track call. |\n| `ajs_trait_` | If `ajs_uid` is provided as querystring, value of this parameter will populate the traits of the identify call made. |\n\n## Adding callbacks to standard methods\n\nYou can also define callbacks to the common methods of the `rudderanalytics` object.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> Currently, the functionality is supported for `syncPixel` method which is called in the SDK when making sync calls in integrations for relevant destinations. For example, you can load `rudderanalytics` with callbacks in your Gatsby browser file as shown below:\n\n```\nwindow.rudderanalytics.syncPixelCallback = obj => {\n  window.rudderanalytics.track(\n    \"sync lotame\",\n    { destination: obj.destination },\n    { integrations: { All: false, S3: true } }\n  )\n}\n```\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> In order for this to work well, you must also set the `manualLoad` option to `true` in your Gatsby configuration, and load RudderStack in the browser’s Gatsby file manually.\n\nIn the above example, we defined a `syncPixelCallback` on the `rudderanalytics` object before the call to load the SDK. This will lead to calling of this registered callback with the parameter `{destination: <destination_name>}` whenever a sync call is made from the SDK for relevant integrations like **Lotame**.\n\nThe callback can be supplied in options parameter like below as well:\n\n```\n// Define the callbacks directly on the load method like:\nrudderanalytics.load(YOUR_WRITE_KEY, DATA_PLANE_URL, {\n  clientSuppliedCallbacks: {\n    syncPixelCallback: () => {\n      console.log(\"sync done!\")\n    },\n  },\n})\n```\n\n## License\n\nThis plugin is released under the [AGPLv3 License](https://www.gnu.org/licenses/agpl-3.0-standalone.html).\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "How to integrate RudderStack with a Gatsby website | RudderStack Docs",
    "description": "You can quickly and easily get RudderStack up and running in your Gatsby application by using RudderStack&rsquo;s Gatsby plugin. It makes it easy to integrate your Gatsby website with the RudderStack API and easily track events.\nKey features of this plugin Use multiple write keys (one for production environment, another optional one for development) Disable page view tracking (just in case you want to add it later manually) Always up-to-date and maintained by the RudderStack team Installing the pluginTo install the plugin, run the following command with the package manager of your choice:",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/user-guides/how-to-guides/rudderstack-jamstack-integration/v1.1/hugo/",
    "markdown": "# Integrate JavaScript SDK v1.1 with your Hugo site\n\n* * *\n\n*     4 minute read  \n    \n\nThis guide will help you integrate RudderStack with your Hugo site using the [RudderStack JavaScript SDK](https://www.rudderstack.com/docs/sources/event-streams/sdks/rudderstack-javascript-sdk/). On successful integration, you can track and send real-time user events to your preferred destinations via RudderStack.\n\nTo set up the event stream on your Hugo site, you need to perform the following steps:\n\n1.  [Integrate the JavaScript SDK with your Hugo site and set up the tracking code](#integrating-the-javascript-sdk-with-your-hugo-site)\n2.  [Configure a destination in RudderStack](#configuring-a-destination-in-rudderstack)\n3.  [Deploy your Hugo site and verify the events](#deploying-your-hugo-site-and-verifying-the-event-stream)\n\n## Prerequisites\n\nThis guide assumes you have installed and set up your Hugo site. Refer to the [Hugo documentation](https://gohugo.io/getting-started/quick-start/) for more information.\n\n## Integrating the JavaScript SDK with your Hugo site\n\nIntegrating the JavaScript SDK with your Hugo site involves the following steps:\n\n1.  [Creating a JavaScript source in RudderStack](#creating-a-javascript-source-in-rudderstack)\n2.  [Installing and configuring the JavaScript SDK in your Hugo site](#installing-and-configuring-the-javascript-sdk-in-your-hugo-site)\n\n### Creating a JavaScript source in RudderStack\n\nThe RudderStack JavaScript source is required track the events from your Hugo app. Follow these steps to set it up in your [RudderStack dashboard](https://app.rudderstack.com/):\n\n1.  Note the data plane URL in your RudderStack dashboard. This is required to set up the JavaScript SDK in your Hugo app.\n\n[![Data plane URL](https://www.rudderstack.com/docs/images/user-guides/jamstack-guides/data-plane-url.webp)](https://www.rudderstack.com/docs/images/user-guides/jamstack-guides/data-plane-url.webp)\n\n2.  Click the **Sources** button in the left navigation bar and select **New Source** to create a source. Under **Sources**, select **Event Streams** > **JavaScript**.\n3.  Assign a name to your source and click **Continue**.\n4.  Your JavaScript source is now configured. Note down the write key for this source:\n\n[![JavaScript source write key](https://www.rudderstack.com/docs/images/user-guides/jamstack-guides/write-key.webp)](https://www.rudderstack.com/docs/images/user-guides/jamstack-guides/write-key.webp)\n\n### Installing and configuring the JavaScript SDK in your Hugo site\n\nTo integrate RudderStack with your Hugo site and set up the tracking code, follow the steps below:\n\n1.  In your Hugo project, navigate to the `layouts` > `partials` folder and open `site-header.html`.\n2.  Place the following script within the `<header>` section of the page’s `<body>`:\n\n```\n<script>\n      rudderanalytics = window.rudderanalytics = [];\n        var methods = [\n          \"load\",\n          \"page\",\n          \"track\",\n          \"identify\",\n          \"alias\",\n          \"group\",\n          \"ready\",\n          \"reset\",\n          // \"requireIntegration\",\n          \"setAnonymousId\",\n          \"getAnonymousId\",\n        ];\n        for (var i = 0; i < methods.length; i++) {\n          var method = methods[i];\n          rudderanalytics[method] = (function (methodName) {\n            return function () {\n              rudderanalytics.push(\n                [methodName].concat(Array.prototype.slice.call(arguments))\n              );\n            };\n          })(method);\n        }\n        rudderanalytics.load(\"WRITE_KEY\",\"DATA_PLANE_URL\");\n        rudderanalytics.track(\"Simple track call\");\n    </script> \n    <script src=\"https://cdn.rudderlabs.com/v1.1/rudder-analytics.min.js\"></script>\n```\n\n3.  Replace `<WRITE_KEY>` and `<DATA_PLANE_URL>` with your JavaScript source write key and the data plane URL obtained in the [Creating a JavaScript source in RudderStack](#creating-a-javascript-source-in-rudderstack) section above.\n\n## Configuring a destination in RudderStack\n\nThis section details the steps required to set up a destination in RudderStack, where you can send all events tracked by the JavaScript SDK you set up above.\n\n1.  In your RudderStack dashboard, click **Destinations** > **New destination**.\n2.  Choose your preferred destination from the list.\n3.  Assign a name to the destination and click **Continue**.\n4.  Select the JavaScript source configured in the above section and click **Continue**.\n5.  Configure the destination with the required settings.\n\n## Deploying your Hugo site and verifying the event stream\n\nTo verify if your event stream is working correctly, deploy your Hugo site and test if the events are tracked and delivered correctly. Follow these steps:\n\n1.  From your terminal, navigate to the folder containing your Hugo site and run following command:\n\n2.  Open the local server URL(generally `http://localhost:1313/`) in your browser to view the site.\n3.  Go to your browser’s developer tools and check the **Network** tab to verify if the RudderStack JavaScript SDK (`rudder-analytics.js`) is loaded correctly. The following image highlights this option for Google Chrome:\n\n[![Chrome Network tab](https://www.rudderstack.com/docs/images/user-guides/browser-network-tab.webp)](https://www.rudderstack.com/docs/images/user-guides/browser-network-tab.webp)\n\n4.  Click the various links or pages in your Hugo site for RudderStack to track these actions.\n5.  Go to the **Live Events** tab of your JavaScript source in the RudderStack dashboard to check if the events are tracked. Note that you may face a minor delay before the events start showing up in your dashboard.\n6.  Go to your destination to verify if the events are received successfully.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Integrate JavaScript SDK v1.1 with your Hugo site | RudderStack Docs",
    "description": "This guide will help you integrate RudderStack with your Hugo site using the RudderStack JavaScript SDK. On successful integration, you can track and send real-time user events to your preferred destinations via RudderStack.\nTo set up the event stream on your Hugo site, you need to perform the following steps:\nIntegrate the JavaScript SDK with your Hugo site and set up the tracking code Configure a destination in RudderStack Deploy your Hugo site and verify the events PrerequisitesThis guide assumes you have installed and set up your Hugo site.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/user-guides/how-to-guides/rudderstack-jamstack-integration/v1.1/nextjs/",
    "markdown": "# Integrate JavaScript SDK v1.1 with your Next.js App\n\n* * *\n\n*     5 minute read  \n    \n\nThis guide will help you integrate RudderStack with your Next.js app using the [RudderStack JavaScript SDK](https://www.rudderstack.com/docs/sources/event-streams/sdks/rudderstack-javascript-sdk/). On successful integration, you can track and send real-time user events to your preferred destinations via RudderStack.\n\nTo set up the event stream on your Next.js app, you need to perform the following steps:\n\n1.  [Integrate the JavaScript SDK with your Next.js app and set up the tracking code](#integrating-the-javascript-sdk-with-your-nextjs-app)\n2.  [Configure a destination in RudderStack](#configuring-a-destination-in-rudderstack)\n3.  [Deploy your Next.js app and verify the events](#deploying-your-nextjs-app-and-verifying-the-event-stream)\n\n## Prerequisites\n\nThis guide assumes you have installed and set up your Next.js app. Refer to the [Next.js documentation](https://nextjs.org/docs/getting-started) for more information.\n\n## Integrating the JavaScript SDK with your Next.js app\n\nIntegrating the JavaScript SDK with your Next.js app involves the following steps:\n\n1.  [Creating a JavaScript source in RudderStack](#creating-a-javascript-source-in-rudderstack)\n2.  [Installing and configuring the JavaScript SDK in your app](#installing-and-configuring-the-javascript-sdk-in-your-nextjs-app)\n\n### Creating a JavaScript source in RudderStack\n\nThe RudderStack JavaScript source is required track the events from your Next.js app. Follow these steps to set it up in your [RudderStack dashboard](https://app.rudderstack.com/):\n\n1.  Note the data plane URL in your RudderStack dashboard. This is required to set up the JavaScript SDK in your Next.js app.\n\n[![Data plane URL](https://www.rudderstack.com/docs/images/user-guides/jamstack-guides/data-plane-url.webp)](https://www.rudderstack.com/docs/images/user-guides/jamstack-guides/data-plane-url.webp)\n\n2.  Click the **Sources** button in the left navigation bar and select **New Source** to create a source. Under **Sources**, select **Event Streams** > **JavaScript**.\n3.  Assign a name to your source and click **Continue**.\n4.  Your JavaScript source is now configured. Note down the write key for this source:\n\n[![JavaScript source write key](https://www.rudderstack.com/docs/images/user-guides/jamstack-guides/write-key.webp)](https://www.rudderstack.com/docs/images/user-guides/jamstack-guides/write-key.webp)\n\n### Method 1: Installing and configuring the JavaScript SDK in your Next.js app\n\nTo integrate RudderStack with your Next.js app and set up the tracking code, follow the steps below:\n\n1.  In your Next.js project folder, create a `rudderInitialize.js` file with the following code:\n\n```\nexport async function rudderInitialize() {\n  (function() {\n    var e = (window.rudderanalytics = window.rudderanalytics || []);\n    (e.methods = [\n      'load',\n      'page',\n      'track',\n      'identify',\n      'alias',\n      'group',\n      'ready',\n      'reset',\n      'getAnonymousId',\n      'setAnonymousId',\n      'getUserId',\n      'getUserTraits',\n      'getGroupId',\n      'getGroupTraits',\n      'startSession',\n      'endSession',\n    ]),\n    (e.factory = function(t) {\n      return function() {\n        e.push([t].concat(Array.prototype.slice.call(arguments)));\n      };\n    });\n    for (var t = 0; t < e.methods.length; t++) {\n      var r = e.methods[t];\n      e[r] = e.factory(r);\n    }\n    (e.loadJS = function(e, t) {\n      var r = document.createElement('script');\n      (r.type = 'text/javascript'),\n      (r.async = !0),\n      (r.src = 'https://cdn.rudderlabs.com/v1.1/rudder-analytics.min.js');\n      var a = document.getElementsByTagName('script')[0];\n      a.parentNode.insertBefore(r, a);\n    }),\n    e.loadJS(),\n      e.load('WRITE-KEY', 'DATAPLANE-URL'), // Replace 'WRITE-KEY' and 'DATAPLANE-URL'\n      e.page();\n  })();\n}\n```\n\n2.  Replace `<WRITE_KEY>` and `<DATA_PLANE_URL>` with your JavaScript source write key and the data plane URL obtained in the [Creating a JavaScript source in RudderStack](#creating-a-javascript-source-in-rudderstack) section above.\n3.  In the `pages` folder, open the `index.js` file and add the following code snippet:\n\n```\nimport React from 'react';\nimport { rudderInitialize } from '../rudderInitialize';\nexport default function Home() {\n  React.useEffect(() => {\n    rudderInitialize();\n  }, []);\n  const search = () => {\n    window.rudderanalytics.page();\n    window.rudderanalytics.track(\"Track Event.\");\n  };\n```\n\n### Method 2: Installing and configuring the JavaScript SDK using NPM package in your Next.js app\n\nAlternatively, you can integrate RudderStack with your Next.js app and set up the tracking code by following the steps below:\n\n1.  In your Next.js project, go to the `pages` folder and create a new `rudderInitialize.js` file with the following code:\n\n```\nexport async function rudderInitialize() {\n  window.rudderanalytics = await import(\"rudder-sdk-js\");\n\n  rudderanalytics.load(\"<WRITE_KEY>\", \"<DATA_PLANE_URL>\", {\n    integrations: { All: true }, // load call options\n  });\n\n  rudderanalytics.ready(() => {\n    console.log(\"All set!\");\n  });\n}\n```\n\n2.  Replace `<WRITE_KEY>` and `<DATA_PLANE_URL>` with your JavaScript source write key and the data plane URL obtained in the [Creating a JavaScript source in RudderStack](#creating-a-javascript-source-in-rudderstack) section above.\n3.  In the `pages` folder, open the `index.js` file and add the following code snippet:\n\n```\nimport React from \"react\";\nimport { rudderInitialize } from \"./rudderInitialize\";\n\nexport default function Home() {\n  React.useEffect(() => {\n    rudderInitialize();\n  }, []);\n\n  const search = () => {\n    window.rudderanalytics.page();\n    window.rudderanalytics.track(\"Sample Track Event\");\n  };\n```\n\n## Configuring a destination in RudderStack\n\nThis section details the steps required to set up a destination in RudderStack, where you can send all events tracked by the JavaScript SDK you set up above.\n\n1.  In your RudderStack dashboard, click **Destinations** > **New destination**.\n2.  Choose your preferred destination from the list.\n3.  Assign a name to the destination and click **Continue**.\n4.  Select the JavaScript source configured in the above section and click **Continue**.\n5.  Configure the destination with the required settings.\n\n## Deploying your Next.js app and verifying the event stream\n\nTo verify if your event stream is working correctly, deploy your Next.js app and test if the events are tracked and delivered correctly. To do so, follow these steps:\n\n1.  From your terminal, navigate to the folder containing your Next.js app and run following command:\n\n2.  Open the local server URL(generally `http://localhost:3000/`) in your browser to view the app.\n3.  Go to your browser’s developer tools and check the **Network** tab to verify if the RudderStack JavaScript SDK (`rudder-analytics.js`) is loaded correctly. The following image highlights this option for the Google Chrome browser:\n\n[![Chrome Network tab](https://www.rudderstack.com/docs/images/user-guides/browser-network-tab.webp)](https://www.rudderstack.com/docs/images/user-guides/browser-network-tab.webp)\n\n4.  Click the various links or pages in your app for RudderStack to track these actions.\n5.  Go to the **Live Events** tab of your JavaScript source in the RudderStack dashboard to check if the events are tracked. Note that you may face a minor delay before the events start showing up in your dashboard.\n6.  Go to your destination to verify if the events are received successfully.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Integrate JavaScript SDK v1.1 with your Next.js App | RudderStack Docs",
    "description": "This guide will help you integrate RudderStack with your Next.js app using the RudderStack JavaScript SDK. On successful integration, you can track and send real-time user events to your preferred destinations via RudderStack.\nTo set up the event stream on your Next.js app, you need to perform the following steps:\nIntegrate the JavaScript SDK with your Next.js app and set up the tracking code Configure a destination in RudderStack Deploy your Next.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/user-guides/how-to-guides/rudderstack-jamstack-integration/v1.1/svelte/",
    "markdown": "# Integrate JavaScript SDK v1.1 with your Svelte app\n\n* * *\n\n*     3 minute read  \n    \n\nThis guide will help you integrate RudderStack with your Svelte app using the [RudderStack JavaScript SDK](https://www.rudderstack.com/docs/sources/event-streams/sdks/rudderstack-javascript-sdk/). On successful integration, you can track and send real-time user events to your preferred destinations via RudderStack.\n\nTo set up the event stream on your Svelte app, you need to perform the following steps:\n\n1.  [Integrate the JavaScript SDK with your Svelte app and set up the tracking code](#integrating-the-javascript-sdk-with-your-svelte-app)\n2.  [Configure a destination in RudderStack](#configuring-a-destination-in-rudderstack)\n3.  [Deploy your Svelte app and verify the events](#deploying-your-svelte-app-and-verifying-the-event-stream)\n\n## Prerequisites\n\nThis guide assumes you have installed and set up your Svelte app. Refer to the [Svelte blog post](https://svelte.dev/blog/the-easiest-way-to-get-started) for more information.\n\n## Integrating the JavaScript SDK with your Svelte app\n\nIntegrating the JavaScript SDK with your Svelte app involves the following steps:\n\n1.  [Creating a JavaScript source in RudderStack](#creating-a-javascript-source-in-rudderstack)\n2.  [Installing and configuring the JavaScript SDK in your Svelte app](#installing-and-configuring-the-javascript-sdk-in-your-svelte-app)\n\n### Creating a JavaScript source in RudderStack\n\nThe RudderStack JavaScript source is required track the events from your Svelte app. Follow these steps to set it up in your [RudderStack dashboard](https://app.rudderstack.com/):\n\n1.  Note the data plane URL in your RudderStack dashboard. This is required to set up the JavaScript SDK in your Svelte app.\n\n[![Data plane URL](https://www.rudderstack.com/docs/images/user-guides/jamstack-guides/data-plane-url.webp)](https://www.rudderstack.com/docs/images/user-guides/jamstack-guides/data-plane-url.webp)\n\n2.  Click the **Sources** button in the left navigation bar and select **New Source** to create a source. Under **Sources**, select **Event Streams** > **JavaScript**.\n3.  Assign a name to your source and click **Continue**.\n4.  Your JavaScript source is now configured. Note down the write key for this source:\n\n[![JavaScript source write key](https://www.rudderstack.com/docs/images/user-guides/jamstack-guides/write-key.webp)](https://www.rudderstack.com/docs/images/user-guides/jamstack-guides/write-key.webp)\n\n### Installing and configuring the JavaScript SDK in your Svelte app\n\nTo integrate RudderStack with your Svelte app and set up the tracking code, follow the steps below:\n\n1.  In your Svelte project folder, navigate to the `public` folder and open `index.html`.\n2.  Place the following script within the `<head>` section of the page:\n\n```\n<script>\n                rudderanalytics = window.rudderanalytics = [];\n                var methods = [\n                  \"load\",\n                  \"page\",\n                  \"track\",\n                  \"identify\",\n                  \"alias\",\n                  \"group\",\n                  \"ready\",\n                  \"reset\",\n                  \"getAnonymousId\",\n                  \"setAnonymousId\",\n                ];\n                for (var i = 0; i < methods.length; i++) {\n                  var method = methods[i];\n                  rudderanalytics[method] = (function (methodName) {\n                        return function () {\n                          rudderanalytics.push(\n                                [methodName].concat(Array.prototype.slice.call(arguments))\n                          );\n                        };\n                  })(method);\n                }\n          \n                rudderanalytics.load(\"WRITE_KEY\",\"DATA_PLANE_URL\");\n                //rudderanalytics.page();\n  </script>\n  <script src=\"https://cdn.rudderlabs.com/v1.1/rudder-analytics.min.js\"></script>\n```\n\n3.  Replace `WRITE_KEY` and `DATA_PLANE_URL` with your JavaScript source write key and the data plane URL obtained in the [Creating a JavaScript source in RudderStack](#creating-a-javascript-source-in-rudderstack) section above.\n\n## Configuring a destination in RudderStack\n\nThis section details the steps required to set up a destination in RudderStack, where you can send all events tracked by the JavaScript SDK you set up above.\n\n1.  In your RudderStack dashboard, click **Destinations** > **New destination**.\n2.  Choose your preferred destination from the list.\n3.  Assign a name to the destination and click **Continue**.\n4.  Select the JavaScript source configured in the above section and click **Continue**.\n5.  Configure the destination with the required settings.\n\n## Deploying your Svelte app and verifying the event stream\n\nTo verify if your event stream is working correctly, deploy your Svelte app and test if the events are tracked and delivered correctly. To do so, follow these steps:\n\n1.  From your terminal, navigate to your Svelte app’s root folder and run the following command:\n\n2.  Open the local server URL(generally `http://localhost:8080/`) in your browser to view the app.\n3.  Go to your browser’s developer tools and check the **Network** tab to verify if the RudderStack JavaScript SDK (`rudder-analytics.js`) is loaded correctly. The following image highlights this option for the Google Chrome browser:\n\n[![Chrome Network tab](https://www.rudderstack.com/docs/images/user-guides/browser-network-tab.webp)](https://www.rudderstack.com/docs/images/user-guides/browser-network-tab.webp)\n\n4.  Click the various links or pages in your app for RudderStack to track these actions.\n5.  Go to the **Live Events** tab of your JavaScript source in the RudderStack dashboard to check if the events are tracked. Note that you may face a minor delay before the events start showing up in your dashboard.\n6.  Go to your destination to verify if the events are received successfully.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Integrate JavaScript SDK v1.1 with your Svelte app | RudderStack Docs",
    "description": "This guide will help you integrate RudderStack with your Svelte app using the RudderStack JavaScript SDK. On successful integration, you can track and send real-time user events to your preferred destinations via RudderStack.\nTo set up the event stream on your Svelte app, you need to perform the following steps:\nIntegrate the JavaScript SDK with your Svelte app and set up the tracking code Configure a destination in RudderStack Deploy your Svelte app and verify the events PrerequisitesThis guide assumes you have installed and set up your Svelte app.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/user-guides/how-to-guides/rudderstack-jamstack-integration/v1.1/nuxtjs/",
    "markdown": "# Integrate JavaScript SDK v1.1 with your Nuxt.js App\n\n* * *\n\n*     3 minute read  \n    \n\nThis guide will help you integrate RudderStack with your Nuxt.js app using the [RudderStack JavaScript SDK](https://www.rudderstack.com/docs/sources/event-streams/sdks/rudderstack-javascript-sdk/). On successful integration, you can track and send real-time user events to your preferred destinations via RudderStack.\n\nTo set up the event stream on your Nuxt.js app, you need to perform the following steps:\n\n1.  [Integrate the JavaScript SDK with your Nuxt.js app and set up the tracking code](#integrating-the-javascript-sdk-with-your-nuxtjs-app)\n2.  [Configure a destination in RudderStack](#configuring-a-destination-in-rudderstack)\n3.  [Deploy your Nuxt.js app and verify the events](#deploying-your-nuxtjs-app-and-verifying-the-event-stream)\n\n## Prerequisites\n\nThis guide assumes you have installed and set up your Nuxt.js app. You can refer to the official [Nuxt.js documentation](https://nuxtjs.org/docs/2.x/get-started/installation/) for more information.\n\n## Integrating the JavaScript SDK with your Nuxt.js app\n\nIntegrating the JavaScript SDK with your Nuxt.js app involves the following steps:\n\n1.  [Creating a JavaScript source in RudderStack](#creating-a-javascript-source-in-rudderstack)\n2.  [Installing and configuring the JavaScript SDK in your Nuxt.js app](#installing-and-configuring-the-javascript-sdk-in-your-nuxtjs-app)\n\n### Creating a JavaScript source\n\nThe RudderStack JavaScript source is required track the events from your Nuxt.js app. Follow these steps to set it up in your [RudderStack dashboard](https://app.rudderstack.com/):\n\n1.  Note the data plane URL in your RudderStack dashboard. This is required to set up the JavaScript SDK in your Nuxt.js app.\n\n[![Data plane URL](https://www.rudderstack.com/docs/images/user-guides/jamstack-guides/data-plane-url.webp)](https://www.rudderstack.com/docs/images/user-guides/jamstack-guides/data-plane-url.webp)\n\n2.  Click the **Sources** button in the left navigation bar and select **New Source** to create a source. Under **Sources**, select **Event Streams** > **JavaScript**.\n3.  Assign a name to your source and click **Continue**.\n4.  Your JavaScript source is now configured. Note down the write key for this source:\n\n[![JavaScript source write key](https://www.rudderstack.com/docs/images/user-guides/jamstack-guides/write-key.webp)](https://www.rudderstack.com/docs/images/user-guides/jamstack-guides/write-key.webp)\n\n### Installing and configuring the JavaScript SDK in your Nuxt.js app\n\nTo integrate RudderStack with your Nuxt.js app and set up the tracking code, follow the steps below:\n\n1.  In your app’s folder, open `nuxt.config.js` and include the following snippet within the `head:` section:\n\n```\nscript: [{\n    hid: 'Rudder-JS',\n    src: 'http://cdn.rudderlabs.com/v1.1/rudder-analytics.min.js',\n    defer: true\n  },\n  {\n    hid: 'rudder-js',\n    innerHTML: `\n        rudderanalytics = window.rudderanalytics = [];\n        var  methods = [\n            'load',\n            'page',\n            'track',\n            'identify',\n            'alias',\n            'group',\n            'ready',\n            'reset',\n            'getAnonymousId',\n            'setAnonymousId'\n        ];\n        for (var i = 0; i < methods.length; i++) {\n              var method = methods[i];\n              rudderanalytics[method] = function (methodName) {\n                    return function () {\n                                       rudderanalytics.push([methodName].concat(Array.prototype.slice.call(arguments)));\n                    };\n                  }(method);\n        }\n        rudderanalytics.load(\"WRITE_KEY\", \"DATA_PLANE_URL\");\n        rudderanalytics.ready(()=>{\n          console.log(\"We are all set\");\n        });\n        //rudderanalytics.page();\n        `,\n    type: 'text/javascript',\n    charset: 'utf-8'\n  }\n],\n```\n\n2.  Replace `<WRITE_KEY>` and `<DATA_PLANE_URL>` with your JavaScript source write key and the data plane URL obtained in the [Creating a JavaScript source in RudderStack](#creating-a-javascript-source-in-rudderstack) section above.\n\n[![Nuxt.js configuration](https://www.rudderstack.com/docs/images/user-guides/nuxtjs-config.webp)](https://www.rudderstack.com/docs/images/user-guides/nuxtjs-config.webp)\n\nThis section details the steps required to set up a destination in RudderStack, where you can send all events tracked by the JavaScript SDK you set up above.\n\n1.  In your RudderStack dashboard, click **Destinations** > **New destination**.\n2.  Choose your preferred destination from the list.\n3.  Assign a name to the destination and click **Continue**.\n4.  Select the JavaScript source configured in the above section and click **Continue**.\n5.  Configure the destination with the required settings.\n\n## Deploying your Nuxt.js app and verifying the event stream\n\nTo verify if your event stream is working correctly, deploy your Nuxt.js app and test if the events are tracked and delivered correctly. Follow these steps:\n\n1.  Deploy your Nuxt.js app by running the following command:\n\n2.  Open the local server URL(generally `http://localhost:3000/`) in your browser to view the app.\n3.  To verify if the RudderStack JavaScript SDK(`rudder-analytics.js`) is loaded correctly, go to your browser’s developer tools and check the **Network** tab. The following image highlights this option for the Google Chrome browser:\n\n[![Chrome Network tab](https://www.rudderstack.com/docs/images/user-guides/browser-network-tab.webp)](https://www.rudderstack.com/docs/images/user-guides/browser-network-tab.webp)\n\n4.  Click the various links or pages in your app for RudderStack to track these actions.\n5.  Go to the **Live Events** tab of your JavaScript source in the RudderStack dashboard to check if the events are tracked. Note that you may face a minor delay before the events start showing up in your dashboard.\n6.  Go to your destination to verify if the events are received successfully.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Integrate JavaScript SDK v1.1 with your Nuxt.js App | RudderStack Docs",
    "description": "This guide will help you integrate RudderStack with your Nuxt.js app using the RudderStack JavaScript SDK. On successful integration, you can track and send real-time user events to your preferred destinations via RudderStack.\nTo set up the event stream on your Nuxt.js app, you need to perform the following steps:\nIntegrate the JavaScript SDK with your Nuxt.js app and set up the tracking code Configure a destination in RudderStack Deploy your Nuxt.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/user-guides/how-to-guides/rudderstack-jamstack-integration/v3/",
    "markdown": "# Integrate JavaScript SDK v3 with your Jamstack site\n\nPrivacy Overview\n\nThis site uses cookies to improve your experience while you navigate through the website. Out of these cookies, the cookies that are categorized as necessary are stored on your browser as they are as essential for the working of basic functionalities of the website. We also use third-party cookies that help us analyze and understand how you use this website. These cookies will be stored in your browser only with your consent. You also have the option to opt-out of these cookies. But opting out of some of these cookies may have an effect on your browsing experience.\n\nNecessary cookies are absolutely essential for the website to function properly. This category only includes cookies that ensures basic functionalities and security features of the website. These cookies do not store any personal information.",
    "title": "Integrate JavaScript SDK v3 with your Jamstack site | RudderStack Docs",
    "description": "The most popular HTML, CSS, and JS library in the world.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/user-guides/how-to-guides/rudderstack-jamstack-integration/v1.1/vue/",
    "markdown": "# Integrate JavaScript SDK v1.1 with your Vue app\n\n* * *\n\n*     3 minute read  \n    \n\nThis guide will help you integrate RudderStack with your Vue app using the [RudderStack JavaScript SDK](https://www.rudderstack.com/docs/sources/event-streams/sdks/rudderstack-javascript-sdk/). On successful integration, you can track and send real-time user events to your preferred destinations via RudderStack.\n\nTo set up the event stream on your Vue app, you need to perform the following steps:\n\n1.  [Integrate the JavaScript SDK with your Vue app and set up the tracking code](#integrating-the-javascript-sdk-with-your-vue-app)\n2.  [Configure a destination in RudderStack](#configuring-a-destination-in-rudderstack)\n3.  [Deploy your Vue app and verify the events](#deploying-your-vue-app-and-verifying-the-event-stream)\n\n## Prerequisites\n\nThis guide assumes you have installed and set up your Vue app. Refer to the [Vue documentation](https://vuejs.org/v2/guide/installation.html) for more information.\n\n## Integrating the JavaScript SDK with your Vue app\n\nIntegrating the JavaScript SDK with your Vue app involves the following steps:\n\n1.  [Creating a JavaScript source in RudderStack](#creating-a-javascript-source-in-rudderstack)\n2.  [Installing and configuring the JavaScript SDK in your Vue app](#installing-and-configuring-the-javascript-sdk-in-your-vue-app)\n\n### Creating a JavaScript source in RudderStack\n\nThe RudderStack JavaScript source is required track the events from your Vue app. Follow these steps to set it up in your [RudderStack dashboard](https://app.rudderstack.com/):\n\n1.  Note the data plane URL in your RudderStack dashboard. This is required to set up the JavaScript SDK in your Vue app.\n\n[![Data plane URL](https://www.rudderstack.com/docs/images/user-guides/jamstack-guides/data-plane-url.webp)](https://www.rudderstack.com/docs/images/user-guides/jamstack-guides/data-plane-url.webp)\n\n2.  Click the **Sources** button in the left navigation bar and select **New Source** to create a source. Under **Sources**, select **Event Streams** > **JavaScript**.\n3.  Assign a name to your source and click **Continue**.\n4.  Your JavaScript source is now configured. Note down the write key for this source:\n\n[![JavaScript source write key](https://www.rudderstack.com/docs/images/user-guides/jamstack-guides/write-key.webp)](https://www.rudderstack.com/docs/images/user-guides/jamstack-guides/write-key.webp)\n\n### Installing and configuring the JavaScript SDK in your Vue app\n\nTo integrate RudderStack with your Vue app and set up the tracking code, follow the steps below:\n\n1.  In your Vue app’s project folder, navigate to the `public` folder and open `index.html`.\n2.  Place the following script within the `<head>` section of the page:\n\n```\n <script>\n\n      rudderanalytics=window.rudderanalytics=[];\n      for(var methods=[\"load\",\"page\",\"track\",\"identify\",\"reset\"],i=0;i<methods.length;i++){var method=methods[i];rudderanalytics[method]=function(a){return function(){rudderanalytics.push([a].concat(Array.prototype.slice.call(arguments)))}}(method)}\n        rudderanalytics.load(\"WRITE_KEY\",\"DATA_PLANE_URL\");\n  </script>\n  <script src=\"https://cdn.rudderlabs.com/rudder-analytics.min.js\"></script>\n```\n\n3.  Replace `WRITE_KEY` and `DATA_PLANE_URL` with your JavaScript source write key and the data plane URL obtained in the [Creating a JavaScript source in RudderStack](#creating-a-javascript-source-in-rudderstack) section above.\n\n## Configuring a destination in RudderStack\n\nThis section details the steps required to set up a destination in RudderStack, where you can send all events tracked by the JavaScript SDK you set up above.\n\n1.  In your RudderStack dashboard, click **Destinations** > **New destination**.\n2.  Choose your preferred destination from the list.\n3.  Assign a name to the destination and click **Continue**.\n4.  Select the JavaScript source configured in the above section and click **Continue**.\n5.  Configure the destination with the required settings.\n\n## Deploying your Vue app and verifying the event stream\n\nTo verify if your event stream is working correctly, deploy your Vue app and test if the events are tracked and delivered correctly. To do so, follow these steps:\n\n1.  From your terminal, navigate to your Vue app’s root folder and run the following command:\n\n2.  Open the local server URL(generally `http://localhost:8080/`) in your browser to view the app.\n3.  Go to your browser’s developer tools and check the **Network** tab to verify if the RudderStack JavaScript SDK (`rudder-analytics.js`) is loaded correctly. The following image highlights this option for the Google Chrome browser:\n\n[![Chrome Network tab](https://www.rudderstack.com/docs/images/user-guides/browser-network-tab.webp)](https://www.rudderstack.com/docs/images/user-guides/browser-network-tab.webp)\n\n4.  Click the various links or pages in your app for RudderStack to track these actions.\n5.  Go to the **Live Events** tab of your JavaScript source in the RudderStack dashboard to check if the events are tracked. Note that you may face a minor delay before the events start showing up in your dashboard.\n6.  Go to your destination to verify if the events are received successfully.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Integrate JavaScript SDK v1.1 with your Vue app | RudderStack Docs",
    "description": "This guide will help you integrate RudderStack with your Vue app using the RudderStack JavaScript SDK. On successful integration, you can track and send real-time user events to your preferred destinations via RudderStack.\nTo set up the event stream on your Vue app, you need to perform the following steps:\nIntegrate the JavaScript SDK with your Vue app and set up the tracking code Configure a destination in RudderStack Deploy your Vue app and verify the events PrerequisitesThis guide assumes you have installed and set up your Vue app.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/user-guides/how-to-guides/rudderstack-jamstack-integration/v3/gatsby/",
    "markdown": "# Integrate JavaScript SDK v3 with your Gatsby website\n\n* * *\n\n*     4 minute read  \n    \n\nYou can quickly and easily get RudderStack up and running in your Gatsby application using RudderStack’s NPM package. It makes it easy to integrate your Gatsby website with the RudderStack API and easily track events.\n\nTo set up the event stream on your Gatsby app, you need to perform the following steps:\n\n1.  [Integrate JavaScript SDK with your Gatsby app and set up the tracking code](#integrate-javascript-sdk-with-your-gatsby-app).\n2.  [Configure a destination in RudderStack](#configure-a-destination-in-rudderstack).\n3.  [Deploy your Gatsby app and verify the events](#deploy-your-app-and-verify-events).\n\n## Integrate JavaScript SDK with your Gatsby app\n\nIntegrating the JavaScript SDK with your Gatsby app involves the following steps:\n\n1.  [Creating a JavaScript source in RudderStack](#create-a-javascript-source).\n2.  [Installing and configuring the JavaScript SDK in your Gatsby app](#install-and-configure-the-sdk-in-your-app).\n\n### Create a JavaScript source\n\nThe RudderStack JavaScript source is required track the events from your Gatsby app. Follow these steps to set it up in your [RudderStack dashboard](https://app.rudderstack.com/):\n\n1.  Note the data plane URL in your RudderStack dashboard. This is required to set up the JavaScript SDK in your Gatsby app.\n\n[![Data plane URL](https://www.rudderstack.com/docs/images/user-guides/jamstack-guides/data-plane-url.webp)](https://www.rudderstack.com/docs/images/user-guides/jamstack-guides/data-plane-url.webp)\n\n2.  Click the **Sources** button in the left navigation bar and select **New Source** to create a source. Under **Sources**, select **Event Streams** > **JavaScript**.\n3.  Assign a name to your source and click **Continue**.\n4.  Your JavaScript source is now configured. Note down the write key for this source:\n\n[![JavaScript source write key](https://www.rudderstack.com/docs/images/user-guides/jamstack-guides/js-write-key.webp)](https://www.rudderstack.com/docs/images/user-guides/jamstack-guides/js-write-key.webp)\n\n### Install and configure the SDK in your app\n\nYou can integrate RudderStack with your Gatsby app and set up the tracking code by following the steps below:\n\n1.  Navigate to the root of your Gatsby project and install `@rudderstack/analytics-js` NPM package. For installation instructions, see [here](https://www.rudderstack.com/docs/sources/event-streams/sdks/rudderstack-javascript-sdk/installation/#using-npm).\n2.  Go to the `pages` folder in `src` and add the following code in the `index.tsx` file:\n\n```\nimport { useEffect } from 'react';\nimport { RudderAnalytics } from '@rudderstack/analytics-js';\n\nconst page = () => {\n  (window.rudderanalytics as RudderAnalytics).page('Cart Viewed');\n};\nconst identify = () => {\n  (window.rudderanalytics as RudderAnalytics).identify('sample-user-123');\n};\nconst track = () => {\n  (window.rudderanalytics as RudderAnalytics).track('Order Completed');\n};\n\nconst buttons = [\n  {\n    text: 'Page',\n    color: '#E95800',\n    action: page,\n  },\n  {\n    text: 'Identify',\n    color: '#1099A8',\n    action: identify,\n  },\n  {\n    text: 'Track',\n    color: '#BC027F',\n    action: track,\n  }\n];\n\nconst IndexPage: React.FC<PageProps> = () => {\n  useEffect(() => {\n    if (window.rudderanalytics as RudderAnalytics) {\n      return;\n    }\n    const analytics = new RudderAnalytics();\n\n    analytics.load('WRITE_KEY', 'DATA_PLANE_URL');\n\n    analytics.ready(() => {\n      console.log('We are all set!!!');\n    });\n  }, []);\n\n  return (\n    <main style={pageStyles}>\n      <ul style={listStyles}>\n        {buttons.map(btn => (\n          <button onClick={btn.action} style={{ ...listItemStyles, color: btn.color }}>\n            {btn.text}\n          </button>\n        ))}\n        <span></span>\n      </ul>\n    </main>\n  );\n};\n\nexport default IndexPage;\n\nexport const Head: HeadFC = () => <title>Home Page</title>;\n```\n\n2.  Replace `WRITE_KEY` and `DATA_PLANE_URL` with your JavaScript source write key and the data plane URL obtained in the [Creating a JavaScript source in RudderStack](#create-a-javascript-source) section above.\n\nSee the [sample Gatsby app](https://github.com/rudderlabs/rudder-sdk-js/tree/develop/examples/gatsby/sample-gatsby-site) for more information.\n\n## Configure a destination in RudderStack\n\nThis section details the steps required to set up a destination in RudderStack, where you can send all events tracked by the JavaScript SDK you set up above.\n\n1.  In your RudderStack dashboard, click **Destinations** > **New destination**.\n2.  Choose your preferred destination from the list.\n3.  Assign a name to the destination and click **Continue**.\n4.  Select the JavaScript source configured in the above section and click **Continue**.\n5.  Configure the destination with the required settings.\n\nOptionally, you can add a [user transformation](https://www.rudderstack.com/docs/transformations/overview/) to this destination to transform your events.\n\n## Deploy your app and verify events\n\nTo verify if your event stream is working correctly, deploy your Gatsby app and test if the events are tracked and delivered correctly. To do so, follow these steps:\n\n1.  From your terminal, navigate to the folder containing your Gatsby app and run following command:\n\n2.  Open the local server URL(generally `http://localhost:8000/`) in your browser to view the app.\n3.  Go to your browser’s developer tools and open the **Network** tab. Click the buttons in your app to trigger the RudderStack events.\n4.  Verify in the **Network** tab if the events are sent successfully:\n\n[![Verifying events](https://www.rudderstack.com/docs/images/user-guides/jamstack-guides/angular-network-tab.webp)](https://www.rudderstack.com/docs/images/user-guides/jamstack-guides/angular-network-tab.webp)\n\n5.  Go to the **Live Events** tab of your JavaScript source in the RudderStack dashboard to check if the events are tracked. Note that you may face a minor delay before the events start showing up in your dashboard.\n6.  Go to your destination to verify if the events are received successfully.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Integrate JavaScript SDK v3 with your Gatsby website | RudderStack Docs",
    "description": "You can quickly and easily get RudderStack up and running in your Gatsby application using RudderStack&rsquo;s NPM package. It makes it easy to integrate your Gatsby website with the RudderStack API and easily track events.\nTo set up the event stream on your Gatsby app, you need to perform the following steps:\nIntegrate JavaScript SDK with your Gatsby app and set up the tracking code. Configure a destination in RudderStack. Deploy your Gatsby app and verify the events.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/user-guides/how-to-guides/rudderstack-jamstack-integration/v3/nextjs/",
    "markdown": "# Integrate JavaScript SDK v3 with your Next.js App\n\n* * *\n\n*     5 minute read  \n    \n\nThis guide will help you integrate RudderStack with your Next.js app using the [JavaScript SDK v3](https://www.rudderstack.com/docs/sources/event-streams/sdks/rudderstack-javascript-sdk/). On successful integration, you can track and send real-time user events to your preferred destinations via RudderStack.\n\nTo set up the event stream on your Next.js app, you need to perform the following steps:\n\n1.  [Integrate JavaScript SDK with your Next.js app and set up the tracking code](#integrate-javascript-sdk-with-your-nextjs-app).\n2.  [Configure a destination in RudderStack](#configure-a-destination-in-rudderstack).\n3.  [Deploy your Next.js app and verify the events](#deploy-your-app-and-verify-events).\n\n## Prerequisites\n\nThis guide assumes you have installed and set up your Next.js app. Refer to the [Next.js documentation](https://nextjs.org/docs/getting-started) for more information.\n\n## Integrate JavaScript SDK with your Next.js app\n\nIntegrating the JavaScript SDK with your Next.js app involves the following steps:\n\n1.  [Creating a JavaScript source in RudderStack](#create-a-javascript-source).\n2.  [Installing and configuring the JavaScript SDK in your Next.js app](#install-and-configure-the-sdk-in-your-app).\n\n### Create a JavaScript source\n\nThe RudderStack JavaScript source is required track the events from your Next.js app. Follow these steps to set it up in your [RudderStack dashboard](https://app.rudderstack.com/):\n\n1.  Note the data plane URL in your RudderStack dashboard. This is required to set up the JavaScript SDK in your Next.js app.\n\n[![Data plane URL](https://www.rudderstack.com/docs/images/user-guides/jamstack-guides/data-plane-url.webp)](https://www.rudderstack.com/docs/images/user-guides/jamstack-guides/data-plane-url.webp)\n\n2.  Click the **Sources** button in the left navigation bar and select **New Source** to create a source. Under **Sources**, select **Event Streams** > **JavaScript**.\n3.  Assign a name to your source and click **Continue**.\n4.  Your JavaScript source is now configured. Note down the write key for this source:\n\n[![JavaScript source write key](https://www.rudderstack.com/docs/images/user-guides/jamstack-guides/js-write-key.webp)](https://www.rudderstack.com/docs/images/user-guides/jamstack-guides/js-write-key.webp)\n\n### Install and configure the SDK in your app\n\nYou can integrate RudderStack with your Next.js app and set up the tracking code by following the steps below:\n\n1.  In your Next.js project folder, create a `useRudderAnalytics.ts` file with the following code:\n\n```\nimport { useEffect, useState } from 'react';\nimport type { RudderAnalytics } from '@rudderstack/analytics-js';\n\nconst useRudderStackAnalytics = (): RudderAnalytics | undefined => {\n  const [analytics, setAnalytics] = useState<RudderAnalytics>();\n\n  useEffect(() => {\n    if (!analytics) {\n      const initialize = async () => {\n        const { RudderAnalytics } = await import('@rudderstack/analytics-js');\n        const analyticsInstance = new RudderAnalytics();\n\n        analyticsInstance.load('WRITE_KEY', 'DATA_PLANE_URL');\n\n        analyticsInstance.ready(() => {\n          console.log('We are all set!!!');\n        });\n        \n        setAnalytics(analyticsInstance);\n      };\n\n      initialize().catch(e => console.log(e));\n    }\n  }, [analytics]);\n\n  return analytics;\n};\n\nexport default useRudderStackAnalytics;\n```\n\n2.  Replace `WRITE_KEY` and `DATA_PLANE_URL` with your JavaScript source write key and the data plane URL obtained in the [Creating a JavaScript source in RudderStack](#create-a-javascript-source) section above.\n3.  To buffer the events triggered before the SDK is loaded, add the following script in the `layout.tsx` file within the `src` folder:\n\n```\nimport Script from 'next/script';\n\nexport default function RootLayout({ children }: { children: React.ReactNode }) {\n  return (\n    <html lang='en'>\n      <head>\n        <Script id='bufferEvents'>\n          {`\n            window.rudderanalytics = [];\n            var methods = [\n              'setDefaultInstanceKey',\n              'load',\n              'ready',\n              'page',\n              'track',\n              'identify',\n              'alias',\n              'group',\n              'reset',\n              'setAnonymousId',\n              'startSession',\n              'endSession',\n              'consent'\n            ];\n            for (var i = 0; i < methods.length; i++) {\n              var method = methods[i];\n              window.rudderanalytics[method] = (function (methodName) {\n                return function () {\n                  window.rudderanalytics.push([methodName].concat(Array.prototype.slice.call(arguments)));\n                };\n              })(method);\n            }\n            // Below line is only for demonstration purpose, SPA code is better place for auto page call\n            window.rudderanalytics.page('sample page call');\n        `}\n        </Script>\n      </head>\n      <body className={inter.className}>{children}</body>\n    </html>\n  );\n}\n```\n\n4.  If you are using an app router, go to the `app` folder and open the `page.tsx` file. If you are using a page router, go to the `pages` folder and open the `index.tsx` file. Then, add the following code:\n\n```\nimport { useEffect } from \"react\";\nimport useRudderStackAnalytics from './useRudderAnalytics';\n\nexport default function Home() {\n  const analytics = useRudderStackAnalytics();\n\n  const page = () => {\n    analytics?.page('Cart Viewed');\n  };\n  const identify = () => {\n    analytics?.identify('sample-user-123');\n  };\n  const track = () => {\n    analytics?.track('Order Completed');\n  };\n\n  useEffect(() => {\n    if(analytics) {\n      analytics.page('Auto track');\n    }\n  }, [analytics])\n\nreturn (\n    <main className='flex min-h-screen flex-col items-center justify-between p-24'>\n      <div className='mb-32 grid text-center lg:max-w-5xl lg:w-full lg:mb-0 lg:grid-cols-4 lg:text-left'>\n        <button\n          onClick={page}\n          className='group rounded-lg border border-transparent px-5 py-4 transition-colors hover:border-gray-300 hover:bg-gray-100 hover:dark:border-neutral-700 hover:dark:bg-neutral-800/30'>\n          <h2 className={`mb-3 text-2xl font-semibold`}>Page </h2>\n          <p className={`m-0 max-w-[30ch] text-sm opacity-50`}>\n            Clicking this tile will trigger a page event.\n          </p>\n        </button>\n     </div>\n\t</main>\n );\n}\n```\n\nSee the [sample Next.js app](https://github.com/rudderlabs/rudder-sdk-js/tree/develop/examples/nextjs/hooks/sample-app) for more information.\n\n## Configure a destination in RudderStack\n\nThis section details the steps required to set up a destination in RudderStack, where you can send all events tracked by the JavaScript SDK you set up above.\n\n1.  In your RudderStack dashboard, click **Destinations** > **New destination**.\n2.  Choose your preferred destination from the list.\n3.  Assign a name to the destination and click **Continue**.\n4.  Select the JavaScript source configured in the above section and click **Continue**.\n5.  Configure the destination with the required settings.\n\nOptionally, you can add a [user transformation](https://www.rudderstack.com/docs/transformations/overview/) to this destination to transform your events.\n\n## Deploy your app and verify events\n\nTo verify if your event stream is working correctly, deploy your Next.js app and test if the events are tracked and delivered correctly. To do so, follow these steps:\n\n1.  From your terminal, navigate to the folder containing your Next.js app and run following command:\n\n2.  Open the local server URL(generally `http://localhost:3000/`) in your browser to view the app.\n3.  Go to your browser’s developer tools and open the **Network** tab. Click the buttons in your app to trigger the RudderStack events.\n4.  Verify in the **Network** tab if the events are sent successfully:\n\n[![Verifying events](https://www.rudderstack.com/docs/images/user-guides/jamstack-guides/angular-network-tab.webp)](https://www.rudderstack.com/docs/images/user-guides/jamstack-guides/angular-network-tab.webp)\n\n5.  Go to the **Live Events** tab of your JavaScript source in the RudderStack dashboard to check if the events are tracked. Note that you may face a minor delay before the events start showing up in your dashboard.\n6.  Go to your destination to verify if the events are received successfully.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Integrate JavaScript SDK v3 with your Next.js App | RudderStack Docs",
    "description": "This guide will help you integrate RudderStack with your Next.js app using the JavaScript SDK v3. On successful integration, you can track and send real-time user events to your preferred destinations via RudderStack.\nTo set up the event stream on your Next.js app, you need to perform the following steps:\nIntegrate JavaScript SDK with your Next.js app and set up the tracking code. Configure a destination in RudderStack. Deploy your Next.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/user-guides/how-to-guides/rudderstack-jamstack-integration/v3/angular/",
    "markdown": "# Integrate JavaScript SDK v3 with your Angular app\n\n* * *\n\n*     4 minute read  \n    \n\nThis guide will help you integrate RudderStack with your Angular app using the [JavaScript SDK v3](https://www.rudderstack.com/docs/sources/event-streams/sdks/rudderstack-javascript-sdk/). On successful integration, you can track and send real-time user events to your preferred destinations via RudderStack.\n\nTo set up the event stream on your Angular app, you need to perform the following steps:\n\n1.  [Integrate the JavaScript SDK with your Angular app and set up the tracking code](#integrate-javascript-sdk-with-your-angular-app).\n2.  [Configure a destination in RudderStack](#configure-a-destination-in-rudderstack).\n3.  [Deploy your Angular app and verify the events](#deploy-your-app-and-verify-events).\n\n## Prerequisites\n\nThis guide assumes you have installed and set up your Angular app. Refer to the [Angular documentation](https://angular.io/guide/setup-local) for more information.\n\n## Integrate JavaScript SDK with your Angular app\n\nIntegrating the JavaScript SDK with your Angular app involves the following steps:\n\n1.  [Creating a JavaScript source in RudderStack](#create-a-javascript-source)\n2.  [Installing and configuring the JavaScript SDK in your Angular app](#install-and-configure-the-sdk-in-your-app)\n\n### Create a JavaScript source\n\nThe RudderStack JavaScript source is required track the events from your Angular app. Follow these steps to set it up in your [RudderStack dashboard](https://app.rudderstack.com/):\n\n1.  Note the data plane URL in your RudderStack dashboard. This is required to set up the JavaScript SDK in your Angular app.\n\n[![Data plane URL](https://www.rudderstack.com/docs/images/user-guides/jamstack-guides/data-plane-url.webp)](https://www.rudderstack.com/docs/images/user-guides/jamstack-guides/data-plane-url.webp)\n\n2.  Click the **Sources** button in the left navigation bar and select **New Source** to create a source. Under **Sources**, select **Event Streams** > **JavaScript**.\n3.  Assign a name to your source and click **Continue**.\n4.  Your JavaScript source is now configured. Note down the write key for this source:\n\n[![JavaScript source write key](https://www.rudderstack.com/docs/images/user-guides/jamstack-guides/js-write-key.webp)](https://www.rudderstack.com/docs/images/user-guides/jamstack-guides/js-write-key.webp)\n\n### Install and configure the SDK in your app\n\nTo integrate RudderStack with your Angular app and set up the tracking code, follow the steps below:\n\n1.  Navigate to the root of your Angular project and install the `@rudderstack/analytics-js` NPM package. For installation instructions, see [here](https://www.rudderstack.com/docs/sources/event-streams/sdks/rudderstack-javascript-sdk/installation/#using-npm).\n2.  In the `src` folder of your project, open `app.component.ts`, and add the following:\n\na. Import the `RudderAnalytics` package:\n\n```\nimport { RudderAnalytics } from '@rudderstack/analytics-js';\n```\n\nb. Modify `AppComponent` as follows:\n\n```\nexport class AppComponent implements OnInit, OnDestroy {\n  title = 'sample-app';\n  analytics: RudderAnalytics | undefined;\n  routerEventSubscription: Subscription | undefined;\n\n  constructor(private _router: Router) {}\n  ngOnInit() {\n    this.initialize();\n\n    this.routerEventSubscription = this._router.events.subscribe(event => {\n      if (event instanceof NavigationStart) {\n        this.analytics?.page(event.url);\n      }\n    });\n  }\n\n  initialize() {\n    if (window.rudderanalytics as RudderAnalytics) {\n      return;\n    }\n    this.analytics = new RudderAnalytics();\n\n    this.analytics.load('WRITE_KEY', 'DATA_PLANE_URL');\n\n    this.analytics.ready(() => {\n      console.log('We are all set!!!');\n    });\n  }\n\n  page() {\n    this.analytics?.page('Cart Viewed');\n  }\n  identify() {\n    this.analytics?.identify('sample-user-123');\n  }\n  track() {\n    this.analytics?.track('Order Completed');\n  }\n\n  public ngOnDestroy(): void {\n    this.routerEventSubscription?.unsubscribe();\n  }\n}\n```\n\nc. Add buttons in the `app.component.html` file to trigger your `page`, `identify`, and `track` events:\n\n```\n<button class=\"card\" (click)=\"page()\">\n    <span>Page</span>\n</button>\n<button class=\"card\" (click)=\"identify()\">\n    <span>Identify</span>\n</button>\n```\n\n3.  Replace `WRITE_KEY` and `DATA_PLANE_URL` with your JavaScript source write key and the data plane URL obtained in the [Creating a JavaScript source in RudderStack](#create-a-javascript-source) section above.\n4.  To buffer the events triggered before the SDK is loaded, add the following script in the `<head>` section of your `index.html` file within the `src` folder:\n\n```\n<script>\nwindow.rudderanalytics = [];\nvar methods = [\n  'setDefaultInstanceKey',\n  'load',\n  'ready',\n  'page',\n  'track',\n  'identify',\n  'alias',\n  'group',\n  'reset',\n  'setAnonymousId',\n  'startSession',\n  'endSession',\n  'consent'\n];\nfor (var i = 0; i < methods.length; i++) {\n  var method = methods[i];\n  window.rudderanalytics[method] = (function(methodName) {\n    return function() {\n      window.rudderanalytics.push([methodName].concat(Array.prototype.slice.call(arguments)));\n    };\n  })(method);\n}\n// Below line is only for demonstration purpose, SPA code is better place for auto page call\nwindow.rudderanalytics.page('sample page call');\n</script>\n```\n\nSee the [sample Angular app](https://github.com/rudderlabs/rudder-sdk-js/tree/develop/examples/angular/sample-app) for more information.\n\n## Configure a destination in RudderStack\n\nThis section details the steps required to set up a destination in RudderStack, where you can send all events tracked by the JavaScript SDK you set up above.\n\n1.  In your RudderStack dashboard, click **Destinations** > **New destination**.\n2.  Choose your preferred destination from the list.\n3.  Assign a name to the destination and click **Continue**.\n4.  Select the JavaScript source configured in the above section and click **Continue**.\n5.  Configure the destination with the required settings.\n\nOptionally, you can add a [user transformation](https://www.rudderstack.com/docs/transformations/overview/) to this destination to transform your events.\n\n## Deploy your app and verify events\n\nTo verify if your event stream is working correctly, deploy your Angular app and test if the events are tracked and delivered correctly. To do so, follow these steps:\n\n1.  From your terminal, navigate to your Angular project’s root folder and run the following command:\n\n2.  Open the local server URL(generally `http://localhost:4200/`) in your browser to view the app.\n3.  Go to your browser’s developer tools and open the **Network** tab. Click the buttons in your app (Step 2.c in [Installing and configuring the JavaScript SDK in your Angular app](#install-and-configure-the-sdk-in-your-app)) to trigger the RudderStack events.\n4.  Verify in the **Network** tab if the events are sent successfully:\n\n[![Verifying events](https://www.rudderstack.com/docs/images/user-guides/jamstack-guides/angular-network-tab.webp)](https://www.rudderstack.com/docs/images/user-guides/jamstack-guides/angular-network-tab.webp)\n\n5.  Go to the **Live Events** tab of your JavaScript source in the RudderStack dashboard to check if the events are tracked. Note that you may face a minor delay before the events start showing up in your dashboard.\n6.  Go to your destination to verify if the events are received successfully.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Integrate JavaScript SDK v3 with your Angular app | RudderStack Docs",
    "description": "This guide will help you integrate RudderStack with your Angular app using the JavaScript SDK v3. On successful integration, you can track and send real-time user events to your preferred destinations via RudderStack.\nTo set up the event stream on your Angular app, you need to perform the following steps:\nIntegrate the JavaScript SDK with your Angular app and set up the tracking code. Configure a destination in RudderStack. Deploy your Angular app and verify the events.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/user-guides/how-to-guides/rudderstack-jamstack-integration/v3/react/",
    "markdown": "# Integrate JavaScript SDK v3 with your React App\n\n* * *\n\n*     4 minute read  \n    \n\nThis guide will help you integrate RudderStack with your React app using the [JavaScript SDK v3](https://www.rudderstack.com/docs/sources/event-streams/sdks/rudderstack-javascript-sdk/). On successful integration, you can track and send real-time user events to your preferred destinations via RudderStack.\n\nTo set up the event stream on your React app, you need to perform the following steps:\n\n1.  [Integrate JavaScript SDK with your React app and set up the tracking code](#integrate-javascript-sdk-with-your-react-app).\n2.  [Configure a destination in RudderStack](#configure-a-destination-in-rudderstack).\n3.  [Deploy your React app and verify the events](#deploy-your-app-and-verify-events).\n\n## Integrate JavaScript SDK with your React app\n\nIntegrating the JavaScript SDK with your React app involves the following steps:\n\n1.  [Creating a JavaScript source in RudderStack](#create-a-javascript-source).\n2.  [Installing and configuring the JavaScript SDK in your React app](#install-and-configure-the-sdk-in-your-app).\n\n### Create a JavaScript source\n\nThe RudderStack JavaScript source is required track the events from your React app. Follow these steps to set it up in your [RudderStack dashboard](https://app.rudderstack.com/):\n\n1.  Note the data plane URL in your RudderStack dashboard. This is required to set up the JavaScript SDK in your React app.\n\n[![Data plane URL](https://www.rudderstack.com/docs/images/user-guides/jamstack-guides/data-plane-url.webp)](https://www.rudderstack.com/docs/images/user-guides/jamstack-guides/data-plane-url.webp)\n\n2.  Click the **Sources** button in the left navigation bar and select **New Source** to create a source. Under **Sources**, select **Event Streams** > **JavaScript**.\n3.  Assign a name to your source and click **Continue**.\n4.  Your JavaScript source is now configured. Note down the write key for this source:\n\n[![JavaScript source write key](https://www.rudderstack.com/docs/images/user-guides/jamstack-guides/js-write-key.webp)](https://www.rudderstack.com/docs/images/user-guides/jamstack-guides/js-write-key.webp)\n\n### Install and configure the SDK in your app\n\nYou can integrate RudderStack with your React app and set up the tracking code by following the steps below:\n\n1.  In your React project folder, create a `useRudderAnalytics.ts` file within the `src` folder, and add the following code:\n\n```\nimport { useEffect, useState } from 'react';\nimport { RudderAnalytics } from '@rudderstack/analytics-js';\n\nconst useRudderStackAnalytics = (): RudderAnalytics | undefined => {\n  const [analytics, setAnalytics] = useState<RudderAnalytics>();\n\n  useEffect(() => {\n    if (!analytics) {\n      const analyticsInstance = new RudderAnalytics();\n      analyticsInstance.load('WRITE_KEY', 'DATA_PLANE_URL');\n\n      analyticsInstance.ready(() => {\n        console.log('We are all set!!!');\n      });\n      \n      setAnalytics(analyticsInstance);\n    }\n  }, [analytics]);\n\n  return analytics;\n};\n\nexport default useRudderStackAnalytics;\n```\n\n2.  Replace `WRITE_KEY` and `DATA_PLANE_URL` with your JavaScript source write key and the data plane URL obtained in the [Creating a JavaScript source in RudderStack](#create-a-javascript-source) section above.\n3.  Add the following code in your `App.tsx` file:\n\n```\nimport { useEffect } from 'react';\nimport useRudderStackAnalytics from './useRudderAnalytics';\n\nfunction App() {\n  const analytics = useRudderStackAnalytics();\n\n  useEffect(() => {\n    if (analytics) {\n      analytics.page('Auto track');\n    }\n  }, [analytics]);\n\n  const page = () => {\n    analytics?.page('Cart Viewed');\n  };\n  const identify = () => {\n    analytics?.identify('sample-user-123');\n  };\n  const track = () => {\n    analytics?.track('Order Completed');\n  };\n  \n\n  return (\n    <div className='App'>\n      <header className='App-header'>\n        <img src={logo} className='App-logo' alt='logo' />\n        <p>\n          Edit <code>src/App.tsx</code> and save to reload.\n        </p>\n        <div className='card'>\n          <button onClick={page}>page</button>\n          <button onClick={identify}>identify</button>\n          <button onClick={track}>track</button>\n        </div>\n      </header>\n    </div>\n  );\n}\n```\n\n4.  To buffer the events triggered before the SDK is loaded, add the following script in the `index.html` file within the `public` folder:\n\n```\n<script>\nwindow.rudderanalytics = [];\nvar methods = [\n  'setDefaultInstanceKey',\n  'load',\n  'ready',\n  'page',\n  'track',\n  'identify',\n  'alias',\n  'group',\n  'reset',\n  'setAnonymousId',\n  'startSession',\n  'endSession',\n  'consent'\n];\nfor (var i = 0; i < methods.length; i++) {\n  var method = methods[i];\n  window.rudderanalytics[method] = (function(methodName) {\n    return function() {\n      window.rudderanalytics.push([methodName].concat(Array.prototype.slice.call(arguments)));\n    };\n  })(method);\n}\n\n// Below line is only for demonstration purpose, SPA code is better place for auto page call\n\nwindow.rudderanalytics.page('sample page call');\n</script>\n```\n\nSee the [sample React app](https://github.com/rudderlabs/rudder-sdk-js/tree/develop/examples/reactjs/hooks/sample-app) for more information.\n\n## Configure a destination in RudderStack\n\nThis section details the steps required to set up a destination in RudderStack, where you can send all events tracked by the JavaScript SDK you set up above.\n\n1.  In your RudderStack dashboard, click **Destinations** > **New destination**.\n2.  Choose your preferred destination from the list.\n3.  Assign a name to the destination and click **Continue**.\n4.  Select the JavaScript source configured in the above section and click **Continue**.\n5.  Configure the destination with the required settings.\n\nOptionally, you can add a [user transformation](https://www.rudderstack.com/docs/transformations/overview/) to this destination to transform your events.\n\n## Deploy your app and verify events\n\nTo verify if your event stream is working correctly, deploy your React app and test if the events are tracked and delivered correctly. To do so, follow these steps:\n\n1.  From your terminal, navigate to the folder containing your React app and run following command:\n\n2.  Open the local server URL(generally `http://localhost:3000/`) in your browser to view the app.\n3.  Go to your browser’s developer tools and open the **Network** tab. Click the buttons in your app to trigger the RudderStack events.\n4.  Verify in the **Network** tab if the events are sent successfully:\n\n[![Verifying events](https://www.rudderstack.com/docs/images/user-guides/jamstack-guides/angular-network-tab.webp)](https://www.rudderstack.com/docs/images/user-guides/jamstack-guides/angular-network-tab.webp)\n\n5.  Go to the **Live Events** tab of your JavaScript source in the RudderStack dashboard to check if the events are tracked. Note that you may face a minor delay before the events start showing up in your dashboard.\n6.  Go to your destination to verify if the events are received successfully.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Integrate JavaScript SDK v3 with your React App | RudderStack Docs",
    "description": "This guide will help you integrate RudderStack with your React app using the JavaScript SDK v3. On successful integration, you can track and send real-time user events to your preferred destinations via RudderStack.\nTo set up the event stream on your React app, you need to perform the following steps:\nIntegrate JavaScript SDK with your React app and set up the tracking code. Configure a destination in RudderStack. Deploy your React app and verify the events.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/javascript-sdk/",
    "markdown": "# JavaScript SDK Archives | RudderStack Docs\n\nPrivacy Overview\n\nThis site uses cookies to improve your experience while you navigate through the website. Out of these cookies, the cookies that are categorized as necessary are stored on your browser as they are as essential for the working of basic functionalities of the website. We also use third-party cookies that help us analyze and understand how you use this website. These cookies will be stored in your browser only with your consent. You also have the option to opt-out of these cookies. But opting out of some of these cookies may have an effect on your browsing experience.\n\nNecessary cookies are absolutely essential for the website to function properly. This category only includes cookies that ensures basic functionalities and security features of the website. These cookies do not store any personal information.",
    "title": "JavaScript SDK Archives | RudderStack Docs",
    "description": "The most popular HTML, CSS, and JS library in the world.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/user-guides/how-to-guides/implement-native-js-sdk-integration/",
    "markdown": "# How to implement a native JavaScript SDK integration\n\nAdd a destination SDK to the base RudderStack JavaScript SDK when sending events through the native integration.\n\n* * *\n\n*     2 minute read  \n    \n\nThis guide is helpful if you want to add a destination SDK to the [RudderStack JavaScript SDK](https://www.rudderstack.com/docs/sources/event-streams/sdks/rudderstack-javascript-sdk/) while sending events directly via your native integration **without RudderStack support**.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> This guide uses the terms **integration** and **destination** interchangeably.\n\nThe RudderStack JavaScript SDK reads the config data from the control plane to fetch native integrations. The native integrations must have the following methods/attributes defined by you as shown below. The RudderStack SDK calls these for initializing the destination global object and forwarding event data namely `identify`, `page` and `track`.\n\n[![](https://www.rudderstack.com/docs/images/screenshot-2019-12-10-at-7.57.43-pm.webp)](https://www.rudderstack.com/docs/images/screenshot-2019-12-10-at-7.57.43-pm.webp)\n\n*   **constructor**: RudderStack JS SDK, constructs an integration object with the destination specific keys such as `name`, `apiKey,` `custom mappings etc`, fetched from your config plane. These information are required by the below calls.\n*   **init** : Add the destination script (JavaScript snippet provided by the destination to initialize a global queue on window object)\n*   **identify**: RudderStack JS SDK calls this method to pass identify event data. One can write custom logic specific to the destination before calling destination specific implementation.\n*   **page**: Similar to identify, with page event data\n*   **track**: Similar to identify, with track event data\n*   **isLoaded**: RudderStack JS SDK checks the output of this method, once true, events are forwarded to above methods.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> RudderStack JS SDK makes a call to the config plane to fetch all native SDK **true** destinations before constructing and initializing the integration object with the fetched config.\n> \n> The **isLoaded** method return true when the global destination object is ready. RudderStack JS SDK waits for a maximum of 10 secs and polls for this flag to be ready every 1sec. Once all native SDK enabled integrations are ready, all event data accumulated from the time of page load is replayed on these integrations.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> The input to identify, page, track methods/property of the integration object is the entire event data object constructed by utility methods of the core analytics object.\n\n### Code structure\n\n[![](https://www.rudderstack.com/docs/images/screenshot-2019-12-10-at-7.21.35-pm.webp)](https://www.rudderstack.com/docs/images/screenshot-2019-12-10-at-7.21.35-pm.webp)\n\n[![](https://www.rudderstack.com/docs/images/screenshot-2019-12-10-at-7.22.25-pm.webp)](https://www.rudderstack.com/docs/images/screenshot-2019-12-10-at-7.22.25-pm.webp)\n\nTo add a new integration, add the above said methods to a JS object and export that object to be picked up by the **integrations** map.\n\nThis map is iterated and matched against the config fetched from the control plane to construct only those enabled integrations. This is matched against the `name` property of the integration object and destination config `name` .\n\n`All the activities of fetching configs and constructing and event forwarding to integrations is handled by the core analytics object.`\n\n> Refer the existing integrations in case of any parameter reference.  \n> For adding destination JS snippet, use the init method of the new integration.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "How to implement a native JavaScript SDK integration | RudderStack Docs",
    "description": "Add a destination SDK to the base RudderStack JavaScript SDK when sending events through the native integration.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/user-guides/how-to-guides/implement-native-js-sdk-integration/add-device-mode-sdk-to-js/",
    "markdown": "# How to add a device mode SDK to the RudderStack JavaScript SDK\n\nAdd an integration to the RudderStack JavaScript SDK to enable native device mode.\n\n* * *\n\n*     5 minute read  \n    \n\n## Connection Modes\n\nThere are two main modes of routing your event data to the required destinations:\n\n*   **Cloud Mode**: Our web and mobile SDKs provide APIs that can be called in case of events like `identify` ,`page` ,`screen` ,`track` etc. The SDK formats the data and sends to the Data Plane. The Data Plane transforms the data, and routes the events to the destination. RudderStack [Transformers](https://github.com/rudderlabs/rudder-transformer) transform the event payload to a destination-specific payload. _\\*\\*_\n*   **Device Mode**: Another way to send events to the destinations is with their client libraries. RudderStack SDKs can initialize and transform the events, and call into the required destination APIs to route your events. These destination libraries once initialized can auto-track events without any explicit call from RudderStack. For example - the destination **Hotjar**, once initialized through RudderStack, starts capturing all forms of event data.\n\n## Destination Setup\n\nBefore adding a **Device mode** destination libraries to RudderStack JS, the corresponding destination should be added via the Control Plane.\n\nThe following screenshot demonstrates how to enable the native SDK to send events:\n\n[![](https://www.rudderstack.com/docs/images/screenshot-2019-12-10-at-7.57.43-pm.webp)](https://www.rudderstack.com/docs/images/screenshot-2019-12-10-at-7.57.43-pm.webp)\n\nMove the Native SDK slider to enabled\n\n## Sample Integration Template\n\n```\nclass SampleIntegration {\n  constructor(apiKey) {\n    this.apiKey = apiKey // destination api key\n    this.name = \"SampleDestination\"\n  }\n\n  // Provides an iife for downloading and initializing the destination library/javascript\n  // Once initialized, the destination object will be available for making a call and pushing event data\n  init() {}\n\n  // rudderElement.message contains event data\n  // Add custom implementation here\n  identify(rudderElement) {\n    // sd('set', 'userId', rudderElement.message.anonymous_id);\n  }\n\n  // rudderElement.message contains event data\n  // Add custom implementation here\n  track(rudderElement) {\n    // sd('send', 'event', rudderElement.message.event);\n  }\n\n  // rudderElement.message contains event data\n  // Add custom implementation here\n  page(rudderElement) {\n    // sd('set', 'page', rudderElement.properties.path);\n    // sd ('send', 'pageview');\n  }\n\n  // Depend on the destination, where it enlists how to ensure if\n  // the destination is ready\n  isLoaded() {\n    return !!window.sdplugins\n  }\n}\n\nexport { SampleIntegration }\n```\n\n## Where to Add Your Integration\n\n1.  Add your integration under [https://github.com/rudderlabs/rudder-sdk-js/tree/production-staging/src/integrations](https://github.com/rudderlabs/rudder-sdk-js/tree/production-staging/src/integrations) with the code as shown above in `browser.js` and an `index.js` for exporting the integration.\n2.  Import the above integration in [https://github.com/rudderlabs/rudder-sdk-js/blob/production-staging/src/integrations/GA/index.js](https://github.com/rudderlabs/rudder-sdk-js/blob/production-staging/src/integrations/GA/index.js) and add it to the exported list to be picked up by the base RudderStack SDK.\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> The exported integration object must be under the **key name that matches the config-plane destination name.**\n\n[![](https://www.rudderstack.com/docs/images/screenshot-2019-12-10-at-7.21.35-pm.webp)](https://www.rudderstack.com/docs/images/screenshot-2019-12-10-at-7.21.35-pm.webp)\n\nThis exports the list of all native integrations\n\n[![](https://www.rudderstack.com/docs/images/screenshot-2019-12-10-at-7.22.25-pm.webp)](https://www.rudderstack.com/docs/images/screenshot-2019-12-10-at-7.22.25-pm.webp)\n\nThis exports the GoogleAds integration\n\nGet the config specific to your integration from the config object fetched by the [base RudderStack SDK](https://github.com/rudderlabs/rudder-sdk-js/) from the config plane and construct your integration object with the related configs.\n\n## Reference\n\nWe have a few native integrations already present in our JavaScript SDK. Please debug through them for any parameter references and call flow.\n\nBelow is a sample for **Google Analytics** native integration:\n\nThe following code implements the Google Analytics native integration under the file `integrations/GA/browser.js`:\n\n```\n// browser.js\n\nimport logger from \"../../utils/logUtil\"\nclass GA {\n  constructor(trackingID) {\n    this.trackingID = trackingID //UA-1010101-1\n    this.name = \"GA\"\n  }\n\n  init() {\n    // iife that initailizes the destination\n\n    ;(function (i, s, o, g, r, a, m) {\n      i[\"GoogleAnalyticsObject\"] = r\n      ;(i[r] =\n        i[r] ||\n        function () {\n          ;(i[r].q = i[r].q || []).push(arguments)\n        }),\n        (i[r].l = 1 * new Date())\n      ;(a = s.createElement(o)), (m = s.getElementsByTagName(o)[0])\n      a.async = 1\n      a.src = g\n      m.parentNode.insertBefore(a, m)\n    })(\n      window,\n      document,\n      \"script\",\n      \"https://www.google-analytics.com/analytics.js\",\n      \"ga\"\n    )\n\n    //window.ga_debug = {trace: true};\n\n    ga(\"create\", this.trackingID, \"auto\")\n    ga(\"send\", \"pageview\")\n\n    logger.debug(\"===in init GA===\")\n  }\n\n  identify(rudderElement) {\n    ga(\"set\", \"userId\", rudderElement.message.anonymous_id)\n    logger.debug(\"in GoogleAnalyticsManager identify\")\n  }\n\n  track(rudderElement) {\n    var eventCategory = rudderElement.message.event\n    var eventAction = rudderElement.message.event\n    var eventLabel = rudderElement.message.event\n    var eventValue = \"\"\n    if (rudderElement.message.properties) {\n      eventValue = rudderElement.message.properties.value\n        ? rudderElement.message.properties.value\n        : rudderElement.message.properties.revenue\n    }\n\n    var payLoad = {\n      hitType: \"event\",\n      eventCategory: eventCategory,\n      eventAction: eventAction,\n      eventLabel: eventLabel,\n      eventValue: eventValue,\n    }\n    ga(\"send\", \"event\", payLoad)\n    logger.debug(\"in GoogleAnalyticsManager track\")\n  }\n\n  page(rudderElement) {\n    logger.debug(\"in GoogleAnalyticsManager page\")\n    var path =\n      rudderElement.properties && rudderElement.properties.path\n        ? rudderElement.properties.path\n        : undefined\n    if (path) {\n      ga(\"set\", \"page\", path)\n    }\n    ga(\"send\", \"pageview\")\n  }\n\n  isLoaded() {\n    logger.debug(\"in GA isLoaded\")\n    return !!window.gaplugins\n  }\n}\n\nexport { GA }\n```\n\nThe following is the code for `index.js` under `integrations/GA/index.js` :\n\n```\n// index.js\n\n// Ignore the node counter-part, we already have a seperate node sdk\n// that sends events directly to the data-plane (server-mode)\n\nimport { GANode } from \"./node\"\nimport { GA } from \"./browser\"\n\nexport default process.browser ? GA : GANode\n```\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> You can ignore the Node counterpart as it’s legacy. We have a separate [Node.js SDK](https://www.rudderstack.com/docs/sources/event-streams/sdks/rudderstack-node-sdk/) for sending events to RudderStack (in the server-side connection mode)\n\nThe following is the code for adding the newly-added integration to the export list. The key name must be the same as the one configured in the config plane.\n\n```\n// root integrations/index.js\n\nimport * as HubSpot from \"./HubSpot\"\nimport * as GA from \"./GA\"\nimport * as Hotjar from \"./Hotjar\"\nimport * as GoogleAds from \"./GoogleAds\"\n\n// Note the key name is same as we have configured in our config-plane\n\nlet integrations = {\n  HS: HubSpot.default,\n  GA: GA.default,\n  HOTJAR: Hotjar.default,\n  GOOGLEADS: GoogleAds.default,\n}\n\nexport { integrations }\n```\n\n## Native Integration Methods\n\nThe native integrations should have the following methods for initializing the destination global object and forwarding event data.\n\n| Function | Description |\n| --- | --- |\n| `constructor` | The RudderStack JavaScript SDK constructs an integration object with the destination config such as `name`, `apiKey`, `custom mappings` etc. fetched from your config plane. This information is required by the subsequent calls. |\n| `isLoaded` | This function is polled to check if the destination is ready |\n| `init` | Adds the destination script, i.e. the JavaScript snippet provided by the destination to initialize a global queue on window object. |\n| `identify` | RudderStack’s JavaScript SDK calls this method to pass the `identify` event data. Destination-specific implementation can be added here. |\n| `page` | This method is called to pass the `page` event data. |\n| `track` | This method is called to pass the `track` event data. |\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> RudderStack JavaScript SDK makes a call to the config plane to fetch all native SDK-enabled destinations, before constructing and initializing the integration object with the fetched configuration. The **`isLoaded`** method should return true when the destination is ready to accept events.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "How to add a device mode SDK to the RudderStack JavaScript SDK | RudderStack Docs",
    "description": "Add an integration to the RudderStack JavaScript SDK to enable native device mode.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/javascript-sdk/1.1/",
    "markdown": "# JavaScript SDK v1.1 | RudderStack Docs\n\nPrivacy Overview\n\nThis site uses cookies to improve your experience while you navigate through the website. Out of these cookies, the cookies that are categorized as necessary are stored on your browser as they are as essential for the working of basic functionalities of the website. We also use third-party cookies that help us analyze and understand how you use this website. These cookies will be stored in your browser only with your consent. You also have the option to opt-out of these cookies. But opting out of some of these cookies may have an effect on your browsing experience.\n\nNecessary cookies are absolutely essential for the website to function properly. This category only includes cookies that ensures basic functionalities and security features of the website. These cookies do not store any personal information.",
    "title": "JavaScript SDK v1.1 | RudderStack Docs",
    "description": "Track event data and send it your specified destinations using the RudderStack JavaScript SDK v1.1.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/user-guides/how-to-guides/how-to-submit-an-integration-pull-request/",
    "markdown": "# How to submit a pull request for a new integration\n\nSubmit a pull request to a RudderStack GitHub repository.\n\n* * *\n\n*     3 minute read  \n    \n\nThis guide will take you through the steps to contribute to our open-source code while creating your RudderStack integration. It will also make sure that your integration code fits into our existing codebase without any issues.\n\nTo create and submit an integration, you must contribute to the following repositories:\n\n| Repository | Link | Description |\n| --- | --- | --- |\n| RudderStack Transformer | [rudder-transformer](https://github.com/rudderlabs/rudder-transformer) | Transforms the standard RudderStack payload from the source to the required destination payload format. |\n| Control Plane Lite | [config-generator](https://github.com/rudderlabs/config-generator) | Gathers the connection-specific configurations specific to each user. |\n| RudderStack documentation | [rudderstack-docs](https://github.com/rudderlabs/rudderstack-docs) | Includes the documentation to set up, configure, and use the integration. |\n\n## Creating a pull request\n\nFollow the steps below to create a pull request and submit your integration to our open-source code:\n\n1.  Fork and clone the above-mentioned respositories in your local machine. If you’ve done it already, ensure that the local repositories are updated to prevent any merge conflicts.\n2.  Create a branch with a clear, descriptive name.\n3.  Push your changes to the remote origin.\n4.  Click **New pull request** in the RudderStack repository.\n5.  Click **Compare across forks** link.\n6.  Change the **Head Repository** to your forked repository and change **Compare** to be the associated branch name.\n7.  Make the necessary title or description changes. For example, you can add tags such as WIP (work in progress), etc. to add your work status.\n8.  Click **Create pull request**.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> Repeat the above steps for each of the repositories: `rudder-transformer`, `config-generator`, and `rudderstack-docs`.\n\n## RudderStack transformer\n\n[GitHub repository](https://github.com/rudderlabs/rudder-transformer)\n\nThe `rudder-transformer` repository is responsible for transforming the source payload into the destination-specific format. As it contains a lot of code, keep the following tips in mind when contributing to this repository:\n\n*   Format the code to match the existing structure. In case of any queries, [contact us](mailto:%20docs@rudderstack.com).\n*   Include any `eslint` logic at the top of the file.\n*   Include test cases that provide around 80% code coverage.\n\n## Control Plane Lite\n\n[GitHub repository](https://github.com/rudderlabs/config-generator)\n\nThe `config-generator` repository can be used to upload the settings required for configuring the integration. The best practice is to follow the pre-existing structure to the best of your ability and [reach out to us](https://rudderstack.com/join-rudderstack-slack-community) in case of any questions.\n\nAlso, note the following:\n\n*   Include the regex validation rules for all text input fields.\n*   The icon file must be in the SVG format.\n\n## RudderStack documentation\n\n[GitHub repository](https://github.com/rudderlabs/rudderstack-docs)\n\nThe `rudderstack-docs` repository houses all RudderStack documentation. You can refer to the [destination template](https://github.com/rudderlabs/rudderstack-docs/blob/master/DESTINATION_TEMPLATE.mdx) while creating the documentation for the integration; it will help you understand the pre-defined documentation structure.\n\nAlso, a few tips while documenting your integration:\n\n*   Ensure that the document is detailed and thorough to avoid any confusion and errors.\n*   Include clear and concise steps to set up, configure, and use your integration.\n*   Include tips and suggestions wherever possible.\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> Do not include any images or screenshots in the documentation. They will be added once all changes are merged.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "How to submit a pull request for a new integration | RudderStack Docs",
    "description": "Submit a pull request to a RudderStack GitHub repository.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/user-guides/how-to-guides/using-aws-lambda-functions-with-rudderstack/",
    "markdown": "# How to integrate Lambda Functions with RudderStack\n\nIntegrate AWS lambda functions with the RudderStack Node SDK.\n\n* * *\n\n*     2 minute read  \n    \n\nAWS Lambda helps you leverage the full potential of **AWS Serverless Compute**, allowing you to run your code without having to manage or provision servers.\n\n## Key features\n\nWith AWS Lambda, you can:\n\n*   Run any kind of application or web service without requiring any server administration\n*   Pay only for computing resources that you use for your code or application\n*   Scale your code and make your applications highly available without requiring any additional configuration or cost\n*   AWS Lambda offers high performance, regardless of the scale or complexity of your code\n\nRudderStack integrates seamlessly with AWS Lambda, allowing you to use any lambda function to transform your event streams and route them to the [destinations](https://www.rudderstack.com/docs/destinations/overview/) of your choice.\n\n## Integrating AWS Lambda with RudderStack’s Node SDK\n\nThe AWS Lambda functions can be coded in Node.js. This makes integration with RudderStack easier, since RudderStack provides a **Node.js SDK** that can be seamlessly used in the AWS Lambda code. The RudderStack SDK is called by the lambda function to perform the necessary data mappings. The data is then routed to the specified analytics destinations by RudderStack.\n\nThe following steps are required for integrating the RudderStack Node SDK with the AWS Lambda function:\n\n*   Before getting started, make sure you have a Docker version of RudderStack server in an EC2 instance. More instructions on the setup can be found on our [GitHub page](https://github.com/rudderlabs/rudder-server#setup-instructions-docker).\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> You must have the [AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html) installed in your development environment.\n\n1.  Install the RudderStack Node SDK at the location where the lambda function artifacts are maintained in the development environment:\n\n```\n[ec2-user@ip-172-31-44-230 ~]$ npm install --prefix=~/lambda-apps @rudderstack/rudder-sdk-node  \n```\n\n2.  Archive all contents of the lambda function development directory in a ZIP file:\n\n```\n[ec2-user@ip-172-31-44-230 lambda-apps]$ zip -r function.zip .  \n```\n\n3.  Update the lambda function deployment:\n\n```\n[ec2-user@ip-172-31-44-230 lambda-apps]$ aws lambda update-function-code --function-name lambda-apps-dev-helloWorld --zip-file fileb://~/lambda-apps/function.zip  \n```\n\n## Testing the integration\n\nTo ensure that the integration is successful, you can test the availability of the function at the **AWS-designated web endpoint**. This endpoint is created at the time of deployment of the function for the first time.\n\n```\n'use strict';\nconst Analytics = require(\"@rudderstack/rudder-sdk-node\");\n\nmodule.exports.helloWorld = (event, context, callback) => {\n    const response = {\n   \t statusCode: 200,\n   \t headers: {\n   \t\t 'Access-Control-Allow-Origin': '*', // Required for CORS support to work\n   \t },\n   \t body: JSON.stringify({\n   \t\t message: 'Go Serverless v1.0! Your function executed successfully!',\n   \t\t input: event,\n   \t }),\n    };\n```\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "How to integrate Lambda Functions with RudderStack | RudderStack Docs",
    "description": "Integrate AWS lambda functions with the RudderStack Node SDK.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/javascript-sdk/1.1/supported-api/",
    "markdown": "# JavaScript SDK API | RudderStack Docs\n\nImplement the JavaScript SDK API.\n\n* * *\n\n*     11 minute read  \n    \n\nThe JavaScript SDK provides a comprehensive API that lets you track and send your event data to any destination.\n\n## Identify\n\nThe [`identify`](https://www.rudderstack.com/docs/event-spec/standard-events/identify/) method lets you identify a user and associate them to their actions. It also lets you record any traits about them like their name, email, etc.\n\nOnce you make the `identify` call, the SDK persists the user information and passes it to the subsequent calls.\n\nThe JavaScript SDK defines the `identify` method as shown:\n\n```\nrudderanalytics.identify(userId, [traits], [apiOptions], [callback]);\n```\n\nThe following table describes the above (optional) parameters in detail:\n\n| Parameter | Type | Description |\n| --- | --- | --- |\n| `userId` | String | The unique user identifier in the database. When provided, the SDK sends this argument to the destinations instead of `anonymousId`. |\n| `traits` | Dictionary | Contains the user’s traits or the properties associated with `userId` such as email, address, etc. [Reference](https://www.rudderstack.com/docs/event-spec/standard-events/identify/#identify-traits). |\n| `callback` | Function | Called after the successful execution of the `identify` method. |\n| `apiOptions` | Dictionary | Provides information such as `integrations`, `anonymousId`, and `originalTimestamp`. |\n\n#### apiOptions\n\nThe structure of the **apiOptions** parameter is as shown:\n\n```\n{\n  integrations: IntegrationOpts,\n  anonymousId: string,\n  originalTimestamp: ISO 8601 date string,\n  <other keys>: <value> // merged with event's contextual information\n}\n```\n\nThe following table describes the above parameters in detail:\n\n| **Parameter** | **Type** | **Description** |\n| --- | --- | --- |\n| `integrations` | [IntegrationOpts](https://www.rudderstack.com/docs/archive/javascript-sdk/1.1/load-js-sdk/#integrationopts) | Use to send event data only to the [selective destinations](https://www.rudderstack.com/docs/archive/javascript-sdk/1.1/filtering/#filtering-destinations). |\n| `anonymousId` | String | Overrides the current event’s `anonymousId` at the top level. |\n| `originalTimestamp` | ISO 8601 Date string | Overrides the current event’s `originalTimestamp` at the top level. [Reference.](https://www.rudderstack.com/docs/event-spec/standard-events/common-fields/#clock-skew-considerations) |\n| `<other keys>: <value>` | \\-  | Merged with the event’s contextual information. |\n\nA sample `identify` call is shown below:\n\n```\nrudderanalytics.identify(\n  \"1hKOmRA4el9Zt1WSfVJIVo4GRlm\", {\n    firstName: \"Alex\",\n    lastName: \"Keener\",\n    email: \"alex@example.com\",\n    phone: \"+1-202-555-0146\"\n  }, {\n    page: {\n      path: \"/best-seller/1\",\n      referrer: \"https://www.google.com/search?q=estore+bestseller\",\n      search: \"estore bestseller\",\n      title: \"The best sellers offered by EStore\",\n      url: \"https://www.estore.com/best-seller/1\"\n    }\n  },\n  () => {\n    console.log(\"identify call\");\n  }\n);\n```\n\nIn the above example, the JavaScript SDK captures the `userId`, `email` and the [contextual information](https://www.rudderstack.com/docs/event-spec/standard-events/common-fields/#contextual-fields).\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> If you specify the IP address in the event payload, RudderStack uses that instead of automatically capturing it in the backend. You can use this feature to anonymize your users’ IP addresses.\n\n### Anonymous ID\n\nThe `anonymousId` is an auto-generated **UUID** (Universally Unique Identifier) that gets assigned to each unique and unidentified visitor to your website.\n\n#### Retrieving anonymous ID\n\nTo retrieve the anonymous ID of any visitor, run the following:\n\n```\nrudderanalytics.getAnonymousId();\n```\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> If the `anonymousId` is `null` in the SDK and you call the above function, RudderStack will automatically set a new `anonymousId`.\n\n#### How the SDK uses `anonymousId`\n\nThe JavaScript SDK generates a unique `anonymousId`, stores it in the `rl_anonymous_id` cookie in the top-level domain, and attaches it to every subsequent event. This helps RudderStack to identify the anonymous users from other sites hosted under a sub-domain.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> If you identify a user with your application’s unique identifier like email, database ID, etc., RudderStack stores this ID in the `rl_user_id` cookie and attaches it to every event.\n\nRefer to the [Data Storage](https://www.rudderstack.com/docs/archive/javascript-sdk/1.1/data-storage-cookies/) guide for more information on how the JavaScript SDK stores persistent user data in cookies.\n\n#### Overriding anonymous ID\n\nYou can use any of the following three methods to override the `anonymousId` generated by the JavaScript SDK:\n\n*   Provide the `anonymousId` in the **apiOptions** parameter of the `identify` call.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> Note that all other events will have the `anonymousId` persisted from the cookie except the particular event where you override the **apiOptions** parameter.\n\nAn example is shown below:\n\n```\nrudderanalytics.identify(\n  \"1hKOmRA4el9Zt1WSfVJIVo4GRlm\", {\n    email: \"alex@example.com\"\n  }, {\n    anonymousId: \"my-anonymous-id\"\n  },\n  () => {\n    console.log(\"identify call\");\n  }\n);\n```\n\n*   Set the `anonymousId` for all future events using the `setAnonymousId()` method. An example is shown below:\n\n```\nrudderanalytics.setAnonymousId(\"my-anonymous-id\");\n// All event payloads will have the anonymousId key set \"my-anonymous-id\".\nrudderanalytics.identify(\"1hKOmRA4el9Zt1WSfVJIVo4GRlm\", {\n  email1: \"alex@example.com\"\n}, () => {\n  console.log(\"identify call\");\n});\n```\n\n*   Parse the AMP Linker ID and set the `anonymousId` using AMP Analytics:\n\n```\nrudderanalytics.setAnonymousId(\n  null,\n  \"<version>*<checkSum>*<idName1>*<idValue1>*<idName2>*<idValue2>...\"\n);\n```\n\nHere, the second parameter is the AMP Linker ID format in the [specified structure](https://github.com/ampproject/amphtml/blob/master/extensions/amp-analytics/linker-id-receiving.md#format). For websites using the [RudderStack AMP Analytics SDK](https://www.rudderstack.com/docs/sources/event-streams/sdks/rudderstack-amp-analytics/#amp-linker), the `<idName1>` value will be `rs_amp_id`.\n\nCalling the above method will parse the Linker ID and set the `rs_amp_id` key value as the `anonymousId`.\n\n### Setting a blank user ID\n\nTo set a blank user ID, you can pass an empty string or `\" \"` to `userId`.\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> Do not pass an `identify` call with `null` as RudderStack will not allow you to pass a traits object and retain the current `userId`.\n\n#### Use-case\n\nSuppose an anonymous user is identified with a `userId` and then logs out of their account. You can then call `identify(\"\", {isLoggedIn: false})` and the user will continue to be identified by their `anonymousId` for the future events.\n\n### Identifying new users\n\nTo identify new users in scenarios like new logins, you can use any one of the following approaches:\n\n*   Call `identify` with a new `userId`  \n    OR\n*   Call `reset` followed by the `identify`\n\nRudderStack resets all cookies related to the user (associated with the `userId` and `traits`) and updates them with the newly provided values.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> The `anonymousId` will remain unchanged in this case. It will be the auto-generated value set by the SDK or the one explicitly set using the `setAnonymousId` method.\n\n### Updating user traits\n\nFor updating the user traits, you can call `identify` method with the same `userId` multiple times with the new traits. This will append or modify all traits associated with that user. An example is shown below:\n\n```\nrudderanalytics.identify(\"1hKOmRA4el9Zt1WSfVJIVo4GRlm\", {\n    email1: \"alex@example.com\"\n}, () => {\n    console.log(\"identify call\");\n});\n\nrudderanalytics.identify(\"1hKOmRA4el9Zt1WSfVJIVo4GRlm\", {\n    email2: \"john@example.com\"\n}, () => {\n    console.log(\"identify call\");\n});\n```\n\nIn the above example, both `email1` and `email2` will be sent in the payload for the second `identify` call.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> Calling `reset()` method resets the existing user traits and calling `identify()` method with new traits updates the new user traits.\n\n### Setting a custom user ID\n\nYou can pass a custom `userId` along with the standard `userId` in your `identify` call. RudderStack adds this value under `context.externalId`.\n\nThe following code snippet highlights how you can add an `externalId` to your `identify` request:\n\n```\nrudderanalytics.identify(\n  \"1hKOmRA4GRlm\", {\n    firstName: \"Alex\",\n    city: \"New Orleans\",\n    country: \"Louisiana\",\n    phone: \"+1-202-555-0146\",\n    email: \"alex@example.com\",\n    custom_flavor: \"chocolate\",\n  } {\n    externalId: [{\n      id: \"some_external_id_1\",\n      type: \"brazeExternalId\",\n    }, ],\n  }\n);\n```\n\n## Page\n\nThe [`page`](https://www.rudderstack.com/docs/event-spec/standard-events/page/) call lets you record your website’s page views with any additional relevant information about the viewed page.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> Many destinations require the `page` call to be made at least once every page load.\n\nThe JavaScript SDK defines the `page` method as shown:\n\n```\nrudderanalytics.page(category, name, [properties], [apiOptions], [callback]);\n```\n\nThe following table describes the above (optional) parameters in detail:\n\n| **Parameter** | **Type** | **Description** |\n| --- | --- | --- |\n| `category` | String | Category of the page. |\n| `name` | String | Name of the page. |\n| `properties` | Dictionary | Properties of the page auto-captured by the SDK. |\n| `apiOptions` | Dictionary | Provides information such as `integrations`, `anonymousId`, and `originalTimestamp`. [Reference](#apioptions). |\n| `callback` | Function | Called after the successful execution of `page` method. |\n\nA sample `page` call is shown below:\n\n```\nrudderanalytics.page(\n  \"Cart\",\n  \"Cart Viewed\", {\n    path: \"/best-seller/1\",\n    referrer: \"https://www.google.com/search?q=estore+bestseller\",\n    search: \"estore bestseller\",\n    title: \"The best sellers offered by EStore\",\n    url: \"https://www.estore.com/best-seller/1\"\n  },\n  () => {\n    console.log(\"page call\");\n  }\n);\n```\n\nIn the above example, the JavaScript SDK captures the page `category`, `name` and the [contextual information](https://www.rudderstack.com/docs/event-spec/standard-events/common-fields/#contextual-fields).\n\n## Track\n\nThe [`track`](https://www.rudderstack.com/docs/event-spec/standard-events/track/) call lets you capture user events along with the associated properties.\n\nThe JavaScript SDK defines the `track` method as shown:\n\n```\nrudderanalytics.track(event, [properties], [apiOptions], [callback]);\n```\n\nThe following table describes the above parameters in detail:\n\n| **Parameter** | **Type** | **Presence** | **Description** |\n| --- | --- | --- | --- |\n| `event` | String | Required | The name of the tracked event. |\n| `properties` | Dictionary | Optional | The event-related properties. |\n| `apiOptions` | Dictionary | Optional | Provides information such as `integrations`, `anonymousId`, and `originalTimestamp`. [Reference](#apioptions). |\n| `callback` | Function | Optional | Called after the successful execution of the `track` call. |\n\nA sample `track` call is shown below:\n\n```\nrudderanalytics.track(\n  \"Order Completed\", {\n    revenue: 30,\n    currency: \"USD\",\n    user_actual_id: 12345\n  },\n  () => {\n    console.log(\"track call\");\n  }\n);\n```\n\nIn the above example, the `track` method tracks the `Order Completed` event along with other information like `revenue`, `currency`, and the `user_actual_id`.\n\nRefer to the [Ecommerce Events Specification](https://www.rudderstack.com/docs/event-spec/ecommerce-events-spec/) for more information on the ecommerce events captured by RudderStack.\n\n## Alias\n\nThe [`alias`](https://www.rudderstack.com/docs/event-spec/standard-events/alias/) call lets you merge different identities of a known user.\n\nThe JavaScript SDK defines the `alias` method as shown:\n\n```\nrudderanalytics.alias(to, from, [apiOptions], [callback]);\n```\n\nThe following table describes the above parameters in detail:\n\n| **Parameter** | **Type** | **Presence** | **Description** |\n| --- | --- | --- | --- |\n| `to` | String | Required | New user identifier. |\n| `from` | String | Optional | Old user identifier which will be an alias for the `to` parameter. If not provided, the SDK populates this as the currently identified `userId`, or `anonymousId` in case of anonymous users. |\n| `apiOptions` | Dictionary | Optional | Provides information such as `integrations`, `anonymousId`, and `originalTimestamp`. [Reference](#apioptions). |\n| `callback` | Function | Optional | Called after the successful execution of the `alias` call. |\n\nA sample `alias` call is shown below:\n\n```\nrudderanalytics.alias(\"new_id\", \"old_id\", () => {\n  console.log(\"alias call\");\n});\n```\n\n## Group\n\nThe [`group`](https://www.rudderstack.com/docs/event-spec/standard-events/group/) call lets you associate an identified user with a group such as a company, organization, or an account.\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> RudderStack does not support associating a user to more than one group per `group` call.\n\nThe JavaScript SDK defines the `group` method as shown:\n\n```\nrudderanalytics.group(groupId, [traits], [apiOptions], [callback]);\n```\n\nThe following table describes the above parameters in detail:\n\n| **Parameter** | **Type** | **Presence** | **Description** |\n| --- | --- | --- | --- |\n| `groupId` | String | Required | The unique group identifier in the database. RudderStack calls the relevant destination APIs to associate the identified user to this group. |\n| `traits` | Dictionary | Optional | The group-related traits. RudderStack passes these traits to the destination to enhance the group properties. |\n| `apiOptions` | Dictionary | Optional | Provides information such as `integrations`, `anonymousId`, and `originalTimestamp`. [Reference](#apioptions). |\n| `callback` | Function | Optional | Called after the successful execution of the `group` call. |\n\nA sample `group` call is shown:\n\n```\nrudderanalytics.group(\"sample_group_id\", {\n  name: \"Apple Inc.\",\n  location: \"USA\",\n});\n```\n\n## Reset\n\nThe `reset` method resets the ID and traits of both the user and the group.\n\nIn [session tracking](https://rudderstack.com/docs/sources/event-streams/sdks/session-tracking/#:~:text=tracking%20is%20enabled%3A-,sessionId,-%28Number%29%3A%20The%20session), calling the `reset` method clears the current `sessionId` and generates a new one.\n\nThe JavaScript SDK defines the `reset` method as shown:\n\nYou can also reset the `anonymousId` along with the above-mentioned details by passing `true` to the `reset()` method:\n\n```\nrudderanalytics.reset(true);\n```\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> The `reset()` method only clears the [cookies and local storage](https://www.rudderstack.com/docs/archive/javascript-sdk/1.1/data-storage-cookies/) set by RudderStack. It does not clear the information stored by the integrated destinations.\n\n## Callbacks to common methods\n\nRudderStack lets you define callbacks to the common methods of the `rudderanalytics` object.\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> This functionality is supported only for the `syncPixel` method which is called by the SDK while making synchronization calls for the relevant destinations.\n\nAn example is shown below:\n\n```\nrudderanalytics.syncPixelCallback = obj => {\n  rudderanalytics.track(\n    \"sync lotame\", {\n      destination: obj.destination\n    }, {\n      integrations: {\n        All: false,\n        S3: true\n      }\n    }\n  );\n};\n\n<script src=\"https://cdn.rudderlabs.com/rudder-analytics.min.js\"></script>\n```\n\nHere, RudderStack defines `syncPixelCallback` on the `rudderanalytics` object while loading the SDK. Further, RudderStack calls this registered callback with the parameter `{destination: <destination_name>}` whenever a SDK makes a sync call the SDK to the relevant integration - in this case, Lotame.\n\nYou can also add the callback in the **loadOptions** parameter as shown below:\n\n```\nrudderanalytics.load(WRITE_KEY, DATA_PLANE_URL, {\n  clientSuppliedCallbacks: {\n    syncPixelCallback: () => {\n      console.log(\"sync done\");\n    },\n  },\n});\n```\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n\n## Querystring API\n\nRudderStack’s Querystring API lets you trigger the `track` and `identify` calls using the query parameters within the URL. The query parameters are listed in the following table:\n\n| Parameter | Action |\n| --- | --- |\n| `ajs_uid` | Triggers a `rudderanalytics.identify()` call with `userId` having the parameter value. |\n| `ajs_aid` | Triggers a `rudderanalytics.setAnonymousId()` call with `anonymousId` having the parameter value. |\n| `ajs_event` | Triggers a `rudderanalytics.track()` call with `event` name as the parameter value. |\n| `ajs_prop_<property>` | If `ajs_event` is passed as a query string, the value of this parameter populates the `properties` of the corresponding event in the `track` call. |\n| `ajs_trait_<trait>` | If `ajs_uid` is provided as a query string, the value of this parameter populates the `traits` of the `identify` call. |\n\nFor example, consider the following URL containing the query string parameters:\n\n`http://hostname.com/?ajs_uid=12345&ajs_event=test%20event&ajs_aid=abcde&ajs_prop_testProp=prop1&ajs_trait_name=Firstname+Lastname`\n\nIt will trigger the following API calls:\n\n```\n// Sets the user ID/anonymous ID\nrudderanalytics.setAnonymousId(\"abcde\");\n\n// Identify call\nrudderanalytics.identify(\"12345\", {\n  name: \"Firstname Lastname\"\n});\n\n// Track call\nrudderanalytics.track(\"test event\", {\n  testProp: \"prop1\"\n});\n```\n\n## Context and traits\n\nThe JavaScript SDK automatically captures certain event-specific and user-specific data based on the event type.\n\nThe `context` and `traits` dictionaries are provided in the [apiOptions](https://www.rudderstack.com/docs/archive/javascript-sdk/1.1/supported-api/#apioptions) parameter of the supported API methods.\n\n### Context\n\nThe `context` object is a dictionary of additional information about a particular event data, such as a user’s locale.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> A context is a complete and specific piece of information. Any other information provided outside this specification is ignored.\n\n### Traits\n\nThe `traits` object is an optional dictionary included within [`context`](#context) specifying the user’s unique traits. This is a very useful field for linking the user’s information from a previously made [`identify`](#identify) call to the subsequent calls, for example, [`track`](#track) or [`page`](#page).\n\n#### Use-case\n\nTo understand the concept of context and traits better, refer to the following `identify` event:\n\n```\nrudderanalytics.identify(\"1hKOmRA4el9Zt1WSfVJIVo4GRlm\", {\n  name: \"Alex Keener\",\n  email: \"alex@example.com\",\n  subscriptionStatus: \"subscribed\",\n  plan: \"Silver\"\n});\n```\n\nThe traits in the above event are `name`, `email`, `subscriptionStatus`, and `plan`. If you wish to add or override any traits in the subsequent `track` or `page` event triggered by the user, you can do so by passing it in `traits` as shown:\n\n```\nrudderanalytics.track(\n  \"Subscription Update\", {\n    campaign: \"Subscribe\"\n  }, {\n    traits: {\n      plan: \"Gold\",\n      addOn: true\n    }\n  }\n);\n```\n\nThe above snippet will add a new trait `addOn` and update the user trait `plan` to `Gold`.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "JavaScript SDK API | RudderStack Docs",
    "description": "Implement the JavaScript SDK API.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/user-guides/how-to-guides/developing-integrations-for-rudderstack/",
    "markdown": "# How to develop new integrations for RudderStack\n\nUseful tips for developers who wish to send events to their own destination platforms via RudderStack.\n\n* * *\n\n*     3 minute read  \n    \n\n> ![success](https://www.rudderstack.com/docs/images/tick.svg)\n> \n> RudderStack currently supports over 150 [destinations](https://www.rudderstack.com/docs/destinations/overview/). This guide is useful if you wish to send events to a target platform **not supported by us currently**.\n\n## What is an integration?\n\n‌A RudderStack integration is a package that includes the client-side (**SDK**) enhancements as well as the server-side (**Transformer**) additions required to support a new destination platform.\n\n## How does an integration work?\n\nThe RudderStack [SDKs](https://www.rudderstack.com/docs/sources/event-streams/sdks/) help the developers capture business events within their enterprise systems and route them to RudderStack. The events sent from the SDKs to RudderStack follow a canonical model and are independent of the programming language or the source platform.\n\nThe RudderStack engine is message-agnostic and performs resilient but optimized routing functions. The [transformer](https://github.com/rudderlabs/rudder-transformer) service is responsible for the final mapping from the canonical message to the destination-specific semantics.\n\n## Why not just develop the transformer, then?\n\nGiven the above high-level distribution of responsibilities among the various RudderStack components, it might first appear that developing a new integration should involve only the development of the transformer, since the canonical payload that undergoes the transformation would already be part of the existing SDK.\n\nHowever, this is not the case due to a few underlying reasons:\n\n*   Most destination platforms provide their own SDKs. While RudderStack relies on the destinations to offer some means of server-to-server communication, this communication support might not always be there or might not provide all functionalities offered by the destination SDK. In such cases, the RudderStack SDK has to rely on the destination SDK to support the events not addressed as part of the platform’s server-side APIs.\n*   Certain platforms (especially attribution) rely on establishing a cross-device identifier for delivering actionable insights. The proprietary systems typically generate such an identifier within the platform that leverages cross-references from various other sources. To integrate with such platforms, RudderStack leverages the platform SDK for establishing the identifier.\n*   In certain cases, the destination SDK captures events over and above its advertised APIs. In such cases, the RudderStack SDK has no option but to include the destination SDK.\n\n‌The following sections familiarize you with the RudderStack SDK components and then delve into the details of enhancing the SDKs.\n\n## RudderStack SDK components\n\nThe RudderStack SDKs have two major components‌:\n\n*   The **Core SDK** provides the APIs that application developers need to use to route events via RudderStack. Apart from routing events to the RudderStack backend, the core SDK also implements a factory pattern wherein the concrete implementations are the wrappers for the supported destination SDKs.\n*   The **integration sub-component** combines the wrapper implementation and the actual destination SDK that the wrapper adapts to the RudderStack framework.\n\n### Enhancing the core SDKs\n\nYou can control the inclusion of a platform during development by including or excluding the corresponding **integration** **sub-component** in the build. The core SDK prepares an inventory of the supported integration modules and includes only those in the factory calls. Using this approach, you can restrict the application size to include only the required components.\n\nEven after an integration component has been included in the build, you can control its usage at runtime through appropriate server-side configurations which the client SDK downloads and uses. Using this approach, the developers can enable or disable the direct flow of events to the destination.\n\n## Submitting an integration pull request\n\nWhen developing a new integration for RudderStack, you need to submit a pull request (PR) to a couple of RudderStack repositories. For more details on submitting these PRs, refer to our [step-by-step guide](https://www.rudderstack.com/docs/user-guides/how-to-guides/how-to-submit-an-integration-pull-request/).\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "How to develop new integrations for RudderStack | RudderStack Docs",
    "description": "Useful tips for developers who wish to send events to their own destination platforms via RudderStack.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/javascript-sdk/1.1/quick-start-guide/",
    "markdown": "# Quickstart | RudderStack Docs\n\nInstall and use the RudderStack JavaScript SDK on your website.\n\n* * *\n\n*     6 minute read  \n    \n\nThis guide lists the steps to quickly install and use the JavaScript SDK to identify your website users and track their actions.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n\n## Prerequisites\n\nTo set up and use the RudderStack JavaScript SDK, the following prerequisites must be met:\n\n*   Set up a [RudderStack account](https://app.rudderstack.com/signup).\n*   [Set up a JavaScript source](https://www.rudderstack.com/docs/dashboard-guides/sources/#adding-a-source) in the RudderStack dashboard.\n\n## Step 1: Install JavaScript SDK\n\n### Using a CDN\n\nTo integrate the SDK with your website and load it **asynchronously**:\n\n1.  Go to the **Setup** tab of your JavaScript source in the dashboard.\n2.  Copy the following snippet by clicking on **Copy Snippet** and paste it in your website’s `<head>` section:\n\n[![JavaScript source write key](https://www.rudderstack.com/docs/images/event-stream-sources/javascript-setup-new.webp)](https://www.rudderstack.com/docs/images/event-stream-sources/javascript-setup-new.webp)\n\n> ![success](https://www.rudderstack.com/docs/images/tick.svg)\n\nAlternatively, you can copy and use the below snippet:\n\n```\n<script type=\"text/javascript\">\n!function(){var e=window.rudderanalytics=window.rudderanalytics||[];e.methods=[\"load\",\"page\",\"track\",\"identify\",\"alias\",\"group\",\"ready\",\"reset\",\"getAnonymousId\",\"setAnonymousId\",\"getUserId\",\"getUserTraits\",\"getGroupId\",\"getGroupTraits\",\"startSession\",\"endSession\",\"getSessionId\"],e.factory=function(t){return function(){e.push([t].concat(Array.prototype.slice.call(arguments)))}};for(var t=0;t<e.methods.length;t++){var r=e.methods[t];e[r]=e.factory(r)}e.loadJS=function(e,t){var r=document.createElement(\"script\");r.type=\"text/javascript\",r.async=!0,r.src=\"https://cdn.rudderlabs.com/v1.1/rudder-analytics.min.js\";var a=document.getElementsByTagName(\"script\")[0];a.parentNode.insertBefore(r,a)},e.loadJS(),\ne.load(WRITE_KEY,DATA_PLANE_URL),\ne.page()}();\n</script>\n```\n\nTo integrate the SDK with your website and load it **synchronously**, add any of the following scripts in your website’s `<head>` section.\n\n**Minified code**\n\n```\n<script>\nrudderanalytics=window.rudderanalytics=[];for(var methods=[\"load\",\"page\",\"track\",\"identify\",\"alias\",\"group\",\"ready\",\"reset\",\"getAnonymousId\",\"setAnonymousId\",\"getUserId\",\"getUserTraits\",\"getGroupId\",\"getGroupTraits\",\"startSession\",\"endSession\",\"getSessionId\"],i=0;i<methods.length;i++){var method=methods[i];rudderanalytics[method]=function(a){return function(){rudderanalytics.push([a].concat(Array.prototype.slice.call(arguments)))}}(method)}rudderanalytics.load(WRITE_KEY,DATA_PLANE_URL),rudderanalytics.page();\n</script>\n\n<script src=\"https://cdn.rudderlabs.com/v1.1/rudder-analytics.min.js\"></script>\n```\n\n**Non-minified code**\n\n```\n<script>\n  rudderanalytics = window.rudderanalytics = [];\n  var methods = [\n    \"load\",\n    \"page\",\n    \"track\",\n    \"identify\",\n    \"alias\",\n    \"group\",\n    \"ready\",\n    \"reset\",\n    \"getAnonymousId\",\n    \"setAnonymousId\",\n    \"getUserId\",\n    \"getUserTraits\",\n    \"getGroupId\",\n    \"getGroupTraits\",\n    \"startSession\",\n    \"endSession\",\n    \"getSessionId\"\n  ];\n  for (var i = 0; i < methods.length; i++) {\n    var method = methods[i];\n    rudderanalytics[method] = (function (methodName) {\n      return function () {\n        rudderanalytics.push(\n          [methodName].concat(Array.prototype.slice.call(arguments))\n        );\n      };\n    })(method);\n  }\n\n  rudderanalytics.load(WRITE_KEY,DATA_PLANE_URL);\n  rudderanalytics.page();\n</script>\n\n<script src=\"https://cdn.rudderlabs.com/v1.1/rudder-analytics.min.js\"></script>\n```\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> There is an explicit `page` call at the end in all above snippets. This is to ensure that a `page` call is sent whenever the SDK loads on a page. You can remove this call or modify it with any extra `page` properties. You can also add `page` calls in your application in places not tied directly to page load, for example, virtual page views and page renders on route change, such as in [single-page applications](#single-page-application).\n\n### Using NPM\n\nWhile it is recommended to use the above method to integrate the JavaScript SDK with your website, you can alternatively use the [NPM module](https://www.npmjs.com/package/rudder-sdk-js) for packaging RudderStack directly into your project.\n\nTo install the JavaScript SDK via NPM, run the following command:\n\n```\nnpm install rudder-sdk-js --save\n```\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> Use this NPM module only for browser installation. To integrate RudderStack with your Node.js apps, refer to the [RudderStack Node.js SDK](https://www.rudderstack.com/docs/sources/event-streams/sdks/rudderstack-node-sdk/) documentation.\n\nSince the NPM module exports the related APIs on an already-defined object combined with the Node.js module caching, you should run the following code snippet **once** and use the exported object throughout your project:\n\n```\nimport * as rudderanalytics from \"rudder-sdk-js\";\nrudderanalytics.load(WRITE_KEY, DATA_PLANE_URL);\nrudderanalytics.ready(() => {\n  console.log(\"We are all set!!!\");\n});\nexport { rudderanalytics };\n```\n\nAlternatively, you can do this with **ES5** using the `require` method:\n\n```\nvar rudderanalytics = require(\"rudder-sdk-js\");\nrudderanalytics.load(WRITE_KEY, DATA_PLANE_URL);\nrudderanalytics.ready(() => {\n  console.log(\"We are all set!!!\");\n});\nexports.rudderanalytics = rudderanalytics;\n```\n\nThe related APIs exported by the module are `load`, `ready`, `identify`, `alias`, `page`, `track`, `group`, `reset`, `getAnonymousId`, `setAnonymousId`, `getUserId`, `getUserTraits`, `getGroupId`, and `getGroupTraits`.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> Refer to the sample [Angular](https://github.com/rudderlabs/rudder-analytics-angular) and [React](https://github.com/rudderlabs/rudder-analytics-react) projects for a detailed walkthrough of the above steps.\n\nThe installation code snippets discussed above perform the following actions:\n\n*   Creates an array to store the events until the `analytics` object is ready.\n*   Stores the following methods to replay them when the `analytics` object is ready:\n\n| Method | Description |\n| --- | --- |\n| [`load()`](https://www.rudderstack.com/docs/archive/javascript-sdk/1.1/load-js-sdk/) | Loads `analytics.js` with the specified write key. |\n| [`track()`](https://www.rudderstack.com/docs/archive/javascript-sdk/1.1/supported-api/#track) | Tracks user events along with the associated properties. |\n| [`identify()`](https://www.rudderstack.com/docs/archive/javascript-sdk/1.1/supported-api/#identify) | Identifies the users, records their traits, and associates them with their actions. |\n| [`alias()`](https://www.rudderstack.com/docs/archive/javascript-sdk/1.1/supported-api/#alias) | Maps a new user ID with an old ID. |\n| [`group()`](https://www.rudderstack.com/docs/archive/javascript-sdk/1.1/supported-api/#group) | Links an identified user with a group such as a company, organization, or an account. |\n| [`ready()`](https://www.rudderstack.com/docs/archive/javascript-sdk/1.1/quick-start-guide/#step-2-check-ready-state) | Fired when the SDK has initialized itself and the other third-party native SDK destinations. |\n| [`reset()`](https://www.rudderstack.com/docs/archive/javascript-sdk/1.1/supported-api/#reset) | Resets the information related to the previously identified user. |\n| `getAnonymousId` | Fetches the current `anonymousId`. |\n| `setAnonymousId` | Sets the `anonymousId`. |\n| `getUserId` | Fetches the current `userId`. |\n| `getUserTraits` | Fetches the current user traits. |\n| `getGroupId` | Fetches the current `groupId`. |\n| `getGroupTraits` | Fetches the current group traits. |\n| `startSession` | Starts a new session. |\n| `endSession` | Clears the `sessionId` and ends the current session. |\n| `getSessionId` | Fetches the session ID of the current session. Returns `null` if the session ID is not available. |\n\n*   Loads the analytics object with the specified write key.\n*   Makes a `page()`call to track the page views. It captures the whole URL including the UTM parameters as part of the `page` call payload, such as `path`, `referrer`, `search`, `title`, and `URL`. Refer to the [`page`](https://www.rudderstack.com/docs/event-spec/standard-events/page/#properties) method to override these properties.\n\n## Step 2: Check ready state\n\nThe JavaScript SDK provides the `ready` API with a `callback` parameter that triggers when the SDK is done initializing itself and the other third-party native SDK destinations.\n\nAn example is shown below:\n\n```\nrudderanalytics.ready(() => {\n    console.log(\"All set!\");\n});\n```\n\n## Step 3: Identify users\n\nThe [`identify`](https://www.rudderstack.com/docs/event-spec/standard-events/identify/) method lets you identify a user and associate them with their actions. It also enables you to record any traits about them like their name, email, etc.\n\nA sample `identify` call is shown below:\n\n```\nrudderanalytics.identify(\n    \"1hKOmRA4el9Zt1WSfVJIVo4GRlm\", {\n        firstName: \"Alex\",\n        lastName: \"Keener\",\n        email: \"alex@example.com\",\n        phone: \"+1-202-555-0146\"\n    }, {\n        page: {\n            path: \"/best-seller/1\",\n            referrer: \"https://www.google.com/search?q=estore+bestseller\",\n            search: \"estore bestseller\",\n            title: \"The best sellers offered by EStore\",\n            url: \"https://www.estore.com/best-seller/1\"\n        }\n    },\n    () => {\n        console.log(\"identify call\");\n    }\n);\n```\n\nThe JavaScript SDK captures the `userId`, `email` and the [contextual information](https://www.rudderstack.com/docs/event-spec/standard-events/common-fields/#contextual-fields) from the above snippet.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> The anonymous visitors are automatically assigned an `anonymousId`. Refer to the [Anonymous ID](https://www.rudderstack.com/docs/archive/javascript-sdk/1.1/supported-api/#anonymous-id) section for more information.\n\n## Step 4: Track user actions\n\nThe [](https://www.rudderstack.com/docs/event-spec/standard-events/track/)method lets you capture user events along with the associated properties.\n\nA sample `track` call is shown below:\n\n```\nrudderanalytics.track(\n    \"Order Completed\", {\n        revenue: 77.6,\n        currency: \"USD\",\n    },\n    () => {\n        console.log(\"track call\");\n    }\n);\n```\n\nThe JavaScript SDK captures the `Order Completed` event along with `revenue`, `currency`, and `anonymousId` from the above snippet.\n\n> ![success](https://www.rudderstack.com/docs/images/tick.svg)\n> \n> You can use the `track` method to track various success metrics for your website like user signups, item purchases, article bookmarks, and more.\n\n## Supported browsers\n\nThe JavaScript SDK supports the following browsers and their corresponding versions:\n\n| Browser | Supported Versions |\n| --- | --- |\n| Safari | v7 or later |\n| IE  | v10 or later |\n| Edge | v80 or later |\n| Mozilla Firefox | v47 or later |\n| Chrome | v54 or later |\n| Opera | v43 or later |\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> You can try adding the browser [polyfills](https://developer.mozilla.org/en-US/docs/Glossary/Polyfill) to your application if the SDK does not work on your browser.\n\n## Single-page application\n\nThe JavaScript SDK makes a `page` call after its initialization (explicitly called at the end of the installation script). However, in the case of a single-page application (SPA) where a route change does not reload the page, you need to make the `page` call explicitly after the route change on the frontend. For more information, refer to the [RudderStack-Next.js Integration](https://www.rudderstack.com/docs/user-guides/how-to-guides/rudderstack-jamstack-integration/v1.1/nextjs/#method-2-installing-and-configuring-the-javascript-sdk-using-npm-package-in-your-nextjs-app) guide.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Quickstart | RudderStack Docs",
    "description": "Install and use the RudderStack JavaScript SDK on your website.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/javascript-sdk/1.1/load-js-sdk/",
    "markdown": "# Load JavaScript SDK | RudderStack Docs\n\nLoad the JavaScript SDK.\n\n* * *\n\n*     11 minute read  \n    \n\nYou can load the JavaScript SDK using the `load` API method to track and send events from your website to RudderStack. It can be defined as:\n\n```\nrudderanalytics.load(WRITE_KEY, DATA_PLANE_URL, [loadOptions]);\n```\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n\n## Loading options\n\nYou can use **loadOptions** in the above `load` call to define various options while loading the SDK. It includes the following parameters:\n\n```\n{\n  logLevel: \"DEBUG\" | \"INFO\" | \"WARN\",\n  integrations: IntegrationOpts,\n  configUrl: string,  // defaults to https://api.rudderlabs.com\n  getSourceConfig: function,\n  queueOptions: QueueOpts,\n  loadIntegration: boolean, // defaults to true.\n  sessions: SessionOpts,\n  secureCookie: boolean, // defaults to false.\n  destSDKBaseURL: string, // defaults to https://cdn.rudderlabs.com/v1.1/js-integrations\n  useBeacon: boolean, // defaults to false.\n  beaconQueueOptions: BeaconQueueOpts,\n  cookieConsentManager: cookieConsentManager,\n  anonymousIdOptions: AnonymousIdOptions,\n  setCookieDomain: string, // defaults to current domain.\n  sameSiteCookie: \"Strict\" | \"Lax\" | \"None\", // defaults to Lax.\n  lockIntegrationsVersion: boolean, // defaults to false.\n  polyfillIfRequired: boolean, // defaults to true.\n  onLoaded: callback function,\n  uaChTrackLevel: \"none\" | \"default\" | \"full\", // defaults to None.\n  sendAdblockPage: boolean,\n  sendAdblockPageOptions: object,\n  useGlobalIntegrationsConfigInEvents: boolean // defaults to false.\n  sameDomainCookiesOnly: boolean // defaults to false.\n}\n```\n\nThe detailed description of these parameters is as follows:\n\n| Parameter | Type | Description |\n| --- | --- | --- |\n| `logLevel` | String | Values include `DEBUG`, `INFO`, and `WARN`. |\n| `integrations` | [IntegrationOpts](#integrationopts) | Sends event data to the [selective destinations](https://www.rudderstack.com/docs/archive/javascript-sdk/1.1/filtering/). |\n| `configUrl` | String | The [Control Plane](https://www.rudderstack.com/docs/get-started/rudderstack-open-source/control-plane-lite/) endpoint serving your destination configurations. Default value is `https://api.rudderlabs.com`. Note that `sourceConfig` is automatically appended to this endpoint in the format `https://api.rudderlabs.com/<source_config_url>/sourceConfig` |\n| [`getSourceConfig`](#getsourceconfig) | Function | Returns a custom configuration which can be used in place of the [control plane’s dashboard](https://www.rudderstack.com/docs/get-started/rudderstack-open-source/control-plane-lite/) configuration. |\n| `queueOpts` | [QueueOpts](#queueopts) | Contains the options to control the behavior of the persistence queue that buffers the events before sending them to the data plane. |\n| `loadIntegration` | Boolean | Determines whether the destination SDKs are fetched by the SDK. Default value is `true`. Supported for **Amplitude** and **Google Analytics** only. |\n| `sessions` | [SessionOpts](#sessionopts) | Captures the details specific to session tracking. |\n| `secureCookie` | Boolean | Determines whether the SDK sends the cookie to the storage backend via HTTPS. Default value is `false`. |\n| `destSDKBaseURL` | String | Path used by the SDK to load the integrations. Default value is `https://cdn.rudderlabs.com/v1.1/js-integrations`. [Reference](https://www.rudderstack.com/docs/archive/javascript-sdk/1.1/version-migration-guide/#case-3-self-hosting-rudderstacks-cdn). |\n| `useBeacon` | Boolean | Determines whether the SDK [sends event payloads via Beacon](https://www.rudderstack.com/docs/archive/javascript-sdk/1.1/javascript-sdk-enhancements/#sending-events-using-beacon). Default value is `false`. |\n| `beaconQueueOptions` | [BeaconQueueOpts](https://www.rudderstack.com/docs/archive/javascript-sdk/1.1/javascript-sdk-enhancements/#sending-events-using-beacon) | Internal queue to hold the data and send it through the Beacon utility in batches. |\n| `cookieConsentManager` | Object | See the [Consent manager integration](#cookieconsentmanager) section. |\n| `anonymousIdOptions` | Object | [Automatically captures the anonymous ID](#anonymousidoptions) from a source and sets it as RudderStack’s `anonymousId`. |\n| `setCookieDomain` | String | Sets the cookie domain. SDK captures and uses the current domain as the default value. Refer to the [Data Storage](https://www.rudderstack.com/docs/archive/javascript-sdk/1.1/data-storage-cookies/) guide for more information. |\n| `sameSiteCookie` | String | Sets the `SameSite` attribute of a cookie. Default value is `Lax`. |\n| `lockIntegrationsVersion` | Boolean | Determines if the JavaScript SDK should use the version of the native destination SDKs as used by the core SDK. This is particularly useful for [NPM installations](https://www.rudderstack.com/docs/archive/javascript-sdk/1.1/quick-start-guide/#using-npm) where a specific version of the core SDK is used. The default value is `false`, meaning the latest versions of the destination SDKs are used in case of both CDN and NPM installations. Note that if `destSDKBaseURL` is set to a specific path, it gets the highest priority. |\n| `polyfillIfRequired` | Boolean | Loads the polyfills for unsupported features in older browsers. Default value is `true`. |\n| [`onLoaded`](#onloaded) | Function | Callback function that executes after the JavaScript SDK loads and before the native device-mode destination SDKs load. |\n| [`uaChTrackLevel`](#uachtracklevel) | String | Configures the information a user should get in the `context` object regarding the [client hints](https://developer.mozilla.org/en-US/docs/Web/HTTP/Client_hints). The JavaScript SDK fetches this information using the [user-agent client hints API](https://developer.mozilla.org/en-US/docs/Web/API/User-Agent_Client_Hints_API). |\n| `sendAdblockPage` | Boolean | Enables the JavaScript SDK to load the [Google AdSense](https://adsense.google.com/start/) library. If RudderStack fails to load this library, it concludes that an adblocker is enabled on the page. [Reference](https://www.rudderstack.com/docs/archive/javascript-sdk/1.1/detecting-adblocked-pages/). |\n| `sendAdblockPageOptions` | Object | If the `sendAdblockPage` option is set to true, the JavaScript SDK makes an implicit `page` call about the ad-blocked pages. You can use the `sendAdblockPageOptions` option (containing the [`IntegrationOpts`](https://www.rudderstack.com/docs/archive/javascript-sdk/1.1/load-js-sdk/#integrationopts) object) to specify the destinations to which you want to forward this `page` call. [Reference](https://www.rudderstack.com/docs/archive/javascript-sdk/1.1/detecting-adblocked-pages/). |\n| [`useGlobalIntegrationsConfigInEvents`](#useglobalintegrationsconfiginevents) | Boolean | Lets you use the `integrations` object of the `load` method at the event level when it is not present at the event level. Default value is `false`. |\n| `sameDomainCookiesOnly` | Boolean | If set to true, the SDK reads cookies from the exact domain it is set at. Default value is `false`.<br><br>For example, if this load option is set to true then the cookies set from the site’s top-level domain are not accessible by the sub-domains and vice versa. |\n\nThe following sections contain the detailed definitions and usage of some of the above parameters:\n\n#### `IntegrationOpts`\n\nYou can use this parameter to send the event data only to the [selective destinations](https://www.rudderstack.com/docs/archive/javascript-sdk/1.1/filtering/#filtering-destinations). Its structure is defined as follows:\n\n```\nrudderanalytics.load(WRITE_KEY, DATA_PLANE_URL, {\n  integrations: {\n    All: boolean, // default true\n    <Destination1>: boolean, // specify destination like Google Analytics, Amplitude etc.\n    <Destination2>: boolean, // specify destination like Google Analytics, Amplitude etc.\n    ...\n  }\n});\n```\n\nThe following table describes the above (optional) parameters in detail:\n\n| **Parameter** | **Type** | **Description** |\n| --- | --- | --- |\n| `All` | Boolean | All destinations to which the event data must be sent. The default value is `true`. If set to `false`, RudderStack will not send the event data to any destination. |\n| `<Destination>` | Boolean | Specific destination to which the event data must be sent/not, depending on its Boolean value. |\n\nYou **must** specify the actual destination name (as listed in the [RudderStack dashboard](https://app.rudderstack.com/directory)) in the `<Destination>` parameter and **not** the name you have assigned to the destination. For example, the below sample snippet sends the event data only to **Google Analytics** and **Intercom** destinations:\n\n```\nrudderanalytics.load(WRITE_KEY, DATA_PLANE_URL, {\n    integrations: {\n        All: false,\n        \"Google Analytics\": true,\n        \"Intercom\": true\n    }\n});\n```\n\n#### `getSourceConfig`\n\nThe `getSourceConfig` function returns the custom configuration which can be used in place of [control plane’s dashboard](https://www.rudderstack.com/docs/get-started/rudderstack-open-source/control-plane-lite/) configuration:\n\n```\nrudderanalytics.load(WRITE_KEY, DATA_PLANE_URL, {\n  getSourceConfig: function() {\n    return {\n      // custom configuration\n    };\n  },\n  // other load options\n});\n```\n\n#### `QueueOpts`\n\nThe `queueOpts` object contains the options to control the behavior of the persistence queue that buffers the events before sending them to RudderStack. Its structure is defined as follows:\n\n```\n{\n  maxRetryDelay: 360000,\n  minRetryDelay: 1000,\n  backoffFactor: 2,\n  maxAttempts: 10,\n  maxItems: 100,\n}\n```\n\nThe following table describes the above `integer` type (optional) parameters in detail:\n\n| **Parameter** | **Description** | **Default value** |\n| --- | --- | --- |\n| `maxRetryDelay` | Upper limit on the maximum delay for an event (in ms). | 360000 |\n| `minRetryDelay` | Minimum delay expected before sending an event (in ms). | 1000 |\n| `backoffFactor` | Exponential base. | 2   |\n| `maxAttempts` | Maximum number of attempts to send the event to the destination. | 10  |\n| `maxItems` | Maximum number of events kept in the storage. | 100 |\n\n#### `SessionOpts`\n\nThe `SessionOpts` object contains the options related to the SDK’s automatic session tracking behavior. Refer to the [Session Tracking](https://www.rudderstack.com/docs/sources/event-streams/sdks/session-tracking/#automatic-session-tracking) guide for more information. Its structure is defined as follows:\n\n| **Parameter** | **Description** | **Default value** |\n| --- | --- | --- |\n| `autoTrack` | Determines if the SDK should automatically track the user sessions. | `true` |\n| `timeout` | The maximum inactivity period before the session expires. | `1800000 ms` (30 minutes) |\n\n#### `cookieConsentManager`\n\nOnce a user provides the consent, you can load the JavaScript SDK and enable the [OneTrust integration](https://www.rudderstack.com/docs/data-governance/consent-management/onetrust/javascript/) via the `cookieConsentManager` object:\n\n```\nrudderanalytics.load(WRITE_KEY, DATA_PLANE_URL, {\n  cookieConsentManager: {\n    oneTrust: {\n      enabled: true\n    }\n  }\n});\n```\n\n#### `anonymousIdOptions`\n\nYou can use the `anonymousIdOptions` object to automatically capture the anonymous ID from a source and set it as RudderStack’s `anonymousId`.\n\nFor example, if you are migrating from a particular source and want to retain its anonymous ID, you can enable the `anonymousIdOptions` to set the source’s anonymous ID as the `anonymousId` in RudderStack.\n\nThe structure of `anonymousIdOptions` is defined as follows:\n\n```\nrudderanalytics.load(WRITE_KEY, DATA_PLANE_URL, {\n  anonymousIdOptions: {\n    autoCapture: {\n      enabled: true,\n      source: \"segment\"\n    }\n  }\n});\n```\n\nThe following table describes the above (required) parameters in detail:\n\n| Parameter | Type | Description |\n| --- | --- | --- |\n| `enabled` | Boolean | Determines if the anonymous ID should be auto-captured. |\n| `source` | String | Determines the external source of anonymous ID. |\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> If the RudderStack `anonymousId` is already set in your browser, `anonymousIdOptions` will not take effect.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> You can call the **reset** API to clear the persisted anonymous ID and force the SDK to generate a new ID when the next tracking API is called (irrespective of whether `anonymousIdOptions` is enabled or disabled). However, if the `anonymousIdOptions` object is enabled and the SDK is loaded again (as a result of webpage reload, navigate to a different webpage, etc.), the `setAnonymousId` call will trigger automatically and the specified source’s anonymous ID will again be set as the RudderStack `anonymousId`.\n\n#### `uaChTrackLevel`\n\nYou can use the `uaChTrackLevel` option to configure the information a user should get in the `context` object regarding the [client hints](https://developer.mozilla.org/en-US/docs/Web/HTTP/Client_hints). The JavaScript SDK fetches this information using the [user-agent client hints API](https://developer.mozilla.org/en-US/docs/Web/API/User-Agent_Client_Hints_API). It can take the below values:\n\n*   `none`: Specifies that `uaChTrackLevel` field is absent in the `context` object.\n*   `default`: Specifies that `uaChTrackLevel` field is present in the `context` object and contains an object similar to the one below:\n\n```\n{\n  \"brands\": [{\n    \"brand\": \"Chromium\",\n    \"version\": \"110\"\n  }, {\n    \"brand\": \"Not A(Brand\",\n    \"version\": \"24\"\n  }, {\n    \"brand\": \"Google Chrome\",\n    \"version\": \"110\"\n  }],\n  \"mobile\": false,\n  \"platform\": \"macOS\"\n}\n```\n\n*   `full`: Specifies that `uaChTrackLevel` field is present in the `context` object and contains an object similar to the one below:\n\n```\n{\n  \"architecture\": \"arm\",\n  \"bitness\": \"64\",\n  \"brands\": [{\n    \"brand\": \"Chromium\",\n    \"version\": \"110\"\n  }, {\n    \"brand\": \"Not A(Brand\",\n    \"version\": \"24\"\n  }, {\n    \"brand\": \"Google Chrome\",\n    \"version\": \"110\"\n  }],\n  \"fullVersionList\": [{\n    \"brand\": \"Chromium\",\n    \"version\": \"110.0.5481.77\"\n  }, {\n    \"brand\": \"Not A(Brand\",\n    \"version\": \"24.0.0.0\"\n  }, {\n    \"brand\": \"Google Chrome\",\n    \"version\": \"110.0.5481.77\"\n  }],\n  \"mobile\": false,\n  \"model\": \"\",\n  \"platform\": \"macOS\",\n  \"platformVersion\": \"13.1.0\",\n  \"uaFullVersion\": \"110.0.5481.77\",\n  \"wow64\": false\n}\n```\n\n#### `onLoaded`\n\nThe `onLoaded` callback function takes the `rudderanalytics` instance as an argument and executes after the JavaScript SDK loads and before the native device-mode destination SDKs are loaded.\n\n```\nrudderanalytics.load(WRITE_KEY, DATA_PLANE_URL, {\n  onLoaded: function(rudderanalytics) {\n    console.log(\"All set!\");\n  }\n});\n```\n\n#### `useGlobalIntegrationsConfigInEvents`\n\nYou can use this option to use the `integrations` object of the `load` method at the event level when it is not present at the event level.\n\nFor example, if the `integrations` object is defined in the `load` method and the `useGlobalIntegrationsConfigInEvents` option is set to `true`:\n\n```\nrudderanalytics.load(WRITE_KEY, DATA_PLANE_URL, {\n  integrations: {\n    All: true,\n    \"Google Analytics\": false,\n    ...\n  },\n  useGlobalIntegrationsConfigInEvents: true,\n  // other load options\n});\n```\n\n**Case 1**: `integrations` option is present at the event level:\n\n```\nrudderanalytics.track(\n  \"Order Completed\", {\n    revenue: 30,\n    currency: \"USD\",\n    user_actual_id: 12345\n  }, {\n    integrations: {\n      All: true,\n      Amplitude: false\n    },\n  },\n  () => {\n    console.log(\"track call\");\n  }\n);\n```\n\nIn this case, the JavaScript SDK uses the `integrations` option from the `track` event.\n\n**Case 2**: `integrations` option is not present at the event level:\n\n```\nrudderanalytics.track(\n  \"Order Completed\", {\n    revenue: 30,\n    currency: \"USD\",\n    user_actual_id: 12345\n  }, {},\n  () => {\n    console.log(\"track call\");\n  }\n);\n```\n\nIn this case, the SDK uses the `integrations` option from the `load` method.\n\n## Loading SDK for self-hosted control plane\n\nIf you are self-hosting the control plane using the [Control Plane Lite](https://www.rudderstack.com/docs/get-started/rudderstack-open-source/control-plane-lite/#using-sdk-sources-set-up-in-self-hosted-control-plane) utility, the `load` call should be made as below:\n\n```\nrudderanalytics.load(WRITE_KEY, DATA_PLANE_URL, {\n  configUrl: < CONTROL_PLANE_URL > ,\n});\n```\n\n## Verifying if the SDK has loaded correctly\n\nYou can verify if the JavaScript SDK has loaded correctly by opening the JavaScript console in your browser:\n\n*   **Safari**: `Ctrl+Alt+I` (Windows) or `Command+Option+I` (Mac) and go to the `Console` tab.\n*   **Chrome**: `Ctrl+Shift+J` (Windows) or `Command+Option+J` (Mac).\n*   **Firefox**: `Ctrl+Shift+K` (Windows) or `Command+Option+K` (Mac) and select the `Console` tab.\n*   **Internet Explorer**: Press `F12` and go to the `Console` tab.\n\nRun the `rudderanalytics` command in the console. If it returns the following code snippet, it means that the `analytics.js` file has loaded successfully:\n\n```\n{Integrations: Object, _integrations: Object, _readied: true, _timeout: 300, _user: n_}\n```\n\nIf it gives an `undefined` error, you need to verify the SDK installation.\n\n## Allowlist destination domain\n\nWhile using the JavaScript SDK with destinations supporting the [device mode](https://www.rudderstack.com/docs/destinations/rudderstack-connection-modes/#device-mode), you might need to allowlist the domain from where the destination SDK will load in your content security policy (CSP).\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> A content security policy adds an extra layer of protection from any type of cross-site scripting, clickjacking, and data injection attacks.\n\nSee the specific [destination’s documentation](https://www.rudderstack.com/docs/destinations/streaming-destinations/) to obtain the domain to be allowlisted. For example, [Braze](https://www.rudderstack.com/docs/destinations/streaming-destinations/braze/#connection-mode).\n\nRudderStack expects the following (minimum) CSP header configuration to load the JavaScript SDK without any errors:\n\n```\n<meta http-equiv=\"Content-Security-Policy\" content=\"script-src 'self' 'unsafe-inline' https: //cdn.rudderlabs.com/ https://cdn.rudderstack.com/;\">\n```\n\nIf you don’t want to allow unsafe inline and use the CDN package with its loading snippet, use `nonce` attribute to the script tag for the loading snippet:\n\n```\n<meta http-equiv=\"Content-Security-Policy\" content=\"script-src 'self' 'nonce-rAnd0m' https: //cdn.rudderlabs.com/ https://cdn.rudderstack.com/;\">\n```\n\nIn case you use the npm package, no inline loading snippet is required:\n\n```\n<meta http-equiv=\"Content-Security-Policy\" content=\"script-src 'self' https: //cdn.rudderlabs.com/ https://cdn.rudderstack.com/;\">\n```\n\n## Tracking user sessions\n\nBy default, the JavaScript SDK automatically tracks the user sessions. This means that RudderStack automatically determines the start and end of a user session depending on the inactivity time configured in the SDK (default time is 30 minutes).\n\n```\nrudderanalytics.load(WRITE_KEY, DATA_PLANE_URL, {\n  sessions: {\n    autoTrack: true,\n    timeout: 10 * 60 * 1000,  // 10 min in milliseconds\n  },\n  ...<otherLoadOptions>\n});\n```\n\nTo disable automatic session tracking, you can set the load option `autoTrack` to `false`.\n\nFor more information on the user sessions and how to track them using the JavaScript SDK, refer to the [Session Tracking](https://www.rudderstack.com/docs/sources/event-streams/sdks/session-tracking/) guide.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Load JavaScript SDK | RudderStack Docs",
    "description": "Load the JavaScript SDK.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/user-guides/migration-guides/",
    "markdown": "# Migration Guides | RudderStack Docs\n\nPrivacy Overview\n\nThis site uses cookies to improve your experience while you navigate through the website. Out of these cookies, the cookies that are categorized as necessary are stored on your browser as they are as essential for the working of basic functionalities of the website. We also use third-party cookies that help us analyze and understand how you use this website. These cookies will be stored in your browser only with your consent. You also have the option to opt-out of these cookies. But opting out of some of these cookies may have an effect on your browsing experience.\n\nNecessary cookies are absolutely essential for the website to function properly. This category only includes cookies that ensures basic functionalities and security features of the website. These cookies do not store any personal information.",
    "title": "Migration Guides | RudderStack Docs",
    "description": "Migrate from other platforms to RudderStack.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/javascript-sdk/1.1/detecting-adblocked-pages/",
    "markdown": "# Detect Ad-blocked Pages | RudderStack Docs\n\nPrivacy Overview\n\nThis site uses cookies to improve your experience while you navigate through the website. Out of these cookies, the cookies that are categorized as necessary are stored on your browser as they are as essential for the working of basic functionalities of the website. We also use third-party cookies that help us analyze and understand how you use this website. These cookies will be stored in your browser only with your consent. You also have the option to opt-out of these cookies. But opting out of some of these cookies may have an effect on your browsing experience.\n\nNecessary cookies are absolutely essential for the website to function properly. This category only includes cookies that ensures basic functionalities and security features of the website. These cookies do not store any personal information.",
    "title": "Detect Ad-blocked Pages | RudderStack Docs",
    "description": "Detect ad-blocked pages via the RudderStack JavaScript SDK.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/javascript-sdk/1.1/filtering/",
    "markdown": "# Events and Destinations Filtering in JavaScript SDK\n\nPrivacy Overview\n\nThis site uses cookies to improve your experience while you navigate through the website. Out of these cookies, the cookies that are categorized as necessary are stored on your browser as they are as essential for the working of basic functionalities of the website. We also use third-party cookies that help us analyze and understand how you use this website. These cookies will be stored in your browser only with your consent. You also have the option to opt-out of these cookies. But opting out of some of these cookies may have an effect on your browsing experience.\n\nNecessary cookies are absolutely essential for the website to function properly. This category only includes cookies that ensures basic functionalities and security features of the website. These cookies do not store any personal information.",
    "title": "Events and Destinations Filtering in JavaScript SDK | RudderStack Docs",
    "description": "Filter events for your allowlist or denylist via the RudderStack JavaScript SDK.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/javascript-sdk/1.1/version-migration-guide/",
    "markdown": "# JavaScript SDK Version Migration | RudderStack Docs\n\nMigrate the RudderStack JavaScript SDK from v1 to v1.1.\n\n* * *\n\n*     4 minute read  \n    \n\nThe latest **JavaScript SDK (v1.1)** is the lightweight, efficient, and optimized version of the SDK with a size reduction of approximately 70%, thereby increasing its loading speed considerably.\n\nAnother significant improvement in the latest version (v1.1) is that the [device mode](https://www.rudderstack.com/docs/destinations/rudderstack-connection-modes/#device-mode) destinations are published as individual plugins and loaded dynamically as per the dashboard configurations. In v1, device mode destinations were bundled with the core SDK.\n\n## Migrating to JavaScript SDK v1.1\n\nIf you have installed the JavaScript SDK v1 from the RudderStack CDN, you can simply upgrade it to v1.1 by updating the script tag in your website:\n\nFrom **v1**:\n\n```\n<script src=\"https://cdn.rudderlabs.com/v1/rudder-analytics.min.js\" />\n```\n\nTo **v1.1**:\n\n```\n<script src=\"https://cdn.rudderlabs.com/v1.1/rudder-analytics.min.js\" />\n```\n\n## Other migration scenarios\n\nThis section covers the detailed steps on migrating to JavaScript SDK v1.1 depending on how you installed JavaScript SDK v1.\n\n### Forwarded/proxied RudderStack CDN\n\n> ![success](https://www.rudderstack.com/docs/images/tick.svg)\n> \n> Refer to the [Custom Domains](https://www.rudderstack.com/docs/user-guides/how-to-guides/custom-domains/#setup-for-serving-the-sdk) guide to use a custom domain to forward or proxy the JavaScript SDK hosted on the RudderStack CDN.\n\nThe following steps assume that you are using AWS CloudFront to forward or proxy the RudderStack CDN:\n\n1.  Go to **Behaviors** and verify that the sub-path `/v1.1/*` is **not configured to be blocked** in any way. This is required to ensure that both the core SDK and destination SDKs are forwarded properly.\n2.  Update the script tag in your website:\n\nFrom **v1**:\n\n```\n<script src=\"https://<subdomain>.<yourdomain>.com/v1/rudder-analytics.min.js\" />\n```\n\nTo **v1.1**:\n\n```\n<script src=\"https://<subdomain>.<yourdomain>.com/v1.1/rudder-analytics.min.js\" />\n```\n\n### Self-hosted JavaScript SDK\n\nTo migrate the self-hosted JavaScript SDK to v1.1, follow any of these options based on your folder structure.\n\nSee [Host your JavaScript SDK in CDN/Storage](https://www.rudderstack.com/docs/user-guides/how-to-guides/self-hosting-js-sdk/) for details.\n\n#### Recommended structure\n\nIn this structure, the filename for the JavaScript SDK is `rudder-analytics.min.js` and device mode destination SDKs are located next to the core SDK file under the `js-integrations` directory.\n\n[![Recommended structure for self-hosting RudderStack&rsquo;s CDN](https://www.rudderstack.com/docs/images/js-sdk-recommended-folder-structure.webp)](https://www.rudderstack.com/docs/images/js-sdk-recommended-folder-structure.webp)\n\nTo migrate to v1.1 with this setup, update the script tag in your website:\n\n```\n<script src=\"https://<subdomain>.<yourdomain>.com/<path-to-sdk-base-directory>/rudder-analytics.min.js\"></script>\n```\n\n#### Destination SDKs are located elsewhere\n\nIf the filename of your self-hosted JavaScript SDK is `rudder-analytics.min.js` but the destination SDKs are **not** located next to the core SDK file under the `js-integrations` directory, update the script tag in your website:\n\n```\n<script>\n// rudderanalytics object initialization\n// provide the location of the destination SDKs in the load options\nrudderanalytics.load(WRITE_KEY, DATA_PLANE_URL, {\n    destSDKBaseURL: \"https://<subdomain>.<yourdomain>.com/<path-to-integration-sdks-directory>\"\n});\n// ...\n</script>\n<script src=\"https://<subdomain>.<yourdomain>.com/<path-to-sdk-base-directory>rudder-analytics.min.js\"></script>\n```\n\n#### Destination SDKs are located elsewhere, alternate filename for rudderanalytics\n\nIf your JavaScript SDK file name is **not** `rudder-analytics.min.js`, and the destination SDKs are **not** located under the `js-integrations` directory, update the script tag in your website as follows:\n\n```\n<script> \n// rudderanalytics object initialization\n// provide the location of the destination SDKs in the load options\nrudderanalytics.load(WRITE_KEY, DATA_PLANE_URL, {\n    destSDKBaseURL: \"https://<subdomain>.<yourdomain>.com/<path-to-integration-sdks-directory>\"\n});\n// ...\n</script>\n<script src=\"https://<subdomain>.<yourdomain>.com/<path-to-custom-sdk-file>/<custom-sdk-file-name.js>\"></script>\n```\n\n### Using NPM\n\nThe latest NPM package is based on the JavaScript SDK v1.1 architecture and is released with the 2.x.x version.\n\nTo update the SDK package using NPM, use any of the following options:\n\n*   Upgrade `rudder-sdk-js` package to v1.1 using the below command:\n\n```\nnpm install rudder-sdk-js@2.x.x\n```\n\n*   Manually modify the **package.json** file like below and run `npm install`:\n\n```\n\"dependencies\": {\n  \"rudder-sdk-js\": \"^2.x.x\"\n}\n```\n\n## Loading device mode destinations\n\nDepending on the dashboard settings, all destination SDKs are loaded from [https://cdn.rudderlabs.com/v1.1/js-integrations/](https://cdn.rudderlabs.com/v1.1/js-integrations/) by default. You can locate a specific destination SDK at [https://cdn.rudderlabs.com/v1.1/js-integrations/](https://cdn.rudderlabs.com/v1.1/js-integrations/)<destination\\_name>.min.js.\n\nFor example, the path for HubSpot is: [https://cdn.rudderlabs.com/v1.1/js-integrations/HubSpot.min.js](https://cdn.rudderlabs.com/v1.1/js-integrations/HubSpot.min.js), and Google Analytics is: [https://cdn.rudderlabs.com/v1.1/js-integrations/GA.min.js](https://cdn.rudderlabs.com/v1.1/js-integrations/GA.min.js), etc.\n\nHowever, if you are loading device mode destinations from a custom path using any of the below methods:\n\n*   [Forwarded/proxied RudderStack CDN](#forwardedproxied-rudderstack-cdn)\n*   [Self-hosted JavaScript SDK](#self-hosted-javascript-sdk)\n\nThen, pass the custom path for the required device mode destination in the `destSDKBaseURL` option in the SDK’s `load()` call:\n\n```\nrudderanalytics.load(WRITE_KEY, DATA_PLANE_URL, {\n    destSDKBaseURL: \"<custom-path-for-device-mode-destination>\", // ex: \"https://cdn.<yourdomain>.com/js-integrations\"\n    ...otherOptions\n});\n```\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n\n## FAQ\n\n#### How are the destination SDKs loaded in v1.1?\n\nIn v1.1, the core JavaScript SDK does not contain any destination-specific SDKs by default. It fetches them dynamically from the [hosted location](https://www.rudderstack.com/docs/archive/javascript-sdk/1.1/version-migration-guide/#loading-device-mode-destinations), depending on the device mode destinations configured in your dashboard (control plane).\n\n#### How does RudderStack determine the root location of the destination SDK?\n\nRudderStack follows the below precedence order while determining the root location of the destination SDK:\n\n1.  It refers to the `destSDKBaseURL` value in the `options` parameter of the `load` API call.\n2.  If absent, it checks the `src` attribute of the `<script>` tag (that adds the core JavaScript SDK to your website) if `/js-integrations` is automatically appended to the root location.\n3.  If none of the above options are applicable, it uses the default CDN URL: `https://cdn.rudderlabs.com/v1.1/js-integrations/`.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "JavaScript SDK Version Migration | RudderStack Docs",
    "description": "Migrate the RudderStack JavaScript SDK from v1 to v1.1.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/user-guides/migration-guides/segment-rudderstack-migration/migration-scenarios/",
    "markdown": "# Migration Scenarios and Time Estimates\n\nUnderstand the common scenarios users encounter while migrating from Segment to RudderStack.\n\n* * *\n\n*     2 minute read  \n    \n\nIt is helpful to think about two broad categories for migrations to estimate the resources and time you will need to migrate:\n\n**Simple migrations** - Simple migrations generally involve a lower number of sources and destinations, without heavy use of Segment’s more advanced features or customization.\n\n**Advanced migrations** - Advanced migrations often involve a large number of sources and/or destinations and some level of customization of the Segment configuration.\n\n## Simple migration\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> This migration is relatively quick due to the minimal complexity involved in the process.\n\nFor a straightforward migration scenario, consider a company using Segment with a few sources and a handful of destinations, like a website and mobile app feeding into a CRM, email marketing tool, and customer success system. The process typically involves mapping these sources to their corresponding destinations in RudderStack.\n\nThis migration includes the following steps:\n\n*   Configuring source and destination to match existing integrations\n*   Updating SDKs across platforms, and\n*   Ensuring data consistency between Segment and RudderStack before switching off Segment.\n\nEstimated timelines for a simple migration:\n\n*   **Initial assessment and planning**: 1 week\n*   **Configuration and setup**: 1 week\n*   **Testing and validation**: 1 week\n*   **Deployment**: 1 week\n\n**Summary**: Typically 2-4 weeks (1 sprint)\n\n## Complex migration\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> This process requires careful consideration of data integrity, backward compatibility, and ongoing support for legacy systems during the transition.\n\nA more complex migration often involves numerous data sources, diverse destinations, custom configurations, and legacy app versions utilizing Segment. In addition to migrating pipelines, this migration may include:\n\n*   Implementing additional changes like introducing a new analytics tool\n*   Re-instrumenting events for better event tracking and analysis\n*   Migrating any customization work (i.e., customized integration configurations)\n\nEstimated timelines for a complex migration:\n\n*   **Comprehensive assessment** of Segment implementation and planning: 2 weeks\n*   **Customization and migration**: 4 weeks\n*   **Testing across multiple environments**: 2 weeks\n*   **Deployment and phased rollout**: 4 weeks\n\n**Summary**: Typically 8-12 weeks (4-6 sprints)\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> Note that this timeline is only an estimate, and assumes dedicated resources focused on the project. Timelines can be longer for highly complex Segment configrations, especially when legacy apps need to be incrementally deprecated over a multi-month period, or technical debt is addressed as part of the migration.\n\n## Next steps\n\n[Migration Timeline and Cutover Best Practices](https://www.rudderstack.com/docs/user-guides/migration-guides/segment-rudderstack-migration/timelines-cutover/)\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Migration Scenarios and Time Estimates | RudderStack Docs",
    "description": "Understand the common scenarios users encounter while migrating from Segment to RudderStack.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/user-guides/migration-guides/segment-rudderstack-migration/",
    "markdown": "# Segment to RudderStack Migration Guide\n\nMigrate from Segment to RudderStack with minimal code changes.\n\n* * *\n\n*     4 minute read  \n    \n\nAny technical migration requires careful consideration, but with intentional planning, execution, and monitoring, migrating from Segment to RudderStack can be a straight-forward process.\n\nBy following the steps outlined in this guide, you can successfully migrate from Segment without data disruption, maintaining, and in most cases improving, data quality and consistency. Additionally, you can leverage the power of RudderStack’s more advanced technical capabilities to extend the functionality and value of your customer data platform.\n\n## Overview: why data teams migrate from Segment to RudderStack\n\nIn today’s data-driven world, businesses rely heavily on customer data platforms (CDP) to collect, manage, unify, and activate customer data. Segment started out as a data-focused CDP, but it was built before the mass adoption of the cloud data warehouse, which has become the center of the data stack and the source of truth for all customer data. Segment has also increasingly focused their product development on marketing users, limiting the ability of data teams to implement more advanced use cases.\n\nRudderStack is a powerful warehouse-native CDP that offers a range of benefits over Segment:\n\n**Warehouse native**\n\n*   Data is only stored in your warehouse\n*   Lower cost of ownership\n*   Increased ROI from your warehouse investment\n*   Full transparency and control\n*   Simplified security and compliance\n\n**Built for data teams**\n\n*   Technical tools for shipping complex use cases\n*   Integration with your existing development workflow\n*   Centralized management and monitoring of all customer data integrations\n\n**Flexible, open architecture**\n\n*   Open source core reduces proprietary risk\n*   Portable assets that live in your warehouse eliminate vendor lock-in\n*   Flexible technical tools make it easy to respond to changing busines requirements\n\nMany data teams who migrate also find that RudderStack’s [Data Governance tools](https://www.rudderstack.com/docs/data-governance/overview/) significantly increase their ability to manage data quality across their stack.\n\n> ![success](https://www.rudderstack.com/docs/images/tick.svg)\n> \n> During the migration process, RudderStack’s Data Governance Toolkit makes it easy to implement data quality measures, fix data problems, and address bad instrumentation.\n\n### Other benefits of migrating to RudderStack\n\nData teams also find these features and functionalities are a significant improvement over their experience with Segment:\n\n*   Powerful, real-time event [transformations](https://www.rudderstack.com/docs/transformations/overview/) for fixing bad data, enriching events, customizing integrations, and interacting with APIs\n*   Comprehensive data governance and quality features, such as [data catalog](https://www.rudderstack.com/docs/data-governance/data-catalog/), [tracking plans](https://www.rudderstack.com/docs/data-governance/tracking-plans/), and schema validation\n*   Robust, fully configurable, warehouse-native identity resolution\n*   Ability to unify all customer data in the warehouse to [build a comprehensive customer 360 table](https://www.rudderstack.com/docs/profiles/overview/)\n*   Ability to access the customer 360 table in real-time via the [Activation API](https://www.rudderstack.com/docs/profiles/activation-api/)\n*   Ability to sync the customer 360 from the warehouse to business tools via [reverse ETL](https://www.rudderstack.com/docs/data-pipelines/reverse-etl/)\n\n## A three-phased approach to seamless migration\n\nThis migration guide will walk you through the process of migrating from Segment to RudderStack, highlighting key differences and advantages of our warehouse-native architecture. It will guide you through each step, from preparing for migration to updating your SDK implementation, migrating your data, and taking advantage of RudderStack’s advanced features.\n\n*   **Phase 1: Basic SDK, Event, and Property Migration**: Lay the foundational groundwork by migrating SDKs, events, and properties\n*   **Phase 2: Advanced Migration Techniques**: Implement more complex migration strategies, including custom transformations, migrating user data and traits, and incrementally migrating high-traffic websites and apps\n*   **Phase 3: Testing and Validating Your Migration**: Ensure the integrity and accuracy of your migration through testing and validation processes\n\n### Migration roadmap\n\n| Topic | Description |\n| --- | --- |\n| [Migration Scenarios & Timelines](https://www.rudderstack.com/docs/user-guides/migration-guides/segment-rudderstack-migration/migration-scenarios/) | Get an overview of migration scenarios and estimated timelines for both simple and advanced configurations |\n| [Migration Timeline and Cutover Best Practices](https://www.rudderstack.com/docs/user-guides/migration-guides/segment-rudderstack-migration/timelines-cutover/) | Best practices to plan your migration timeline and cutover. |\n| [Preparing for Migration](https://www.rudderstack.com/docs/user-guides/migration-guides/segment-rudderstack-migration/prerequisites/) | Prerequisites before diving into the migration process |\n| [Migrate Existing Segment Events](https://www.rudderstack.com/docs/user-guides/migration-guides/segment-rudderstack-migration/migrate-segment-events/) | Migrate your existing Segment events by connecting Segment to RudderStack. |\n| [Phase 1: Basic SDK, Event and Property migration](https://www.rudderstack.com/docs/user-guides/migration-guides/segment-rudderstack-migration/basic-migration/) | Update your SDK, event, and property implementations while migrating from Segment to RudderStack. |\n| [Phase 2: Advanced Migration Techniques](https://www.rudderstack.com/docs/user-guides/migration-guides/segment-rudderstack-migration/advanced-migration/) | Advanced migration techniques to ensure a smooth and complete migration from Segment to RudderStack. |\n| [Phase 3: Testing and Validation of Your Migration](https://www.rudderstack.com/docs/user-guides/migration-guides/segment-rudderstack-migration/migration-testing-validation/) | Verify data flow, identify and resolve discrepancies, and monitor for post-migration anomalies. |\n\n## Additional resources\n\nRudderStack offers a wealth of resources and support to help you seamlessly migrate from Segment, including:\n\n*   Comprehensive documentation and guides covering every aspect of the RudderStack platform and migration process.\n*   A dedicated [support team](mailto:support@rudderstack.com) and customer success managers to help you plan and execute your migration and optimize your RudderStack implementation.\n*   A vibrant community of developers, data engineers, and marketers sharing knowledge and best practices on the RudderStack forum and [Slack](https://www.rudderstack.com/join-rudderstack-slack-community/) channels.\n*   Regular webinars, workshops, and events to help you stay up-to-date with the latest features and best practices in customer data management.\n\nBy leveraging these resources and following the steps outlined in this guide, you can confidently migrate from Segment to RudderStack and unlock the full potential of your customer data stack.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Segment to RudderStack Migration Guide | RudderStack Docs",
    "description": "Migrate from Segment to RudderStack with minimal code changes.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/javascript-sdk/1.1/data-storage-cookies/",
    "markdown": "# Data Persistence in JavaScript SDK\n\nUnderstand how our JavaScript SDK persists user data stored in cookies or local storage.\n\n* * *\n\n*     4 minute read  \n    \n\nThe JavaScript SDK stores persistent user data in the cookies by default. If the cookies are not supported, the SDK uses local storage instead.\n\nBy default, the SDK stores all cookies in the top-level domain. It helps you to identify the users visiting websites hosted under a particular sub-domain. For example, if you include the JavaScript SDK in both `admin.samplewebsite.com` and `app.samplewebsite.com`, the SDK will store the cookie in `samplewebsite.com`. However, you can specify the cookie storage location by using the `setCookieDomain` parameter in the [`load`](https://www.rudderstack.com/docs/archive/javascript-sdk/1.1/load-js-sdk/) API options as shown:\n\n```\nrudderanalytics.load(WRITE_KEY, DATA_PLANE_URL, {\n  setCookieDomain: \"samplewebsite.com\",\n});\n```\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n\n## Cookies\n\nThe cookie values are encrypted and their length is directly proportional to the values provided to the SDK. Also, all cookie names are prefixed with `rl_` and the values are prefixed with `RudderEncrypt:`. For example, `rl_user_id —> RudderEncrypt:U2FsdGVkX1+UKmiooYoGmKdNws7sgmWgGfHe`.\n\nThe following table lists the cookies used by the JavaScript SDK to store persistent user data:\n\n| Name | Description | Clearing mechanism  <br>using the SDK |\n| --- | --- | --- |\n| `rl_user_id` | Stores the user ID set via the `identify` API. All the subsequent event payloads will contain this data unless cleared from the storage.<br><br>For example: `4578`, `USER_001` | `rudderanalytics.reset()` |\n| `rl_trait` | Stores the user traits object set via the `identify` API. All the subsequent event payloads will contain this data unless cleared from the storage.<br><br>For example:<br><br>{  <br>  email: “[alex@example.com](mailto:alex@example.com)”,  <br>  accountType: “pro”,  <br>  country: “US”,  <br>  someObj: {  <br>    key1: val1,  <br>    key2: val2,  <br>  }  <br>} | `rudderanalytics.reset()` |\n| `rl_anonymous_id` | Stores the anonymous ID. By default, it would be the auto-generated unique ID by SDK for each visitor unless overridden via `setAnonymousId` API. All the subsequent event payloads will contain this data unless cleared from the storage.<br><br>For example: `5bfe258f-bd2f-49cf-bddd-8b844f74ab4b`, `customAnonId` | `rudderanalytics.reset(true)` |\n| `rl_group_id` | Stores the user group ID set via the `group` API. All the subsequent group event payloads will contain this data unless cleared from the storage.<br><br>For example: `GRP_3`, `98` | `rudderanalytics.reset()` |\n| `rl_group_trait` | Stores the user group traits object set via the `group` API. All the subsequent group event payloads will contain this data unless cleared from the storage.<br><br>For example:<br><br>{  <br>  location: “New Orleans”,  <br>  nationality: “US”,  <br>  someObj: {  <br>    key1: val1,  <br>    key2: val2,  <br>  }  <br>} | `rudderanalytics.reset()` |\n| `rl_page_init_referrer` | Stores the initial referrer of the page when a user visits a site for the first time. All the subsequent event payloads will contain this data.<br><br>For example: `https://www.google.com/` | Cannot be cleared using SDK. |\n| `rl_page_init_referring_domain` | Stores the initial referring domain of the page when a user visits a site for the first time. All the subsequent event payloads will contain this data.<br><br>For example: `google.com` | Cannot be cleared using SDK. |\n| `test_rudder_cookie` | Checks whether the cookie storage of a browser is accessible or not. Once checked, the SDK removes the cookie immediately.<br><br>For example: `test_rudder_cookie:true` | Cleared automatically. |\n| `rl_session` | Stores the session-related information including `sessionId` if session tracking is enabled.<br><br>For example: `1678961874` | Manual session tracking: `rudderanalytics.endSession()`<br><br>Automatic session tracking: Automatically cleared by the SDK (if `autoTrack`: `false`). |\n| `rl_auth_token` | Stores the authentication token passed by the user.<br><br>For example: `MOx2ZmMwLNE2A2IdNKL0N2VhN2I3Z` | `rudderanalytics.reset()` |\n\n## Local storage\n\nRudderStack stores the local storage cookie names with `rudder.<uuid>.` prefix where `uuid` is in the standard UUID v4 format. For example, `rudder.2dc2aee6-2836-4273-be69-79c90c04ddec.reclaimEnd`.\n\nThe JavaScript SDK uses local storage to keep track of the events sent to the RudderStack backend, as listed in the below table:\n\n| Name | Description |\n| --- | --- |\n| `ack` | Timer for other browser tabs to claim control of the retry queue.  <br>For example, `1639734070124` |\n| `reclaimStart` and `reclaimEnd` | Determines if a tab takes over the queue from another tab.  <br>For example, `2dc2aee6-2836-4273-be69-79c90c04ddec` |\n| `inProgress` | Keeps track of the events in progress. For example:  <br><br>{  <br>  “d89d7fb5-945e-4378-bda5-492e4b596fb4”: {  <br>   “item”: {  <br>    “url”: “[https://rudderstack-dataplane.rudderstack.com/v1/track\"](https://rudderstack-dataplane.rudderstack.com/v1/track%22),  <br>    “headers”: {  <br>     “Content-Type”: “application/json”,  <br>      …  <br>    },  <br>    “message”: {   <br>     …  <br>    }  <br>    “attemptNumber”: 1,  <br>    “time”: 1639734792773,  <br>    “id”: “a4d89d7f-b594-4eb3-b8bd-a5492e4b596f”  <br>   }  <br>  }  <br>} |\n| `queue` | Keeps track of the events that are in the processing queue. For example:  <br><br>\\[  <br>  {  <br>   “item”: {  <br>    “url”: “https://rudderstack-dataplane.rudderstack.com/v1/track”,  <br>    “headers”: {  <br>     “Content-Type”: “application/json”,  <br>      …  <br>    },  <br>    “message”: {   <br>     …  <br>    }  <br>    “attemptNumber”: 0,  <br>    “time”: 1639734792773,  <br>    “id”: “a4d89d7f-b594-4eb3-b8bd-a5492e4b596f”  <br>   }  <br>  }  <br>\\] |\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Data Persistence in JavaScript SDK | RudderStack Docs",
    "description": "Understand how our JavaScript SDK persists user data stored in cookies or local storage.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/user-guides/migration-guides/segment-rudderstack-migration/timelines-cutover/",
    "markdown": "# Segment Migration Timeline and Cutover Best Practices\n\nBest practices to plan your migration timeline and cutover while moving from Segment to RudderStack.\n\n* * *\n\n*     4 minute read  \n    \n\nPlanning and executing a successful migration from Segment to RudderStack requires careful consideration of timelines, dependencies, and cutover strategies.\n\nThis guide contains some guidance and best practices that have helped RudderStack customers who have successfully migrated from Segment.\n\n## Plan the migration timeline\n\n*   **Identify key milestones and dependencies**\n\nStart by defining the key milestones and dependencies for your migration project, such as SDK implementation, data mapping and transformation, destination configuration, and testing and validation. Identify any dependencies between these milestones and plan your timeline accordingly.\n\nMigrating to a new platform is an opportunity to reevaluate and potentially restructure your data pipelines for better efficiency and scalability. Some questions to consider are:\n\n*   What sources and destinations are you migrating from Segment to RudderStack?\n*   Do you modify any of the events currently being tracked in Segment?\n*   Do you have any functions or custom data transformations in use on Segment\n*   When does your Segment contract end?\n\n**Allocate resources and assigning responsibilities**\n\nDetermine the resources and personnel required for each phase of the migration project, and assign clear roles and responsibilities. This may include developers for SDK implementation, data engineers for data mapping and transformation, and QA teams for testing and validation.\n\n## Phased migration approach\n\n*   **Migrate non-critical applications first**: Begin your migration by targeting non-critical applications or user segments first, such as internal tools or low-traffic areas of your product. This allows you to test and validate your RudderStack implementation in a lower-risk environment before rolling it out to your entire user base.\n*   **Gradually ramp up traffic to RudderStack**: As you gain confidence in your RudderStack implementation, gradually increase the proportion of traffic that you send to RudderStack. Monitor your data quality and performance metrics closely during this phase, and be prepared to roll back or pause the migration if any issues arise.\n*   **Monitor data consistency and performance**: Throughout the phased migration process, continuously monitor your data consistency and performance, comparing your Segment and RudderStack data side-by-side. Use data validation workflows and alerting to identify and resolve any discrepancies or performance issues.\n\n## Going live with RudderStack\n\n*   **Switch primary data flow to RudderStack**: Once you have successfully migrated a significant portion of your traffic to RudderStack and are confident in your implementation, you can switch your primary data flow to RudderStack. This involves updating your SDK configurations to send all data to RudderStack, and disabling or removing your Segment SDK.\n*   **Run Segment and RudderStack in parallel for a limited time**: To minimize the risk of data loss or disruption during the cutover process, consider running Segment and RudderStack in parallel for a limited time. This allows you to verify that your RudderStack implementation is capturing and forwarding data correctly, and provides a fallback option if any issues arise.\n*   **Closely monitor data quality and consistency**: During the cutover process, closely monitor your data quality and consistency, comparing your Segment and RudderStack data in real-time. Be prepared to quickly identify and resolve any issues that may arise, and have a rollback plan in place if necessary.\n\n## Fully migrating off Segment\n\n*   **Decommission Segment implementation**: Once you have successfully switched your primary data flow to RudderStack and are confident in your implementation, you can begin the process of decommissioning your Segment implementation. This involves removing the Segment SDK from your applications, disabling any Segment destinations or integrations, and archiving your Segment data.\n*   **Clean up residual Segment code and configurations**: As part of the decommissioning process, thoroughly review your codebase and configurations for any residual Segment code or settings. Remove any unused Segment libraries, update any hardcoded Segment API keys or endpoints, and ensure that your RudderStack implementation is fully self-contained.\n*   **Verify data completeness and accuracy**: Before fully sunsetting your Segment implementation, verify that your RudderStack data is complete and accurate, and that you have successfully migrated all historical data and user profiles. Run final data validation checks and compare your Segment and RudderStack data to ensure that no data has been lost or corrupted during the migration process.\n\n## Post-migration monitoring and optimization\n\n*   **Monitor data pipelines for issues and anomalies**: After fully migrating to RudderStack, continue to monitor your data pipelines for any issues or anomalies. Use RudderStack’s monitoring and alerting features to proactively identify and resolve any data quality or performance issues.\n*   **Optimize RudderStack implementation based on usage patterns**: As you gather more data and insights through RudderStack, look for opportunities to optimize your implementation based on your unique usage patterns and requirements. This may involve tweaking your data transformation logic, adjusting your destination configurations, or implementing new features and integrations.\n*   **Continuously improve data quality and governance processes**: Finally, use your migration to RudderStack as an opportunity to strengthen your data quality and governance processes. Continuously monitor and improve your data quality, implement robust data validation and testing processes, and involve stakeholders from across your organization in data governance decisions.\n\nBy carefully planning your migration timeline, adopting a phased approach, and closely monitoring your data quality and performance throughout the cutover process, you can ensure a smooth and successful migration from Segment to RudderStack.\n\n## Next steps\n\n*   [Prepare for Migration](https://www.rudderstack.com/docs/user-guides/migration-guides/segment-rudderstack-migration/prerequisites/)\n*   [Migrate Events from Segment](https://www.rudderstack.com/docs/user-guides/migration-guides/segment-rudderstack-migration/migrate-segment-events/)\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Segment Migration Timeline and Cutover Best Practices | RudderStack Docs",
    "description": "Best practices to plan your migration timeline and cutover while moving from Segment to RudderStack.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/user-guides/migration-guides/segment-rudderstack-migration/prerequisites/",
    "markdown": "# Prepare for Migration | RudderStack Docs\n\nPrivacy Overview\n\nThis site uses cookies to improve your experience while you navigate through the website. Out of these cookies, the cookies that are categorized as necessary are stored on your browser as they are as essential for the working of basic functionalities of the website. We also use third-party cookies that help us analyze and understand how you use this website. These cookies will be stored in your browser only with your consent. You also have the option to opt-out of these cookies. But opting out of some of these cookies may have an effect on your browsing experience.\n\nNecessary cookies are absolutely essential for the website to function properly. This category only includes cookies that ensures basic functionalities and security features of the website. These cookies do not store any personal information.",
    "title": "Prepare for Migration | RudderStack Docs",
    "description": "Set up your RudderStack account and environment before diving into the migration process.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/javascript-sdk/1.1/javascript-sdk-enhancements/",
    "markdown": "# JavaScript SDK Enhancements | RudderStack Docs\n\nUse the navigator.sendBeacon utility to send event payloads with the JavaScript SDK.\n\n* * *\n\n*     5 minute read  \n    \n\nThe following sections cover the finer technical details of the improvements in the JavaScript SDK v1.1 that make it more efficient and easier to use.\n\n## Sending events using Beacon\n\nThe [JavaScript SDK v1.1](https://cdn.rudderlabs.com/v1.1/rudder-analytics.min.js) lets you send the event payloads using the **XHR** (XMLHttpRequest) API (default) or [Beacon](https://developer.mozilla.org/en-US/docs/Web/API/Navigator/sendBeacon) browser utility.\n\nThe **Beacon** browser utility asynchronously sends a small amount of data over HTTP to the RudderStack server. To send the SDK events using this utility, set the `useBeacon` field in the [`load()`](https://www.rudderstack.com/docs/archive/javascript-sdk/1.1/load-js-sdk/#loading-options) call options to `true`.\n\n> ![danger](https://www.rudderstack.com/docs/images/danger.svg)\n> \n> The `sendBeacon` version of the JavaScript SDK located [here](https://cdn.rudderlabs.com/v2/rudder-analytics.min.js?transport=beacon) and its XMLHTTP version located [here](https://cdn.rudderlabs.com/v2/rudder-analytics.min.js) are **no longer supported**.\n\nThe JavaScript SDK internally uses a separate queue (`BeaconQueueOpts`) to hold the data and send it through the Beacon utility in batches. The structure of `BeaconQueueOpts` is shown below:\n\n```\n{\n  maxItems: 10 \n  flushQueueInterval: 600000\n}\n```\n\nThe following table describes the above `integer` type parameters in detail:\n\n| Parameter | Description | Default Value |\n| --- | --- | --- |\n| `maxItems` | The SDK flushes the queue when the default number of payloads is reached. | `10` |\n| `flushQueueInterval` | The SDK flushes the queue after the default time interval (in milliseconds) is reached. | `600000` |\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> The JavaScript SDK flushes the Beacon queue if the total size of the payload exceeds 64 KB before even reaching the default `maxItems` or `flushQueueInterval` values.\n\n### Advantages\n\nThere are two key advantages of using the [Beacon](https://developer.mozilla.org/en-US/docs/Web/API/Navigator/sendBeacon) utility to send your event payload:\n\n*   Pushing events to the Beacon queue is faster than the XHR instrumentation, so there are some performance improvements in the JavaScript SDK.\n*   The Beacon requests are optimized so that the browser waits until the CPU load is lower or until the network is free before making the actual requests, leading to better website performance.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> The Beacon queue maintained by the browsers limits the total size of the elements present in the queue at any point and **peaks at 64 KB**.\n\n### Event delivery and retry mechanism\n\nThis section highlights some important points which will help you choose whether to use Beacon for sending your event payloads:\n\n*   The requests sent from the SDK using the Beacon utility only push the events to the browser’s Beacon queue. Further, it depends on the browser’s engine to send these events from the queue. Hence, RudderStack **does not guarantee** if any events get discarded due to any 5xx or other network-related errors (request timed out, end resource unavailable, etc.).\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> If event delivery and retry is an important requirement for your website, using the XHR API of the JavaScript SDK is highly recommended. RudderStack retries event delivery based on the status codes and other errors.\n\n*   The Beacon queue maintained by the browsers limits the total size of the elements present in the queue at any point and **peaks at 64 KB**. Therefore, you cannot send high-frequency hits from the main thread in one go, as the Beacon queue cannot take up cycles to dequeue itself. The JavaScript SDK handles this by maintaining a separate queue that retries pushing events to the Beacon queue if they are not successfully pushed in the first attempt.\n\n> ![success](https://www.rudderstack.com/docs/images/tick.svg)\n> \n> The RudderStack queue handles approximately 500 hits per 30ms and ensures the eventual successful delivery of events after retries to Beacon.\n\n## Reduced core SDK size\n\n> ![danger](https://www.rudderstack.com/docs/images/danger.svg)\n> \n> This feature is deprecated and no longer supported by RudderStack.\n\nAs RudderStack supports more native destinations through the JavaScript SDK, more instrumentation code is likely to be added to it. This increases the SDK size and requires the browser to evaluate and parse more unused JavaScript.\n\nTherefore, these instrumentation codes will not be bundled for the end destinations in the core JavaScript SDK. Instead, the SDK will only fetch the destination configuration settings from the RudderStack dashboard, such as track ID, API key, secret, etc., using the `requireIntegration` method.\n\n### `requireIntegration` call definition\n\nThe `requireIntegration` method contains the following two parameters:\n\n*   The first parameter is a **string** or an **array of strings** containing the destination names.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> You can also pass `rudderanalytics.requireIntegration(“All”)`. This will fetch the plugins for **all** the native destinations connected to your source in the RudderStack dashboard.\n\n*   The second parameter is a **callback** that accepts an object containing the names of the destinations successfully or unsuccessfully loaded on the page.\n\nAn example is shown below:\n\n```\nrudderanalytics.requireIntegration(\n    [\"GoogleAnalytics\", \"Hotjar\", \"Hubspot\"],\n    function(object) {\n        console.log(JSON.stringify(object));\n    }\n);\n```\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> Currently, RudderStack supports the plugins only for Google Analytics, Hotjar, and HubSpot.\n\n### How it works\n\nOnce the JavaScript SDK receives a `requireIntegration` call, for example, `rudderanalytics.requireIntegration(\"GA\")`, it automatically fetches the Google Analytics instrumentation code (GAPlugin.js).\n\nThe SDK maintains a call queue, and the API calls are processed one after the other. The SDK blocks the processing of this call queue once you call the `requireIntegration` method.\n\n### Use-case\n\nSuppose the user makes a call `rudderanalytics.requireIntegration(\"GA\")`. All the subsequent calls made to the SDK (such as `page`, `track`, `alias`, `group`, etc.) will get enqueued until the `GAPlugin.js` and Google Analytics’ `analytics.js` is loaded on the web page. Once the plugin and the end destination snippet is loaded, the calls in the queue will be processed, and the corresponding calls to `analytics.js` will start flowing.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> For the above example, Google Analytics’ `trackingId` and other configuration settings are fetched from the RudderStack dashboard and the SDK is configured using these settings.\n\n### Sample call flow\n\nThe following workflow sums up the flow of the event payload when the user calls `requireIntegration()`:\n\n[![requireIntegration call flow](https://www.rudderstack.com/docs/images/requireintegration-call-flow.webp)](https://www.rudderstack.com/docs/images/requireintegration-call-flow.webp)\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "JavaScript SDK Enhancements | RudderStack Docs",
    "description": "Use the navigator.sendBeacon utility to send event payloads with the JavaScript SDK.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/user-guides/migration-guides/segment-rudderstack-migration/migrate-segment-events/",
    "markdown": "# Migrate Existing Segment Events | RudderStack Docs\n\nMigrate your existing Segment events by connecting Segment to RudderStack.\n\n* * *\n\n*     3 minute read  \n    \n\nMany users choose to start their migration by importing their existing Segment events. There are two primary benefits:\n\n1.  Simplifying and speeding up the migration process\n2.  De-risking data quality issues and enforcing granular data governance throughout the migration\n\nRudderStack’s [Data Catalog](https://www.rudderstack.com/docs/data-governance/data-catalog/) feature can serve as a central repository for your events and properties flowing in from Segment, and is beneficial for teams to review or make changes before migrating.\n\nYou can also create a RudderStack [Tracking Plan](https://www.rudderstack.com/docs/data-governance/tracking-plans/) to apply data governance rules to your Segment events and manage any violations of those rules, ensuring strict data continuity during the migration.\n\nIn many cases, users also want to enhance or modify their events for cleaner tracking with RudderStack. RudderStack can transform Segment events in real time, which is particularly useful if you need to implement new naming or formatting conventions without updating code in your website or app.\n\n## Segment source setup considerations\n\nWhen setting up Segment as a source in RudderStack, you have a few critical decisions to make that will impact your data governance, event tracking, and user continuity:\n\n### Starting fresh vs. leveraging existing events\n\nOne of the first decisions you will have to make is whether to start with a clean slate in RudderStack or use your existing Segment event taxonomy.\n\nMost companies migrating from Segment have some things they want to clean up. It’s easy for your data schemas to get fragmented when multiple users and several years have elapsed.\n\nThe easiest thing to do is to use the [Segment source](https://www.rudderstack.com/docs/sources/event-streams/cloud-apps/segment/) in RudderStack to populate the RudderStack data catalog and tracking plans by importing your Segment events. This approach offers a few crucial benefits:\n\n*   Maintains a historical record of your event taxonomy\n*   Provides a foundation for future event planning and optimization\n*   Allows for better data governance and consistency\n\n### The user ID dilemma\n\nAnother critical consideration is migrating your existing Segment user IDs to RudderStack. Maintaining data continuity by migrating existing segment and ensuring a smooth transition is recommended. By carrying over your user IDs, you’ll be able to:\n\n*   Preserve user profiles and journeys across tools\n*   Avoid data discrepancies and duplication\n*   Enable more accurate and insightful analysis\n\nFor more information, read the detailed section on [migrating user data](https://www.rudderstack.com/docs/user-guides/migration-guides/segment-rudderstack-migration/advanced-migration/#migrate-user-data).\n\n### Handling legacy app versions\n\nIf you have older app versions (like mobile, Roku or other TV apps) that will continue running the Segment SDK for a period of time until users update, you can use Segment as a source in RudderStack to ensure data continuity during your phased migration. This approach lets you:\n\n*   Gradually sunset Segment SDKs without disrupting data flows (i.e., data for a small percentage of users running old apps will be ingested to RudderStack through Segment as a source)\n*   Maintain a single source of truth in your warehouse (which enalbles comprehensive reporting across RudderStack and legacy app Segment data)\n*   Minimize dependencies and simplify your overall data architecture\n\nFor more information, read the detailed section on [migrating legacy app versions](https://www.rudderstack.com/docs/user-guides/migration-guides/segment-rudderstack-migration/advanced-migration/#migrate-legacy-app-versions-running-the-segment-sdk).\n\n## Data governance advantages\n\nOne of the key advantages of migrating from Segment to RudderStack is the ability to uplevel your data governance. With RudderStack’s Tracking Plans and Data Catalog features, you can:\n\n*   Standardize event naming and properties across your stack\n*   Enforce data quality and consistency with validation rules\n*   Gain full visibility into your event schema and lineage\n*   Streamline data discovery and democratization for your teams\n\nFor more information, read the detailed section on [migrating legacy app versions](https://www.rudderstack.com/docs/user-guides/migration-guides/segment-rudderstack-migration/advanced-migration/#migrate-legacy-app-versions-running-the-segment-sdk).\n\n## Next steps\n\n[Phase 1: Basic SDK, event and property migration](https://www.rudderstack.com/docs/user-guides/migration-guides/segment-rudderstack-migration/basic-migration/)\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Migrate Existing Segment Events | RudderStack Docs",
    "description": "Migrate your existing Segment events by connecting Segment to RudderStack.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/javascript-sdk/1.1/service-worker/",
    "markdown": "# JavaScript SDK Service Worker | RudderStack Docs\n\nUse the JavaScript SDK service worker in browser extensions and serverless runtime.\n\n* * *\n\n*     4 minute read  \n    \n\nRudderStack’s JavaScript SDK provides a service worker that you can use in browser extensions and serverless runtimes. It exposes the same interface and features as the [RudderStack Node SDK](https://www.rudderstack.com/docs/sources/event-streams/sdks/rudderstack-node-sdk/).\n\n[![NPM Badge](https://img.shields.io/npm/v/@rudderstack/analytics-js-service-worker)](https://www.npmjs.com/package/@rudderstack/analytics-js-service-worker)\n\n## Install the package\n\nTo install the package, run the following command:\n\n```\nnpm install @rudderstack/analytics-js-service-worker --save\n```\n\nThen, run the following code snippet and use the exported object throughout your project:\n\n```\nimport { Analytics } from '@rudderstack/analytics-js-service-worker';\n\nconst rudderClient = new Analytics('<WRITE_KEY>', '<DATA_PLANE_URL>/v1/batch');\n```\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> This NPM module is meant to be used only for service worker usage. To integrate RudderStack with your Node.js apps, use the [Node SDK](https://www.rudderstack.com/docs/sources/event-streams/sdks/rudderstack-node-sdk/) instead.\n\n## Usage in Chrome extensions\n\nYou can use the JavaScript SDK in Chrome extensions with the manifest v3 - both as a [content script](#content-script) (via the JavaScript SDK package) or as [background script](#background-script) (via [service worker package](https://www.npmjs.com/package/@rudderstack/analytics-js-service-worker)).\n\nFor more information on usage in Chrome extensions, see the [JavaScript SDK GitHub repository](https://github.com/rudderlabs/rudder-sdk-js/blob/main/examples/chrome-extension/USAGE.md).\n\n### Background script\n\nYou can use the RudderStack [service worker npm package](https://www.npmjs.com/package/@rudderstack/analytics-js-service-worker) as a background script. To do so, place it in your Chrome extension resources by following either of these approaches:\n\n*   Copy the file from the node modules and place it as a part of the resources.\n*   Use a JS bundler and bundle it as a part of your service worker script.\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> You need to enable relevant permissions in the manifest file according to the required capabilities and allowed connections.\n> \n> Also, setting the background script `type` as a `module` is recommended, as it allows to import the script as ESM.\n\n```\n\"permissions\": [\"storage\", \"tabs\"],\n\"host_permissions\": [\n    \"https://*.dataplane.rudderstack.com/*\",\n    \"https://*.rudderlabs.com/*\",\n    \"*://*/*\"\n],\n\"externally_connectable\": {\n    \"matches\": [\n        \"https://*.dataplane.rudderstack.com/*\",\n        \"https://*.rudderlabs.com/*\"\n    ]\n},\n\"background\": {\n    \"service_worker\": \"service-worker.js\",\n    \"type\": \"module\"\n},\n```\n\nThen, follow the [Node SDK documentation](https://www.rudderstack.com/docs/sources/event-streams/sdks/rudderstack-node-sdk/) for further usage.\n\nYou can also react to events available in the background scripts using the [Chrome API](https://developer.chrome.com/docs/extensions/reference/).\n\nThe following example tracks any URL changes:\n\n```\n// If file is copied from node_modules/@rudderstack/analytics-js-service-worker/npm/esm/index.js in extension resources folder\n\nimport { Analytics } from \"./rudderAnalytics.js\";\n\n// If the package is imported directly as umd and then bundled in the background script\n\nimport { Analytics } from \"@rudderstack/analytics-js-service-worker/umd/index.js\";\n\n// If the package is imported directly as es-module and then bundled in the background script\n\nimport { Analytics } from \"@rudderstack/analytics-js-service-worker\";\n```\n\n```\nconst rudderClient = new Analytics(\"<WRITE_KEY>\",\"<DATA_PLANE_URL>/v1/batch\");\n\nchrome.tabs.onUpdated.addListener((tabId, tab) => {\n    if (tab.url) {\n        rudderClient.track({\n            userId: \"123456\",\n            event: \"Event Name\",\n            properties: {\n                data: { url: tab.url },\n            }\n        });\n    }\n});\n```\n\n### Content script\n\nTo use the RudderStack `Analytics` JavaScript SDK as a content script, place it in your Chrome extension resources by following either of these approaches:\n\n*   Download the file and place it as a part of the resources.\n*   Use a JS bundler and bundle it as part of your content script.\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> You need to enable relevant permissions in the manifest file according to the required capabilities and allowed connections.\n\n```\n\"permissions\": [\"storage\", \"tabs\"],\n\"host_permissions\": [\n    \"https://*.dataplane.rudderstack.com/*\",\n    \"https://*.rudderlabs.com/*\",\n    \"*://*/*\"\n],\n\"externally_connectable\": {\n    \"matches\": [\n        \"https://*.dataplane.rudderstack.com/*\",\n        \"https://*.rudderlabs.com/*\"\n    ]\n}\n```\n\nThen, follow the [SDK documentation](https://www.rudderstack.com/docs/archive/javascript-sdk/1.1/) for further usage.\n\nYou can also react to events available in both the content and background scripts by leveraging the [Chrome API](https://developer.chrome.com/docs/extensions/reference/).\n\nThe following sample scripts help you track any URL changes:\n\n```\n# prepend the JS SDK file here\nrudderanalytics.load(\"<WRITE_KEY>\", \"<DATA_PLANE_URL>\");\n\nchrome.runtime.onMessage.addListener((obj, sender, response) => {\n    const { type, value } = obj;\n\n    if (type === \"trackURL\") {\n        rudderanalytics.track(\"URL change\", { url: value });\n    }\n});\n```\n\n```\nchrome.tabs.onUpdated.addListener((tabId, tab) => {\n    if (tab.url) {\n        chrome.tabs.sendMessage(tabId, {\n            type: \"trackURL\",\n            value: {\n                url: tab.url\n            },\n        });\n    }\n});\n```\n\n## Usage in serverless runtimes\n\nYou can use the JavaScript SDK in serverless runtimes like Cloudflare workers or Vercel Edge functions.\n\n### Cloudflare worker\n\nTo use the JavaScript SDK service worker in Cloudflare workers, start with the [sample](https://developers.cloudflare.com/workers/get-started/guide/) and integrate the SDK in the `worker.js` file:\n\n```\nimport { Analytics } from '@rudderstack/analytics-js-service-worker';\n\nconst rudderClient = new Analytics(\n  \"<WRITE_KEY>\",\n  \"<DATA_PLANE_URL>/v1/batch\",\n  {\n    flushAt: 1\n  }\n);\n```\n\nThen, use the JavaScript SDK within the `fetch` methods with promisified flush:\n\n```\nconst flush = () => new Promise((resolve) => rudderClient.flush(resolve));\n\nrudderClient.track({\n  userId: '123456',\n  event: 'test cloudflare worker',\n  properties: {\n    data: {\n      url: 'test cloudflare worker',\n    },\n  },\n});\n\nawait flush();\n```\n\nFor more information, see this [sample implementation](https://github.com/rudderlabs/rudder-sdk-js/tree/main/examples/serverless/cloudflare-worker).\n\n### Vercel Edge\n\nTo use the JavaScript SDK service worker in Vercel Edge functions, start with the [sample](https://vercel.com/docs/functions/edge-functions/quickstart) and integrate the SDK in the `app/api/edge-function-sample/route.ts` file:\n\n```\nimport { Analytics } from '@rudderstack/analytics-js-service-worker';\n\nconst rudderClient = new Analytics(\n  \"<WRITE_KEY>\",\n  \"<DATA_PLANE_URL>/v1/batch\",\n  {\n    flushAt: 1\n  }\n);\n```\n\nThen, use the JavaScript SDK within the `fetch` methods as usual:\n\n```\nrudderClient.track({\n  userId: '123456',\n  event: 'test vercel edge worker',\n  properties: {\n    data: {\n      url: 'test vercel edge worker',\n    },\n  }\n});\n```\n\nFor more information, see this [sample implementation](https://github.com/rudderlabs/rudder-sdk-js/tree/main/examples/serverless/vercel-edge).\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "JavaScript SDK Service Worker | RudderStack Docs",
    "description": "Use the JavaScript SDK service worker in browser extensions and serverless runtime.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/user-guides/migration-guides/segment-rudderstack-migration/advanced-migration/",
    "markdown": "# Phase 2: Advanced Migration Techniques\n\nAdvanced migration techniques to ensure a smooth and complete migration from Segment to RudderStack.\n\n* * *\n\n*     10 minute read  \n    \n\nThis guide explores some more advanced techniques to ensure a smooth and complete migration from Segment to RudderStack.\n\nOne of the critical aspects of migrating from Segment to RudderStack is ensuring that historical user data is appropriately handled. RudderStack provides several options for migrating user data, depending on your specific requirements and the complexity of your data.\n\n## Migrate user data\n\n**Call the Segment SDK from RudderStack to migrate anonymous ID, user ID, and user traits**\n\nIf you have the Segment SDK implemented in your application, you can leverage it to migrate user data to RudderStack by following these steps:\n\n1.  Implement the RudderStack SDK alongside the Segment SDK in your application.\n2.  In the RudderStack SDK initialization code, add a callback function to the `load` method:\n\n```\nrudderanalytics.load(WRITE_KEY, DATA_PLANE_URL, {\n  onLoaded: function(rudderanalytics) {\n    const segmentAnon = analytics.user().anonymousId();\n    const segmentUserId = analytics.user().id();\n    const rudderUserId = rudderanalytics.getUserId();\n    if (segmentAnon) {\n      rudderanalytics.setAnonymousId(segmentAnon)\n    }\n\n    if (segmentUserId && (!rudderUserId || (rudderUserId != segmentUserId))) {\n      rudderanalytics.identify(segmentUserId)\n    }\n  }\n});\n```\n\nThis code snippet retrieves the user’s anonymous ID, user ID, and traits from the Segment SDK and sends them to RudderStack using the `setAnonymousId` and `identify` methods.\n\n3.  Deploy the updated code to your application and verify that user data is being sent to RudderStack.\n\n**Include previously used Segment anonymous IDs**\n\nIf you rely heavily on anonymous user tracking and don’t have access to the Segment SDK, you can migrate anonymous user data using RudderStack’s `anonymousIdOptions` feature.\n\n1.  Enable the `anonymousIdOptions` feature in the RudderStack SDK initialization code:\n\n```\nrudderanalytics.load(\"WRITE_KEY\", \"DATA_PLANE_URL\", {\n  anonymousIdOptions: {\n    localStorage: true,\n    cookie: {\n      name: \"ajs_anonymous_id\",\n      domain: \"your-domain.com\",\n      path: \"/\"\n    }\n  }\n});\n```\n\nThis code snippet instructs RudderStack to retrieve the anonymous ID from the `ajs_anonymous_id` cookie set by Segment and store it in local storage.\n\n2.  Deploy the updated code to your application and verify that anonymous user data is being sent to RudderStack.\n\n**Retrieve user data from local storage or cookies**\n\nIf you have stored user data in local storage or cookies, you can migrate that data to RudderStack using a custom script:\n\n1.  Retrieve the user data from local storage or cookies:\n\n```\n// Just like in the Segment SDK example above but from local storage\n\nrudderanalytics.load(WRITE_KEY, DATA_PLANE_URL, {\n      onLoaded: function(rudderanalytics) {\n          const segAnonymousId = localStorage.getItem(\"ajs_anonymous_id\");\n          const segmentUserId = localStorage.getItem(\"ajs_user_id\");\n          ....\n```\n\n2.  Send the user data to RudderStack using the SDK methods as before:\n\n```\nrudderanalytics.setAnonymousId(segAnonymousId);\nrudderanalytics.identify(segmentUserId, segmentTraits);\n```\n\n3.  Deploy the custom script to your application and verify that user data is being sent to RudderStack.\n4.  Once the data migration is complete, clean up the local storage or cookies to remove the Segment-related data when you are ready.\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> Remember to thoroughly test your data migration to ensure all user data is accurately migrated to RudderStack. Monitor your RudderStack dashboard for any data discrepancies or anomalies during the migration process.\n\n## Incrementally migrate high-traffic applications\n\nFor some customers with high-traffic applications or especially complex environments, an incremental migration approach may be preferable to minimize disruption and risk. Here’s how you can incrementally migrate from Segment to RudderStack:\n\n1.  Identify a subset of your application or user base to start the migration process, such as a specific geographic region, user cohort, or product area.\n2.  Set up a parallel tracking implementation where you send data to both Segment and RudderStack for the selected subset of users or application components. This can be achieved by conditionally loading both the Segment and RudderStack SDKs based on user or application criteria.\n3.  Monitor the data flow and consistency between Segment and RudderStack for the migrated subset, ensuring that events and user traits are being correctly captured and forwarded by RudderStack.\n4.  Gradually expand the scope of the migration by adding more users, regions, or product areas to the RudderStack implementation while continuously monitoring data quality and consistency.\n5.  Once you have migrated all of your applications and are confident in the RudderStack implementation, you can complete the migration by removing the Segment SDK and updating any remaining downstream systems to use the RudderStack data.\n6.  Continuously monitor your RudderStack data pipelines for any anomalies or discrepancies, and address any issues promptly to ensure a smooth post-migration experience.\n\nBy adopting an incremental migration approach, you can minimize the risk of data loss or disruption and ensure a seamless transition from Segment to RudderStack.\n\n## Migrate legacy app versions running the Segment SDK\n\nCompanies who offer apps across multiple platforms often have a subset of users running outdated app versions. This is often due to users not updating their apps or using older devices that do not support newer operating systems. These cases are most common for apps on mobile and TV devices (for example, iOS devices or Roku OTT devices).\n\nWhen migrating from Segment to RudderStack, only the latest version of an app will include the RudderStack SDK. Older app versions will still be running the Segment SDK.\n\nIf users running older app versions represent a meaningful percentage of overall users, you can use RudderStack’s Segment source integration to maintain full data capture across your user base during the migration. Because this data will be ingested into RudderStack, you can manage data quality and destinations integrations in a single platform (RudderStack), simplifying the migration process.\n\nHere’s the high level process for incrementally migrating legacy app versions off of Segment:\n\n1.  Identify the data sources in Segment that represent older app versions.\n2.  Configure [Segment as a source](https://www.rudderstack.com/docs/sources/event-streams/cloud-apps/segment/#getting-started) in RudderStack. This will require setting up a webook destination in Segment. We recommend setting up a dedicated RudderStack Segment source (and related Segment webhook destination) for legacy app versions.\n3.  Point the Segment sources that represent legacy apps at the webhook destination. At this point, that data will be flowing to RudderStack and sent to all RudderStack destinations, fully centralizing your data management.\n4.  **Once you have completed the migration of new app versions to RudderStack SDKs, you can significantly decrease your Segment bill by only using Segment to collect data from legacy app version sources.**\n5.  Monitor the percentage of total users that are running legacy app versions.\n6.  When the percentage of total users reaches your desired threshold (often a few percent), you can fully migrate off of Segment and close your Segment account.\n\n## Ensure data governance during migration\n\nData governance and data quality are essential aspects of any successful customer data platform migration. RudderStack provides a range of features and best practices to help you maintain high-quality data throughout the migration process and beyond.\n\n### Data catalog for data definitions, discovery, and lineage\n\nRudderStack’s [Data Catalog](https://www.rudderstack.com/docs/data-governance/data-catalog/) feature provides a centralized view of your event schema, making it easy to discover, understand, and govern your customer data.\n\nHere’s how you can leverage the data catalog for data discovery and lineage:\n\n*   **Explore event schemas and properties**: The data catalog allows you to browse and search your event schemas, helping you understand the structure and content of your event data. You can view event names, properties, and sample payloads, making identifying relevant data for analysis and activation easier.\n*   **Trace data flow from sources to destinations**: With the data catalog’s lineage view, you can trace data flow from your sources through RudderStack and into your downstream destinations. This helps you understand how your data is being transformed and consumed and identify any potential issues or bottlenecks in your data pipelines.\n*   **Collaborate with teams using data catalog documentation**: The data catalog provides a collaborative platform for documenting and sharing knowledge about your event data. You can add descriptions, tags, and annotations to your event schemas, making it easier for teams across your organization to understand and use your customer data effectively.\n\nBy leveraging RudderStack’s data catalog, you can improve data discovery, lineage, and collaboration, ensuring that your teams have a shared understanding of your customer data and can use it effectively for analysis and activation. Using a data catalog can also help you prevent issues you may have had in your Segment implementation with bad data or inconsistent naming.\n\n### Tracking plans for data quality and violation management\n\n[Tracking Plans](https://www.rudderstack.com/docs/data-governance/tracking-plans/) are a powerful tool for standardizing and governing your event data, ensuring that your data is consistent, accurate, and aligned with your business objectives. Here’s how you can implement tracking plans in RudderStack:\n\n*   **Define standard events and properties in tracking plans**: Create a tracking plan that defines your standard event names, properties, and data types. This helps ensure that your event data is consistent across your sources and destinations, making it easier to analyze and activate.\n*   **Validate event implementation against tracking plans**: Use RudderStack’s tracking plan validation feature to automatically compare your incoming event data against your defined tracking plan. This helps identify discrepancies or errors in your event implementation, such as missing properties or incorrect data types.\n*   **Monitor data quality and identify discrepancies**: Set up data quality monitoring in RudderStack to continuously track the quality and consistency of your event data. Use RudderStack’s data quality reports and alerts to identify any discrepancies or anomalies in your data, such as sudden changes in event volume or property values.\n*   **Evolve tracking plans to support new use cases and data needs**: As your business evolves and new use cases emerge, update your tracking plans to reflect your changing data requirements. This helps ensure that your event data remains relevant and actionable and supports your ongoing analytics and activation needs.\n\nBy implementing tracking plans and data quality monitoring in RudderStack, you can standardize your event data, ensure data cleanliness, consistency, and accuracy, and evolve your data governance practices to support your growing business needs.\n\n### Transformations for data cleansing and enrichment\n\nRudderStack’s [Transformations](https://www.rudderstack.com/docs/transformations/overview/) feature allows you to clean, enrich, and transform your event data in real-time, ensuring that your downstream systems receive high-quality, actionable data.\n\n> ![success](https://www.rudderstack.com/docs/images/tick.svg)\n> \n> RudderStack also provides some [pre-built JavaScript functions](https://www.rudderstack.com/docs/transformations/templates/) that you can use to create transformations and implement a variety of use cases on your event data.\n\nSome common use cases for RudderStack transformations are listed below:\n\n**Fix bad data**\n\nIf you have violations in your tracking plan, you can quickly fix them using a transformation to rename the events. Renaming events is one of the most popular transformations when migrating from Segment to RudderStack.\n\n```\n/***\n * This transformation renames properties to conform with the appropriate naming convention,e.g, rename \"first_name\" to \"firstName.\" This allows you to change property names to those expected by downstream destinations.\n ***/\n\nexport function transformEvent(event, metadata) {\n  const firstName = event.context?.traits?.first_name;\n  if (firstName) {\n    event.context.traits.firstName = firstName;\n    delete event.context.traits.first_name;\n  }\n  return event;\n}\n```\n\n**Mask sensitive PII**\n\nIf your event data contains sensitive personally identifiable information (PII), such as email addresses or phone numbers, you can use transformations to mask or hash that data before forwarding it to downstream destinations.\n\n```\n/***\n * This transformation hashes sensitive personal data, e.g., email, birthday,social security number. This reduces the risk of accidentally disclosing personally identifiable information (PII). It uses the standard RudderStack 'sha256' library.\n ***/\n\nimport {\n  sha256\n} from \"@rs/hash/v1\";\n\nexport function transformEvent(event, metadata) {\n  const email = event.context?.traits?.email;\n  if (email) event.context.traits.email = sha256(email);\n  return event;\n}\n```\n\n**Enrich events with additional context**\n\nYou can use transformations to enrich your event data with additional context, such as user attributes, device information, or external API data.\n\n```\n/***\nThis transformation enriches events with geolocation data using an IP-to-geolocation API.This allows you to easily query events based on geolocation data, e.g., country, city.\n***/\n\nexport async function transformEvent(event, metadata) {\n  if (event.request_ip) {\n    try {\n      const res = await fetchV2(\"<YOUR_API_ENDPOINT>\" + event.request_ip); // Use your paid IP-to-geolocation API endpoint.\n      event.context.geolocation = res.body;\n    } catch {}\n  }\n  return event;\n}\n```\n\nBy leveraging RudderStack transformations, you can ensure clean and consistent event data enriched with valuable context, enabling more accurate analysis and activation.\n\n## Next steps\n\n[Phase 3: Test and validate your migration](https://www.rudderstack.com/docs/user-guides/migration-guides/segment-rudderstack-migration/migration-testing-validation/)\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Phase 2: Advanced Migration Techniques | RudderStack Docs",
    "description": "Advanced migration techniques to ensure a smooth and complete migration from Segment to RudderStack.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/",
    "markdown": "# Profiles | RudderStack Docs\n\nPrivacy Overview\n\nThis site uses cookies to improve your experience while you navigate through the website. Out of these cookies, the cookies that are categorized as necessary are stored on your browser as they are as essential for the working of basic functionalities of the website. We also use third-party cookies that help us analyze and understand how you use this website. These cookies will be stored in your browser only with your consent. You also have the option to opt-out of these cookies. But opting out of some of these cookies may have an effect on your browsing experience.\n\nNecessary cookies are absolutely essential for the website to function properly. This category only includes cookies that ensures basic functionalities and security features of the website. These cookies do not store any personal information.",
    "title": "Profiles | RudderStack Docs",
    "description": "The most popular HTML, CSS, and JS library in the world.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/javascript-sdk/1.1/faq/",
    "markdown": "# JavaScript SDK FAQ | RudderStack Docs\n\nSolutions to some commonly faced issues while using the RudderStack JavaScript SDK.\n\n* * *\n\n*     2 minute read  \n    \n\nThis guide contains the answers to some commonly-asked questions about setting up, using and troubleshooting the JavaScript SDK.\n\nYes, it is important to ensure that no ad blockers are running on your browser, as they restrict the JavaScript SDK script from executing and storing user information in the browser.\n\n#### Can I load multiple instances of RudderStack JavaScript SDK?\n\nNo, it is not possible to load multiple instances of the JavaScript SDK, as it is bound to exceed the maximum stack call size and give you an error.\n\n#### How can I verify if the SDK sends the data to the desired destinations?\n\nTo verify if the SDK is transmitting the events to the specified destinations, go to the **Network tab** of the JavaScript console in your browser.\n\n[![Sample page call](https://www.rudderstack.com/docs/images/sample-page-call.webp)](https://www.rudderstack.com/docs/images/sample-page-call.webp)\n\n[![Sample track call](https://www.rudderstack.com/docs/images/sample-track-call.webp)](https://www.rudderstack.com/docs/images/sample-track-call.webp)\n\nIf you cannot see any outbound requests, verify if you have installed and set up the JavaScript SDK correctly. Also, check if any adblockers are enabled on your browser.\n\n#### What is the size limit on the event requests?\n\nThe [JavaScript SDK](https://www.rudderstack.com/docs/archive/javascript-sdk/1.1/) exhibits the following behavior:\n\n*   If the event size exceeds 32KB, the SDK logs an error message but forwards it to the RudderStack data plane (backend).\n*   If you send the event using [`sendBeacon`](https://www.rudderstack.com/docs/archive/javascript-sdk/1.1/javascript-sdk-enhancements/#sending-events-using-beacon), the SDK batches the events with a size limit of 64KB on the **entire** batch payload. If a single event’s size exceeds 64KB, the browser might drop the event. Note that this is applicable for the JavaScript SDK v1.1.\n\n#### Can I send the event data to specific destinations only?\n\nYes, you can send your [event data only to the specific destinations](https://www.rudderstack.com/docs/archive/javascript-sdk/1.1/filtering/#filtering-destinations) by stopping the SDK from loading the other [device mode](https://www.rudderstack.com/docs/destinations/rudderstack-connection-modes/#device-mode) integrations.\n\n#### What is the “Reserved Keyword” error?\n\nWhen using the JavaScript SDK, you may run into the following error:\n\n```\nWarning! : Reserved keyword used in traits -->  id with track call.\n```\n\nThis is because one or more keys in your `traits` or `properties` object have the same value as a reserved keyword.\n\nRudderStack reserves the following keywords as keys for a standard event payload, and you should avoid using these while naming your event traits and properties:\n\n```\n\"anonymous_id\";\n\"id\";\n\"sent_at\";\n\"received_at\";\n\"timestamp\";\n\"original_timestamp\";\n\"event_text\";\n\"event\";\n```\n\n#### How can I differentiate between events sent from a mobile device or from a website using a laptop?\n\nThe events tracked via JavaScript SDK contain `context.userAgent` that contain information on the user agent of the device.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "JavaScript SDK FAQ | RudderStack Docs",
    "description": "Solutions to some commonly faced issues while using the RudderStack JavaScript SDK.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/user-guides/migration-guides/segment-rudderstack-migration/basic-migration/",
    "markdown": "# Phase 1: Basic SDK, Event, and Property migration\n\nUpdate your SDK, event, and property implementations while migrating from Segment to RudderStack.\n\n* * *\n\n*     6 minute read  \n    \n\nRudderStack provides a wide range of SDKs for different platforms, including JavaScript, iOS, Android, and server-side languages like Node.js, Python, and Ruby.\n\nThis guide covers the process of updating your SDK, events, and properties implementation for web, mobile, and server while migrating from Segment to RudderStack.\n\n## JavaScript SDK\n\n1.  Install the RudderStack JavaScript SDK by including the [JavaScript SDK installation snippet](https://www.rudderstack.com/docs/sources/event-streams/sdks/rudderstack-javascript-sdk/installation/#using-cdn) in thesection of your website.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n\n2.  Update your `page`, `track`, and `identify` calls to use the RudderStack SDK. RudderStack and Segment calls are nearly identical. Here are a few examples:\n\n```\n// Segment\nanalytics.identify(\"user_id\", {\n  name: \"John Doe\",\n  email: \"john@example.com\",\n  plan: \"Premium\"\n});\n\n// RudderStack \nrudderanalytics.identify(\"user_id\", {\n  name: \"John Doe\",\n  email: \"john@example.com\",\n  plan: \"Premium\"\n});\n```\n\n```\n// Segment\nanalytics.page();\n\n// RudderStack \nrudderanalytics.page();\n```\n\n```\n// Segment\n\nanalytics.track(\"Order Completed\", {\n  order_id: \"12345\",\n  revenue: 99.95,\n  shipping_method: \"FedEx\"\n});\n\n// RudderStack \n\nrudderanalytics.track(\"Order Completed\", {\n  order_id: \"12345\",\n  revenue: 99.95,\n  shipping_method: \"FedEx\"\n});\n```\n\n3.  After implementing your calls, verify that the data flows correctly by checking the **Live Events** tab in your [RudderStack dashboard](https://app.rudderstack.com/). You should see events appearing in real time as users interact with your website.\n\n## iOS/Android SDK\n\nThe native iOS or Android SDK implementations can be migrated similarly and the calls will be very similar to Segment as well.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n\n**Example for iOS**\n\n1.  Install the [RudderStack iOS SDK](https://www.rudderstack.com/docs/sources/event-streams/sdks/rudderstack-ios-sdk/) as follows:\n\n```\nRSConfigBuilder *builder = [[RSConfigBuilder alloc] init];\n[builder withDataPlaneUrl:\"DATA_PLANE_URL\"];\n[RSClient getInstance:\"WRITE_KEY\" config:[builder build]];\n```\n\n```\nRSConfigBuilder *builder = [[RSConfigBuilder alloc] init];\n[builder withDataPlaneUrl:\"DATA_PLANE_URL\"];\n[RSClient getInstance:\"WRITE_KEY\" config:[builder build]];\n```\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n\n2.  Implement `identify` and `track` calls in your app as below:\n\n```\nRSClient.sharedInstance()?.identify(\"user_id\", traits: [\n    \"key_1\": \"value_1\",\n    \"key_2\": \"value_2\",\n    \"email\": \"alex@example.com\"\n])\n```\n\n```\nRSClient.sharedInstance()?.track(\"user_id\", properties: [\n    \"key_1\": \"value_1\",\n    \"key_2\": \"value_2\"\n])\n```\n\n3.  Verify the data flow by checking the **Live Events** tab in your [RudderStack dashboard](https://app.rudderstack.com/).\n\n> ![success](https://www.rudderstack.com/docs/images/tick.svg)\n> \n> See the RudderStack [Android SDK](https://www.rudderstack.com/docs/sources/event-streams/sdks/rudderstack-android-sdk/) and [iOS SDK](https://www.rudderstack.com/docs/sources/event-streams/sdks/rudderstack-ios-sdk/) documentation for detailed implementation and examples.\n\n## Server-side SDKs\n\nRudderStack supports eight different [server-side SDKs](https://www.rudderstack.com/docs/sources/event-streams/sdks/#server) that can be implemented similarly to Segment.\n\n**Example for Node.js**\n\n1.  Install the [RudderStack Node.js SDK](https://www.rudderstack.com/docs/sources/event-streams/sdks/rudderstack-node-sdk/) using npm:\n\n```\nnpm install @rudderstack/rudder-sdk-node\n```\n\n2.  Import and initialize the SDK in your Node.js app:\n\n```\nconst RudderAnalytics = require('@rudderstack/rudder-sdk-node');\n\nconst client = new RudderAnalytics(WRITE_KEY, {\n  dataPlaneUrl: DATA_PLANE_URL, // default: https://hosted.rudderlabs.com\n\n  // More initialization options\n});\n```\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n\n3.  Implement `identify` and `track` calls in your server-side code:\n\n```\nclient.identify({\n  userId: \"1hKOmRA4GRlm\",\n  traits: {\n    name: \"Alex Keener\",\n    email: \"alex@example.com\",\n    plan: \"Free\",\n    friends: 21,\n  },\n})\n```\n\n```\nclient.track({\n  userId: \"1hKOmRA4GRlm\",\n  event: \"Item Viewed\",\n  properties: {\n    revenue: 19.95,\n    shippingMethod: \"Premium\",\n  },\n})\n```\n\n4.  Verify the data flow by checking the **Live Events** tab in your [RudderStack dashboard](https://app.rudderstack.com/).\n\n## Historical data imports\n\nIf you have a significant amount of historical data in your Segment warehouse **and** you are moving warehouses, you should migrate your historical Segment data into your new warehouse (Snowflake, BigQuery, Databricks, etc.) using your preferred ETL tool.\n\nOnce data is in your new data warehouse, you can combine SQL tables for a unified historical view of your customer data.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> If you are **not** changing data warehouses, you don’t need to set up pipelines.\n\n## Migrate user traits and event properties\n\nWhen migrating from Segment to RudderStack, follow these steps to ensure that all your custom user traits and event properties are properly mapped and migrated:\n\n1.  Identify the key user traits and event properties that you want to migrate from Segment to RudderStack. The easiest way to do this is to set up [Segment as a source](https://www.rudderstack.com/docs/sources/event-streams/cloud-apps/segment/) in a [RudderStack Tracking Plan](https://www.rudderstack.com/docs/data-governance/tracking-plans/).\n    \n2.  Create new events in the [RudderStack Data Catalog](https://www.rudderstack.com/docs/data-governance/data-catalog/):\n    \n\n[![Add new event](https://www.rudderstack.com/docs/images/data-governance/new-event-1.webp)](https://www.rudderstack.com/docs/images/data-governance/new-event-1.webp)\n\n3.  Update your RudderStack SDK implementation to use the mapped trait and property names, ensuring your data is correctly structured and labeled in RudderStack.\n4.  If you have any downstream destinations or data, consumers that rely on the Segment trait and property names, update their configurations to use the new RudderStack names.\n5.  Verify that your user traits and event properties are correctly captured and forwarded by RudderStack by inspecting your live events stream and destination data.\n\nBy carefully mapping and migrating your user traits and event properties, you can maintain data consistency and ensure that your downstream systems continue to function as expected.\n\n## Handle custom events and mappings\n\nIf your Segment implementation includes custom events or mappings, you can set up Segment as a RudderStack source to import your existing mappings into a RudderStack tracking plan and manage them in the RudderStack data catalog. Otherwise, you must recreate your schema in RudderStack to ensure a seamless migration by following these steps:\n\n1.  Identify any custom events or mappings in your Segment implementation like custom page or screen events, e-commerce events, or user attribute mappings.\n2.  For each custom event, create an equivalent event in RudderStack using the `track` method and the appropriate event name and properties. For example:\n\n```\n// Segment\n\nanalytics.track(\"Custom Event Name\", {\n  property1: \"value1\",\n  property2: \"value2\"\n});\n\n// RudderStack \n\nrudderanalytics.track(\"Custom Event Name\", {\n  property1: \"value1\",\n  property2: \"value2\"\n});\n```\n\n3.  If you have any custom mappings or transformations in Segment like user attribute synchronization or event property renaming, implement them in RudderStack using the JavaScript SDK’s `identify` and `track` methods or by leveraging RudderStack’s server-side transformations.\n4.  Test your custom events and mappings thoroughly to ensure that they are being correctly captured and processed by RudderStack.\n5.  Update any downstream systems or integrations that rely on the custom events or mappings to use the new RudderStack event names and property structures.\n\nBy handling custom events and mappings during the migration process, you can ensure that your data remains consistent and actionable across your entire stack.\n\n## Next steps\n\n[Phase 2: Advanced migration techniques](https://www.rudderstack.com/docs/user-guides/migration-guides/segment-rudderstack-migration/advanced-migration/)\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Phase 1: Basic SDK, Event, and Property migration | RudderStack Docs",
    "description": "Update your SDK, event, and property implementations while migrating from Segment to RudderStack.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/user-guides/migration-guides/segment-rudderstack-migration/migration-testing-validation/",
    "markdown": "# Phase 3: Test and Validate Your Migration\n\nVerify data flow, identify and resolve discrepancies, and monitor for post-migration anomalies.\n\n* * *\n\n*     3 minute read  \n    \n\nThorough testing and validation are critical to successfully migrating from Segment to RudderStack. This guide covers some critical steps to verify your data flow, identify and resolve discrepancies, and monitor for any post-migration anomalies.\n\n## Verify successful event streaming to destinations\n\n**Use RudderStack Live Events for real-time event monitoring**\n\nRudderStack’s [Live Events](https://www.rudderstack.com/docs/dashboard-guides/live-events/) feature allows you to monitor your event stream in real-time, helping you verify that events are being successfully captured and forwarded to your destinations.\n\nUse the destination live event viewer to inspect individual event payloads, check event volumes, and identify any gaps or delays in your data flow.\n\n**Debugging and troubleshooting**\n\nIf you encounter any issues with your event streaming, RudderStack has this helpful article to help you quickly identify and resolve the problem. RudderStack provides detailed logs and error messages for each event, helping you pinpoint the source of the issue and take corrective action.\n\n## Compare data between Segment and RudderStack\n\n**Set up data validation workflows**\n\nTo ensure that your data is being accurately migrated from Segment to RudderStack, set up data validation workflows that compare your Segment and RudderStack data side-by-side. This can involve running SQL queries on your Segment and RudderStack warehouses to compare event counts, property values, and user traits.\n\n**Identify and resolve data discrepancies**\n\nIf you identify any discrepancies between your Segment and RudderStack data during the validation process, investigate the root cause and take corrective action. This may involve updating your RudderStack SDK implementation, modifying your data transformation logic, or adjusting your destination configurations.\n\n**Integrate and validate data in your cloud tools**\n\nWhile data warehouse validation is crucial, double-checking your data in downstream tools like Amplitude, Braze, and Google Analytics is also essential. Spot-checking key metrics and dimensions can cause any discrepancies to surface early on. Also, make sure to double-check RudderStack documentation.\n\nRudderStack’s native integrations often take a different approach than Segment with additional configuration and features. Some examples are:\n\n*   **Google Analytics (GA4)**: Hybrid mode is a [RudderStack connection mode](https://www.rudderstack.com/docs/destinations/rudderstack-connection-modes/), allowing you to send your event data to Google Analytics 4 (GA4) via the native SDK (device mode) and the Google Analytics 4 Measurement Protocol. In hybrid mode, RudderStack sends specific event data to the destination directly from your client (like UTM parameters) while routing the remaining events through the RudderStack server (like page call information). RudderStack then stitches both data sources together automatically.\n*   **Braze**: RudderStack’s [Braze deduplication](https://www.rudderstack.com/docs/destinations/streaming-destinations/braze/deduplication/) feature prevents duplicate data from being sent to Braze, thereby saving data points and avoiding unnecessary billing overages. This feature is handy in Reverse ETL scenarios where a large number of rows are sent with duplicate columns. It is available in both cloud and device mode for Event Stream and for Reverse ETL pipelines.\n\n## Monitor for post-migration anomalies\n\n**Define key metrics and thresholds**\n\nOnce you have fully migrated to RudderStack, define a set of key metrics and thresholds that you can use to monitor the health and quality of your data. This may include metrics such as event volume, property completeness, and user trait consistency.\n\n**Set up alerts for data quality issues**\n\nUse RudderStack’s alerting features to set up notifications for when your key metrics fall outside of your defined thresholds. This helps you quickly identify and respond to any data quality issues that may arise post-migration, ensuring that your data remains accurate and actionable.\n\nImplementing a robust testing and validation process and monitoring your data quality post-migration can ensure a smooth and successful transition from Segment to RudderStack.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Phase 3: Test and Validate Your Migration | RudderStack Docs",
    "description": "Verify data flow, identify and resolve discrepancies, and monitor for post-migration anomalies.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.14/how-profiles-works/",
    "markdown": "# How Profiles Works | RudderStack Docs\n\nKnow how Profiles collect, unify, and activate your data to enhance the overall customer experience.\n\n* * *\n\n*     3 minute read  \n    \n\nRudderStack helps you build a complete CDP on top of your data warehouse in three stages - **Collect**, **Unify**, and **Activate**.\n\nThe following sections highlight RudderStack’s comprehensive solution at every stage to create a complete customer profile.\n\n[![Profiles Overview](https://www.rudderstack.com/docs/images/profiles/profiles-overview.png)](https://www.rudderstack.com/docs/images/profiles/profiles-overview.png)\n\n### Collect\n\nFirst, RudderStack collects and stores all the source data in your warehouse. This includes:\n\n*   Event data, for example, user interaction data from web and mobile apps.\n*   Data from cloud sources, for example, CRM platforms like Salesforce, support tools like Zendesk, etc.\n\n#### Known data\n\nRudderStack collects all the information from:\n\n1.  First-party data: Data collected from the enterprise’s own mobile application, websites, POS systems, etc.\n2.  Third-party apps like SalesForce CRM, Zendesk Support, ecommerce payments via Stripe, etc.\n\nThere is a known ID for all of these by which RudderStack collects the data like email, user ID, etc.\n\n#### Unknown Data\n\nThis includes unknown user attributes like `anonymousId` captured from the RudderStack SDKs on the web/mobile apps. It is helpful in tracking user activities in cases where they are not logged in.\n\nThe difference between known and unknown data is that in the former, we have information about the user. First-party data can be known data if a user is logged in. For example, the data from cloud sources will always be known. However, data from the event stream sources can be known or unknown depending on whether a user had logged in.\n\nAs the data is collected, you can apply relevant transformations to it for compliance/security purposes like data governance, privacy, etc.\n\nThe below image highlights a snapshot of the `identifies` and `tracks` [tables](https://www.rudderstack.com/docs/destinations/warehouse-destinations/warehouse-schema/#schema), that RudderStack leverages for unifying the data.\n\n[![Identifies and Tracks tables for stitching](https://www.rudderstack.com/docs/images/profiles/identifies-tracks-stitch.png)](https://www.rudderstack.com/docs/images/profiles/identifies-tracks-stitch.png)\n\n### Unify\n\nAt this stage, Profiles takes over and does the following:\n\n#### ID Stitching\n\nProfiles stitches together all the known and unknown IDs into a single table. The IDs are linked using an autogenerated ID known as `rudderId`, which is akin to a golden record. Imagine a 1-to-many relationship, in which one `rudderId` has multiple values for other IDs like user ID, anonymous ID, email, etc.\n\nWith a `rudderId`, you can easily identify that a customer - who shopped on your website 6 months ago, anonymously browsed from mobile 4 months ago, raised a complaint with the support team 2 months ago - is actually the same customer. RudderStack represents them as different nodes/edges of the ID stitching graph.\n\n[![Identity stitching](https://www.rudderstack.com/docs/images/profiles/identity-stitching.png)](https://www.rudderstack.com/docs/images/profiles/identity-stitching.png)\n\n#### Feature Views\n\nIf the entity features/traits are spread across multiple entity vars and ML models, you can use Feature views to get them together into a single view. These models are usually defined in the `pb_project.yaml` file by creating entries under `feature_views` key with corresponding entity.\n\n[![Features generation](https://www.rudderstack.com/docs/images/profiles/features-generation.png)](https://www.rudderstack.com/docs/images/profiles/features-generation.png)\n\n#### Feature Table\n\nThe entity vars specified in the project are unified into a view. A **Feature Table** is a unified customer profile containing useful information for each customer. Once all the known and unknown identities are stitched together, you can trace back activities for all such identifiers and aggregate them under the common `rudderId`. This is helpful in calculating features across all such interactions.\n\nSome common use cases are computing the customer’s total LTV (lifetime value), purchase history, number of days a customer was active, etc.\n\n### Activate\n\nActivation is a two-step process. In the first step, the user creates the target audience using RudderStack’s [Audiences](https://www.rudderstack.com/docs/data-pipelines/reverse-etl/features/audiences/) feature. They would then use Reverse ETL to route this audience information (also persisted in the same cloud warehouse) to downstream marketing tools like Braze, Mailchimp, etc.\n\n[![Audience Builder / Cohorts](https://www.rudderstack.com/docs/images/profiles/audience-builder-cohorts.png)](https://www.rudderstack.com/docs/images/profiles/audience-builder-cohorts.png)\n\nAdditionally - you can use the feature tables as inputs to ML models for use cases like churn prediction, lifetime value (LTV) prediction, etc. Again, you can route the output of such models to marketing platforms via Reverse ETL.\n\nAs you keep collecting data, Profiles continues to unify and send it for activation.\n\n[![Profiles lifecycle](https://www.rudderstack.com/docs/images/profiles/profiles-life-cycle.png)](https://www.rudderstack.com/docs/images/profiles/profiles-life-cycle.png)\n\nIn this way, you can make informed decisions, run personalized marketing campaigns, and enhance the overall customer experience across multiple platforms.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "How Profiles Works | RudderStack Docs",
    "description": "Know how Profiles collect, unify, and activate your data to enhance the overall customer experience.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.14/overview/",
    "markdown": "# Profiles Overview | RudderStack Docs\n\nCreate unified customer records in your warehouse using RudderStack Profiles.\n\n* * *\n\n*     3 minute read  \n    \n\nModern data teams rely on their warehouse as a single source of truth for customer data. RudderStack’s **Profiles** unifies every user touchpoint and trait into comprehensive customer profiles, establishing the data warehouse as the core of the customer data platform.\n\nWith Profiles, data teams can efficiently resolve identities and create user features to produce a comprehensive customer 360 table.\n\n![](https://img.shields.io/badge/Plan-Enterprise-7447fc.svg?logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAJYAAACWCAYAAAA8AXHiAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAl8SURBVHgB7Z1NbFzVFcfPfeOAFGR3WpqU4iwmFIli2tSVgHSXaaVK2TQgGhASSG3KKqlUpwvworhxJCyRZJMQJXSXqRSkCBIriQqq5CiZ7JKYxQiBbaoUJlJMRSqBsQMSMDOPe579wsh4PjJ+c979+P8kZ+zxmzzb9zfnfp+rqAPuH9yepyo9FpLKkaJBCsOsfjpLwAlCRSUV0pxSdJECKl4tnSzSbaLavTA3+Hg2qPUM6RvuJkjkGaqs/ymuyWT2zpROlNt6RasLIBSoJyQ6eEem51ArwZqKxVVeWFXH9H+XIwBuocoqDHdcfbdxFZlp9I37Nj05RDU6QYhS4LtkdUj64/fvGaBPP566uNIFK4p136bte3TV9zIB0ARd3eUbyfUdsZakGiUA2mBJrs+0XJeWPf8ti20qukAA3CY6GP26vs11S6zc4NO5TLV6AQ110BmqXM18/cty6fQcfxXETwe1yhCkAp0T5npqPbvjr6KItRitKh8SAKtjrpqpbOSoFUUsXQXuIQBWTzaOWnFVmCcAEiAMaYgfFXqCIGm4hxjo0fU8AZAgYRA+HujQtYUASJJQ5QKlMBcIkkb9giPWIAGQKGE2IACSB2KB7tBDHrKhfz09sc3dPkvhtbdofv5zShMvxerrvYv+svMpcpXxMxdTF8vLqnB+/iZJMj//BfmGn2ItyBb0/E1ZkU3AU7E+F40iMzPXyDe87RXO/u8GSXH9I7l7mYK3Yl2efI+k4Ajpm1zeijX1fpkkuTI5RT7hrVjnzk+SJNIip423YnH1dPltuepw/EyRfMLrKZ0JwaglLXLaeC0WRxHJYYdXXn2DfMFrsTiKFF77F0nBPVFfeofer26QjlqHPYla3ot1/aP/i0atU1pkH6IW1mNpCsffEo1awyNHyXUgFi22tYb/foSk4LYWy+wyEGuJifNXRAv7lVdfd7pKhFh1cGFPz5RJAo6Su4YOOLtWC2LVwYW9c/d+sUjC0zxjB46Ri0CsZXAv8dk/jYrJxb3E4RG59p0UEGsFINfqgVgNiOWSanOxXNuefN6ZBj3EagLL9bunnheb4+M2F8vswmQ1xGoD7i3mt+4SiSYs8zNarrH9BaujF8RqEy7w/NY/R20hiQI/dvzNKHrZuo4LYt0m3BaKBZvu8qpQlvkFfR+OluOWzTF6uRM6CVgw/hh4IEdPPJanB3+ao80PP0TdIBaM2fzIQ/T7bXl69JEB2nDvejIV9ZOfbw/JALhgeOu7zfT1rqUHtWj80dd3F/XrgufneH39qbNFSpoN967T91gX3a+/f3309+PnOKKmHd2MEav47yNGvwNtwgSxvGxj+biBVBpjxJrV7QhXwU7oFJmdlROLJZZcVbCw4OYKhmYYI5b0u1oyd4PUtJBJGCPW9PuyGVkkC5vfNL7lyDJGLMkkHYz0lvfp//h1BpYxYklnZDl3/gpJMiGcKyJtjBpukJwX49Fsl0VOG6PEuvy2bKofaZGRuyEluJ0l+ceX3oLlU3Vo3Mi7ZBRJI5WRL71D48SS3oIumQFGOglJmhg5VyiZOCON6teHqGWkWBy1JAt7+MUjYoXNUevwP14n1zF2dcPYvgJJwT02ycLmZceu9xCNFSvaJby/QFJIF7ZklEwDo9djpVHYUh0HjpKubq9njF/ot3PogGhhSybq4Lakq3lJjReLG7uS292lE3XwnkUX5bJiaXIauRR27paLXC7KZc2ad+lcCpyI7dnn9ojJ7JpcVm2mSCuXgpTMLNczz42SC2vkrdylI1kA0jLzTIDNW+tjrN3+xQXA++e4wCUEixODSBR4vPOZ23m2Rq/MD340MEoWw+NcvNOYj+Pt71/X1m5q3qXTiSB8j4kLk9Fr+T68e7vdn7GTtWYffDgbzS3yz8s7q9vd0BvNRy6ke9i49WIxXOBcePwH5U0ZX335dSTZnXfeseL1nYpVf79YME4M0qrQOxUrhu/B94p/Zv691v0w2/B6E8RyLikI9+b4g0YWE2hwXgPOZxDnhujtXauHEZL5o0fLm5eSg8QRbPPDAzTwwEbq7Vsb5VVIEr7fS0vTXIu/08bofiw1v5H4dzMFY3I3ALdAfizQFSAW6AoQC3QFiAW6AsQCXQFiga4AsUBXgFigKxgz8v7b3zxq1MixzZzT001JzS50ijFi/e2FPyBrckLwqo+0xUJVCLoCxHKQ+ZvpRisGYjlI2tUgA7Ec47oh+fIhlmPMGrKUGWI5hgnVIAOxHMOULDYQyzGkD2JoBMRyjGnhgxEaAbEcgqtBtLFA4ph0GBTEcgje62gKEMsReCu+9EFXzYBYjnBlUva4mFZALEfgpCUmAbEcgHuDpswRxkAsBzAxlxbEshxutJ+CWCBpDhuatxRiWYyp0YqBWBZjarRiIJaljC8lfDMViGUhXAWaNm61HIhlIYejTNFmjVstB2JZhulVYAzEsgiuAl8SPMNxNUAsS2Cp+MSKtNNstwvEsgCWaVd0bqPZ7ap6IJYFjO37Z3RglE1ALMMZfvEonTp7gWzDuZMpXIGrv+GRIzRx3pzlxrcDxDIQbqhzm8q26q8eiGUYLNOuof1WNdRXAmIZROH4m9aMU7UCYhkAV33DI0eN2mWzWiBWynCU4lNibRn4bBeIlRK8AWJsX8HqBnozIJYwLBRHKJeqvZWAWAJwNRetSjhbNCq/QjeBWF2CZeJD0GOZXGtDtQJiJQT37GZmrtElXdVxjirXq7pWGCMWn7zeZ9GRJzyAubDwBc3O3og+9y0itQKHjYOugNUNoCuwWHMEQMJosRTEAokSKioFiqhEACRIENJcEKqaGYnBgTsouhiomjpNACRJjYqKH/WQw6f6IUsArBalyv99542NweLndIgASIKQivwQiVUJKgcJww4gAdZkMnv5MRKrXDo9h6gFVk0YHpopnSjzp7dG3qOopetHAqATtDtretYcjL+8JVYUtWrhDgKgA9idOFoxmfpvfnJjqnz3PQOf6U+3EgBtokLae/Xdk4X65zLLL/rk46lLd68fUHqQK08AtGBJqtHlz2dWulhHriLkAq1QNfrr1fdOvrzi95q98P6fbc+HgTqmW/s5AiBGN9S5TaUjVbHRJZlmr+c21/d+vOlMUKvq6KV+RcBvFM3pqm9fNVPZ8cE74zPNL22T3ODTuUy1MqoF24II5hksVI0OVXoqB3n0oL2XdABXkXqgIq+H77fo/yEbhjRIwA0Uz8AojkylMKxd01XV6WZVXiO+ASW4fHjWfkysAAAAAElFTkSuQmCC&logoColor=&logoWidth=&style=)  \n\nThe following self-guided tour shows how to use Profiles:\n\n## Highlights\n\nSee the following guides to learn more about Profiles features and their usage:\n\n| Guide | Description |\n| --- | --- |\n| [Quickstart](https://www.rudderstack.com/docs/archive/profiles/0.14/get-started/) | Create your first Profiles project using the [RudderStack dashboard](https://www.rudderstack.com/docs/archive/profiles/0.14/get-started/quickstart-ui/) or [Profile Builder (PB) CLI tool](https://www.rudderstack.com/docs/archive/profiles/0.14/get-started/profile-builder/). |\n| [Identity stitching](https://www.rudderstack.com/docs/archive/profiles/0.14/core-concepts/identity-stitching/) | Stitch different identifiers across multiple channels to create a comprehensive user profile. |\n| [Feature development](https://www.rudderstack.com/docs/archive/profiles/0.14/core-concepts/feature-development/) | Enhance the unified profiles with additional data points and features. |\n| [Warehouse permissions](https://www.rudderstack.com/docs/archive/profiles/0.14/permissions/) | Grant RudderStack the required permissions on your data warehouse. |\n| [Project structure](https://www.rudderstack.com/docs/archive/profiles/0.14/cli-user-guide/structure/) | Know the detailed project structure of a Profiles project. |\n| [Commands](https://www.rudderstack.com/docs/archive/profiles/0.14/cli-user-guide/commands/) | List of commands you can use for the Profiles CLI project. |\n| [Cohorts](https://www.rudderstack.com/docs/archive/profiles/0.14/cohorts/) | Build core customer segments and use them for targeted campaigns. |\n| [Activations](https://www.rudderstack.com/docs/archive/profiles/0.14/activations/) | Activate your cohorts data in the downstream destinations. |\n| [Predictions](https://www.rudderstack.com/docs/archive/profiles/0.14/predictions/) | Create predictive features using the data present in your warehouse. |\n| [Examples](https://www.rudderstack.com/docs/archive/profiles/0.14/example/) | Create sample Profiles projects using different model types. |\n| [Glossary](https://www.rudderstack.com/docs/archive/profiles/0.14/resources/glossary/) | Commonly used Profiles terminology. |\n\n## Why use Profiles?\n\nData teams often face challenges when building a comprehensive customer view. Maintaining large, complex SQL models or working around the limitations of rigid SaaS platforms is time-consuming and expensive at scale.\n\nProfiles simplifies this process of unifying customer data by automating the manual data engineering and modeling required to build an identity graph, layer new data sources into customer profiles, and compute user features that leverage data from diverse sources.\n\nUsing Profiles, data teams can quickly build and easily maintain a comprehensive customer 360 table and make it available to downstream teams and tools.\n\n#### Move faster with an end-to-end platform\n\nProfiles integrates directly with RudderStack’s other pipelines:\n\n*   [Event Stream](https://www.rudderstack.com/docs/sources/event-streams/) and [Cloud Extract](https://www.rudderstack.com/docs/sources/extract/) (ETL) pipelines have known schemas and unique identifiers through the ingested customer data. Profiles can produce a baseline identity graph, and user features out of the box. Data teams can then augment the graph and features using any other data in their warehouse.\n*   [Reverse ETL](https://www.rudderstack.com/docs/sources/reverse-etl/) pipeline makes it easy to send data from the customer 360 table directly to the downstream tools used by marketing, customer success, product, and other teams.\n\n#### Enrich user profiles with features\n\nYou can [enhance user profiles](https://www.rudderstack.com/docs/profiles/core-concepts/feature-development/) with additional data points and features. When new data sources are added, discovered, or calculated, data teams can add them to their Profiles configuration without having to clean data and update complex models and dependencies.\n\nThe features/traits can include demographic information, preferences, purchase history, browsing behavior, or other static or computed data points.\n\n#### Unlock deeper insights\n\nProfiles extends its capabilities to support features derived from complex concepts such as funnels, organizational metrics, and machine learning models. You can understand your customer’s journey through your sales funnel or locate each user across the histogram of customer metric values by simply defining a trait.\n\n#### Deliver personalization and recommendations\n\nUsing Profiles, you can ship projects like personalization significantly faster by focusing entirely on activating key user features instead of cleaning and modeling data to build them.\n\n#### Predict user conversions and churn\n\nYou can leverage [Predictions](https://www.rudderstack.com/docs/archive/profiles/0.14/predictions/) to build predictive features that help you predict in advance whether a lead is likely to convert, or a customer is likely to churn or make a purchase.\n\n## Who can leverage Profiles?\n\nProfiles is built for data engineers, data scientists, and technical marketers. You can define identity stitching, and user features as configuration files without requiring deep SQL, Python, or technical knowledge.\n\nYou can define the associated properties or attributes that provide detailed information for each new feature. For example, if the feature is `purchase_history`, its properties can include the date of purchase, product category, or order value.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Profiles Overview | RudderStack Docs",
    "description": "Create unified customer records in your warehouse using RudderStack Profiles.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.14/get-started/",
    "markdown": "# Quickstart | RudderStack Docs\n\nPrivacy Overview\n\nThis site uses cookies to improve your experience while you navigate through the website. Out of these cookies, the cookies that are categorized as necessary are stored on your browser as they are as essential for the working of basic functionalities of the website. We also use third-party cookies that help us analyze and understand how you use this website. These cookies will be stored in your browser only with your consent. You also have the option to opt-out of these cookies. But opting out of some of these cookies may have an effect on your browsing experience.\n\nNecessary cookies are absolutely essential for the website to function properly. This category only includes cookies that ensures basic functionalities and security features of the website. These cookies do not store any personal information.",
    "title": "Quickstart | RudderStack Docs",
    "description": "Know the different ways to create a Profiles project.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/user-guides/migration-guides/switch-to-rudderstack-hosted-data-plane/",
    "markdown": "# Migrating from self-hosted RudderStack to RudderStack Cloud\n\nPrivacy Overview\n\nThis site uses cookies to improve your experience while you navigate through the website. Out of these cookies, the cookies that are categorized as necessary are stored on your browser as they are as essential for the working of basic functionalities of the website. We also use third-party cookies that help us analyze and understand how you use this website. These cookies will be stored in your browser only with your consent. You also have the option to opt-out of these cookies. But opting out of some of these cookies may have an effect on your browsing experience.\n\nNecessary cookies are absolutely essential for the website to function properly. This category only includes cookies that ensures basic functionalities and security features of the website. These cookies do not store any personal information.",
    "title": "Migrating from self-hosted RudderStack to RudderStack Cloud | RudderStack Docs",
    "description": "Migrate from self-hosted RudderStack control plane and data plane to cloud dashboard.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/resources/rudderstack-architecture/",
    "markdown": "# RudderStack architecture | RudderStack Docs\n\nFamiliarize yourself with RudderStack’s architecture and key components—the control plane and data plane.\n\n* * *\n\n*     4 minute read  \n    \n\nRudderStack is a standalone system dependent only on a database (**PostgreSQL**). Its backend is written in Go, with a rich UI written in React.js.\n\nRudderStack’s architecture consists of 2 major components: the **control plane** and **data plane**, as seen in the following diagram:\n\n[![RudderStack Architecture](https://www.rudderstack.com/docs/images/get-started/rudderstack-architecture.webp)](https://www.rudderstack.com/docs/images/get-started/rudderstack-architecture.webp)\n\n## Control plane\n\nThe control plane offers a UI to configure your event data sources and destinations. It consists of:\n\n*   **RudderStack web app**: The [front-end application](https://app.rudderstack.com/) where you set up and configure your data pipelines in RudderStack.\n*   **Configuration backend**: The web app leverages this module to store all relevant information around your configured sources, destinations, and the connections between them.\n\n## Data plane\n\nThe data plane (backend) is RudderStack’s core engine responsible for:\n\n*   Receiving and processing event data\n*   Transforming events in the required destination format\n*   Relaying events to the destination\n\nThe RudderStack data plane consists of three major components:\n\n*   RudderStack server ([rudder-server](https://github.com/rudderlabs/rudder-server))\n*   Transformations module\n*   Standalone streaming database (PostgreSQL) for event data\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> Refer to the [data plane architecture](#data-plane-architecture) section below for more information on each of these components.\n\n## Data plane architecture\n\nRudderStack’s data plane is responsible for receiving, transforming, and routing event data to the destination. To do so, it receives the event data from sources like websites, mobile apps, server-side applications, cloud apps, and data warehouses.\n\nA simplified version of the RudderStack data plane architecture is as shown:\n\n[![RudderStack backend architecture](https://www.rudderstack.com/docs/images/get-started/rudderstack-backend-architecture.webp)](https://www.rudderstack.com/docs/images/get-started/rudderstack-backend-architecture.webp)\n\nThe following sections give an overview of each of the components of the RudderStack data plane.\n\n### Gateway\n\nThe gateway module is primarily responsible for receiving event data from a source (web, mobile, or server-side) and forwarding it for processing and transformation.\n\nIt accepts event requests and sends an acknowledgment back to the source depending on the acceptance (HTTP 200 response) or rejection of the event data.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> The gateway rejects event data in case of the following scenarios:\n> \n> *   Invalid JSON\n> *   Invalid write key\n> *   Improper request size\n\nThe gateway also temporarily stores all received event data into an internal PostgreSQL database before acknowledging successful receipt. Once event is successfully sent to the destination, it is then deleted from the database.\n\n### Processor\n\nThe processor fetches data from the gateway and forwards it to the transformation module. Once event data is transformed, the processor forwards it to the router module that sends it to the required destination.\n\n### Transformation module\n\nRudderStack’s Transformations module transforms received event data into a suitable destination-specific format. All the transformation code is written in JavaScript.\n\nRudderStack also supports **user transformations** that let you code custom JavaScript functions to implement specific use-cases on your event data. These include but are not limited to: event enrichment, filtering, removing sensitive PII information, etc.\n\n> ![success](https://www.rudderstack.com/docs/images/tick.svg)\n> \n> Refer to the [Transformations](https://www.rudderstack.com/docs/transformations/overview/) guide for more information on user transformations.\n\nOnce events are transformed, the transformation module sends them back to the processor. The processor then forwards this data to the router, which in turn relays it to the desired destination.\n\n### Router\n\nThe router module sends the processed and transformed event data to your desired destinations, like marketing and analytics platforms, CRMs, data warehouses, etc.\n\n### Database\n\nThe data plane uses PostgreSQL as a streaming database for the event data. RudderStack temporarily stores the events in the database so that it can retry sending them in case of delivery failures. The events are deleted once they are successfully delivered.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> RudderStack does not persist any events or store any user information in the database.\n\n## Data plane workflow\n\nThe following steps detail the RudderStack data plane workflow:\n\n1.  The gateway module receives event data from the source.\n    \n2.  The gateway then:\n    \n    *   Stores event data in the router database (PostgreSQL).\n    *   Sends an **HTTP 200** status acknowledging receipt of the data.\n3.  The processor module picks the data from the gateway and forwards it to the transformation module.\n    \n4.  The transformation module transforms the event and sends it back to the processor.\n    \n5.  The processor forwards the transformed event to the router. This event is then deleted from the gateway store.\n    \n6.  The router then:\n    \n    i. Sends the transformed event data to the specified destinations.\n    \n    ii. Stores the event information in a separate table in the database.\n    \n7.  Once the event data reaches the destination, the router deletes it from the database.\n    \n\n[![RudderStack Backend Workflow](https://www.rudderstack.com/docs/images/get-started/rudderstack-backend-workflow.webp)](https://www.rudderstack.com/docs/images/get-started/rudderstack-backend-workflow.webp)\n\n## Customizing the RudderStack data plane\n\nAlthough the default configuration works just fine for most use cases, RudderStack also gives you the flexibility to customize the data plane with a variety of configuration options. Some of these include backing up events to S3, rejecting malicious requests by defining the maximum size of the event, and more.\n\nYou can do so by editing the [`config.yaml`](https://github.com/rudderlabs/rudder-server/blob/master/config/config.yaml) file to suit your application’s needs.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "RudderStack architecture | RudderStack Docs",
    "description": "Familiarize yourself with RudderStack's architecture and key components—the control plane and data plane.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/user-guides/migration-guides/snowplow-migration-guide/",
    "markdown": "# Migrating from Snowplow to RudderStack\n\nMigrate from Snowplow to RudderStack.\n\n* * *\n\n*     13 minute read  \n    \n\nThis document covers the steps required to migrate from [Snowplow](https://snowplow.io/) to RudderStack and replace your instrumentation code. You can then start using the RudderStack SDKs to track your events with minimal code changes.\n\n## Set up your RudderStack dashboard\n\n1.  [Sign up](https://app.rudderstack.com/signup) for RudderStack Cloud.\n2.  Set up the source-destination connections in the dashboard to facilitate event data flow. For more information on setting up connections, refer to the [Quickstart](https://www.rudderstack.com/docs/data-pipelines/event-stream/quickstart/) guide.\n\n> ![success](https://www.rudderstack.com/docs/images/tick.svg)\n\n## Update your SDK implementation\n\nDepending on your target platform, follow these steps to move your existing Snowplow SDK implementation to RudderStack.\n\n### Android\n\nRudderStack distributes the Rudder Snowplow adapter SDK through [Maven Central](https://central.sonatype.com/artifact/com.rudderstack.android.snowplow/adapter/1.0.0.beta.1). We recommended you add the SDK to your project through the Android Gradle build system.\n\n1.  Add the following code in your project level `app/build.gradle` file:\n\n```\nbuildscript {\n  repositories {\n    mavenCentral()\n  }\n}\nallprojects {\n  repositories {\n    mavenCentral()\n  }\n}\n```\n\n2.  Add the following dependency under `dependencies` section in your `app/build.gradle` file:\n\n```\nimplementation 'com.rudderstack.android.snowplow:adapter:1.0.0.beta.1'\n```\n\n3.  Update your SDK initialization to any one of the following snippet (method 1, 2, or 3 in the following snippets). Also, replace the `WRITE_KEY` and `DATA_PLANE_URL` with your source write key and data plane URL obtained in the above section.\n\n```\n//Method 1: Default values are considered for all configuration objects \n//except networkConfiguration.\nRSTracker tracker = new RSTracker()\n            .createTracker(\n                this, WRITE_KEY, networkConfiguration\n            );\n\n//Method 2: Default values are considered for all configuration objects.\nRSTracker tracker = new RSTracker()\n            .createTracker(\n                this, WRITE_KEY, DATA_PLANE_URL\n            );\n\n//Method 3: Values for all configuration objects must be provided.\nRSTracker tracker = new RSTracker()\n            .createTracker(\n                this, WRITE_KEY, networkConfiguration,\n                sessionConfiguration, trackerConfiguration,\n                subjectConfiguration\n            );\n```\n\n```\n//Method 1: Default values are considered for all configuration objects \n//except networkConfiguration.\nval tracker = RSTracker()\n            .createTracker(\n                this, WRITE_KEY, networkConfiguration\n            )\n\n//Method 2: Default values are considered for all configuration objects.\nval tracker = RSTracker()\n            .createTracker(\n                this, WRITE_KEY, DATA_PLANE_URL\n            )\n\n//Method 3: Values for all configuration objects must be provided.\nval tracker = RSTracker()\n            .createTracker(\n                this, WRITE_KEY, networkConfiguration,\n                sessionConfiguration, trackerConfiguration,\n                subjectConfiguration\n            )\n```\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> Refer to the [Setting the configuration objects](#setting-configuration-objects) section for more information on using these configuration objects.\n\n### iOS\n\nFollow these steps to migrate to the RudderStack iOS SDK:\n\n1.  Add the SDK to your `Podfile`:\n\n```\npod 'RudderSnowplowMigrator', '1.0.0.beta.1'\n```\n\n2.  Run the following command:\n\n3.  Add the following code in all `.m` and `.h` files (for Objective-C) or `.swift` files(for Swift) where you want to refer or use RudderStack SDK classes:\n\n```\n@import RudderSnowplowMigrator;\n```\n\n```\nimport RudderSnowplowMigrator\n```\n\n4.  Update your SDK initialization to any one of the following snippet (method 1, 2, or 3). Also, replace the `WRITE_KEY` and `DATA_PLANE_URL` with your source write key and data plane URL obtained in the above section.\n\n```\n//Method 1: Default values are considered for all configuration objects \n//except networkConfiguration.\nRSTracker *tracker = [RSTracker createTrackerWithWriteKey:WRITE_KEY network:networkConfig];\n\n//Method 2: Default values are considered for all configuration objects.\nRSTracker *tracker = [RSTracker createTrackerWithWriteKey:WRITE_KEY dataPlaneUrl:DATA_PLANE_URL];\n\n//Method 3: Values for all configuration objects must be provided.\nRSTracker *tracker = [RSTracker createTrackerWithWriteKey:WRITE_KEY network:networkConfig configurations:@[trackerConfig]];\n```\n\n```\n//Method 1: Default values are considered for all configuration objects \n//except networkConfiguration.\nlet tracker = RSTracker.createTracker(\n            writeKey: WRITE_KEY,\n            network: networkConfig\n        )\n\n//Method 2: Default values are considered for all configuration objects.\nlet tracker = RSTracker.createTracker(\n            writeKey: WRITE_KEY,\n            dataPlaneUrl: DATA_PLANE_URL\n        )\n\n//Method 3: Values for all configuration objects must be provided.\nlet tracker = RSTracker.createTracker(\n            writeKey: WRITE_KEY,\n            network: networkConfig,\n            configurations: [trackerConfig, sessionConfig]\n        )\n```\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> Refer to the the [Setting the configuration objects](#setting-configuration-objects) section for more information on using these configuration objects.\n\n### JavaScript\n\nTo migrate to the RudderStack JavaScript SDK, add the following code snippet in the `<head>` section of your website. Also, replace the `WRITE_KEY` and `DATA_PLANE_URL` with your source write key and data plane URL obtained in the above section.\n\n```\n<script>\n  rs=window.rs=[],rs.snoplowAdapter=function(){rs.push(Array.prototype.slice.call(arguments))},rs.snoplowAdapter(\"newTracker\",<WRITE_KEY>,<DATA_PLANE_URL>,{<Configurations>});\n  //Optionally, use RudderStack JavaScript SDK load options\n  rudderanalytics.load();\n</script>\n\n<script src=\"https://cdn.rudderlabs.com/adapter/sp/beta/v1/rs-sp-analytics.min.js\"></script>\n```\n\nThe `<Configurations>` parameter in the above snippet can be replaced by the following Snowplow properties which are mapped to the RudderStack properties as shown:\n\n| Snowplow property | RudderStack property |\n| --- | --- |\n| `cookieDomain` | `setCookieDomain` |\n| `cookieSameSite` | `sameSiteCookie` |\n| `cookieSecure` | `secureCookie` |\n\n## Setting configuration objects for mobile SDKs\n\nRudderStack supports setting values for the following Snowplow configuration objects in the mobile SDKs (iOS/Android). If not set, the default values are assigned.\n\n### NetworkConfiguration\n\nThe [`NetworkConfiguration`](https://docs.snowplow.io/docs/collecting-data/collecting-from-own-applications/mobile-trackers/configuring-how-events-are-sent/) class can be used to specify the collector endpoint:\n\n```\n// 1\nRSNetworkConfiguration *networkConfig = [[RSNetworkConfiguration alloc] initWithDataPlaneUrl:DATA_PLANE_URL];\n// 2\nRSNetworkConfiguration *networkConfig = [[RSNetworkConfiguration alloc] initWithDataPlaneUrl:DATA_PLANE_URL controlPlaneUrl:CONTROL_PLANE_URL];\n```\n\n```\n// 1\nlet networkConfig = NetworkConfiguration(dataPlaneUrl: DATA_PLANE_URL)\n// 2\nlet networkConfig = NetworkConfiguration(dataPlaneUrl: DATA_PLANE_URL, controlPlaneUrl: CONTROL_PLANE_URL)\n```\n\n```\n// 1\nNetworkConfiguration networkConfiguration = new NetworkConfiguration(DATA_PLANE_URL);\n// 2\nNetworkConfiguration networkConfiguration = new NetworkConfiguration(DATA_PLANE_URL, CONTROL_PLANE_URL);\n```\n\n```\n// 1\nval networkConfiguration = NetworkConfiguration(DATA_PLANE_URL)\n// 2\nval networkConfiguration = NetworkConfiguration(DATA_PLANE_URL, CONTROL_PLANE_URL)\n```\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> If not set, the default values for `dataPlaneUrl` and `controlPlaneUrl` are taken as `https://hosted.rudderlabs.com` and `https://api.rudderlabs.com` respectively.\n\n### SessionConfiguration\n\nThe [`SessionConfiguration`](https://docs.snowplow.io/docs/collecting-data/collecting-from-own-applications/mobile-trackers/tracking-events/#session) class can be used to capture sessions to track the user activity in the app:\n\n```\nRSSessionConfiguration *sessionConfig = [[RSSessionConfiguration alloc] initWithForegroundTimeout:[[NSMeasurement alloc] initWithDoubleValue:30 unit:NSUnitDuration.minutes] backgroundTimeout:[[NSMeasurement alloc] initWithDoubleValue:30 unit:NSUnitDuration.minutes]];\n\n// 2\nRSSessionConfiguration *sessionConfig = [[RSSessionConfiguration alloc] initWithForegroundTimeoutInSeconds:60 backgroundTimeoutInSeconds:60];\n```\n\n```\n// 1\nlet sessionConfig = SessionConfiguration(\n  foregroundTimeout: Measurement(value: 30, unit: .minutes),\n  backgroundTimeout: Measurement(value: 30, unit: .minutes)\n)\n\n// 2\nlet sessionConfig = SessionConfiguration(foregroundTimeoutInSeconds: 60, backgroundTimeoutInSeconds: 60)\n```\n\n```\nSessionConfiguration sessionConfiguration = new SessionConfiguration(\n  new TimeMeasure(1, TimeUnit.MINUTES),\n  new TimeMeasure(1, TimeUnit.MINUTES)\n);\n```\n\n```\nval sessionConfiguration = SessionConfiguration(\n  TimeMeasure(1, TimeUnit.MINUTES),\n  TimeMeasure(1, TimeUnit.MINUTES)\n);\n```\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> You need to pass both the arguments for the `SessionConfiguration` class as shown in the previous code snippets. However, RudderStack ignores the first argument (as it is a placeholder argument) and considers only the second argument (`backgroundTimeout`).\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> For mobile SDKs, Snowplow’s default timeout session is 30 minutes whereas RudderStack’s default timeout session is 5 minutes. Refer to the [RudderStack Session Tracking](https://www.rudderstack.com/docs/sources/event-streams/sdks/session-tracking/#mobile-sdks) documentation for more information.\n\n### TrackerConfiguration\n\nThe [`TrackerConfiguration`](https://docs.snowplow.io/docs/collecting-data/collecting-from-own-applications/mobile-trackers/tracking-events/) class can be used to configure contexts and automatic events of the tracker and the general behavior, as shown in the following snippets:\n\n```\nRSTrackerConfiguration * trackerConfig = [\n  [RSTrackerConfiguration alloc] init\n];\n[trackerConfig base64Encoding: YES];\n[trackerConfig logLevel: LogLevelDebug];\n[trackerConfig sessionContext: YES];\n[trackerConfig deepLinkContext: YES];\n[trackerConfig applicationContext: YES];\n[trackerConfig platformContext: YES];\n[trackerConfig geoLocationContext: NO];\n[trackerConfig screenContext: YES];\n[trackerConfig screenViewAutotracking: YES];\n[trackerConfig lifecycleAutotracking: YES];\n[trackerConfig installAutotracking: YES];\n[trackerConfig exceptionAutotracking: YES];\n[trackerConfig diagnosticAutotracking: NO];\n[trackerConfig userAnonymisation: NO];\n[trackerConfig appId: APP_ID];\n```\n\n```\nlet trackerConfig = TrackerConfiguration()\n  .base64Encoding(false)\n  .logLevel(.debug)\n  .deepLinkContext(true)\n  .applicationContext(true)\n  .platformContext(true)\n  .geoLocationContext(true)\n  .lifecycleAutotracking(true)\n  .diagnosticAutotracking(true)\n  .screenViewAutotracking(true)\n  .screenContext(true)\n  .applicationContext(true)\n  .exceptionAutotracking(true)\n  .installAutotracking(true)\n  .userAnonymisation(false)\n  .appId(APP_ID)\n```\n\n```\nTrackerConfiguration trackerConfiguration = new TrackerConfiguration()\n  .base64Encoding(false)\n  .logLevel(LogLevel.VERBOSE)\n  .deepLinkContext(true)\n  .applicationContext(true)\n  .platformContext(true)\n  .geoLocationContext(true)\n  .lifecycleAutotracking(true)\n  .diagnosticAutotracking(true)\n  .screenViewAutotracking(true)\n  .screenContext(true)\n  .applicationContext(true)\n  .exceptionAutotracking(true)\n  .installAutotracking(true)\n  .userAnonymisation(false)\n  .appId(APP_ID);\n```\n\n```\nval trackerConfiguration = TrackerConfiguration()\n  .base64Encoding(false)\n  .logLevel(LogLevel.VERBOSE)\n  .deepLinkContext(true)\n  .applicationContext(true)\n  .platformContext(true)\n  .geoLocationContext(true)\n  .lifecycleAutotracking(true)\n  .diagnosticAutotracking(true)\n  .screenViewAutotracking(true)\n  .screenContext(true)\n  .applicationContext(true)\n  .exceptionAutotracking(true)\n  .installAutotracking(true)\n  .userAnonymisation(false)\n  .appId(APP_ID)\n```\n\nSnowplow automatically captures and tracks the following data. Refer to the [Auto-tracked events and entities](https://docs.snowplow.io/docs/collecting-data/collecting-from-own-applications/mobile-trackers/tracking-events/#auto-tracked-events-and-entities) section for more information.\n\n| Variable name | Default value |\n| --- | --- |\n| `logLevel` | `LogLevel.OFF` |\n| `lifecycleAutotracking` | `true` |\n| `screenViewAutotracking` | `true` |\n| `sessionContext` | `true` |\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> RudderStack ignores any variable other than the ones mentioned above.\n\n### SubjectConfiguration\n\nThe [`SubjectConfiguration`](https://docs.snowplow.io/docs/collecting-data/collecting-from-own-applications/mobile-trackers/tracking-events/) class can be used to capture basic user information which is attached to all events as the context entity.\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> Snowplow does not provide any call to identify a user. However, RudderStack sends an [](https://www.rudderstack.com/docs/event-spec/standard-events/identify/)call if you initialize the `SubjectConfiguration` class while initializing the SDK. **Note that a user is not identified if the SDK is not initialized**.\n\nThe `userId` field is mapped to the RudderStack’s `userId`. All the other properties are mapped to RudderStack’s `traits` object.\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> `userId` is a mandatory field. If not provided, the `identify` call is ignored.\n\n```\nRSSubjectConfiguration * subjectConfig = [\n  [RSSubjectConfiguration alloc] init\n];\n[subjectConfig userId: @ \"user_id\"];\n[subjectConfig traits: @ {\n  @ \"key_1\": @ \"value_1\", @ \"key_2\": @20, @ \"key_3\": @YES\n}];\n```\n\n```\nlet subjectConfig = SubjectConfiguration()\n  .userId(\"user_id\")\n  .traits([\"key_1\": \"value_1\", \"key_2\": 20, \"key_3\": true])\n```\n\n```\nTraits.Address address = new Traits.Address()\n  .putCity(\"city value\")\n  .putCountry(\"country value\")\n  .putPostalCode(\"postalCode value\")\n  .putState(\"state value\")\n  .putStreet(\"street value\");\n\nTraits.Company company = new Traits.Company()\n  .putName(\"name value\")\n  .putId(\"id value\")\n  .putIndustry(\"industry value\");\n\nTraits traits = new Traits()\n  .putCompany(company)\n  .putAddress(address)\n  .putAge(\"age value\")\n  .putBirthday(\"birthday value\")\n  .putCreatedAt(\"createAt value\")\n  .putDescription(\"description value\")\n  .putEmail(\"email value\")\n  .putFirstName(\"fName value\")\n  .putGender(\"gender value\")\n  .putId(\"id value\")\n  .putLastName(\"lName value\")\n  .putName(\"name value\")\n  .putPhone(\"phone value\")\n  .putTitle(\"title value\")\n  .putUserName(\"userName value\")\n  .put(\"Key-1\", \"value-1\")\n  .put(\"Key-2\", 20)\n  .put(\"Key-3\", true);\n\nSubjectConfiguration subjectConfiguration = new SubjectConfiguration()\n  .userId(\"User-1\")\n  .traits(traits);\n```\n\n```\nval address: Traits.Address = Traits.Address()\n  .putCity(\"city value\")\n  .putCountry(\"country value\")\n  .putPostalCode(\"postalCode value\")\n  .putState(\"state value\")\n  .putStreet(\"street value\")\n\nval company: Traits.Company = Traits.Company()\n  .putName(\"name value\")\n  .putId(\"id value\")\n  .putIndustry(\"industry value\")\n\nval traits = Traits()\n  .putCompany(company)\n  .putAddress(address)\n  .putAge(\"age value\")\n  .putBirthday(\"birthday value\")\n  .putCreatedAt(\"createAt value\")\n  .putDescription(\"description value\")\n  .putEmail(\"email value\")\n  .putFirstName(\"fName value\")\n  .putGender(\"gender value\")\n  .putId(\"id value\")\n  .putLastName(\"lName value\")\n  .putName(\"name value\")\n  .putPhone(\"phone value\")\n  .putTitle(\"title value\")\n  .putUserName(\"userName value\")\n  .put(\"Key-1\", \"value-1\")\n  .put(\"Key-2\", 20)\n  .put(\"Key-3\", true)\n\nval subjectConfiguration = SubjectConfiguration()\n  .userId(\"User-1\")\n  .traits(traits)\n```\n\n## Updating class names\n\nThe following table lists the corresponding class names in Snowplow and RudderStack (mobile SDKs) which need to be updated based on your platform:\n\n| Snowplow |     | RudderStack |     |     |\n| --- | --- | --- | --- | --- |\n|     | **Java** | **Kotlin** | **Objective-C** | **Swift** |\n| `NetworkConfiguration` | `NetworkConfiguration` | `NetworkConfiguration` | `RSNetworkConfiguration` | `NetworkConfiguration` |\n| `TrackerConfiguration` | `TrackerConfiguration` | `TrackerConfiguration` | `RSTrackerConfiguration` | `TrackerConfiguration` |\n| `SessionConfiguration` | `SessionConfiguration` | `SessionConfiguration` | `RSSessionConfiguration` | `SessionConfiguration` |\n| `SubjectConfiguration` | `SubjectConfiguration` | `SubjectConfiguration` | `RSSubjectConfiguration` | `SubjectConfiguration` |\n| `Structured` | `Structured` | `Structured` | `RSStructured` | `Structured` |\n| `ScreenView` | `ScreenView` | `ScreenView` | `RSScreenView` | `ScreenView` |\n| `Background` | `Background` | `Background` | `RSBackground` | `Background` |\n| `Foreground` | `Foreground` | `Foreground` | `RSForeground` | `Foreground` |\n| `SelfDescribing` | `SelfDescribing` | `SelfDescribing` | `RSSelfDescribing` | `SelfDescribing` |\n| `SelfDescribingJson` | `SelfDescribingJson` | `SelfDescribingJson` | `RSSelfDescribingJson` | `SelfDescribingJson` |\n| `TimeMeasure` | `TimeMeasure` | `TimeMeasure` | `N/A` | `N/A` |\n| `Snowplow` | `RSTracker` | `RSTracker` | `RSTracker` | `RSTracker` |\n| `LogLevel` | `LogLevel` | `LogLevel` | `LogLevel` | `LogLevel` |\n\n## Sending event data\n\nMigrate your existing events from Snowplow to RudderStack by following the below sections:\n\n### Identifying a user\n\n#### iOS/Android\n\nSnowplow’s [`SubjectConfiguration`](https://docs.snowplow.io/docs/collecting-data/collecting-from-own-applications/mobile-trackers/tracking-events/) class can be used to identify users. See the [Setting configuration objects](#setting-configuration-objects) section for more information.\n\n#### JavaScript\n\nSnowplow supports identifying a user by passing the `setUserId` method. A Snowplow event including the `setUserId` method triggers the [`identify`](https://www.rudderstack.com/docs/event-spec/standard-events/identify/) call in the RudderStack JavaScript SDK.\n\nIn the following sample call, `alex@example.com` is the `userId`, while `firstName`, `lastName`, and `city` are the user traits.\n\n```\nrs.snowplowAdapter('setUserId', 'alex@example.com', {\n  firstName: 'Alex',\n  lastName: 'Keener',\n  city: 'New Orleans'\n});\n```\n\n### Tracking user actions\n\n#### iOS/Android\n\n##### Custom structured events\n\nRudderStack sends a [`track`](https://www.rudderstack.com/docs/event-spec/standard-events/track/) call for Snowplow events containing the [`Structured`](https://docs.snowplow.io/docs/collecting-data/collecting-from-own-applications/mobile-trackers/tracking-events/#creating-a-structured-event) class.\n\nIn the following example, RudderStack maps `Action_example` to the RudderStack event name and the rest of the properties like `Category_example`, `label`, `value`, etc. to the RudderStack properties.\n\n```\nRSStructured * structured = [\n  [RSStructured alloc] initWithCategory: @ \"Category_example\"\n  action: @ \"Action_example\"\n];\n[structured label: @ \"my-label\"];\n[structured property: @ \"my-property\"];\n[structured value: @5];\n[structured properties: @ {\n  @ \"key_1\": @ \"value_1\", @ \"key_2\": @ \"value_2\"\n}];\n\n[tracker track: structured];\n```\n\n```\nlet structured = Structured(category: \"Category_example\", action: \"Action_example\")\n  .label(\"my-label\")\n  .property(\"my-property\")\n  .value(5)\n  .properties([\"key_1\": \"value_1\", \"key_2\": \"value_2\"])\n\ntracker.track(structured)\n```\n\n```\nHashMap  properties = new HashMap  ();\nproperties.put(\"key-1\", \"value 1\");\nproperties.put(\"key-2\", 123);\nproperties.put(\"key-3\", 123.45);\nproperties.put(\"key-4\", true);\nproperties.put(\"key-5\", false);\n\nStructured structured = new Structured(\"Category_example\", \"Action_example\")\n  .label(\"my-label\")\n  .value(1234.23)\n  .property(\"my-property\")\n  .properties(properties);\n\ntracker.track(structured);\n```\n\n```\nval properties = mapOf<string any=\"\">(\n  \"key-1\" to \"value 1\",\n  \"key-2\" to 123,\n  \"key-3\" to 123.45,\n  \"key-4\" to true,\n  \"key-5\" to false\n)\n\nval structured = Structured(\"Category_example\", \"Action_example\")\n  .label(\"my-label\")\n  .value(1234.23)\n  .property(\"my-property\")\n  .properties(properties)\n\ntracker.track(structured)\n```\n\n##### Custom self-described events\n\nRudderStack sends a [track](https://www.rudderstack.com/docs/event-spec/standard-events/track/) call for Snowplow events containing the [`SelfDescribing`](https://docs.snowplow.io/docs/collecting-data/collecting-from-own-applications/mobile-trackers/tracking-events/#creating-a-structured-event) class.\n\nIn the following example, RudderStack maps `action` to the RudderStack event name.\n\n```\n// 1\nRSSelfDescribingJson * selfDescribingJson = [\n  [RSSelfDescribingJson alloc] initWithSchema: @ \"schema\"\n  andDictionary: @ {\n    @ \"action\": @ \"Action_2\", @ \"key_2\": @ \"value_2\"\n  }\n];\nRSSelfDescribing * selfDescribing = [\n  [RSSelfDescribing alloc] initWithEventData: selfDescribingJson\n];\n\n// 2\nRSSelfDescribing * selfDescribing = [\n  [RSSelfDescribing alloc] initWithSchema: @ \"schema\"\n  payload: @ {\n    @ \"action\": @ \"Action_2\", @ \"key_2\": @ \"value_2\"\n  }\n];\n\n[tracker track: selfDescribing];\n```\n\n```\n// 1\nlet selfDescribingJson = SelfDescribingJson(schema: \"schema\", andDictionary: [\"action\": \"Action_2\"])\nlet selfDescribing = SelfDescribing(eventData: selfDescribingJson)\n\n// 2\nlet selfDescribing = SelfDescribing(schema: \"schema\", payload: [\"action\": \"Action_2\"])\n\ntracker.track(selfDescribing)\n```\n\n```\nHashMap  properties = new HashMap  ();\nproperties.put(\"action\", \"Action-2\");\nproperties.put(\"key-1\", \"value 1\");\n\n// 1\nSelfDescribingJson selfDescribingJson = new SelfDescribingJson(\"schema\", payload)\nSelfDescribing selfDescribing = new SelfDescribing(selfDescribingJson)\n\n// 2\nSelfDescribing selfDescribing = new SelfDescribing(\"schema\", payload)\n\ntracker.track(selfDescribing);\n```\n\n```\nval payload = mapOf  (\n\t  \"action\" to \"Action-2\",\n\t  \"key-2\" to \"value-2\"\n\t)\n\n// 1\nval selfDescribingJson = SelfDescribingJson(\"schema\", payload)\nval selfDescribing = SelfDescribing(selfDescribingJson)\n\n// 2\nval selfDescribing = SelfDescribing(\"schema\", payload)\n\ntracker.track(selfDescribing)\n```\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> `action` is a mandatory field. RudderStack does not send any call if it is absent.\n\n##### Custom foreground events\n\nRudderStack sends a [track](https://www.rudderstack.com/docs/event-spec/standard-events/track/) call for Snowplow events containing the [`Foreground`](https://docs.snowplow.io/docs/collecting-data/collecting-from-own-applications/mobile-trackers/previous-versions/objective-c-tracker/objective-c-1-0-0/#foreground-and-background-events) class.\n\nRudderStack sends the event name as `Application Opened` and maps the rest of the properties like `index`, `properties`, etc. to the RudderStack properties.\n\n```\nRSForeground * foreground = [\n  [RSForeground alloc] initWithIndex: @1\n];\n[foreground properties: @ {\n  @ \"key_1\": @ \"value_1\"\n}];\n\n[tracker track: foreground];\n```\n\n```\nlet foreground = Foreground(index: 1)\n  .properties([\"key_1\": \"value_1\"])\n\ntracker.track(foreground)\n```\n\n```\nHashMap  properties = new HashMap  ();\nproperties.put(\"key-1\", \"value 1\");\nproperties.put(\"key-2\", 123);\nproperties.put(\"key-3\", 123.45);\nproperties.put(\"key-4\", true);\nproperties.put(\"key-5\", false);\n\nForeground foreground = new Foreground(1234);\nforeground.setProperties(properties);\n\ntracker.track(foreground);\n```\n\n```\nval properties = mapOf<string any=\"\">(\n  \"key-1\" to \"value 1\",\n  \"key-2\" to 123,\n  \"key-3\" to 123.45,\n  \"key-4\" to true,\n  \"key-5\" to false\n)\n\nForeground foreground = Foreground(1234)\nforeground.properties = properties\n\ntracker.track(background)\n```\n\n##### Custom background events\n\nRudderStack sends a [track](https://www.rudderstack.com/docs/event-spec/standard-events/track/) call for Snowplow events containing the [`Background`](https://docs.snowplow.io/docs/collecting-data/collecting-from-own-applications/mobile-trackers/previous-versions/objective-c-tracker/objective-c-1-0-0/#foreground-and-background-events) class.\n\nRudderStack sends the event name as `Application Backgrounded` and maps the rest of the properties, like `index`, `properties`, etc. to the RudderStack properties.\n\n```\nRSBackground * background = [\n  [RSBackground alloc] initWithIndex: @1\n];\n[background properties: @ {\n  @ \"key_1\": @ \"value_1\"\n}];\n\n[tracker track: background];\n```\n\n```\nlet background = Background(index: 1)\n  .properties([\"key_1\": \"value_1\"])\n\ntracker.track(background)\n```\n\n```\nHashMap  properties = new HashMap  ();\nproperties.put(\"key-1\", \"value 1\");\nproperties.put(\"key-2\", 123);\nproperties.put(\"key-3\", 123.45);\nproperties.put(\"key-4\", true);\nproperties.put(\"key-5\", false);\n\nBackground background = new Background(1234);\nbackground.setProperties(properties);\n\ntracker.track(background);\n```\n\n```\nval properties = mapOf<string any=\"\">(\n  \"key-1\" to \"value 1\",\n  \"key-2\" to 123,\n  \"key-3\" to 123.45,\n  \"key-4\" to true,\n  \"key-5\" to false\n)\n\nBackground background = Background(1234)\nbackground.properties = properties\n\ntracker.track(background)\n```\n\n#### JavaScript\n\nThe RudderStack JavaScript SDK supports Snowplow’s `trackStructEvent` and `trackSelfDescribingEvent` calls. These Snowplow calls capture user events along with their associated properties and trigger the [`track`](https://www.rudderstack.com/docs/event-spec/standard-events/track/) call in the RudderStack JavaScript SDK.\n\nA sample `trackSelfDescribingEvent` call:\n\n```\nrs.snowplowAdapter('trackSelfDescribingEvent', {\n  event: {\n    data: {\n      action: 'order completed',\n      category: 'FCW',\n      product_id: 'P1100DFG9766',\n      revenue: 30,\n      currency: 'USD',\n      user_actual_id: 12345,\n    },\n  },\n}); \n```\n\nIn the previous code snippet, the `trackSelfDescribingEvent` method tracks the `Order Completed` event along with other information like `revenue`, `currency`, and `user_actual_id`, etc.\n\nA sample `trackStructEvent` call:\n\n```\nrs.snowplowAdapter('trackStructEvent', {\n  action: 'order completed',\n  category: 'FCW',\n  label: 'Sample label',\n  property: 'Some property',\n  value: 40.0,\n});\n```\n\nIn the previous code snippet, the `trackStructEvent` method tracks the `Order Completed` event along with other information like `label`, `property`, and `value`, etc.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> The `action` field is mandatory in both calls.\n\n### Tracking page or screen views\n\n#### iOS/Android\n\nSnowplow’s [`ScreenView`](https://docs.snowplow.io/docs/collecting-data/collecting-from-own-applications/mobile-trackers/tracking-events/#creating-a-structured-event) class captures whenever a new screen is loaded. A Snowplow event including the `ScreenView` class triggers the [`screen`](https://www.rudderstack.com/docs/event-spec/standard-events/screen/) call in the RudderStack iOS or Android SDK.\n\nRudderStack maps the `name` property to RudderStack event name and the rest of the properties, like `screenId`, `previousName`, `previousId`, etc. to the RudderStack properties.\n\n```\nRSScreenView * screen = [\n  [RSScreenView alloc] initWithName: @ \"DemoScreenName\"\n  screenId: [\n    [NSUUID alloc] init\n  ]\n];\n[screen type: @ \"type\"];\n[screen previousName: @ \"previousName\"];\n[screen previousId: @ \"previousId\"];\n[screen previousType: @ \"previousType\"];\n[screen transitionType: @ \"transitionType\"];\n[screen viewControllerClassName: @ \"viewControllerClassName\"];\n[screen topViewControllerClassName: @ \"topViewControllerClassName\"];\n[screen properties: @ {\n  @ \"key_1\": @ \"value_1\",\n    @ \"key_2\": @ \"value_2\"\n}];\n\n[tracker track: screen];\n```\n\n```\nlet screen = ScreenView(name: \"DemoScreenName\", screenId: UUID())\n  .type(\"type\")\n  .previousName(\"previousName\")\n  .previousId(\"previousId\")\n  .previousType(\"previousType\")\n  .transitionType(\"transitionType\")\n  .viewControllerClassName(\"viewControllerClassName\")\n  .topViewControllerClassName(\"topViewControllerClassName\")\n  .properties: ([\n    \"key_1\": \"value_1\",\n    \"key_2\": \"value_2\"\n  ])\n\ntracker.track(screen)\n```\n\n```\nScreenView screenView = new ScreenView(\"MainActivity\", UUID.randomUUID())\n  .type(\"type\")\n  .previousName(\"previousName\")\n  .previousId(\"previousId\")\n  .previousType(\"previousType\")\n  .transitionType(\"transitionType\");\n\ntracker.track(screenView);\n```\n\n```\nval screenView = ScreenView(\"MainActivity\", UUID.randomUUID())\n  .type(\"type\")\n  .previousName(\"previousName\")\n  .previousId(\"previousId\")\n  .previousType(\"previousType\")\n  .transitionType(\"transitionType\")\n\ntracker.track(screenView)\n```\n\n#### JavaScript\n\nSnowplow’s `trackPageView` call lets you record your website’s page views with any additional relevant information about the viewed page. A Snowplow event including the `trackPageView` call triggers the [screen](https://www.rudderstack.com/docs/event-spec/standard-events/screen/) call in the RudderStack JavaScript SDK.\n\nA sample `trackPageView` call:\n\n```\nrs.snowplowAdapter(\n  'trackPageView',\n  {\n    title: 'Cart Viewed',\n  },\n  {\n    path: '/best-seller/1',\n    referrer: 'https://www.google.com/search?q=estore+bestseller',\n    search: 'estore bestseller',\n    title: 'The best sellers offered by EStore',\n    url: 'https://www.estore.com/best-seller/1',\n  },\n);\n```\n\nIn the previous code snippet, the SDK captures the page title and other [contextual information](https://www.rudderstack.com/docs/event-spec/standard-events/common-fields/#contextual-fields).\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Migrating from Snowplow to RudderStack | RudderStack Docs",
    "description": "Migrate from Snowplow to RudderStack.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/resources/community/",
    "markdown": "# Community | RudderStack Docs\n\nPrivacy Overview\n\nThis site uses cookies to improve your experience while you navigate through the website. Out of these cookies, the cookies that are categorized as necessary are stored on your browser as they are as essential for the working of basic functionalities of the website. We also use third-party cookies that help us analyze and understand how you use this website. These cookies will be stored in your browser only with your consent. You also have the option to opt-out of these cookies. But opting out of some of these cookies may have an effect on your browsing experience.\n\nNecessary cookies are absolutely essential for the website to function properly. This category only includes cookies that ensures basic functionalities and security features of the website. These cookies do not store any personal information.",
    "title": "Community | RudderStack Docs",
    "description": "Find help, contribute to RudderStack, and participate in the community.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.14/get-started/profile-builder/",
    "markdown": "# Profile Builder CLI | RudderStack Docs\n\nCreate a Profiles project using the Profile Builder (PB) tool.\n\n* * *\n\n*     10 minute read  \n    \n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> While creating a Profiles project, you can choose either of the below:\n> \n> *   **Profile Builder (PB) CLI** which gives you the flexibility to create, develop, and debug your Profiles project using various commands in fine detail. You can explore and implement the exhaustive list of features and functionalities offered by Profiles.\n> *   **Profiles UI** which provides a step-by-step intuitive workflow in the RudderStack dashboard. You can configure your project, schedule its run, explore the outputs and the user profiles.\n\n**Profile Builder (PB)** is a command-line interface (CLI) tool that simplifies data transformation within your warehouse. It generates customer profiles by stitching data together from multiple sources.\n\nThis guide lists the detailed steps to install and use the Profile Builder (PB) tool to create, configure, and run a new project.\n\n## Prerequisites\n\nYou must have:\n\n*   [Python 3](https://www.python.org/downloads/) installed on your machine.\n*   Admin privileges on your machine.\n\n## Steps\n\nTo set up a project using the PB tool, follow these steps:\n\n### 1: Install PB\n\nInstall the Profile Builder tool by running the following command:\n\n```\npip3 install profiles-rudderstack\n```\n\nIf you have already installed PB, use the following command to update its version:\n\n```\npip3 install profiles-rudderstack -U\n```\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> RudderStack recommends using a Python virtual environment to maintain an isolated and clean environment.\n> \n> ```\n> pipx install profiles-rudderstack\n> ```\n\nValidate Profile Builder’s version after install using:\n\nSee also: [Setup and installation FAQ](https://www.rudderstack.com/docs/archive/profiles/0.14/faq/#setup-and-installation)\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> If you are an existing user, migrate your project to the new schema. See [Migrate your existing project](#migrate-your-existing-project) for more information.\n\n### 2: Create warehouse connection\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> RudderStack supports **Snowflake**, **Redshift**, **BigQuery**, and **Databricks** warehouses for Profiles. You must grant certain [warehouse permissions](https://www.rudderstack.com/docs/archive/profiles/0.14/permissions/) to let RudderStack read from schema having the source tables (for example, `tracks` and `identifies` tables generated via Event Stream sources), and write data in a new schema created for Profiles.\n\nCreate a warehouse connection to allow PB to access your data:\n\nThen, follow the prompts to enter details about your warehouse connection.\n\nA sample connection for a Snowflake account is as follows:\n\n```\nEnter Connection Name: test\nEnter target:  (default:dev):  # Press enter, leaving it to default\nEnter account: ina13147.us-east-1\nEnter warehouse: rudder_warehouse\nEnter dbname: your_rudderstack_db \nEnter schema: rs_profiles # A segregated schema for storing tables/views created by Profiles\nEnter user: profiles_test_user\nEnter password: <password>\nEnter role: profiles_role\nAppend to /Users/<user_name>/.pb/siteconfig.yaml? [y/N]\ny\n```\n\n*   **Connection Name**: Name of the connection in the project file.\n*   **Target**: Environment name, such as `dev`, `prod`, `test`, etc. You can specify any target name and create a separate connection for the same.\n*   **Account**: Name of your [Snowflake account](https://docs.snowflake.com/en/user-guide/admin-account-identifier.html). Based on your cloud platform and region, you might need to append `.aws`, `.gcp`, or `.azure` in your account name. See [Snowflake documentation](https://docs.snowflake.com/en/user-guide/admin-account-identifier#non-vps-account-locator-formats-by-cloud-platform-and-region) for more information.\n*   **Warehouse**: Name of the warehouse.\n*   **Database name**: Name of the database inside warehouse where model outputs will be written.\n*   **Schema**: Name of the schema inside database where you’ll store identity stitcher and entity features.\n*   **User**: Name of the user in data warehouse.\n*   **Password**: Password for the above user.\n*   **Role**: Name of the user role.\n\nRudderStack supports various user authentication mechanisms for Redshift:\n\n```\nEnter Connection Name: test\nEnter target:  (default:dev):  # Press enter, leaving it to default\nEnter dbname: your_rudderstack_db \nEnter schema: rs_profiles # A segregated schema for storing tables/views created by Profiles\nEnter user: profiles_test_user\nEnter sslmode: options - [disable require]: disable # Enter \"require\" in case your Redshift connection mandates sslmode. \nHow would you like to authenticate with the warehouse service? Please select your method by entering the corresponding number:\ny\n[1] Warehouse Credentials: Log in using your username and password.\n        Format: [Username, Password]\n[2] AWS Programmatic Credentials: Authenticate using one of the following AWS credentials methods:\n        a) Direct input of AWS Access Key ID, Secret Access Key, and an optional Session Token.\n                Format: [AWS Access Key ID, Secret Access Key, Session Token (Optional)]\n        b) AWS configuration profile stored on your system.\n                Format: [AWS Configuration Profile Name]\n        c) Use an AWS Secrets Manager ARN to securely retrieve credentials.\n                Format: [Secret ARN]\n```\n\n*   **Connection Name**: Name of the connection in the project file.\n*   **Target**: Environment name, such as `dev`, `prod`, `test`, etc. You can specify any target name and create a separate connection for the same.\n*   **Host**: Log in to AWS Console and go to **Clusters** to know about host.\n*   **Port**: Port number to connect to the warehouse.\n*   **Database name**: Name of the database inside warehouse where model outputs will be written.\n*   **Schema**: Name of the schema inside database where you’ll store identity stitcher and entity features.\n*   **User**: Name of the user in data warehouse.\n*   **Password**: Password for the above user.\n\n### Warehouse credentials\n\nIf you choose to use the warehouse credentials (option 1), enter the following details:\n\n```\nEnter host: warehouseabc.us-west-1.redshift.amazonaws.com\nEnter port: 5439\nEnter password: <password>\nEnter tunnel_info: Do you want to use SSH Tunnel to connect with the warehouse? (y/n): y\nEnter ssh user: user\nEnter ssh host: 1234\nEnter ssh port: 12345\nEnter ssh private key file path: /Users/alex/.ssh/id_ed25519.pub\nAppend to /Users/alex/.pb/siteconfig.yaml? [y/N] y\n```\n\n### AWS Programmatic Credentials\n\nIf you choose to use AWS Access Key ID, Secret Access Key, and an optional Session Token from AWS Programmatic Credentials (option 2a), enter the following details:\n\n**For Redshift Cluster**\n\n```\nWhich Redshift Service you want to connect with?\n        [c]Redshift Cluster\n        [s]Redshift Serverless\nc\nEnter Redshift Cluster Identifier: cluster-id\nEnter access_key_id: aid\nEnter secret_access_key: ***\nEnter session_token: If you are using temporary security credentials, please specify the session token, otherwise leave it empty.\nstoken\nAppend to /Users/alex/.pb/siteconfig.yaml? [y/N] y\n```\n\n**For Redshift Serverless**\n\n```\nWhich Redshift Service you want to connect with?\n        [c]Redshift Cluster\n        [s]Redshift Serverless\ns\nEnter Redshift Serverless Workgroup Name: wg-name\nEnter access_key_id: aid\nEnter secret_access_key: ***\nEnter session_token: If you are using temporary security credentials, please specify the session token, otherwise leave it empty.\nAppend to /Users/alex/.pb/siteconfig.yaml? [y/N] y\n```\n\nIf you choose to use AWS configuration profile stored on your system from AWS Programmatic Credentials (option 2b), enter the following details:\n\n**For Redshift Cluster**\n\n```\nWhich Redshift Service you want to connect with?\n        [c]Redshift Cluster\n        [s]Redshift Serverless\nc\nEnter Redshift Cluster Identifier: ci   \nEnter shared_profile: default\nEnter region: us-east-1\nAppend to /Users/alex/.pb/siteconfig.yaml? [y/N] y\n```\n\n**For Redshift Serverless**\n\n```\nWhich Redshift Service you want to connect with?\n        [c]Redshift Cluster\n        [s]Redshift Serverless\ns\nEnter Redshift Serverless Workgroup Name: serverless-wg\nEnter shared_profile: default\nAppend to /Users/alex/.pb/siteconfig.yaml? [y/N] y\n```\n\nIf you choose to use an AWS Secrets Manager ARN to securely retrieve credentials from AWS Programmatic Credentials (option 2c), enter the following details:\n\n**For Redshift Cluster**\n\n```\nWhich Redshift Service you want to connect with?\n        [c]Redshift Cluster\n        [s]Redshift Serverless\nc\nEnter Redshift Cluster Identifier: cluster-identifier\nEnter secrets_arn: ************************\nAppend to /Users/alex/.pb/siteconfig.yaml? [y/N] y\n```\n\nA sample connection for a Databricks account is as follows:\n\n```\nEnter Connection Name: test\nEnter target:  (default:dev):  # Press enter, leaving it to default\nEnter host: a1.8.azuredatabricks.net # The hostname or URL of your Databricks cluster\nEnter port: 443 # The port number used for establishing the connection. Usually it is 443 for https connections.\nEnter http_endpoint: /sql/1.0/warehouses/919uasdn92h # The path or specific endpoint you wish to connect to.\nEnter access_token: <password> # The access token created for authenticating the instance.\nEnter user: profiles_test_user # Username of your Databricks account.\nEnter schema: rs_profiles # A segregated schema for storing tables/views created by Profiles\nEnter catalog: your_rudderstack_db # The database or catalog having data that you’ll be accessing.\nAppend to /Users/<user_name>/.pb/siteconfig.yaml? [y/N]\ny\n```\n\n*   **Connection Name**: Name of the connection in the project file.\n*   **Target**: Environment name, such as `dev`, `prod`, `test`, etc. You can specify any target name and create a separate connection for the same.\n*   **Host**: Host name or URL of your Databricks cluster.\n*   **Port**: Port number for establishing the connection, usually `443` for `https` connections.\n*   **http\\_endpoint**: Path or specific endpoint you wish to connect to.\n*   **access\\_token**: Access token for authenticating the instance.\n*   **User**: Username of your Databricks account.\n*   **Schema**: Name of the schema to store your output tables/views.\n*   **Catalog**: Name of the database or catalog from where you want to access the data.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> RudderStack currently supports Databricks on Azure. To get the Databricks connection details:\n> \n> 1.  Log in to your Azure’s Databricks Web UI.\n> 2.  Click on **SQL Warehouses** on the left.\n> 3.  Select the warehouse to connect to.\n> 4.  Select the **Connection Details** tab.\n\nA sample connection for a BigQuery account is as follows:\n\n```\nEnter Connection Name: test\nEnter target:  (default:dev):  # Press enter, leaving it to default\nEnter credentials: json file path: # File path of your BQ JSON file, for example, /Users/alexm/Downloads/big.json. Entering an incorrect path will exit the program.\nEnter project_id: profiles121\nEnter schema: rs_profiles\nAppend to /Users/<user_name>/.pb/siteconfig.yaml? [y/N]\ny\n```\n\nThis creates a [site configuration file](https://www.rudderstack.com/docs/archive/profiles/0.14/cli-user-guide/structure/) inside your home directory: `~/.pb/siteconfig.yaml`. If you don’t see the file, enable the **View hidden files** option.\n\n### 3: Create project\n\nRun the following command to create a sample project:\n\n```\npb init pb-project -o MyProfilesProject\n```\n\nThe above command creates a new project in the **MyProfilesProject** folder with the following structure:\n\n[![Project structure](https://www.rudderstack.com/docs/images/profiles/project-structure.webp)](https://www.rudderstack.com/docs/images/profiles/project-structure.webp)\n\nSee [Project structure](https://www.rudderstack.com/docs/archive/profiles/0.14/cli-user-guide/structure/) for more information on the PB project files.\n\nNavigate to the `pb_project.yaml` file and set the value of `connection:` to the connection name as defined in the previous step.\n\n### 4: Change input sources\n\n*   Navigate to your project and open the `models/inputs.yaml` file. Here, you will see a list of tables/views along with their respective ID types.\n*   Replace the placeholder table names with the actual table names in the `table` field.\n\nSee [Project structure](https://www.rudderstack.com/docs/archive/profiles/0.14/cli-user-guide/structure/#inputs) for more information on setting these values.\n\n### 5: Validate project\n\nNavigate to your project and validate your warehouse connection and [inputInputs refers to the input data sources used to create the material (output) tables in the warehouse.](https://www.rudderstack.com/docs/resources/glossary/#input) sources:\n\nIf there are no errors, proceed to the next step. In case of errors, check if your warehouse schemas and tables have the [required permissions](https://www.rudderstack.com/docs/archive/profiles/0.14/permissions/).\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> Currently, this command is not supported for BigQuery warehouse.\n\n### 6: Generate SQL files\n\nCompile the project:\n\nThis generates SQL files in the `output/` folder that you can run directly on the warehouse. In case of any compilation errors, you will see them on your screen and also in the `logs/logfile.log` file.\n\n### 7: Generate output tables\n\nRun the project and generate [material tables](https://www.rudderstack.com/docs/archive/profiles/0.14/resources/glossary/#material-tables):\n\nThis command generates and runs the SQL files in the warehouse, creating the material tables.\n\n### 8: View generated tables\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> The view `user_default_id_stitcher` will always point to the latest generated ID stitcher and `user_profile` to the latest feature table.\n\nYou can run the `pb show models` command to get the exact name and path of the generated ID stitcher/feature table. See [show](https://www.rudderstack.com/docs/archive/profiles/0.14/cli-user-guide/commands/#show) command for more information.\n\nThen, execute the below query to view the generated tables in the warehouse:\n\n```\nselect * from <table_name> limit 10;\n```\n\nHere’s what the columns imply:\n\n![ID Stitcher Table](https://www.rudderstack.com/docs/images/profiles/idstitcher-table.webp)\n\n*   **user\\_main\\_id**: Rudder ID generated by Profile Builder. Think of a 1-to-many relationship, with one Rudder ID connected to different IDs belonging to same user such as User ID, Anonymous ID, Email, Phone number, etc.\n*   **other\\_id**: ID in input source tables that is stitched to a Rudder ID.\n*   **other\\_id\\_type**: Type of the other ID to be stitched (User ID, Anonymous ID, Email, etc).\n*   **valid\\_at**: Date at which the corresponding ID value occurred in the source tables. For example, the date at which a customer was first browsing anonymously, or when they logged into the CRM with their email ID, etc.\n\n![Feature Table](https://www.rudderstack.com/docs/images/profiles/feature-table.webp)\n\n*   **user\\_main\\_id**: Rudder ID generated by Profile Builder.\n*   **valid\\_at**: Date when the feature table entry was created for this record.\n*   **first\\_seen, last\\_seen, country, first\\_name, etc.** - All features for which values are computed.\n\n## See Also\n\n*   [Basic Profiles project](https://github.com/rudderlabs/rudderstack-profiles-basic-example): Get started with RudderStack Profiles by creating a basic project including the identity stitcher and feature table models.\n\n## Migrate your existing project\n\nTo migrate an existing PB project to the [schema version](https://www.rudderstack.com/docs/archive/profiles/0.14/resources/glossary/#schema-versions) supported by your PB binary, navigate to your project’s folder. Then, run the following command to replace the contents of the existing folder with the new one:\n\n```\npb migrate auto --inplace\n```\n\nA confirmation message appears on screen indicating that the migration is complete. A sample message for a user migrating their project from version 25 to 44:\n\n```\n2023-10-17T17:48:33.104+0530\tINFO\tmigrate/migrate.go:161\t\nProject migrated from version 25 to version 44\n```\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Profile Builder CLI | RudderStack Docs",
    "description": "Create a Profiles project using the Profile Builder (PB) tool.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.14/get-started/quickstart-ui/",
    "markdown": "# Profiles UI | RudderStack Docs\n\nCreate your Profiles project from the RudderStack dashboard.\n\n* * *\n\n*     5 minute read  \n    \n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> While creating a Profiles project, you can choose either of the below:\n> \n> *   **Profile Builder (PB) CLI** which gives you the flexibility to create, develop, and debug your Profiles project using various commands in fine detail. You can explore and implement the exhaustive list of features and functionalities offered by Profiles.\n> *   **Profiles UI** which provides a step-by-step intuitive workflow in the RudderStack dashboard. You can configure your project, schedule its run, explore the outputs and the user profiles.\n\nThis guide lists the detailed steps to create a Profiles project in the RudderStack dashboard.\n\n## Create Profiles project\n\n1.  Log in to the [RudderStack dashboard](https://app.rudderstack.com/) and go to **Unify** > **Profiles** option in the left sidebar.\n2.  Click **Create project**.\n\n[![Choose predefined template](https://www.rudderstack.com/docs/images/profiles/create-project.webp)](https://www.rudderstack.com/docs/images/profiles/create-project.webp)\n\n3.  Select **Basic Entity Setup** on the next screen.\n\n[![Choose predefined template](https://www.rudderstack.com/docs/images/profiles/basic-setup-profiles.webp)](https://www.rudderstack.com/docs/images/profiles/basic-setup-profiles.webp)\n\n4.  Enter a unique name and description for your Profiles project.\n5.  Select a data warehouse from the dropdown and the source(s) connected to the warehouse.\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> RudderStack currently supports the [Snowflake](https://www.rudderstack.com/docs/destinations/warehouse-destinations/snowflake/), [Redshift](https://www.rudderstack.com/docs/destinations/warehouse-destinations/redshift/), [Databricks](https://www.rudderstack.com/docs/destinations/warehouse-destinations/delta-lake/), and [BigQuery](https://www.rudderstack.com/docs/destinations/warehouse-destinations/bigquery/) warehouses for creating a Profiles project.\n> \n> You must connect an [event stream source](https://www.rudderstack.com/docs/sources/event-streams/) to one of the above warehouses and sync data at least once so that they populate in the dropdown.\n> \n> For example, in the below image, the Node event stream source (_es src_) is connected to the Snowflake warehouse destination (_cohort sample_ ) containing the schema _SAMPLE\\_SHOPIFY\\_DATA_.\n\n[![Choose predefined template](https://www.rudderstack.com/docs/images/profiles/source.webp)](https://www.rudderstack.com/docs/images/profiles/source.webp)\n\n6.  Click **Add mapping** and provide values to map the existing ID types from the above-selected event stream source:\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> You can see some predefined mappings for the identifiers which comes from the [Profiles library project](https://github.com/rudderlabs/profiles-multieventstream-features/tree/main).\n> \n> Here, **Event** column represents the tables present in the warehouse, **Property** column represents the column name in that table, and **ID type** represents the type of identifier.\n\n[![Choose predefined template](https://www.rudderstack.com/docs/images/profiles/map-id.webp)](https://www.rudderstack.com/docs/images/profiles/map-id.webp)\n\n7.  Define features either by adding a custom feature or selecting a pre-defined feature as shown:\n\n[![Choose predefined template](https://www.rudderstack.com/docs/images/profiles/define-features.webp)](https://www.rudderstack.com/docs/images/profiles/define-features.webp)\n\n*   To add a custom feature, click **Add a custom feature** and enter the relevant feature details:\n\n[![Choose predefined template](https://www.rudderstack.com/docs/images/profiles/custom-feature.webp)](https://www.rudderstack.com/docs/images/profiles/custom-feature.webp)\n\n*   To use a pre-defined feature, select the required feature from the **Template features library**.\n\n8.  Select the [schedule type](https://www.rudderstack.com/docs/sources/reverse-etl/sync-schedule-settings/).\n9.  Enter the warehouse details where you want to store this Profiles project.\n10.  Finally, review all the provided details and click **Create Profiles project**.\n\n### Convert to a Git project\n\nOnce you set up a Profiles project in the dashboard, you can convert it to a configuration-based Git repository by [uploading it either to GitHub, GitLab, or Bitbucket](https://www.rudderstack.com/docs/archive/profiles/0.14/example/packages/#supported-git-urls):\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> You cannot revert to your UI-based project after Git conversion. Once converted, all the project edits and updates can only be done via Git repository.\n\n1.  Navigate to the **Unify** > **Profiles** option in the left sidebar to view all your Profiles projects.\n2.  Click **Convert to Git** button next to the project you want to convert.\n3.  [Create a new Git repository](https://git-scm.com/book/en/v2/Git-Basics-Getting-a-Git-Repository).\n4.  Enter the [SSH URL of your Git repository](https://git-scm.com/book/en/v2/Git-on-the-Server-The-Protocols) to be used for your Profiles project and click **Continue**.\n5.  Copy the SSH public key and click **Add deploy key** which will take you to your git repository’s **Deploy keys** section. See deploy keys section in the [GitHub](https://docs.github.com/en/authentication/connecting-to-github-with-ssh/managing-deploy-keys#set-up-deploy-keys), [GitLab](https://docs.gitlab.com/ee/user/project/deploy_keys/), or [Bitbucket](https://bitbucket.org/blog/deployment-keys) documentation for more information.\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> Make sure your Git repository has at least one commit for successful validation.\n\n7.  Click **Add deploy key**, add a **Title**, paste your SSH public key, and click **Add key**.\n8.  Click **Continue** to let RudderStack verify read access.\n9.  Click **Download project** to download the Profiles project you set up in the dashboard.\n10.  Commit the downloaded project to your Git repository.\n11.  Once the project is committed successfully, return to the dashboard and click **Convert Profiles project**.\n\n## Run project\n\nOnce created, you can run your Profiles project using either of the following ways:\n\n*   Clicking **Run** in the **History** tab of the project.\n*   Programmatically using the [Profiles API](https://www.rudderstack.com/docs/api/profiles-api/).\n\n## Download project\n\nTo download the Profiles project, click the arrow icon corresponding to your Profiles project and click **Download this project**:\n\n[![Activation API](https://www.rudderstack.com/docs/images/profiles/cohorts-view-ui.webp)](https://www.rudderstack.com/docs/images/profiles/cohorts-view-ui.webp)\n\nOnce downloaded, you can view the [project folder structure](https://www.rudderstack.com/docs/archive/profiles/0.14/cli-user-guide/structure/), modify the files, or run various [commands](https://www.rudderstack.com/docs/archive/profiles/0.14/cli-user-guide/commands/) to execute the desired use-cases.\n\n## Project details\n\nTo view the Profile project details, click the arrow icon corresponding to your Profiles project:\n\n| Option | Description |\n| --- | --- |\n| **Entities** | Lists the entities, cohorts, features, activations, etc. for your Profiles project. |\n| **History** | Displays the history of Profile runs. |\n| **Settings** | Displays your profile settings and lets you delete your Profiles project. You can edit the project by clicking the edit icon next to each section. |\n\n### Profile details\n\nYou can also view the details of a specific profile in your Profiles project:\n\n1.  In your Profiles project’s **Entities** tab, click **View** button across the entity for which you want to see the profile:\n    \n    [![Activation API](https://www.rudderstack.com/docs/images/profiles/entity_view.webp)](https://www.rudderstack.com/docs/images/profiles/entity_view.webp)\n    \n2.  Click **Profile Lookup** tab to search a profile record.\n    \n3.  Type an available unique identifier like `email`, `phone number`, `user id`, `anonymous id` etc. and click **Search user profile**.\n    \n    [![Activation API](https://www.rudderstack.com/docs/images/profiles/profiles-lookup.webp)](https://www.rudderstack.com/docs/images/profiles/profiles-lookup.webp)\n    \n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> Note that:\n> \n> *   It can take a few minutes for the data preview to show up in your profile’s **History** tab.\n> *   If you keep getting a blank screen, it may be because you do not have sufficient access. Make sure you have a [Connections Admin](https://www.rudderstack.com/docs/dashboard-guides/user-management/#resource-roles) resource role with [access to PII](https://www.rudderstack.com/docs/dashboard-guides/data-management/#limiting-access-to-pii-related-features). In case the problem persists, contact [RudderStack support](mailto:support@rudderstack.com).\n\n## FAQ\n\n**When trying to fetch data for a lib project, then data/columns are shown as blank. What should I do?**\n\nYou’ll need to sync data from a source to a destination. If data is synced from the source you are using and not from some pre-existing tables in the destination, the missing column/data issues should not occur.\n\n**I am not able to see Unify tab on the web app though I have admin privileges. What should I do?**\n\nDisable any adblockers on your web browser.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Profiles UI | RudderStack Docs",
    "description": "Create your Profiles project from the RudderStack dashboard.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.14/get-started/import-from-git/",
    "markdown": "# Import Profiles Project from Git\n\nPrivacy Overview\n\nThis site uses cookies to improve your experience while you navigate through the website. Out of these cookies, the cookies that are categorized as necessary are stored on your browser as they are as essential for the working of basic functionalities of the website. We also use third-party cookies that help us analyze and understand how you use this website. These cookies will be stored in your browser only with your consent. You also have the option to opt-out of these cookies. But opting out of some of these cookies may have an effect on your browsing experience.\n\nNecessary cookies are absolutely essential for the website to function properly. This category only includes cookies that ensures basic functionalities and security features of the website. These cookies do not store any personal information.",
    "title": "Import Profiles Project from Git | RudderStack Docs",
    "description": "Import an existing Profiles project from the Git repository in RudderStack dashboard.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/resources/early-access-program/",
    "markdown": "# Early Access Program | RudderStack Docs\n\nPrivacy Overview\n\nThis site uses cookies to improve your experience while you navigate through the website. Out of these cookies, the cookies that are categorized as necessary are stored on your browser as they are as essential for the working of basic functionalities of the website. We also use third-party cookies that help us analyze and understand how you use this website. These cookies will be stored in your browser only with your consent. You also have the option to opt-out of these cookies. But opting out of some of these cookies may have an effect on your browsing experience.\n\nNecessary cookies are absolutely essential for the website to function properly. This category only includes cookies that ensures basic functionalities and security features of the website. These cookies do not store any personal information.",
    "title": "Early Access Program | RudderStack Docs",
    "description": "Collaborate with RudderStack's Product team on building out new, exciting features.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/resources/glossary/",
    "markdown": "# RudderStack Glossary | RudderStack Docs\n\nFamiliarize yourself with the RudderStack-specific features and terminology.\n\n* * *\n\n*     7 minute read  \n    \n\nThis guide lists the definitions of the RudderStack-related terms that you are likely to encounter throughout the documentation and while using RudderStack.\n\n### Airflow Provider\n\n[Airflow Provider](https://www.rudderstack.com/docs/sources/reverse-etl/airflow-provider/) is a tool that lets you programmatically schedule and trigger your Reverse ETL syncs from outside RudderStack and integrate them with your existing Airflow workflows.\n\n### Anonymous ID\n\nAn anonymous ID is an auto-generated **UUID** (Universally Unique Identifier) that gets assigned to each unique and unidentified visitor to your website.\n\n### Audit logs\n\n[Audit Logs](https://www.rudderstack.com/docs/dashboard-guides/audit-logs/) give you a detailed log of all user activities happening within your RudderStack. These include various operations related to sources, destinations, transformations, user management, and more.\n\n[![](https://www.rudderstack.com/docs/images/audit-logs.webp)](https://www.rudderstack.com/docs/images/audit-logs.webp)\n\n[Cloud Extract](https://www.rudderstack.com/docs/sources/extract/) is RudderStack’s ELT feature. It lets you collect data from various third-party cloud platforms and send it to your data warehouse.\n\n### Cloud mode\n\nIn this [connection mode](https://www.rudderstack.com/docs/destinations/rudderstack-connection-modes/), the RudderStack SDKs track and send the event data to RudderStack for processing. RudderStack then routes this data to the specified destination. Use this mode when you want to use RudderStack’s [Transformations](https://www.rudderstack.com/docs/transformations/) feature to transform your events before sending them to your destination.\n\n### Connection\n\nA connection is a one-to-one directional event flow between a RudderStack source and a destination. You can set up different types of connections in RudderStack to send your events:\n\n*   [Event Stream](https://www.rudderstack.com/docs/sources/event-streams/): One source to many destinations\n*   [Cloud Extract](https://www.rudderstack.com/docs/sources/extract/): Multiple sources to one warehouse destination\n*   [Reverse ETL](https://www.rudderstack.com/docs/sources/reverse-etl/): One warehouse source to multiple event stream destinations\n\n[![](https://www.rudderstack.com/docs/images/connections.webp)](https://www.rudderstack.com/docs/images/connections.webp)\n\n### Control plane\n\nThe control plane manages the configuration of your sources and destinations. The interface for the control plane is the [RudderStack dashboard](https://app.rudderstack.com/) (web app).\n\n### Control Plane Lite\n\nThe [Control Plane Lite](https://www.rudderstack.com/docs/get-started/rudderstack-open-source/control-plane-lite/) utility lets you self-host your source and destination configurations. You can manage these configurations by locally exporting to or importing them from a JSON file.\n\n### Customer Data Platform (CDP)\n\nA Customer Data Platform (CDP) is software or a collection of tools that unifies and persists all customer records across multiple data sources in a centralized location. It is accessible to other tools in your tech stack and lets you build a comprehensive customer profile for a variety of use cases.\n\n### Data governance\n\nRudderStack’s [Data Governance](https://www.rudderstack.com/docs/data-governance/overview/) capabilities let you access all your events and their metadata programmatically and identify any inconsistencies in them. This includes vital information related to the event schema, event payload versions, data types, and more.\n\n### Data plane\n\nRudderStack’s core engine responsible for receiving, processing, and relaying your event data to the specified destination. For more information, refer to the [Architecture](https://www.rudderstack.com/docs/resources/rudderstack-architecture/) guide.\n\n### Data plane URL\n\nRudderStack requires the data plane URL for routing and processing events in the backend. You can find the URL in the home page of your RudderStack dashboard. For more information, see the [Dashboard Overview](https://www.rudderstack.com/docs/dashboard-guides/overview/) guide.\n\n[![Data Plane URL](https://www.rudderstack.com/docs/images/data-plane-url.webp)](https://www.rudderstack.com/docs/images/data-plane-url.webp)\n\n### Data retention\n\nRudderStack’s data retention settings let you define and manage your event data storage. You can disable event data storage completely, store the events in your own cloud storage, or store them in the RudderStack-hosted cloud storage. For more information, refer to the [Data Management](https://www.rudderstack.com/docs/dashboard-guides/data-management/) guide.\n\n### Destination\n\nA destination is a tool or platform where you want to send the event data via RudderStack. RudderStack currently supports over 150 [Cloud destinations](https://www.rudderstack.com/docs/destinations/streaming-destinations/) and [Warehouse destinations](https://www.rudderstack.com/docs/destinations/warehouse-destinations/).\n\n[![](https://www.rudderstack.com/docs/images/destinations.webp)](https://www.rudderstack.com/docs/images/destinations.webp)\n\n### Device mode\n\nIn this connection mode, you can send the source events to the destinations using the native client-specific libraries present on your website/mobile app. These libraries allow RudderStack to use the data you collect on your device to call the destination APIs without sending it to the RudderStack server first. Use device mode when you want to send events to a destination directly, without any transformation. For more information, see the [Connection Modes](https://www.rudderstack.com/docs/destinations/rudderstack-connection-modes/) guide.\n\n### ELT\n\nThe ELT (Extract, Load, Transform) process involves obtaining the data from the source, replicating it into the target system (typically data warehouse or data lake), and transforming it depending on the use case.\n\n### Event\n\nEvents are the fundamental components of clickstream data. They correspond to the user actions on websites or mobile apps such as clicks, page or screen views, logins, registrations, etc. Tracking events in real-time helps businesses to better understand the users and their product journey.\n\n### Event spec\n\nThe [RudderStack Event Spec](https://www.rudderstack.com/docs/event-spec/standard-events/) helps you plan your event data and provides various options for tracking your events across all the RudderStack SDKs and APIs. RudderStack has a unified event semantic for different destination platforms, so you can easily translate your event data to different downstream tools by following this spec.\n\n### Event Stream\n\nRudderStack’s [Event Streams](https://www.rudderstack.com/docs/sources/event-streams/) feature lets you collect your event data from all of your web and mobile apps and route it to a wide array of customer tools and data warehouses.\n\n### Identity stitching\n\n[Identity Stitching](https://www.rudderstack.com/docs/profiles/core-concepts/identity-stitching/) is the process of matching different identifiers across multiple devices and digital touchpoints to build a cohesive and omnichannel customer profile. With RudderStack’s warehouse-first architecture, you can send all your cross-platform data to your warehouse and perform identity stitching on it.\n\n### Live events\n\nRudderStack’s [Live Events](https://www.rudderstack.com/docs/dashboard-guides/live-events/) feature lets you view the live events collected from your sources and sent to the connected destinations in real-time. With this feature, you can easily debug any errors in the failing events at a destination level and reduce your troubleshooting time and efforts.\n\n[![Live Events](https://www.rudderstack.com/docs/images/rs-cloud/source-live-events.webp)](https://www.rudderstack.com/docs/images/rs-cloud/source-live-events.webp)\n\n### Models\n\nRudderStack’s [Models](https://www.rudderstack.com/docs/data-pipelines/reverse-etl/features/models/) feature lets you create models by defining custom SQL queries. You can then run these queries on your warehouse and send the resulting data to specific destinations.\n\n### Personal access token\n\n[Personal Access Token](https://www.rudderstack.com/docs/dashboard-guides/personal-access-token/) is a unique key associated with your RudderStack account. It is required to consume all the public RudderStack APIs.\n\n[![New personal access token in RudderStack dashboard](https://www.rudderstack.com/docs/images/rudderstack-api/personal-access-token-1.webp)](https://www.rudderstack.com/docs/images/rudderstack-api/personal-access-token-1.webp)\n\n### Profiles\n\n[Profiles](https://www.rudderstack.com/docs/profiles/overview/) are the collection of all the events associated with a user. RudderStack lets you create and manage these profiles in your warehouse.\n\n### Properties\n\nProperties are additional contextual information you can add to a `track` call to further describe the action a user takes. RudderStack has reserved some standard properties listed in the following table and handles them in a special manner.\n\n### Reverse ETL\n\n[Reverse ETL](https://www.rudderstack.com/docs/sources/reverse-etl/) is the process of routing the data residing in your data warehouse to various downstream tools within your customer data stack. This includes various SaaS marketing, analytics, sales, and customer support tools.\n\n### Source\n\nA [source](https://www.rudderstack.com/docs/sources/overview/) is a platform or an application (web, mobile, server-side, or a third-party cloud app) from where RudderStack tracks and collects your event data.\n\n[![](https://www.rudderstack.com/docs/images/sources.webp)](https://www.rudderstack.com/docs/images/sources.webp)\n\n### Tracking plans\n\n[Tracking Plans](https://www.rudderstack.com/docs/data-governance/tracking-plans/) is a RudderStack feature that lets you proactively monitor and act on non-compliant event data coming into your RudderStack sources based on predefined plans. This can help you prevent or de-risk situations where missing or improperly configured event data can break your downstream destinations.\n\n[![Create blank tracking plan](https://www.rudderstack.com/docs/images/data-governance/blank-tracking-plan.webp)](https://www.rudderstack.com/docs/images/data-governance/blank-tracking-plan.webp)\n\n### Traits\n\n[Traits](https://www.rudderstack.com/docs/event-spec/standard-events/identify/#identify-traits) are attributes that describe a user. They can be added to an identify call in the `traits` object. Some examples of traits include age, gender, or some specific details - for example, a user’s product plan (basic, premium, and so on). After making an identify call, you don’t need to include all the user traits in the subsequent calls every time. You can include only the changed/updated traits since the last identify call.\n\n### Transformations\n\n[Transformations](https://www.rudderstack.com/docs/transformations/overview/)is a RudderStack feature that lets you leverage custom JavaScript functions to implement a variety of use cases like event filtering, sampling, removing sensitive PII, or implementing custom logic to enrich your events.\n\n[![](https://www.rudderstack.com/docs/images/transformations-glossary.webp)](https://www.rudderstack.com/docs/images/transformations-glossary.webp)\n\n### User management\n\nRudderStack’s [user management](https://www.rudderstack.com/docs/dashboard-guides/user-management/) feature lets you manage users and their permissions in your RudderStack workspace. It lets you set access controls and collaborate with other members of your organization.\n\n### User suppression API\n\nThe [User Suppression API](https://www.rudderstack.com/docs/api/user-suppression-api/) is RudderStack’s enterprise feature. It lets you programmatically suppress user data identified by a user ID. With this feature, you can block all the user data for all the sources and destinations in RudderStack.\n\n### Visual data mapper\n\n[Visual Data Mapper](https://www.rudderstack.com/docs/sources/reverse-etl/visual-data-mapper/) (VDM) is RudderStack’s Reverse ETL feature. It offers an intuitive UI to map your data warehouse columns to specific destination fields without any second-guessing.\n\n### Warehouse schema\n\nWhen sending your events to a data warehouse via RudderStack, you don’t need to define a schema. RudderStack automatically does that for you by following a predefined warehouse schema that defines the different tables and columns created based on different event types. For more information, refer to the [Warehouse Schema](https://www.rudderstack.com/docs/destinations/warehouse-destinations/warehouse-schema/) guide.\n\n### Workspace ID\n\nRudderStack uses the workspace ID for tracking the data internally. You can find your workspace ID by navigating to **Settings** > **Company** in the RudderStack dashboard.\n\n[![](https://www.rudderstack.com/docs/images/workspace-id.webp)](https://www.rudderstack.com/docs/images/workspace-id.webp)\n\n### Workspace token\n\nThe workspace token uniquely identifies your RudderStack workspace. You can find your workspace ID by navigating to **Settings** > **Workspace** in the RudderStack dashboard. The workspace token is hidden by default - you must have administrative privileges to access the token.\n\n[![Workspace Token](https://www.rudderstack.com/docs/images/rs-cloud/workspace-token.webp)](https://www.rudderstack.com/docs/images/rs-cloud/workspace-token.webp)\n\n### Write key\n\nThe write key (or source write key) is a unique identifier for your source. RudderStack uses this key to send events from a source to the specified destination.\n\n[![JavaScript SDK source write key](https://www.rudderstack.com/docs/images/get-started/quickstart/js-write-key.webp)](https://www.rudderstack.com/docs/images/get-started/quickstart/js-write-key.webp)\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "RudderStack Glossary | RudderStack Docs",
    "description": "Familiarize yourself with the RudderStack-specific features and terminology.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.14/get-started/sample-data/",
    "markdown": "# Snowflake Sample Data | RudderStack Docs\n\nSample data for Snowflake\n\n* * *\n\n*     2 minute read  \n    \n\nRudderStack provides a sample data set for the Snowflake warehouse, available in the [Snowflake marketplace](https://app.snowflake.com/marketplace/listing/GZT0Z856CMJ/rudderstack-inc-rudderstack-event-data-for-quickstart). You can use this data to run the Profiles project and [Predictive features](https://www.rudderstack.com/docs/archive/profiles/0.14/predictions/) through the UI or the CLI.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> The number of columns in this data set are intentionally limited to make the data set easily understandable. Also, all email addresses are generated randomly and no PII is used in the generation of this data set.\n\nThe following tables, properties, and user information is included in the data set:\n\n## Tables\n\nThis data set includes below-mentioned RudderStack event data tables:\n\n*   `PAGES` - Page view events from anonymous and known users.\n*   `TRACKS` - Summarized tracked user actions (like `login`, `signup`, `order_completed`, etc.).\n*   `IDENTIFIES` - Identify calls run when a user provides a unique identifier (i.e., upon `signup`).\n*   `ORDER_COMPLETED` - Detailed payloads from tracked `order_completed` events.\n\nAs of January 2023, here are the approximate number of rows in each table:\n\n*   `PAGES`: ~43k\n*   `TRACKS`: ~14k\n*   `IDENTIFIES`: ~4.8k\n*   `ORDER_COMPLETED`: ~2.2k\n\nThese volumes follow the pattern of a normal eCommerce conversion funnel (pageview, signup, order). Specifically, here’s a rough breakdown of the user journey by volume:\n\n*   30% - Never sign in\n*   10% - Sign in but never add an item to cart\n*   40% - Add to cart and abandon\n*   20% - Make purchases\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> Note that this data includes _future_ data until Apr 2024, and starts in June 2023. This is to ensure that future users can still run the project with ‘current’ data. RudderStack team will refresh the data periodically throughout the year.\n\n## Properties\n\nThis data set includes a _subset_ of the standard properties found in the [Warehouse schema spec](https://www.rudderstack.com/docs/destinations/warehouse-destinations/warehouse-schema/) for each table. The required columns for running Profiles and Predictions projects are also present.\n\n### User information\n\nThe user data includes a subset of our standard properties for `identify` calls.\n\nThis data set contains a total of ~10k unique users by `anonymousId`. About half of these unique users (~4.8k) are known users (with an associated `identify` call).\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Snowflake Sample Data | RudderStack Docs",
    "description": "Sample data for Snowflake",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.14/core-concepts/data-modeling/",
    "markdown": "# Data Modeling | RudderStack Docs\n\nModel your unorganized and scattered warehouse data using RudderStack’s Profiles.\n\n* * *\n\n*     3 minute read  \n    \n\nProfiles models your organizational data by analyzing all the data in your warehouse to create unified customer profiles and enrich them with features to help you scale your business efficiently and swiftly.\n\nWhen you run the Profiles project, it creates an identity graph and feature views as outputs. You can augment the graph and create new user features by writing simple definitions in a configuration file or via SQL models.\n\n[![Profiles data modeling](https://www.rudderstack.com/docs/images/profiles/data-modeling.webp)](https://www.rudderstack.com/docs/images/profiles/data-modeling.webp)\n\n## Highlights\n\n*   Flexibility to use event stream, ETL, or any external tools as input sources.\n*   Support to define various entities like user, product, organization, etc.\n*   Intelligent merging of entities with different identifiers, like stitching Salesforce IDs.\n*   Ease of creating features/traits for any entity and using them to deliver personalization.\n*   Support to create core customer segments and activate them in downstream systems.\n*   Deal with advanced use-case scenarios using entity\\_vars/ML models.\n\n## Use varied input sources\n\nRudderStack Profiles gives you the flexibility of using a variety of input sources. These sources are essentially the tables or views which you can create using:\n\n*   [Event Stream](https://www.rudderstack.com/docs/sources/event-streams/) (loaded from event data)\n*   ETL extract (loaded from [Cloud Extract](https://www.rudderstack.com/docs/sources/extract/) (loaded from event data))\n*   Existing tables in the warehouse (generated by external tools like DBT).\n\n## Define entities\n\nEntities refer to an object for which you can create a profile. RudderStack allows you to use the desired object as an entity. For example, user, customer, product, or any other object that requires profiling.\n\nYou can define the entities in `pb_project.yaml` file and use them declaratively while describing the columns of your input sources.\n\n## Unify entities\n\nOnce you define the entities, you can resolve different identities for an entity using the process of [identity stitching](https://www.rudderstack.com/docs/archive/profiles/0.14/core-concepts/identity-stitching/). It matches the different identifiers across multiple devices, digital touchpoints, and other data (like offline point-of-sale interactions) to build a comprehensive identity graph. The identity graph includes nodes (identifiers) and their relationships (edges), and it is generated as a transparent table in the warehouse.\n\nFor example, you can stitch Salesforce IDs or other ID types.\n\n## Enrich with features\n\nOnce you map all the available identifiers to an individual user or entity, it is easier to collect their traits and compute the user features you want in your customer 360 table.\n\nUsing the identity graph as a map, the Profiles **entity var** models let you define or perform calculations over the customer data in your warehouse. Each `var` materialises as a column in the `entity_var` table and represents a distinct feature. In addition, ML models can also use the identify graph as well as other entity vars, to create new features. Finally, **feature view** model lets you unify entity vars as well as ML faetures into a single view.\n\nYou don’t need any other tool or deep technical/SQL expertise to create these features. Trait definition is in a single unified framework and there is no need to move data across silos.\n\nTo implement advanced use cases, you can use [custom SQL queries](https://www.rudderstack.com/docs/archive/profiles/0.14/example/sql-model/) to define user features.\n\n## Define cohorts\n\nUsing cohorts, you can define core customer segments in the warehouse via a simple YAML config and the entire business can use them as a single source of truth. It is a subset of instances of an entity meeting a specified set of characteristics, behaviours, or attributes. For example, if you have user as an entity, you can define cohorts such as known users, new users, or North American users, etc.\n\nBy leveraging cohorts, you can target specific customer segments by enabling targeted campaigns and analysis. See [Cohorts](https://www.rudderstack.com/docs/sources/event-streams/cloud-apps/auth0/) for more information.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Data Modeling | RudderStack Docs",
    "description": "Model your unorganized and scattered warehouse data using RudderStack's Profiles.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/resources/faq/",
    "markdown": "# FAQ | RudderStack Docs\n\nGet answers to frequently asked questions about RudderStack.\n\n* * *\n\n*     17 minute read  \n    \n\nThis section aims to address the queries and issues you might encounter while using RudderStack.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> If you come across any issue not listed in this guide, feel free to start a conversation in our [Slack](https://rudderstack.com/join-rudderstack-slack-community) community.\n\n## RudderStack Cloud\n\n#### How do I get started with RudderStack?\n\nHead over to our [Quickstart](https://www.rudderstack.com/docs/get-started/quickstart/) guide to configure your sources and destinations and start building your data pipelines.\n\n#### Which RudderStack features are not supported in Open Source?\n\nSee the [RudderStack Cloud vs. Open Source](https://www.rudderstack.com/docs/get-started/cloud-vs-open-source/) guide for a detailed comparison of the features available in RudderStack Open Source and RudderStack Cloud.\n\n#### Where can I see my monthly event usage/volume?\n\nTo see your monthly event usage/volume, go to **Settings** > **Organization** > **Usage** tab in your [RudderStack dashboard](https://app.rudderstack.com/).\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> RudderStack calculates the monthly event volume based on the events ingested at source and **not** the events sent to the destinations.\n> \n> Filtering selective events to destinations using [transformations](https://www.rudderstack.com/docs/transformations/overview/) will not result in a lower event volume.\n\n#### Is there any event volume limit for RudderStack Cloud Free?\n\nThere is an event volume limit of 1000 events/minute for [RudderStack Cloud Free](https://rudderstack.com/pricing/) users. You will get a `429` (rate limit exceeded) error if this limit is breached.\n\n## Installation and setup\n\n#### What is a workspace token? Where do I get it?\n\nThe workspace token is a unique identifier of your RudderStack workspace.\n\nTo get your workspace token, go to **Settings** > **Workspace**. The workspace token is present in the **General** tab.\n\n[![Workspace Token](https://www.rudderstack.com/docs/images/rs-cloud/workspace-token.webp)](https://www.rudderstack.com/docs/images/rs-cloud/workspace-token.webp)\n\nTo view the workspace token, click the show icon and enter the password associated with your RudderStack account.\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> By default, the workspace token is hidden for security purposes. You must have [administrative privileges](https://www.rudderstack.com/docs/dashboard-guides/user-management/#organization-roles) to access the workspace token.\n\n#### What is a data plane URL? Where do I get it?\n\nFor routing and processing the events to the RudderStack backend, a data plane URL is required.\n\nThe location of your data plane URL depends on your RudderStack plan:\n\n*   **RudderStack Open Source**: [Set up your own data plane](https://www.rudderstack.com/docs/get-started/rudderstack-open-source/data-plane-setup/) in the preferred environment.\n\n> ![success](https://www.rudderstack.com/docs/images/tick.svg)\n> \n> An open source data plane URL looks like `http:localhost:8080` where `8080` is typically the port where your data plane is hosted.\n\n*   **RudderStack Cloud**: The data plane URL is provided in the dashboard at the top of the **Connections** page.\n*   **RudderStack Pro/Enterprise**: [Contact us](https://rudderstack.com/join-rudderstack-slack-community) for the data plane URL with the email ID you used to sign up for RudderStack.\n\n#### Is it possible to change the data plane URL?\n\nFor [RudderStack Cloud Free and Starter](https://www.rudderstack.com/pricing/) plans, the data plane URL is provided in the dashboard, at the top of the **Connections** page. It is is not possible to change this URL.\n\nIf you are a RudderStack Cloud [Growth and Enterprise](https://www.rudderstack.com/pricing/) customer, contact your [Customer Success manager](mailto:support@rudderstack.com) to change their data plane URL.\n\n#### To get started with RudderStack on my local machine, is it mandatory to get the workspace token from RudderStack dashboard?\n\nThe [workspace token](https://www.rudderstack.com/docs/dashboard-guides/overview/#workspace-token) is required if you are installing RudderStack in your own environment and wish to use the RudderStack-hosted control plane. It is a unique identifier for your configuration settings which RudderStack can fetch to track your instrumentations.\n\nIf you are [self-hosting your control plane](https://www.rudderstack.com/docs/get-started/rudderstack-open-source/control-plane-lite/), this token is not required.\n\n#### Can I self-host the RudderStack control plane?\n\nYes. Use the open source [Control Plane Lite](https://www.rudderstack.com/docs/get-started/rudderstack-open-source/control-plane-lite/) utility to self-host the control plane and configure your sources and destinations. Refer to the **Control Plane Lite** section below for more information.\n\n#### While running `git submodule update`, I get this error:\n\n```\nPlease make sure you have the correct access rights and the repository exists.\nfatal: clone of 'git@github.com:rudderlabs/rudder-transformer.git' into submodule path '/home/ubuntu/rudder-server/rudder-transformer' failed\nFailed to clone 'rudder-transformer'. Retry scheduled.\nCloning into '/home/ubuntu/rudder-server/rudder-transformer'...\ngit@github.com: Permission denied (publickey).\nfatal: Could not read from remote repository.\n```\n\nVerify if the SSH keys are correctly set in your GitHub account as they are used when cloning using the git protocol. For more information, refer to this [Stack Overflow thread](https://stackoverflow.com/questions/25957125/git-submodule-permission-denied).\n\n#### How do I verify my RudderStack installation?\n\nYou can verify your RudderStack installation by sending test events and checking if they are delivered correctly. For more information, refer to the [Sending Test Events](https://www.rudderstack.com/docs/get-started/rudderstack-open-source/sending-test-events/) guide.\n\n#### My open source RudderStack setup keeps creating a new database automatically. What could be the reason?\n\nThis can happen if you have changed your [workspace token](https://www.rudderstack.com/docs/dashboard-guides/overview/#workspace-token). Also, ensure that the RudderStack server is running on the latest version.\n\n### Docker\n\n#### Is there any recommended size for the EC2 instance? I am running a self-hosted Docker setup.\n\nA **c4.xlarge** or **c4.2xlarge** machine should work just fine for your setup.\n\n#### I’m running RudderStack in Docker on a GCP VM instance. I upgraded the instance to have more CPU and now the RudderStack container is stuck on this message:\n\n```\nsh -c '/wait-for db:5432 -- /rudder-server'\n```\n\nThis message indicates that the RudderStack server is waiting on the PostgreSQL database dependency to be up and running. Verify if your PostgreSQL container is up.\n\n## RudderStack backend (server)\n\n#### How do I check the status of the RudderStack data plane?\n\nTo check the status of the data plane, run the following command:\n\n```\nCURL <DATA_PLANE_URL>/health\n```\n\nA sample command would look something like:\n\n```\nCURL https://hosted.rudderlabs.com/health\n```\n\n#### How many events can a single RudderStack node handle?\n\nThe number of events a single RudderStack node can handle depends on the destinations that you are sending the event data to. It also depends on the transformations you are running.\n\nHere are some ballpark figures:\n\n| Activity | Events handled |\n| --- | --- |\n| Dumping to S3 | Approx. 1.5K events/sec |\n| Dumping to warehouse | Approx. 1K events/sec |\n| Dumping to warehouse and a few cloud destinations | Approx. 750 events/sec |\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> These are conservative numbers. A single RudderStack node can handle close to 5x event load at the peak— just that those events get cached locally and are drained as per the regular throughput.\n\n#### How can I speed up the number of events sent to a destination?\n\nThere is a [config variable](https://github.com/rudderlabs/rudder-server/blob/master/config/config.yaml#L107) to configure the number of workers that send data to destinations. The default value is `64`, which itself is an aggressive number. You can increase the number of workers. However, note that some destinations generally throttle the number of requests per account.\n\n#### How I can know the number of events that are sent to a destination?\n\nGo to the **Events** tab of the destination page to see the event-related metrics, as shown below:\n\n[![Destination event metrics](https://www.rudderstack.com/docs/images/general/destination-event-metrics.webp)](https://www.rudderstack.com/docs/images/general/destination-event-metrics.webp)\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> Events sent through [device mode](https://www.rudderstack.com/docs/destinations/rudderstack-connection-modes/#device-mode) are not visible in this option.\n\n#### Do I need to change the data plane URL associated with the RudderStack Cloud to my self-hosted data plane?\n\nNo, you need not change the URL. As long as your self-hosted data plane has the same [workspace token](https://www.rudderstack.com/docs/dashboard-guides/overview/#workspace-token), the RudderStack-hosted control plane will use your data plane for processing events.\n\n#### While trying to start rudder-server, I get the following error:\n\n```\nbackend_1 | 2021/06/09 08:12:14 notifying bugsnag: During db.vlog.open: Value log truncate required to run DB. This might result in data loss \nbackend_1 | 2021/06/09 08:12:14 bugsnag.Notify: bugsnag/payload.deliver: invalid api key: '' \nbackend_1 | 2021/06/09 08:12:14 bugsnag/sessions/publisher.publish invalid API key: '' \nbackend_1 | panic: During db.vlog.open: Value log truncate required to run DB. This might result in data loss [recovered] \nbackend_1 | panic: During db.vlog.open: Value log truncate required to run DB. This might result in data loss [recovered] \nbackend_1 | panic: During db.vlog.open: Value log truncate required to run DB. This might result in data loss\n```\n\nCheck for the folder `/tmp/badgerdbv2` and delete it. This should resolve the issue and you should be able to start rudder-server.\n\n## Control Plane Lite\n\n#### How do I self-host the UI configuration?\n\nFor self-hosting the UI, you can use the [RudderStack Control Plane Lite](https://github.com/rudderlabs/config-generator) utility.\n\nNote that this utility will only generate the source-destination configurations which are required by RudderStack.\n\n#### I am using the Control Plane Lite to generate the `workspaceConfig.json` file. But when I import this file, I get this error:\n\n```\nTypeError: Cannot read property 'name' of undefined\"\n```\n\nThis issue can occur when you have some old data left in your browser’s local storage. Use the latest version of the Control Plane Lite after clearing your browser cache and local storage.\n\n#### For a self-hosted environment, how do I obtain the control plane URL?\n\nTo use the control plane URL to initialize your SDKs, follow these steps:\n\n1.  Set up the control plane using the Control Plane Lite utility.\n2.  Go to dashboard, configure the source, and export the source configuration by clicking the **EXPORT SOURCE CONFIG** button as shown:\n\n[![Export source config option](https://www.rudderstack.com/docs/images/rudderstack-open-source/export-source-config.webp)](https://www.rudderstack.com/docs/images/rudderstack-open-source/export-source-config.webp)\n\n3.  Host the exported file on your own server such that the configuration is available at `<CONTROL_PLANE_URL>/sourceConfig`.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> This solution assumes that you have already set up the RudderStack data plane (backend) locally.\n\n#### I don’t want to configure my API keys and secrets with RudderStack’s control plane. But I want to use its features like Transformations. How can I do this?\n\nRudderStack lets you provide your API keys and secrets as environment variables prepended with `env.` and run the data plane.\n\nSuppose you are configuring Amazon S3 as a destination but you don’t want to enter the AWS access key credentials in the destination settings. Fill the value with a placeholder that starts with `env.` It should look like this `env.MY_AWS_ACCESS_KEY`. Then set the value of the environment variable `MY_AWS_ACCESS_KEY`while running the data plane.\n\n## Transformations\n\n#### How do I add user transformations in RudderStack?\n\nRudderStack lets you implement your own custom transformations that leverage the event data to implement specific use-cases based on your business requirements. Refer to the [Adding a transformation](https://www.rudderstack.com/docs/transformations/create/#adding-a-transformation) section to add transformations in RudderStack.\n\n#### Can I apply a transformation to a source configured in RudderStack?\n\nCurrently, transformations can only be configured and used for destinations. If you want to write custom logic specific to the source, you can get the **Source ID** in the transformation function and use it to include the logic. Refer to the [Accessing metadata](https://www.rudderstack.com/docs/transformations/overview/#accessing-event-metadata) section for more information.\n\n## RudderStack Cloud\n\n#### Can I change my workspace name?\n\nUnfortunately, your workspace name is not changeable currently. We are planning to include this feature in our future releases.\n\n#### What is the difference between source write key and the workspace token?\n\nThe write key is different from your workspace token.\n\nThe write key is associated with the source and is used by RudderStack to track the events.\n\n[![Source Write Key](https://www.rudderstack.com/docs/images/rudderstack-open-source/write-key.webp)](https://www.rudderstack.com/docs/images/rudderstack-open-source/write-key.webp)\n\nOn the other hand, the [workspace token](https://www.rudderstack.com/docs/dashboard-guides/overview/#workspace-token) is a unique identifier for the configuration settings which RudderStack uses to fetch and track your instrumentations. You can find it by navigating to **Settings** > **Workspace** in your RudderStack dashboard:\n\n[![Workspace Token](https://www.rudderstack.com/docs/images/rs-cloud/workspace-token.webp)](https://www.rudderstack.com/docs/images/rs-cloud/workspace-token.webp)\n\n#### I see a few events that show up in the live stream but do not reach the destination. How do I see the logs or data that is sent to my destination?\n\nTo view the data or events that are sent to your destination, you can use the [Live Events](https://www.rudderstack.com/docs/dashboard-guides/live-events/) tab in your destination’s page.\n\n#### Do I need to change the data plane URL associated with the cloud-hosted RudderStack to my self-hosted data plane?\n\nNo, you need not change the URL. As long as your self-hosted data plane has the same [workspace token](https://www.rudderstack.com/docs/dashboard-guides/overview/#workspace-token), the RudderStack-hosted control plane will use your data plane for processing events.\n\n#### How can I switch from RudderStack Open Source to RudderStack Cloud and vice-versa?\n\nSwitching between RudderStack Open Source and RudderStack Cloud is quite straightforward. Replace the URL of your self-hosted data plane to the RudderStack-hosted data plane URL. You can use the same sources and destinations as before - all you need to do is just change the URL to where the events are sent.\n\n#### What is a personal access token? Where do I find it?\n\nThe personal access token is a unique token associated with your RudderStack account. It is required to access and consume the RudderStack APIs. For more information, see [Personal Access Token](https://www.rudderstack.com/docs/dashboard-guides/personal-access-token/).\n\n## Integrations\n\n### SDKs\n\n#### I want to use the RudderStack JavaScript SDK to track impressions in an ecommerce site. How can I send the impression data in batches? I could not find the `batch` method in the SDK.\n\nYou should use the `track` method instead. For the JavaScript SDK’s `track` method parameters specific to ecommerce, you can refer to the [Ecommerce Events Spec](https://www.rudderstack.com/docs/event-spec/ecommerce-events-spec/).\n\n#### Is Shopify compatible as a data source for RudderStack?\n\nYes, Shopify is compatible as an event stream data source. For more information, refer to [Shopify](https://www.rudderstack.com/docs/sources/event-streams/cloud-apps/shopify/). We also have users that integrate the JavaScript SDK into their Shopify sites. In some cases, they even do it through Google Tag Manager. However, RudderStack strongly recommends using the Shopify source integration for better tracking.\n\n```\n\"\"Unauthorized\"\",\"\"description\"\": The server could not verify that you are authorized to access the requested resource.\"\"\n```\n\nIf you encounter this error, it is most likely because of faulty permissions. Try editing the Zendesk Chat source and reauthorizing it again.\n\nRudderStack does not persist any data on its own. Rather, it fetches the data from the source based on the last timestamp it was extracted.\n\n### Destinations\n\n#### Would a destination connected with a source work if it is connected to a new source?\n\nYes, you can connect a destination to multiple sources with no issues.\n\n#### How do I see the logs or the data that is sent to my destination?\n\nTo view the data or events that are sent to your destination, you can use the [Live Events](https://www.rudderstack.com/docs/dashboard-guides/live-events/) tab on your destination page.\n\n#### I would like to send events to Mixpanel via RudderStack. However, I would like to set a filtering condition on the source events before routing them to Mixpanel. How do I do this?\n\nYou can use [Transformations](https://www.rudderstack.com/docs/transformations/overview/) to set custom logic on your events before sending them to Mixpanel.\n\n#### I am seeing a `Message type not supported` error. What does this mean?\n\nThis error is being returned from the RudderStack back end. It means that a particular destination does not support the event you are trying to send.\n\nFor example, Salesforce only supports `identify` events. Therefore, if a `track` call is sent to Salesforce, the `Message type not supported` error will be returned. This error does not affect any other events and is harmless. However, a simple user transformation can be written to filter out these events so you will no longer see this error.\n\n### Warehouse destinations\n\n> ![success](https://www.rudderstack.com/docs/images/tick.svg)\n> \n> For a comprehensive list of warehouse destinations-specific FAQ, refer to the [Warehouse FAQ](https://www.rudderstack.com/docs/destinations/warehouse-destinations/faq/) guide.\n\n#### How can I force RudderStack to push all data to a data warehouse in real-time with no delay? During the implementation, it would be better to see how the data is collected in real-time, rather than 30 minutes later.\n\nYou can override the UI-set sync frequency by setting `warehouseSyncFreqIgnore` to true in [`config.yaml`](https://github.com/rudderlabs/rudder-server/blob/master/config/config.yaml) (or `config.toml`, in case you have an older RudderStack deployment). You can set your desired frequency by changing the `uploadFreqInS` parameter.\n\n#### I am using RudderStack to mirror my source tables to PostgreSQL. I have all of the data in the S3 staging folders. But RudderStack doesn’t create the corresponding PostgreSQL tables when I click ‘sync’. What do I do?\n\n1.  Firstly, make sure you have set up the required [user permissions](https://www.rudderstack.com/docs/destinations/warehouse-destinations/postgresql/#setting-user-permissions-in-postgresql) for PostgreSQL.\n2.  Then, check the status of the sync in the [RudderStack dashboard](https://app.rudderstack.com/syncs).\n3.  Check if the database is accessible by allowlisting all RudderStack IPs listed [here](https://www.rudderstack.com/docs/destinations/warehouse-destinations/faq/#which-rudderstack-ips-should-i-allowlist).\n4.  Ensure that all security group policies for S3 are set as specified [here](https://www.rudderstack.com/docs/destinations/streaming-destinations/amazon-s3/#permissions).\n\n#### When sending data into a data warehouse, how can I change the table where this data is sent?\n\nBy default, RudderStack sends the data to the table/dataset based on the source it is connected to. For example, if the source is Google Tag Manager, RudderStack sets the schema name as **`gtm_*`**.\n\nYou can override this by setting a **Namespace** in the destination settings as shown:\n\n[![Set namespace in the destination settings](https://www.rudderstack.com/docs/images/general/wh-destination-namespace.webp)](https://www.rudderstack.com/docs/images/general/wh-destination-namespace.webp)\n\n#### I am trying to set `warehouseSyncFreqIgnore = true` to have a real-time sync with BigQuery but I can’t find the `config.yaml` file. How can I do this using the Docker setup?\n\nYou can do so by setting this value via the following `.env` variables:\n\n*   `RSERVER_WAREHOUSE_WAREHOUSE_SYNC_FREQ_IGNORE`\n*   `RSERVER_WAREHOUSE_UPLOAD_FREQ_IN_S`\n\n#### I’m looking to send data to my data warehouse through RudderStack and I’m trying to understand what data is populated in each column. How do I go about this?\n\nRefer to the [Warehouse Schema](https://www.rudderstack.com/docs/destinations/warehouse-destinations/warehouse-schema/) guide for details on how RudderStack generates the schemas in the warehouse.\n\n#### I am trying to load data into my BigQuery destination and I get this error:\n\n```\nbackend_1 | {Location: “”; Message: “Cannot read and write in different locations: \nsource: US, destination: us-central1\"\"; Reason: “invalid”}\"\n```\n\nMake sure that both your BigQuery dataset and the bucket have the same region. For more information, refer to the [BigQuery documentation](https://cloud.google.com/bigquery/docs/locations#data-locations).\n\n#### When sending data to BigQuery, I can set the bucket but not a folder within the bucket. Is there a way to put RudderStack data in a specific bucket folder?\n\nYes, you can set the desired folder name in the **Prefix** input field while configuring your BigQuery destination. For more information, refer to the [BigQuery setup guide](https://www.rudderstack.com/docs/destinations/warehouse-destinations/bigquery/#configuring-google-bigquery-destination-in-rudderstack).\n\n## RudderStack failover, hardening, and security\n\n#### What cloud infrastructure is the RudderStack hosted solution running on? Do you have failover to alternate availability zones?\n\nRudderStack’s hosted solution is running on AWS EKS with the cluster spanning 3 availability zones (`east-1a`, `east-1b`, `east-1c`).\n\n#### How does RudderStack ensure uptime with a single node?\n\n*   At the infrastructure layer, RudderStack runs on a multi-availability zone EKS cluster. So hardware failures, if any, are handled by Kubernetes by relocating pods.\n*   At an application level, RudderStack operates in one of the following 3 modes:\n    *   **Normal** mode: everything is normal and there are no issues.\n    *   If for some reason the system fails (e.g. because of a bug), it enters the **Degraded** mode, where RudderStack processes incoming requests but doesn’t send them to destinations.\n    *   If the system continues to fail to process the data, e.g., internal database corruption, it enters **Maintenance** mode. In this mode we save the previous state (which can be debugged and processed) and start from scratch— still receiving requests.\n*   All of RudderStack’s SDKs also have failure handling. They can store events in local storage and retry on failure.\n*   RudderStack provides isolation between the data and control planes. For example, if the control plane (where you manage the source and destination configurations) goes offline, the data plane continues to operate.\n\nAll this is done to ensure that RudderStack can always receive events and no events are lost.\n\n#### Would adding an additional node to RudderStack cause an outage, and if so what is the expected downtime? How long would it take to recover from backup?\n\nAdding a new node requires a bit of downtime. However, RudderStack is built in a way that minimizes this downtime as much as possible.\n\nWhen a new node is added, the users need to be rebalanced across nodes to keep event ordering in place. While the rebalancing takes place (can take a few minutes), RudderStack does not send events to downstream destinations, but continues to receive events so that your SDKs don’t see any failures, ignoring the small ELB switch over time.\n\nAdditionally, the SDKs have a built-in local caching and retrying capability. So even if there is a failure, no events are lost.\n\n## Monitoring and alerting\n\n#### How do I set up alerts in my Grafana dashboard?\n\nYou can use the Grafana dashboard to set up time-critical alerts for various use cases like:\n\n*   Aborted/failed events to destinations\n*   Event volume spikes crossing a defined threshold.\n\nThe following video tutorial walks you through setting up and enabling alerts in your Grafana dashboard:\n\n## IP allowlisting\n\n#### What are the IPs to be allowlisted?\n\nTo enable network access to RudderStack, allowlist the following RudderStack IPs depending on your region and [RudderStack Cloud plan](https://www.rudderstack.com/pricing):\n\n| Plan | Region |     |\n| --- | --- | --- |\n|     | **<br><br>US<br><br>** | **<br><br>EU<br><br>** |\n| Free, Starter, and Growth | *   3.216.35.97<br>*   18.214.35.254<br>*   23.20.96.9<br>*   34.198.90.241<br>*   34.211.241.254<br>*   52.38.160.231<br>*   54.147.40.62 | *   3.123.104.182<br>*   3.125.132.33<br>*   18.198.90.215<br>*   18.196.167.201 |\n| Enterprise | *   3.216.35.97<br>*   34.198.90.241<br>*   44.236.60.231<br>*   54.147.40.62<br>*   100.20.239.77 | *   3.66.99.198<br>*   3.64.201.167<br>*   3.123.104.182<br>*   3.125.132.33 |\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> All the outbound traffic is routed through these RudderStack IPs.\n\n#### Which URLs should be allowlisted for a content security policy?\n\nFor a content security policy, the following URLs should be allowlisted:\n\n**Control plane**\n\n*   `https://api.rudderstack.com`\n*   `https://api.rudderlabs.com`\n\n**Data plane**\n\n*   `DATA_PLANE_URL`\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> Refer to this [FAQ](#what-is-a-data-plane-url-where-do-i-get-it) for more information on obtaining your data plane URL.\n\n**SDK**\n\n*   `https://cdn.rudderlabs.com`\n\n## Retry behavior\n\n#### How does RudderStack handle retries for failed events in case of destination failure?\n\nSometimes the downstream destination can be unavailable or send a failure code for a variety of reasons. RudderStack retries sending the events depending on the type of failure:\n\n| Failure Code | Retry Behavior |\n| --- | --- |\n| `5XX`, `429` | Retry for a time window of 3 hours with exponential backoff and a minimum of 3 times. |\n| `4XX` | Abort without any retries. |\n\nThe above behavior is configurable via config variables in [`config.yaml`](https://github.com/rudderlabs/rudder-server/blob/master/config/config.yaml).\n\n```\n[Router]\nretryTimeWindowInMins = 180\nminRetryBackoffInS = 10\nmaxRetryBackoffInS = 300\nmaxFailedCountForJob = 8\n```\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> If a user event fails, the other events are not sent until the failed event is successfully sent or aborted, as per above behavior. This is to ensure event ordering for all events from a single user.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> For more information on the SDK-specific retry and backoff logic, refer to the [SDK FAQ](https://www.rudderstack.com/docs/sources/event-streams/sdks/faq/#what-is-the-rudderstack-retry-and-backoff-logic-after-the-connection-fails) guide.\n\n## Throttling behavior\n\n#### Some destinations have limits on the number of events they accept. How does RudderStack handle this?\n\nSome downstream destinations have limits on the number of events they accept at an account or user/device level. RudderStack tries to throttle the API requests as per the destination’s limits.\n\nSome examples are:\n\n*   [Customer.io](https://customer.io/docs/api/#api-documentationlimits)\n*   [Amplitude upload limit](https://www.docs.developers.amplitude.com/analytics/apis/http-v2-api/#upload-limit)\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> If you are a RudderStack-managed customer, contact [Customer Success](mailto:support@rudderstack.com) to set up throttling for your destination.\n\nFor [RudderStack Open Source](https://www.rudderstack.com/docs/get-started/rudderstack-open-source/), you can configure these limits using the variables in [`config.yaml`](https://github.com/rudderlabs/rudder-server/blob/6da163be3d0d777f07c18154085a6bd9cc386af4/config/config.yaml#L120).\n\n```\nthrottler:\n    MARKETO:\n      limit: 45\n      timeWindow: 20s\n\n# throttling by destinationID\n    <destination_id>:\n      limit: 90\n      timeWindow: 10s\n```\n\nTo get the destination ID, go to the **Settings** tab of your destination:\n\n[![Redis destination ID](https://www.rudderstack.com/docs/images/rudderstack-api/redis-destination-id.webp)](https://www.rudderstack.com/docs/images/rudderstack-api/redis-destination-id.webp)\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "FAQ | RudderStack Docs",
    "description": "Get answers to frequently asked questions about RudderStack.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.14/cohorts/",
    "markdown": "# Cohorts | RudderStack Docs\n\nCreate core customer segments in your warehouse and use them for targeted campaigns.\n\n* * *\n\n*     5 minute read  \n    \n\n**Cohort** is a subset of [entityEntity refers to a digital representation of a class of real world distinct objects for which you can create a profile.](https://www.rudderstack.com/docs/resources/glossary/#entity) instances meeting a specified set of characteristics, behaviors, or attributes. For example, if you have user as an entity, you can define cohorts like known users, new users, or North American users.\n\nUsing RudderStack Profiles, you can create the desired cohorts for entities and target specific user segments by enabling targeted campaigns and analysis.\n\n## Define cohorts\n\nProfiles lets you define cohorts as a model under `model_type` field in your `profiles.yaml` file:\n\n*   **Default cohort**: When you define an entity, a default cohort `<entity>/all` (`user/all` for the `user` entity) is created automatically. It contains the set of all instances of that entity. Any other cohort you define for that entity is derived from it.\n*   **Derived cohort**: When you define a cohort based on a pre-existing cohort (base cohort), it becomes a derived cohort. A derived cohort inherits the features of the base cohort. You can filter out the member instances of the base cohort based on a set of characteristics, behaviors, or attributes for the derived cohort. You must specify the base cohort in the derived cohort’s definition using the `extends` field.\n\nFor example, `known_users` is a cohort derived from the base cohort `user/all` (set of all users), whereas `known_mobile_users` is derived from its base cohort `known_users`.\n\nWhen you run a Profiles project including cohorts, the output of the cohort is stored in a table/view with the same name.\n\n### Sample cohort\n\nYou can apply filters using the `include`/`exclude` clauses to specify a boolean expression over any of the entity vars defined on the base cohort or its ancestors. Certain features might hold relevance only for the specific cohorts. For example, `SSN` feature may only be applicable for American users.\n\n**Example 1**: Let’s consider the following `profiles.yaml` file which defines a cohort `knownUsUsers` to include users from US with a linked email address.\n\n```\nmodels:\n  - name: knownUsUsers\n    model_type: entity_cohort\n    model_spec:\n      extends: user/all\n      materialization:\n        output_type: table\n      filter_pipeline:\n        - type: exclude\n            # exclude users which don't have any linked email.\n          value: \"{{ user.Var('id_type_email_count') }} = 0\"\n        - type: include\n            # include users with country US.\n          value: \"{{ user.Var('country') }} = 'US'\"   \n```\n\nHere, the `extends` keyword specifies the base cohort `users/all`. You can also specify the path of a custom defined base cohort, if applicable. The `value` field in `filter_pipeline` must be a boolean expression over any of the `entity_vars` defined on the base cohort or its ancestor cohorts.\n\n**Example 2**: Let’s derive the `us_credit_card_users` cohort from the `knownUsUsers` as a base cohort. It filters the known US users who possess a credit card and have spent more than 10 thousand USD in last transaction. The `extends` field specifies the path of the base cohort which is `models/knownUsUsers`.\n\n```\nmodels:\n\t-  name: us_credit_card_users\n     model_type: entity_cohort\n     model_spec:\n       extends: models/knownUsUsers\n       materialization:\n         output_type: view\n       filter_pipeline:\n         - type: include\n           value: \"{{ knownUsUsers.Var('has_credit_card') }} = 1\"\n\t       - type: include\n\t\t       value: \"{{ user.Var('last_transaction_value') }} > 10000\"\n```\n\n**Example 3**: Let’s consider another scenario where you can unify different cohorts (`north_american_users`, and `south_american_users`) to create a new cohort.\n\n```\nmodels:\n  - name: american_users\n    model_type: entity_cohort\n    model_spec:\n       extends: user/all\n       filter_pipeline:\n         - type: include\n           sql:\n             select: user_main_id\n             from: \"{{ this.DeRef('models/north_american_users') }}\"\n         - type: include\n           sql:\n             select: user_main_id\n             from: \"{{ this.DeRef('models/south_american_users') }}\"\n             where: user_main_id != null\n```\n\n### Associate features with cohort\n\nYou can also use `var_groups` to target a cohort instead of an entire entity which will provide a comprehensive 360-degree view combining relevant features.\n\nTo do so, associate features with a cohort by specifying the `entity_cohort` key and passing the cohort’s path to it within a `var_group`, as shown:\n\n```\nvar_groups:\n  - name: known_us_users_vars #vars targeted to knownUsUsers cohort\n    entity_cohort: models/knownUsUsers\n    vars:\n      - entity_var:\n          name: has_credit_card\n          select: max(case when lower(payment_details_credit_card_company) in ('visa','american express','mastercard') then 1 else 0 end)\n          from: inputs/rsOrderCreated\n          description: If the user has a credit card.\n          default: false\n          \n  - name: user_vars #vars targeted to default user/all cohort\n    entity_key: user\n    vars:\n      - entity_var:\n          name: last_transaction_value\n          from: inputs/rsOrderCreated\n          select: first_value(total_price_usd)\n          window:\n            order_by:\n              - case when TOTAL_PRICE_USD is not null then 2 else 1 end desc\n              - timestamp desc\n      - entity_var:\n          name: country\n          from: inputs/rsIdentifies\n          select: first_value(address_country)\n          where: address_country is not null and address_country != ''\n          window:\n            order_by:\n              - timestamp desc\n```\n\nTo apply the features to the entire user entity, you can use an `entity_key` in `user_vars`.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> In a `var_group`, you can use either `entity_key` or `entity_cohort` but not both. Setting `entity_key` as `user` is equivalent to setting `entity_cohort` as `user/all`.\n\n### Feature view of cohort\n\nYou can establish a holistic **360 feature view** of a cohort within its definition. This view consolidates all the features associated with the specified identifiers, providing a complete overview of the cohort.\n\nThe following example shows how to define a feature view for the `knownUsUsers` cohort:\n\n```\nmodels:\n  - name: knownUsUsers\n    model_type: entity_cohort\n    model_spec:\n      extends: users/all\n      materialization:\n        output_type: table\n      filter_pipeline:\n        - type: exclude\n            # exclude users which don't have any linked email.\n          value: \"{{ user.Var('id_type_email_count') }} = 0\"\n        - type: include\n            # include users with country US.\n          value: \"{{ user.Var('country') }} = 'US'\"\n      # to define a 360 feature view of knownUsUsers cohort [optional]\n      feature_views:\n        # view with entity's `main_id` as identifier\n        name: known_us_users_feature_view\n        using_ids:\n          - id: email\n            # view with `email` as identifier\n            name: us_users_with_email\n```\n\nHere, the `known_us_users_feature_view` view contains all the features of the `knownUsUsers` cohort and uses `main_id` as the identifier. There is another `us_users_with_email` view which also contains all the features of the `knownUsUsers` cohort but uses `email` as the identifier (specified in `using_ids` field).\n\n## Use cohorts\n\nOnce you have defined cohorts in your `profiles.yaml` file, you can choose to run your project in either of the following ways:\n\n### Profile CLI\n\nRun your [Profiles CLI project](https://www.rudderstack.com/docs/archive/profiles/0.14/get-started/profile-builder/) using the `pb run` command to generate output tables.\n\n### Profiles UI\n\nTo view cohorts in the RudderStack dashboard, you can make your Profiles CLI project available in a Git repository and import it in the RudderStack dashboard. See [Import Profiles Project from Git](https://www.rudderstack.com/docs/archive/profiles/0.14/get-started/import-from-git/) for more information.\n\nOnce imported, you can run your project by navigating to the **History** tab and clicking **Run**. After a successful run of the project, you can view the output for cohorts in the **Entities** tab of the project:\n\n[![Activation API](https://www.rudderstack.com/docs/images/profiles/cohorts-view-ui.webp)](https://www.rudderstack.com/docs/images/profiles/cohorts-view-ui.webp)\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> Contact Profiles support team in RudderStack’s [Community Slack](https://rudderstack.com/join-rudderstack-slack-community) if you are unable to see the **Entities** tab.\n\nYou can further activate your cohorts data by syncing it to the downstream destinations. See [Activations](https://www.rudderstack.com/docs/archive/profiles/0.14/activations/) for more information.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Cohorts | RudderStack Docs",
    "description": "Create core customer segments in your warehouse and use them for targeted campaigns.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.14/cli-user-guide/commands/",
    "markdown": "# Commands | RudderStack Docs\n\nLearn about the Profiles commands and how to use them.\n\n* * *\n\n*     12 minute read  \n    \n\nThe Profile Builder tool supports specific commands, making executing the usual operations easier. The basic syntax of executing a command is:\n\n```\n$ pb <command> <subcommand> [parameters]\n```\n\n## Supported commands\n\nYou can use the following Profile Builder commands:\n\n### cleanup\n\nDisplays and removes materials, older than the retention time period specified by the user (default value is 180 days).\n\n```\npb cleanup materials -r <number of days>\n```\n\n**Optional Parameter**\n\n| Parameter | Description |\n| --- | --- |\n| `-r` | Retention time in number of days.<br><br>**Example**: If you pass 1, then all the materials created prior to one day (24 hours) are listed. This is followed by prompts asking you for confirmation, after which you can view the material names and delete them. |\n\n### compile\n\nGenerates SQL queries from models.\n\nIt creates SQL queries from the `models/profiles.yaml` file, storing the generated results in the **Output** subfolder in the project’s folder. With each run, a new folder is created inside it. You can manually execute these SQL files on the warehouse.\n\n**Optional parameters**\n\n| Parameter | Description |\n| --- | --- |\n| `clean_output` | Empties the output folder(s) before executing the command. |\n| `-c` | Uses a site configuration file other than the one in `.pb` directory.<br><br>**Example**: `$ pb compile -c MyOtherConnection/siteconfig.yaml` |\n| `-t` | Defines target name (mentioned in `siteconfig.yaml`) or timestamp in building the model.<br><br>**Example**: If your `siteconfig.yaml` has two targets, `dev` and `test`, and you want to use the `test` instance: `$ pb compile -t test` |\n| `--begin_time` | Timestamp to be used as a start time in building model. |\n| `--end_time` | Timestamp to be used as an end time in building model. |\n| `--migrate_on_load` | Whether to automatically migrate the project and packages to the latest version. Defaults to false. |\n| `--migrated_folder_path` | Folder location of the migrated project. Defaults to sub-directory of the project folder. |\n| `-p` | *   Uses a project file (`pb_project.yaml`) other than the one in current directory.  <br>    **Example**: `$ pb compile -p MyOtherProject`.<br>  <br>*   Fetches project from a URL such as GitHub.  <br>    **Example**:`$ pb compile -p git@github.com:<orgname>/<repo>`. You can also fetch a specific tag, like `$ pb compile -p git@github.com:<orgname>/<repo>/tag/<tag_version>/<folderpath>` |\n| `--rebase_incremental` | Rebases any incremental models (build afresh from their inputs) instead of starting from a previous run. You can do this every once in a while to address the stale data or migration/cleanup of an input table. |\n\n### discover\n\nDiscovers elements in the warehouse, such as models, entities, features and sources.\n\nIt allows you to discover all the registered elements in the warehouse.\n\n**Subcommands**\n\nDiscover all the `models`, `entities`, `features`, `sources`, and `materials` in the warehouse.\n\n```\n$ pb discover models\n$ pb discover entities\n$ pb discover features\n$ pb discover sources\n$ pb discover materials\n```\n\n**Optional parameters**\n\n| Parameter | Description |\n| --- | --- |\n| `-e` | Discovers specific entities with their name.<br><br>**Example**: `$ pb discover -e 'Name'` |\n| `-m` | Discovers a specific model.<br><br>**Example**: `$ pb discover -m 'MY_DATABASE.PROD_SCHEMA.CREATED_MODEL'` |\n| `-c` | Uses a site config other than the default one.<br><br>**Example**: `$ pb discover -c siteconfig.yaml` |\n| `-s` | Discovers entities in a specified schema. |\n| `-s \"*\"` | Discovers entities across all schemas (case-sensitive). |\n| `-u` | Discovers entities having the specified source URL’s.<br><br>**Example**: To discover all the entities coming from GitHub: `$ pb discover -u %github%` |\n| `-t` | Selects target (mentioned in `siteconfig.yaml`). |\n| `-p` | Uses project folder other than the one in current directory.<br><br>**Example**: `$ pb discover -p ThisFolder/ThatSubFolder/SomeOtherProject/` |\n| `-f` | Specifies a file path to dump the discovery output into a csv file.<br><br>**Example**: `$ pb discover -f path/to/csv_file.csv` |\n| `-k` | Restricts discovery of the specified model keys.<br><br>**Example**: `$ pb discover -k entity_key:mode_type:model_name` |\n| `--csv_file` | Specify this flag with a file path to dump the discovery output into a csv file. |\n\n**Examples**\n\n```\n# Discover all the models\n$ pb discover models\n\n# Discover a model with specific name\n$ pb discover -m 'RUDDER_WEB_EVENTS.PROD_SCHEMA.feature_profile'\n\n# Discover all features having 'max' in their name\n$ pb discover features -u %max%\n\n# Discover all the entities for a specific profile\n$ pb discover entities -c siteconfig.yaml\n\n# Discover all materials for target dev\n$ pb discover materials -t dev\n\n# Export output of discover command to a CSV file in output folder\n$ pb discover -f my-custom-name.csv\n\n# Export all sources to a CSV file in output folder\n$ pb discover sources -f my-custom-name.csv\n```\n\n### help\n\nProvides list information for any command.\n\n**Subcommand**\n\nGet usage information for a specific command, with subcommands, and optional parameters.\n\n### init\n\nCreates connection and initializes projects.\n\n**Subcommands**\n\nInputs values for a warehouse connection and then stores it in the `siteconfig.yaml` file.\n\nGenerates files in a folder named **HelloPbProject** with sample data. You can change it as per project information, models, etc.\n\n**Optional parameters**\n\n| Parameter | Description |\n| --- | --- |\n| `pb-project -o` | Creates a Profile Builder project with a different name by specifying it as an additional parameter.<br><br>**Example**: To create a Profile Builder project with the name **SomeOtherProject**: `$ pb init pb-project -o SomeOtherProject` |\n| `connection -c` | Creates `siteconfig.yaml` at a location other than `.pb` inside home directory.<br><br>**Example**: To create `myconfig.yaml` in the current folder: `$ pb init connection -c myconfig.yaml`. |\n\n### insert\n\nAllows you to store the test dataset in your (Snowflake) warehouse . It creates the tables `sample_rs_demo_identifies` and `sample_rs_demo_tracks` in your warehouse schema specified in the `test` connection.\n\n```\n# Select the first connection named test having target and output as dev, of type Snowflake.\n$ pb insert\n# By default it'll pick up connection named test. To use connection named red:\n$ pb insert -n red\n# To pick up connection named red, with target test .\n$ pb insert -n red -t test\n```\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> This command is supported only for Snowflake currently.\n\n### migrate\n\nMigrate your project to the latest schema.\n\n**Subcommands**\n\nBased on the current schema version of your project, it enlists all the steps needed to migrate it to the latest one.\n\nAutomatically migrate from one version to another.\n\nTo migrate your models:\n\n**Schema 44 onwards**\n\nNavigate to the folder where your project files are stored. Then execute one of the following:\n\n*   `pb migrate auto --inplace`: Replaces contents of existing folder with the migrated folder.\n*   `pb migrate auto -d <MigratedFolder>`: Keeps the original project intact and stores the migrated project in another folder.\n\n**Schema 43 -> 44:**\n\nUse `{{entity-name.Var(var-name)}}` to refer to an `entity-var` or an `input-var`.\n\nFor example, for entity\\_var `user_lifespan` in your HelloPbProject, change `select: last_seen - first_seen` to `select: '{{user.Var(\"last_seen\")}} - {{user.Var(\"first_seen\")}}'`.\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> Note that:\n> \n> *   You must use two curly brackets.\n> *   Anything contained within double curly brackets must be written in double quotes (`\" \"`). If you use single quotes within double quotes, then use the escape character (`\\`) that comes when using macros.\n\nFurther, navigate to the folder where your project files are stored. Then execute one of the following:\n\n*   `pb migrate auto --inplace`: Replaces contents of existing folder with the migrated folder.\n*   `pb migrate auto -d <MigratedFolder>`: Keeps the original project intact and stores the migrated project in another folder.\n\n**Linear dependency**\n\nSpecify this parameter when entity as vars migration is not done (till version 43). After the migration is done, it’s not necessary to mention this parameter and can be removed.\n\n```\n  compatibility_mode:\n    linear_dependency_of_vars: true\n```\n\n**Optional parameters**\n\n| Parameter | Description |\n| --- | --- |\n| `-p` | Uses a project file other than the one in current directory. |\n| `-c` | Uses a `siteconfig.yaml` file other than the one in your home directory. |\n| `-t` | Target name (defaults to the one specified in `siteconfig.yaml` file). |\n| `-v` | Version to which the project needs to be migrated (defaults to the latest version). |\n| `-d` | Destination folder to store the migrated project files.<br><br>**Example**: `pb migrate auto -d FolderName` |\n| `--force` | Ignores warnings (if any) and migrates the project. |\n| `--inplace` | Overwrites the source folder and stores migrated project files in place of original.<br><br>**Example**: `pb migrate auto --inplace` |\n| `-p` | Uses a project folder other than the one in current directory.<br><br>**Example**: `$ pb discover -p ThisFolder/ThatSubFolder/SomeOtherProject/` |\n| `-f` | Specifies a file path to dump the discovery output into a csv file.<br><br>**Example**: `$ pb discover -f path/to/csv_file.csv` |\n| `-k` | Restricts discovery of the specified model keys.<br><br>**Example**: `$ pb discover -k entity_key:mode_type:model_name` |\n\n### run\n\nCreates identity stitcher or feature table model in the Warehouse.\n\nIt generates the SQL files from models and executes them in the warehouse. Once executed, you can see the output table names, which are accessible from the warehouse.\n\n**Optional parameters**\n\nThe `run` command shares the same parameters as the [`compile`](#compile) command, in addition to the following ones:\n\n| Parameter | Description |\n| --- | --- |\n| `--force` | Does a force run even if the material already exists. |\n| `-- write_output_csv` | Writes all the generated tables to CSV files in the specified directory.<br><br>**Example**: `$ pb run -- WriteOutputHere.csv` |\n| `--model_args` | Customizes behavior of an individual model by passing configuration params to it.<br><br>The only argument type supported currently is `breakpoint` for feature table models.<br><br>The `breakpoint` parameter lets you generate and run SQL only till a specific feature/tablevar. You can specify it in the format `modelName:argType:argName` where argName is the name of feature/tablevar.<br><br>**Example**: `$ pb run --model_args domain_profile:breakpoint:salesforceEvents` |\n| `--model_refs` | Restricts the operation to a specified model. You can specify model references like `pb run --model_refs models/<model-name> --seq_no latest` |\n| `--seq_no` | Sequence number for the run, for example, 0, 1, 2,…, latest/new. The default value is `new`. You can check run logs or use discover commands to know about existing sequence numbers. |\n| `--ignore_model_errors` | Allows the project to continue to run in case of an erroneous model. The execution will not stop due to one bad model. |\n| `--grep_var_dependencies` | Uses regex pattern matching over fields from vars to find references to other vars and set dependencies. By default, it is set to `true`. |\n| `--concurrency` | (_Experimental_) Lets you run the models concurrently in a warehouse (wherever possible) based on the dependency graph. In CLI, you can specify the concurrency level for running models in a project via `pb run --concurrency <int>` (default int value is 1). Currently, this is supported only for Snowflake warehouse. It is recommended to use this option judiciously as applying a large value may not be supported by your warehouse. The concurrency limit for Snowflake is 20. To increase the limit, see [Snowflake docs](https://community.snowflake.com/s/question/0D50Z00008VjQDkSAN/how-to-handle-thenumberofwaitersexceedsthe20statementslimit-error). |\n| `--begin_time` | Timestamp to be used as a start time in building model. |\n| `--end_time` | Timestamp to be used as an end time in building model. |\n| `--migrate_on_load` | Whether to automatically migrate the project and packages to the latest version. Defaults to false. |\n| `--migrated_folder_path` | Folder location of the migrated project. Defaults to sub-directory of the project folder. |\n| `--include_untimed` | Whether to include data without timestamps when running models. Defaults to true. |\n\n### show\n\nObtains a comprehensive overview of models, id\\_clusters, packages, and more in a project. Its capacity to provide detailed information makes it particularly useful when searching for specific details, like all the models in your project.\n\n**Subcommands**\n\n1.  `pb show models`\n\nThis command lets you view information about the models in your project. The output includes the following information about each model:\n\n*   **Warehouse name**: Name of the table/view to be created in the warehouse.\n*   **Model type**: Whether its an identity stitching, feature table, SQL model etc.\n*   **Output type**: Whether the output type is `ephemeral`, `table`, or `view`.\n*   **Run type**: Whether the model’s run type is `discrete` or `incremental`.\n*   **SQL type**: Whether the SQL type of the model is `single_sql` or `multi_sql`.\n\n2.  `pb show models --json`\n\nThis subcommand saves all model details in a JSON file.\n\n3.  `pb show dependencies`\n\nThis subcommand generates a graph file (`dependencies.png`) highlighting the dependencies of all models in your project.\n\n4.  `pb show dataflow`\n\nThis subcommand generates a graph file (`dataflow.png`) highlighting the data flow of all models in your project.\n\n5.  `pb show idstitcher-report --id_stitcher_model models/<ModelName> --migrate_on_load`\n\nThis subcommand creates a detailed report about the identity stitching model runs. To know the exact modelRef to be used, you can execute `pb show models`. By default, it picks up the last run, which can be changed using flag `-l`. The output consists of:\n\n*   **ModelRef**: The model reference name.\n*   **Seq No**: Sequence number of the run for which you are creating the report.\n*   **Material Name**: Output name as created in warehouse.\n*   **Creation Time**: Time when the material object was created.\n*   **Model Converged**: Indicates a successful run if `true`.\n*   **Pre Stitched IDs before run**: Count of all the IDs before stitching.\n*   **Post Stitched IDs after run**: Count of unique IDs after stitching.\n\nProfile Builder also generates a HTML report with relevant results and graphics including largest cluster, ID graph, etc. It is saved in `output` folder and the exact path is shown on screen when you execute the command.\n\n6.  `pb show entity-lookup -v '<trait value>'`\n\nThis subcommand lists all the features associated with an entity using any of the traits (flag `-v`) as ID types (email, user id, etc. that you are trying to discover).\n\n**Optional parameters**\n\n| Parameter | Description |\n| --- | --- |\n| `--entity string` | (Optional) Passes the entity value. (default `user`). |\n| `-h` | Displays help information for the command. |\n| `-p` | Specifies the project path to list the models. If not specified, it uses the project in the current directory. |\n| `-c` | File location of the `siteconfig.yaml` (defaults to the one in your home directory). |\n| `-t` | Target name (defaults to the target specified in `siteconfig.yaml` file). |\n| `--include_disabled` | Lets the disabled models be a part of the generated graph image (applicable to [`dataflow` and `dependencies`](#show)). |\n| `--seq_no` | Specifies a particular run for an ID stitcher model (applicable for [`idstitcher-report`](#show)). |\n\n### query\n\nExecutes SQL query on the warehouse and prints the output on screen (10 rows by default).\n\nFor example, if you want to print the output of a specific table/view named `user_id_stitcher`, run the following query:\n\n```\npb query \"select * from user_id_stitcher\"\n```\n\nTo reference a model with the name `user_default_id_stitcher` for a previous run with seq\\_no 26, you can execute:\n\n```\npb query \"select * from {{this.DeRef('path/to/user_default_id_stitcher')}} limit 10\" --seq_no=26\n```\n\n**Optional parameters**:\n\n| Parameter | Description |\n| --- | --- |\n| `-f` | Exports output to a CSV file. |\n| `-max_rows` | Maximum number of rows to be printed (default is 10). |\n| `-seq_no` | Sequence number for the run. |\n\n### validate\n\nValidates aspects of the project and configuration.\n\nIt allows you to run various tests on the project-related configurations and validate those. This includes but is not limited to validating the project configuration, privileges associated with the role specified in the site configuration of the project’s connection, etc.\n\n**Subcommands**\n\nRuns tests on the role specified in the site configuration file and validates if the role has privileges to access all the related objects in the warehouse. It throws an error if the role does not have required privileges to access the input tables or does not have the permissions to write the material output in the output schema.\n\n### version\n\nShows the Profile Builder’s current version along with its GitHash and native schema version.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Commands | RudderStack Docs",
    "description": "Learn about the Profiles commands and how to use them.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.14/predictions/",
    "markdown": "# Predictions (Early Access) | RudderStack Docs\n\nUse Profiles’ predictive features to train machine learning models.\n\n* * *\n\n*     7 minute read  \n    \n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> Predictions is part of our [Early Access Program](https://www.rudderstack.com/docs/resources/early-access-program/), where we work with early users and customers to test new features and get feedback before making them generally available. These features are functional but can change as we improve them. We recommend connecting with our team before running them in production.\n> \n> [Contact us](https://www.rudderstack.com/contact/) to get access to this feature.\n\nPredictions extends Profiles’ standard [feature development](https://www.rudderstack.com/docs/archive/profiles/0.14/core-concepts/feature-development/) functionality. It lets you easily create predictive features in your warehouse and answer questions like:\n\n*   Is a customer likely to churn in the next 30 days?\n*   Will a user make a purchase in the next 7 days?\n*   Is a lead going to convert?\n*   How much is a user likely to spend in the next 90 days?\n\nFurther, you can add the predicted feature to user profiles in your warehouse automatically and deliver ML-based segments and audiences to your marketing, product, and customer success teams.\n\nThe following self-guided tour shows you how to build the predictive traits. You can also follow the [Predictions sample project](https://www.rudderstack.com/docs/archive/profiles/0.14/example/predictive-features-snowflake/) guide and build the project yourself, including sample data.\n\n## Use cases\n\n*   **Churn prediction**: Predicting churn is one of the crucial initiatives across businesses. Without a predicted churn score, your actions are reactive, whereas you can act proactively with a user trait like `is_likely_to_churn`. Once you have such features, you can activate them with the appropriate outreach programs to prevent user churn.\n    \n*   **Customer LTV prediction**: Predictions helps you understand your customers’ purchasing behavior over time. You can predict how much amount a particular customer is likely to spend within the predicted time range.\n    \n\n## Python model\n\nYou can generate predictive features using a `python_model` which involves two key steps - `train` and `predict`.\n\nThe following `profiles.yaml` file shows how to use a `python_model`:\n\n```\nmodels:\n  - name: shopify_churn\n    model_type: python_model\n    model_spec:\n      occurred_at_col: insert_ts\n      entity_key: user\n      validity_time: 24h # 1 day\n      py_repo_url: https://github.com/rudderlabs/rudderstack-profiles-classifier.git # Do not modify \n      # this value as the actual logic resides in this repo.\n      train:\n        file_extension: .json\n        file_validity: 60m\n        inputs: &inputs\n          - packages/feature_table/models/shopify_user_features\n        config:\n          data:\n            label_column: is_churned_7_days \n            label_value: 1\n            prediction_horizon_days: 7\n            output_profiles_ml_model: *model_name\n            eligible_users: lower(country) = 'us' and amount_spent_overall > 0\n            inputs: *inputs\n            entity_column: user_main_id\n            recall_to_precision_importance: 1.0\n          preprocessing: \n            ignore_features: [name, gender, device_type]\n      predict:\n        inputs:\n          - packages/feature_table/models/shopify_user_features\n        config:\n          outputs:\n            column_names:\n              percentile: &percentile_name percentile_churn_score_7_days\n              score: churn_score_7_days\n            feature_meta_data: &feature_meta_data\n              features:\n                - name: *percentile_name\n                  description: 'Percentile of churn score. Higher the percentile, higher the probability of churn'\n```\n\n#### Model parameters\n\nThe detailed list of parameters used in the `python_model` along with their description are listed below:\n\n| Parameter | Description |\n| --- | --- |\n| `py_repo_url`  <br>Required | The actual logic for Predictions resides in this remote repository. DO NOT modify this value. |\n| `file_extension`  <br>Required | Indicates the file type. This is a static value and does not need to be modified. |\n| `file_validity`  <br>Required | If the last trained model is older than this duration, then the model is trained again. |\n| `inputs`  <br>Required | Path to the base feature table project. You must add `&inputs` to it. |\n| `label_column`  <br>Required | Name of the feature (`entity_var`) you want to predict. It is defined in the feature table model. |\n| `label_value` | Expected label value for users who performed the event |\n| `prediction_horizon_days`  <br>Required | Number of days in future for which you want to make the prediction.<br><br>See [Prediction horizon days](https://www.rudderstack.com/docs/profiles/glossary/#prediction-horizon-days) for more information. |\n| `output_profiles_ml_model`  <br>Required | Name of the output model. |\n| `eligible_users` | Eligibilty criteria for the users for which you want to define predictive features. You can set this criteria by defining a SQL statement referring to the different `entity_vars`. To build a model for all the available users, you can leave this parameter as blank.<br><br>For example, if you want to train the model and make predictions only for the paying users from US, then define `country='US' and is_payer=true`. |\n| `config.data.inputs` | Path to the referenced project. |\n| `entity_column` | If you change the value of`id_column_name` in the ID stitcher model, you should specify it here. This field is optional otherwise. |\n| `recall_to_precision_importance` | Also referred to as **beta** in f-beta score, this field is used in classification models to fine-tune the model threshold and give more weight to recall against precision.<br><br>**Note**: This is an optional parameter. If not specified, it defaults to `1.0`, giving equal weight to precision and recall. |\n| `ignore_features` | List of columns from the feature table which the model should ignore while training. |\n| `percentile`  <br>Required | Name of column in output table having percentile score. |\n| `score`  <br>Required | Name of column in output table having probabilistic score. |\n| `description`  <br>Required | Custom description for the predictive feature. |\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> If you want to run your python model locally using a [CLI setup](https://www.rudderstack.com/docs/archive/profiles/0.14/get-started/profile-builder/), you must set up a python environment with the required packages and add the python path to your [siteconfig.yaml](https://www.rudderstack.com/docs/archive/profiles/0.14/cli-user-guide/structure/#site-configuration-file-configuration.md) file.\n\n## Project setup\n\nThis section highlights the project setup steps for a sample churn prediction and LTV model.\n\n### Prerequisites\n\n*   You must be using a [Snowflake](https://www.rudderstack.com/docs/destinations/warehouse-destinations/snowflake/), [BigQuery](https://www.rudderstack.com/docs/destinations/warehouse-destinations/bigquery/), or [Redshift](https://www.rudderstack.com/docs/destinations/warehouse-destinations/redshift/) warehouse.\n*   You must set up a standard Profiles project with a [feature table model](https://www.rudderstack.com/docs/archive/profiles/0.14/example/feature-table/).\n*   **Optional**: If you are using Snowflake, you might need to create a [Snowpark](https://www.snowflake.com/en/data-cloud/snowpark/)\\-optimized warehouse if your dataset is significantly large.\n\n### Churn prediction/LTV model\n\n#### 1\\. Create a Profiles project with Feature Table model\n\nFollow the [Feature table](https://www.rudderstack.com/docs/archive/profiles/0.14/example/feature-table/) guide to create a Profiles project. Your project must include the definition of the feature you want to predict.\n\nFor example, to predict 30-day inactive churn, you should define it as a feature (`entity_var`) in the feature table so that the model knows how to compute this for historic users.\n\n```\nentity_var:\n  name: churn_30_days\n  select: case when days_since_last_seen >= 30 then 1 else 0 end\n```\n\n#### 2\\. Create a python model and train it\n\nCreate a [`python_model`](#python-model) and pass the Feature table model as an input.\n\nAdd the following set of parameters in the `train` block:\n\n```\ntrain:\n    file_extension: .json\n    file_validity: 168h\n    inputs: &inputs\n      - packages/feature_table/models/shopify_user_features\n    config:\n      data: &model_data_input_configs\n        label_column: churn_30_days\n        label_value: 1\n        prediction_horizon_days: 30\n        output_profiles_ml_model: *model_name\n        eligible_users: ''\n        inputs: *inputs\n        entity_column: user_main_id\n        recall_to_precision_importance: 1.0\n      preprocessing: \n        ignore_features: [name, gender, device_type]\n```\n\n```\ntrain:\n    file_extension: .json\n    file_validity: 168h\n    inputs: &inputs\n      - packages/feature_table/models/shopify_user_features\n    config:\n      data: &model_data_input_configs\n        label_column: amount_spent_past_7_days\n        task: regression\n        prediction_horizon_days: 7\n        output_profiles_ml_model: *model_name\n        eligible_users: ''\n        inputs: *inputs\n        entity_column: user_main_id\n      preprocessing: \n        ignore_features: [name, gender, device_type]\n```\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> Note that:\n> \n> *   Unlike churn prediction, you should not specify the `label_value` and `recall_to_precision_importance` fields.\n> *   The LTV model introduces a new parameter called `task` which you must set to `regression`. Profiles assumes a classification model by default, unless explicitly specified otherwise.\n\n#### 3\\. Define predictive features\n\nAdd the following set of parameters in the `predict` block:\n\n```\npredict:\n    inputs:\n      - packages/feature_table/models/shopify_user_features\n    config:\n      data: *model_data_input_configs\n      outputs:\n        column_names:\n          percentile: &percentile_name percentile_churn_score_7_days\n          score: churn_score_7_days\n        feature_meta_data: &feature_meta_data\n          features:\n            - name: *percentile_name\n              description: 'Percentile of churn score. Higher the percentile, higher the probability of churn'\n```\n\n```\npredict:\n    inputs:\n      - packages/feature_table/models/shopify_user_features\n    config:\n      data: *model_data_configs\n      preprocessing: *model_prep_configs\n      outputs:\n        column_names:\n          percentile: &percentile_name percentile_predicted_amount_spent\n          score: predicted_amount_spent\n        feature_meta_data: &feature_meta_data\n          features:\n            - name: *percentile_name\n              description: 'Percentile of predicted future LTV. Higher the percentile, higher the expected LTV.'\n```\n\n#### 4\\. Run your project\n\nOnce you have created the project, you can choose to run it using either of the following ways:\n\n**Using Profile CLI**\n\nIf you have created your Predictions Profiles project locally, run it using the `pb run` [CLI](https://www.rudderstack.com/docs/archive/profiles/0.14/get-started/profile-builder/) command to generate output tables.\n\n**Using Profiles UI**\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> [Contact us](mailto:support@rudderstack.com) to enable this feature for your account.\n\nRun your Predictions Profiles project by first uploading it to a Git repository and then [importing it in the RudderStack dashboard](https://www.rudderstack.com/docs/archive/profiles/0.14/get-started/import-from-git/#steps).\n\n## Output\n\nOnce your project run is completed, you can:\n\n*   View the output materials in your warehouse for the predictive features.\n*   Check the predicted value for any given user in the RudderStack dashboard’s [Profile Lookup](https://www.rudderstack.com/docs/archive/profiles/0.14/get-started/quickstart-ui/#profile-details) section.\n*   View all predictive features in the **Entities** tab of your Profiles project:\n\n[![New personal access token in RudderStack dashboard](https://www.rudderstack.com/docs/images/profiles/predictive-features-2.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-2.webp)\n\nClick **Predictive features** to see the following view:\n\n[![New personal access token in RudderStack dashboard](https://www.rudderstack.com/docs/images/profiles/predictive-features.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features.webp)\n\nThe value of a predictive feature is a probability. You can consider it as `true` or `false` based on your threshold.\n\n## See Also\n\n*   [Predictive features](https://github.com/rudderlabs/rudderstack-profiles-classifier): Builds predictive features such as churn prediction, conversion prediction, etc.\n*   [Shopify churn model](https://github.com/rudderlabs/rudderstack-profiles-shopify-churn/): Builds a churn prediction score on top of the [Shopify library project](https://github.com/rudderlabs/profiles-shopify-features).\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Predictions (Early Access) | RudderStack Docs",
    "description": "Use Profiles' predictive features to train machine learning models.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.14/core-concepts/identity-stitching/",
    "markdown": "# Identity Stitching | RudderStack Docs\n\nStitch multiple identifiers together to create a unified and comprehensive profile.\n\n* * *\n\n*     4 minute read  \n    \n\nIdentity stitching combines unique identifiers across your digital touchpoints to identify users and create a unified, omnichannel view of your customers. Using this feature, you can:\n\n*   **Understand user behavior**: Consolidate and connect customer data from various sources to better understand customers’ preferences, behaviors, and interactions across multiple touchpoints.\n*   **Provide personalized support**: Deliver personalized marketing messages and experiences to your customers. Ensures that the right message reaches the right person at the right time, increasing the effectiveness of marketing campaigns.\n*   **Enrich user profile with features**: Enhance user profiles with additional data points and features. These features can include demographic information, preferences, purchase history, browsing behavior, or any other static or computed data points.\n\n## Problem of multiple identities\n\nCompanies gather user data across digital touchpoints like websites, mobile apps, enterprise systems like CRMs, marketing platforms, etc. During this process, a single user is identified with multiple identifiers across their product journey, like their email ID, phone number, device ID, anonymous ID, and more. Also, the user information is spread across dozens of devices, accounts, or products as they often change their devices and use work and personal emails together.\n\nThe user data stored in the warehouse contains unstructured objects that represent one or more user (or entity) identities. Competitive businesses need to clarify this mess of data points into an accurate model of customer behavior and build personalized relationships.\n\nTo create a unified user profile, it is essential to correlate all of the different user identifiers into one canonical identifier so that all the data related to a particular user or entity can be associated with that user or entity.\n\nThis unification step, called **Identity Stitching**, ties all the user data from these tables into a unified view, giving you a 360-degree view of the user.\n\n## Perform identity stitching\n\nYou can use the RudderStack’s [identity stitching model](https://www.rudderstack.com/docs/archive/profiles/0.14/example/id-stitcher/) to define the identifiers you want to combine together. You can also define the input sources, like [Event Stream](https://www.rudderstack.com/docs/sources/event-streams/), [Cloud Extract](https://www.rudderstack.com/docs/sources/extract/) sources, which automatically produce an identity graph because all the schemas and unique identifiers are known.\n\n## Unify identities across all platforms and devices\n\nIdentity stitching is the process of matching different identifiers across multiple devices, digital touchpoints, and other data (like offline point-of-sale interactions) to build a comprehensive identity graph. This identity graph includes nodes (identifiers) and their relationships (edges), and it is generated as a transparent table in the warehouse.\n\nRudderStack performs identity stitching by mapping all the unique identifiers into a single canonical ID (for example, `rudder_id`), then uses that ID to make user feature development easier (for example, summing the payment events against a single `rudder_id`).\n\n[![single identity created from different identities](https://www.rudderstack.com/docs/images/profiles/id-stitching.webp)](https://www.rudderstack.com/docs/images/profiles/id-stitching.webp)\n\n## Identity graph\n\nIdentity stitching starts with the creation of an identity graph. The identity graph is a database housing the entity identifiers where you can identify and connect details related to your customer journeys. Further, it stitches them together in one customer profile representing their whole identity.\n\nThe most fundamental data in an identity graph is the ID tag associated with a device, account, network, session, transaction, or any other anonymous identifier that can engage with your company. Once you’ve collected this data and associated it with a single customer identity (wherever possible), your customer data becomes more reliable, and you can move on to achieve higher goals.\n\nAn identity graph incorporates models that help it in ingesting new information. As you add a new data point with whatever connections are immediately known, the graph database determines if it fits into any existing customer identifier. If there is a clear link - such as a matching device ID or conclusive biographical data like a credit card number - the graph incorporates the data into a relevant user node.\n\n[![identity graph](https://www.rudderstack.com/docs/images/profiles/identity-graph.webp)](https://www.rudderstack.com/docs/images/profiles/identity-graph.webp)\n\n## Notable features\n\n*   Use different input sources like RudderStack’s [Event Stream](https://www.rudderstack.com/docs/sources/event-streams/), [Cloud Extract](https://www.rudderstack.com/docs/sources/extract/) sources, or any existing tables in the warehouse.\n*   Merge identities for entities like a user, customer, product, business account, etc.\n*   Stitch identities from all the required channels like web, mobile, marketing platforms, etc.\n*   Use identity stitching results to develop user features and deliver personalized campaigns.\n\n#### See also\n\n*   [Sample identity stitching project](https://www.rudderstack.com/docs/archive/profiles/0.14/example/id-stitcher/)\n*   [Problem of Identity resolution](https://www.rudderstack.com/blog/the-tale-of-identity-graph-and-identity-resolution/)\n*   [How to achieve ID mapping in a data warehouse](https://www.rudderstack.com/blog/identity-graph-and-identity-resolution-in-sql/)\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Identity Stitching | RudderStack Docs",
    "description": "Stitch multiple identifiers together to create a unified and comprehensive profile.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.14/core-concepts/feature-development/",
    "markdown": "# Feature Development | RudderStack Docs\n\nEnrich unified profiles with the required features/traits to drive targeted campaigns.\n\n* * *\n\n*     3 minute read  \n    \n\nOnce you have [performed identity stitching](https://www.rudderstack.com/docs/archive/profiles/0.14/core-concepts/identity-stitching/#how-to-perform-identity-stitching) to map the individual entities to known identifiers, you can use its output to enhance the unified profiles with additional data points and features.\n\nYou can define the features/traits in your warehouse tables and further perform calculations over this data to devise meaningful outcomes, which can help marketing teams to run effective campaigns.\n\n## Define features\n\nYou can use `var_group` to define features. Each `var_group` can have multiple entity vars which can be considered as features for that entity. The Profiles project generates and runs SQL in the background and automatically adds the resulting features to a [feature view](https://www.rudderstack.com/docs/archive/profiles/0.14/example/feature-views/).\n\nYou can produce a customer [feature view](https://www.rudderstack.com/docs/archive/profiles/0.14/example/feature-views/) or a feature view for specific projects like personalization, recommendations, or analytics.\n\nYou can combine the features to create even more features. You can also use [custom SQL queries](https://www.rudderstack.com/docs/archive/profiles/0.14/example/sql-model/) to enrich unified user profiles for advanced use cases.\n\nA sample `pb_project.yaml` file with a definition of a feature\\_view::\n\n```\n...\nentities:\n  - name: user\n    id_types:\n      - main_id\n      - user_id\n    feature_views:\n      using_ids:\n        - id: user_id\n```\n\nA sample configuration file to create `var_groups`:\n\n```\nvar_groups:\n  - name: user_vars\n    entity_key: user\n    vars:\n      - entity_var:\n          name: first_seen\n          select: min(timestamp::date)\n          from: inputs/rsTracks\n          is_feature: false\n      - entity_var:\n          name: last_seen\n          select: max(timestamp::date)\n          from: inputs/rsTracks\n          is_feature: false\n      - entity_var:\n          name: user_lifespan\n          select: '{{user.Var(\"last_seen\")}} - {{user.Var(\"first_seen\")}}'\n          description: Life Time Value of a customer\n      - entity_var:\n          name: days_active\n          select: count(distinct timestamp::date)\n          from: inputs/rsTracks\n          description: No. of days a customer was active\n# ID stitcher\nmodels:\n  - name: domain_profile_id_stitcher\n    model_type: id_stitcher\n    model_spec:\n      validity_time: 24h # 1 day\n      entity_key: user\n      materialization:\n        run_type: incremental\n      edge_sources:\n        - from: inputs/rsIdentifies\n        - from: inputs/rsTracks\n```\n\n## Benefits\n\n*   You can use the output of the identity graph to define or compute features based on given ID types. Feature views creates a view which will have all or a specified set of features on that entity from across the project based on the identifier column provided.\n*   As the number of features/traits increases, Profiles makes the maintenance process much easier by using a configuration file (as opposed to large and complex SQL queries).\n*   Profiles generates highly performant SQL to build feature views, which helps mitigate computing costs and engineering resources when the data sets become large, dependencies become complex, and features require data from multiple sources.\n\n### Use-cases\n\n*   Create analytics queries like demographic views, user activity views, etc.\n*   Send data using a [Reverse ETL](https://www.rudderstack.com/docs/sources/reverse-etl/) pipeline to various cloud destinations.\n*   Use RudderStack Audiences to send customer profiles to marketing tools (available for beta customers).\n*   Define traits/features using `entity_vars` and ML models and unify them using the Feature Views model.\n\n#### See also\n\n*   [Sample feature views project](https://www.rudderstack.com/docs/archive/profiles/0.14/example/feature-views/)\n*   [Sample SQL model project](https://www.rudderstack.com/docs/archive/profiles/0.14/example/sql-model/)\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Feature Development | RudderStack Docs",
    "description": "Enrich unified profiles with the required features/traits to drive targeted campaigns.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.14/permissions/",
    "markdown": "# Warehouse Permissions | RudderStack Docs\n\nGrant RudderStack the required permissions on your data warehouse.\n\n* * *\n\n*     7 minute read  \n    \n\nRudderStack supports **Snowflake**, **Redshift**, **Databricks**, and **BigQuery** for creating unified user profiles.\n\nTo read and write data to the warehouse, RudderStack requires specific warehouse permissions as explained in the following sections.\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> Keeping separate schemas for projects running via CLI and web is recommended. This way projects run from the CLI will never risk overwriting your production data.\n\n## Snowflake\n\nSnowflake uses a combination of DAC and RBAC models for [access control](https://docs.snowflake.com/en/user-guide/security-access-control-overview.html). However, RudderStack chooses an RBAC-based access control mechanism as multiple users can launch the [Profile Builder CLI](https://www.rudderstack.com/docs/archive/profiles/0.14/get-started/profile-builder/).\n\nAlso, it is not ideal to tie the result of an individual user run with that user. Hence, it is recommended to create a generic role (for example, `PROFILES_ROLE`) with the following privileges:\n\n*   Read access to all the inputs to the model (can be shared in case of multiple schemas/tables).\n*   Write access to the schemas and common tables as the PB project creates material (output) tables.\n\nIf you want to access any material created from the project run, the role (`PROFILES_ROLE`) must also have read access to all of those schemas.\n\nBelow are some sample commands which grant the required privileges to the role (`PROFILES_ROLE`) in a Snowflake warehouse:\n\n```\n-- Create role\nCREATE ROLE PROFILES_ROLE;\nSHOW ROLES; -- To validate\n```\n\n```\n-- Create user\nCREATE USER PROFILES_TEST_USER PASSWORD='<StrongPassword>' DEFAULT_ROLE='PROFILES_ROLE';\nSHOW USERS; -- To validate\n```\n\n```\n-- Grant role to user and database\nGRANT ROLE PROFILES_ROLE TO USER PROFILES_TEST_USER;\nGRANT USAGE ON DATABASE YOUR_RUDDERSTACK_DB TO ROLE PROFILES_ROLE;\n```\n\n```\n-- Create separate schema for Profiles and grant privileges to role\nCREATE SCHEMA YOUR_RUDDERSTACK_DB.RS_PROFILES;\nGRANT ALL PRIVILEGES ON SCHEMA YOUR_RUDDERSTACK_DB.RS_PROFILES TO ROLE PROFILES_ROLE;\nGRANT USAGE ON WAREHOUSE RUDDER_WAREHOUSE TO ROLE PROFILES_ROLE;\nGRANT USAGE ON SCHEMA YOUR_RUDDERSTACK_DB.EVENTSSCHEMA TO ROLE PROFILES_ROLE;\n```\n\nFor accessing input sources, you can individually grant select on tables/views, or give blanket grant to all in a schema.\n\n```\n-- Assuming we want read access to tables/views in schema EVENTSSCHEMA\nGRANT SELECT ON ALL TABLES IN SCHEMA YOUR_RUDDERSTACK_DB.RS_PROFILES TO PROFILES_ROLE;\nGRANT SELECT ON FUTURE TABLES IN SCHEMA YOUR_RUDDERSTACK_DB.RS_PROFILES TO PROFILES_ROLE;\nGRANT SELECT ON ALL VIEWS IN SCHEMA YOUR_RUDDERSTACK_DB.RS_PROFILES TO PROFILES_ROLE;\nGRANT SELECT ON FUTURE VIEWS IN SCHEMA YOUR_RUDDERSTACK_DB.RS_PROFILES TO PROFILES_ROLE;\n```\n\n```\n-- Assuming we want read access to tracks and identifies tables in schema EVENTSSCHEMA\nGRANT SELECT ON TABLE YOUR_RUDDERSTACK_DB.RS_PROFILES.TRACKS TO PROFILES_ROLE;\nGRANT SELECT ON TABLE YOUR_RUDDERSTACK_DB.RS_PROFILES.IDENTIFIES TO PROFILES_ROLE;\n```\n\n## Redshift\n\nSuppose the inputs/edge sources are in a single schema `website_eventstream` and the name of the newly created Profiles user is `rudderstack_admin`. In this case, the requirements are as follows:\n\n*   A separate schema `rs_profiles` (to store all the common and output tables).\n*   The `rudderstack_admin` user should have all the privileges on the above schema and the associated tables.\n*   The `rudderstack_admin` user should have `USAGE` privilege on schemas that have the edge sources and input tables (`website_eventstream`) and read (`SELECT`) privileges on specific tables as well. This privilege can extend to the migration schema and other schemas from where data from warehouses comes in.\n*   The `rudderstack_admin` user should have privileges to use `plpythonu` to create some UDFs.\n\nThe sample commands are as follows:\n\n```\nCREATE USER rudderstack_admin WITH PASSWORD '<strong_unique_password>';\nCREATE SCHEMA rs_profiles;\nGRANT ALL ON SCHEMA \"rs_profiles\" TO rudderstack_admin;\nGRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA \"rs_profiles\" TO rudderstack_admin;\nGRANT USAGE ON SCHEMA \"website_eventstream\" TO rudderstack_admin;\nGRANT USAGE ON LANGUAGE plpythonu TO rudderstack_admin;\n```\n\n```\nGRANT SELECT ON ALL TABLES IN SCHEMA \"website_eventstream\" TO rudderstack_admin;\n```\n\nTo give access to only specific input tables/views referred in your Profiles project, use the below command:\n\n```\nGRANT SELECT ON TABLE \"<YOUR_SCHEMA>\".\"<YOUR_TABLE>\" TO rudderstack_admin;\n```\n\n### Supported inputs\n\nRudderStack supports the following input types for Redshift warehouse/serverless:\n\n*   Redshift cluster with DC2 type nodes with following types as inputs:\n    *   Redshift internal tables\n    *   External schema and tables only for inputs (not supported as output)\n    *   CSV files stored on S3 as inputs\n*   Redshift cluster with RA3 type nodes with following types as inputs:\n    *   Redshift internal tables\n    *   External schema and tables only for inputs (not supported as output)\n    *   CSV files stored on S3 as inputs\n    *   Cross DB input tables\n*   Redshift serverless with following types as inputs:\n    *   Redshift internal tables\n    *   External schema and tables (not supported as output)\n    *   CSV files stored on S3 as inputs\n    *   Cross DB input tables RudderStack also supports various authentication mechanisms to authenticate the user running the Profiles project. Refer [Redshift warehouse connection](https://www.rudderstack.com/docs/archive/profiles/0.14/get-started/profile-builder/#2-create-warehouse-connection) for more information.\n\n## Databricks\n\n1.  Open the Databricks UI.\n2.  Create a new user.\n3.  Reuse an existing catalog or create a new one by clicking **Create Catalog**.\n4.  Grant `USE SCHEMA` privilege on the catalog.\n5.  Create a separate schema to write objects created by RudderStack Profiles.\n6.  Grant all privileges on this schema.\n7.  Grant privileges to access relevant schemas for the input tables. For example, if an input schema is in a schema named `website_eventstream`, then you can run the following commands to assign a blanket grant to all schemas or only specific tables/views referred in your Profiles project:\n\n```\nCREATE USER rudderstack_admin WITH PASSWORD <strong_unique_password>;\nGRANT USE SCHEMA ON CATALOG <catalog name> TO rudderstack_admin;\nCREATE SCHEMA RS_PROFILES;\nGRANT ALL PRIVILEGES ON SCHEMA RS_PROFILES TO rudderstack_admin;\nGRANT SELECT ON SCHEMA website_eventstream TO rudderstack_admin;\n```\n\n```\nGRANT SELECT ON ALL TABLES IN SCHEMA \"website_eventstream\" TO rudderstack_admin;\n```\n\nTo give access to only specific input tables/views referred in your Profiles project, use the below command:\n\n```\nGRANT SELECT ON TABLE public.input_table TO rudderstack_admin; \n```\n\n## BigQuery\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> For BigQuery, RudderStack recommends you to use a view instead of table for streaming data sets.\n\nYou must first assign the `BigQuery Job User` role to your service account. To do so:\n\n1.  Open the [BigQuery UI](https://console.cloud.google.com/) (Google Cloud Console).\n2.  Select **IAM & Admin** > **IAM** from the left sidebar.\n3.  Click **GRANT ACCESS** to grant permissions to your service account.\n4.  Enter your service account email in the **New principals** field.\n5.  In the **Assign roles** section, select `BigQuery Job User` role from the role list.\n6.  Click **Save**.\n\nAlternatively, you can add the following **IAM Policy Binding**:\n\n```\n{\n  \"bindings\": [\n    {\n      \"role\": \"roles/bigquery.jobUser\",\n      \"members\": [\n        \"serviceAccount:your-service-account@your-project.iam.gserviceaccount.com\"\n      ]\n    }\n  ]\n}\n```\n\nFurther, assign the dataset or project level roles to your service account:\n\n1.  Open the [BigQuery UI](https://console.cloud.google.com/) (Google Cloud Console).\n2.  Select **BigQuery** from the sidebar.\n3.  Select the dataset from your schema to which you want to grant access.\n4.  In the **Dataset info** window, click **SHARING** > **Permissions**.\n5.  Click **Add Principal**.\n6.  Enter the service account email in the **New principals** field.\n7.  In the **Assign roles** section, select the following roles from the role list:\n    *   **BigQuery Data Viewer**: Allows read access to the dataset.\n    *   **BigQuery Data Editor**: Allows read and write access to the dataset.\n\nAlternatively, you can add the following **IAM Policy Binding**:\n\n```\n{\n  \"bindings\": [\n    {\n      \"role\": \"roles/bigquery.dataViewer\",\n      \"members\": [\n        \"serviceAccount:your-service-account@your-project.iam.gserviceaccount.com\"\n      ],\n      \"condition\": {\n        \"title\": \"Access to dataset1\",\n        \"description\": \"Allow data viewing access to dataset1\",\n        \"expression\": \"resource.name.startsWith('projects/your-project-id/datasets/dataset1')\"\n      }\n    },\n    {\n      \"role\": \"roles/bigquery.dataEditor\",\n      \"members\": [\n        \"serviceAccount:your-service-account@your-project.iam.gserviceaccount.com\"\n      ],\n      \"condition\": {\n        \"title\": \"Access to dataset2\",\n        \"description\": \"Allow data editing access to dataset2\",\n        \"expression\": \"resource.name.startsWith('projects/your-project-id/datasets/dataset2')\"\n      }\n    }\n  ]\n}\n```\n\nIf you have input relations in many datasets from a single project, you can assign the required privileges for the complete Google Cloud project. This allows your service account to access the relation from all the datasets of the project.\n\n1.  Open the [BigQuery UI](https://console.cloud.google.com/) (Google Cloud Console).\n2.  Select **IAM & Admin** > **IAM** from the left sidebar.\n3.  Click **GRANT ACCESS** to grant permissions to your service account.\n4.  Enter the service account email in the **New principals** field.\n5.  In the **Assign roles** section, select the following roles from the role list:\n    *   **BigQuery Data Viewer**: Allows read access to the dataset.\n    *   **BigQuery Data Editor**: Allows read and write access to the dataset.\n\nAlternatively, you can add the following **IAM Policy Binding**:\n\n```\n{\n  \"bindings\": [\n    {\n      \"role\": \"roles/bigquery.dataViewer\",\n      \"members\": [\n        \"serviceAccount:your-service-account@your-project.iam.gserviceaccount.com\"\n      ]\n    },\n    {\n      \"role\": \"roles/bigquery.dataEditor\",\n      \"members\": [\n        \"serviceAccount:your-service-account@your-project.iam.gserviceaccount.com\"\n      ]\n    }\n  ]\n}\n```\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Warehouse Permissions | RudderStack Docs",
    "description": "Grant RudderStack the required permissions on your data warehouse.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.14/activations/",
    "markdown": "# Activations | RudderStack Docs\n\nPrivacy Overview\n\nThis site uses cookies to improve your experience while you navigate through the website. Out of these cookies, the cookies that are categorized as necessary are stored on your browser as they are as essential for the working of basic functionalities of the website. We also use third-party cookies that help us analyze and understand how you use this website. These cookies will be stored in your browser only with your consent. You also have the option to opt-out of these cookies. But opting out of some of these cookies may have an effect on your browsing experience.\n\nNecessary cookies are absolutely essential for the website to function properly. This category only includes cookies that ensures basic functionalities and security features of the website. These cookies do not store any personal information.",
    "title": "Activations | RudderStack Docs",
    "description": "Activate your cohorts data in downstream systems to run targeted campaigns.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.14/cli-user-guide/structure/",
    "markdown": "# Project Structure | RudderStack Docs\n\nKnow the specifications of a site configuration file, PB project structure, configuration files, and their parameters.\n\n* * *\n\n*     11 minute read  \n    \n\nOnce you complete the [Profile Builder CLI](https://www.rudderstack.com/docs/archive/profiles/0.14/get-started/profile-builder/) steps, you will be able to see the Profiles project on your machine.\n\n## Site configuration file\n\nRudderStack creates a site configuration file (`~/.pb/siteconfig.yaml`) while [creating a warehouse connection](https://www.rudderstack.com/docs/archive/profiles/0.14/get-started/profile-builder/#2-create-warehouse-connection). It contains the following details including secrets (if any):\n\n*   Warehouse connection details and its credentials.\n*   Git repository connection credentials (if any). See [Associate SSH Key to Git Project](https://www.rudderstack.com/docs/archive/profiles/0.14/example/packages/#associate-ssh-key-to-git-project) for more information.\n\n> ![success](https://www.rudderstack.com/docs/images/tick.svg)\n> \n> If you have multiple Profiles projects and they use different warehouse connections, you can store the details for multiple connections in the same site configuration file.\n\nA sample site configuration file containing multiple warehouse connection details is shown below:\n\n```\nconnections:\n  prod-db-profile:\n      target: dev\n      outputs:\n          dev:\n              account: inb828.us-west-3\n              dbname: MAT_STORE\n              password: password\n              role: PROFILES_ROLE\n              schema: AB_SCHEMA\n              type: snowflake\n              user: rik\n              warehouse: PROD_WAREHOUSE\n  test-db-profile:\n      target: test\n      outputs:\n          db:\n              access_token: dabasihasdho\n              catalog: rs_dev\n              host: adb-98.18.azuredatabricks.net\n              http_endpoint: /sql/1.0/warehouses/919uasdn92h\n              port: 443\n              schema: rs_profiles\n              type: databricks\n              user: johndoe@abc.onmicrosoft.com\n          dev:\n              account: uk12.us-west-1\n              dbname: RUDDERSTACK_DB\n              password: password\n              role: RS_ROLE\n              schema: RS_PROFILES\n              type: snowflake\n              user: johndoe\n              warehouse: RS_WAREHOUSE\n          redshift_v1:\n              dbname: warehouse_rs\n              host: warehouse.abc.us-east-3.redshift.amazonaws.com\n              password: password\n              port: 5419\n              schema: rs_profiles\n              type: redshift\n              user: redshift_user\n          redshift_v2:\n              workgroup_name: warehouse_workgroup\n              region: us-east-1\n              driver: v2\n              sslmode: require\n              dbname: warehouse_rs\n              schema: rs_profiles\n              type: redshift\n              access_key_id: ******************\n              secret_access_key: ******************************\n           big:\n              credentials:\n                auth_provider_x509_cert_url: https://www.googleapis.com/oauth2/v1/certs\n                auth_uri: https://accounts.google.com/o/oauth2/auth\n                client_email: johndoe@big-query-integration-poc.iam.gserviceaccount.com\n                client_id: \"123345678909872\"\n                client_x509_cert_url: https://www.googleapis.com/robot/v1/metadata/x509/johndoe%40big-query-integration-poc.iam.gserviceaccount.com\n                private_key: |\n                    -----BEGIN PRIVATE KEY-----                    \n                   ## private key\n                    -----END PRIVATE KEY-----\n                private_key_id: 5271368bhjbd72y278222e233w23e231e\n              project_id: big-query-integration-poc\n                token_uri: https://oauth2.googleapis.com/token\n                type: service_account\n                project_id: rs_profiles\n              schema: rs_profiles\n              type: bigquery\n              user: johndoe@big-query-integration-poc.iam.gserviceaccount.com\ngitcreds:\n - reporegex: \"git@github.com:REPO_OWNER/*\" # in case of ssh url\n   key: |\n       -----BEGIN OPENSSH PRIVATE KEY-----\n       **********************************************************************\n       **********************************************************************\n       **********************************************************************\n       **********************************************************************\n       ****************************************************************\n       -----END OPENSSH PRIVATE KEY-----       \n - reporegex: \"https://github.com/rudderlabs/*\" # https url\n   basic_auth:\n     username: oauth2\n     password: ... # your personal access token with read permission\npy_models:\n    enabled: true # in case you are using Python models in your project, else set it to false\n    python_path: /opt/anaconda3/bin/python # the path where Python is installed (run `which python` to get the full path). If `py_models` is not enabled, set it to `\"\"`. For Windows, you may pass the path value as: python.exe\n    credentials_presets: null\n    allowed_git_urls_regex: \"\"\ncache_dir: /Users/YOURNAME/.pb/WhtGitCache/ # For Windows, the directory path will have forward slash (\\)\nfilepath: /Users/YOURNAME/.pb/siteconfig.yaml # For Windows, the file path will have forward slash (\\)\n```\n\n## Profiles project structure\n\nThe following image shows the folder structure of the project:\n\n[![Project structure](https://www.rudderstack.com/docs/images/profiles/project-structure.webp)](https://www.rudderstack.com/docs/images/profiles/project-structure.webp)\n\n### `pb_project.yaml`\n\nThe `pb_project.yaml` file contains the project details like the name, schema version, warehouse connection, [entityEntity refers to a digital representation of a class of real world distinct objects for which you can create a profile.](https://www.rudderstack.com/docs/resources/glossary/#entity) names along with ID types, etc.\n\nA sample `pb_project.yaml` file with entity type as `user`:\n\n```\n# Project name\nname: sample_attribution\n\n# Project's yaml schema version\nschema_version: 67\n\n# WH Connection to use\nconnection: test\n\n# Model folders to use\nmodel_folders:\n  - models\n\n# Entities in the project and their ids\nentities:\n  - name: user\n    # Change the following to set a custom ID stitcher(optional).\n    # id_stitcher: models/user_id_stitcher\n    id_types:\n      - main_id\n      - user_id\n      - anonymous_id\n      - email\n    # Feature views - to get all features/traits of an entity into a single view (optional)\n    feature_views: \n      using_ids: \n        - id: user_id \n          name: with_user_id\n          \n# lib packages can be imported to signify that this project's properties are inherited\npackages:\n  - name: corelib\n    url: \"https://github.com/rudderlabs/rudderstack-profiles-corelib/tag/schema_{{best_schema_version}}\"\n\n# Profiles can also use certain model types defined in Python.\n# Examples include ML models. Those dependencies are specified here.\npython_requirements:\n  - profiles-pycorelib==0.1.0\n```\n\nThe following table explains the fields used in the above file:\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `name` | String | Name of the project. |\n| `schema_version` | Integer | Project’s YAML version. Each new schema version comes with improvements and added functionalities. |\n| `connection` | String | Connection name from [`siteconfig.yaml`](https://www.rudderstack.com/docs/archive/profiles/0.14/cli-user-guide/structure/) used for connecting to the warehouse. |\n| `model_folders` | String | Names of folders where model files are stored. |\n| [`entities`](#entities) | List | Lists all the entities used in the project for which you can define models. Each entry for an entity here is a JSON object specifying entity’s name and attributes. |\n| [`packages`](#packages) | List | List of packages with their name and URL. Optionally, you can also extend ID types filters for including or excluding certain values from this list. |\n\n##### `entities`\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `name` | String | Name of the entity used in the project. |\n| `id_stitcher` | String | (Optional) Reference path of the [custom ID stitcher model](https://www.rudderstack.com/docs/archive/profiles/0.14/example/id-stitcher/#sample-project-for-custom-id-stitcher) (for example, `models/name_of_id_stitcher`). |\n| [`id_types`](https://www.rudderstack.com/docs/archive/profiles/0.14/example/packages/#modify-id-types) | List | List of all identifier types associated with the current entity. |\n| `feature_views` | List | (Optional) Lists all the view names along with their ID’s being served for [feature views model](https://www.rudderstack.com/docs/archive/profiles/0.14/example/feature-views/). |\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> The identifiers listed in `id_types` may have a many-to-one relationship with an entity but each ID must belong to a single entity.\n> \n> For example, a `user` entity might have `id_types` as the `salesforce_id`, `anonymous_id`, `email`, and `session_id` (a user may have many session IDs over time). However, it should not include something like `ip_address`, as a single IP can be used by different users at different times and it is not considered as a user identifier.\n\n##### `packages`\n\nYou can import library packages in a project signifying where the project inherits its properties from.\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `name` | String | Specify a name. |\n| `url` | String | HTTPS URL of the lib package, with a tag for the best schema version. |\n\n### `inputs.yaml`\n\nThe `inputs.yaml` file lists all the input sources which should be used to run [models](#models) and eventually create outputs. You can also define specific constraints on the input sources using the [`contract`](https://www.rudderstack.com/docs/archive/profiles/0.14/example/packages/#model-contracts) key.\n\nRudderStack supports the following input sources:\n\n*   **Table**: Specify the table’s name in the `table` key.\n*   **View**: Specify the view’s name in the `view` key.\n*   **S3 bucket**: Specify the path of the CSV file in your bucket in the `s3` key. See [Use Amazon S3 bucket as input](https://www.rudderstack.com/docs/archive/profiles/0.14/example/packages/#use-amazon-s3-bucket-as-input) for more information.\n*   **Local CSV file**: Specify the file path in the `csv` key. See [Use CSV file as input](https://www.rudderstack.com/docs/archive/profiles/0.14/example/packages/#use-csv-file-as-input) for more information.\n\nYou can also specify the table/view along with the column name and SQL expression for retrieving values. The input specification may also include metadata and the constraints on those columns.\n\nA sample `inputs.yaml` file:\n\n```\ninputs:\n  - name: salesforceTasks\n    contract:\n      is_optional: false\n      is_event_stream: true\n      with_entity_ids:\n        - user\n      with_columns:\n        - name: activitydate\n        - name: whoid\n    app_defaults:\n      table: salesforce.task\n      # For BigQuery, it is recommended to use view (view: _views_<view_name>) instead of table for event streaming data sets.\n      occurred_at_col: activitydate\n      row_identifier:\n        - activitydate\n        - whoid\n      ids:\n        # column name or sql expression\n        - select: \"whoid\" \n          type: salesforce_id\n          entity: user\n          to_default_stitcher: true\n  - name: salesforceContact\n    contract:\n      is_optional: false\n      is_event_stream: true\n      with_entity_ids:\n        - user\n      with_columns:\n        - name: createddate\n        - name: id\n        - name: email\n    app_defaults:\n      table: salesforce.contact\n      # For BigQuery, it is recommended to use view (view: _views_<view_name>) instead of table for event streaming data sets.\n      occurred_at_col: createddate\n      ids:\n        - select: \"id\"\n          type: salesforce_id\n          entity: user\n          to_default_stitcher: true\n        - select: \"case when lower(email) like any ('%gmail%', '%yahoo%') then lower(email)  else split_part(lower(email),'@',2) end\"\n          type: email\n          entity: user\n          to_default_stitcher: true\n  - name: websitePageVisits\n    contract:\n      is_optional: false\n      is_event_stream: true\n      with_entity_ids:\n        - user\n      with_columns:\n        - name: timestamp\n        - name: anonymous_id\n        - name: context_traits_email\n        - name: user_id\n    app_defaults:\n      table: autotrack.pages\n      # For BigQuery, it is recommended to use view (view: _views_<view_name>) instead of table for event streaming data sets.\n      occurred_at_col: timestamp\n      ids:\n        - select: \"anonymous_id\"\n          type: rudder_anon_id\n          entity: user\n          to_default_stitcher: true\n        # below sql expression check the email type, if it is gmail and yahoo return email otherwise spilt email return domain of email.  \n        - select: \"case when lower(coalesce(context_traits_email, user_id)) like any ('%gmail%', '%yahoo%') then lower(coalesce(context_traits_email, user_id))  \\\n              else split_part(lower(coalesce(context_traits_email, user_id)),'@',2) end\"\n          type: email\n          entity: user\n          to_default_stitcher: true\n```\n\nThe following table explains the fields used in the above file:\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `name` | String | Name of the input model. |\n| `contract` | Dictionary | A model contract provides essential information about the model like the necessary columns and entity IDs that it should contain. This is crucial for other models that depend on it, as it helps find errors early and closer to the point of their origin. |\n| `app_defaults` | Dictionary | Values that input defaults to when you run the project directly. For library projects, you can remap the inputs and override the app defaults while importing the library projects. |\n\n##### `contract`\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `is_optional` | Boolean | Whether the model’s existence in the warehouse is mandatory. |\n| `is_event_stream` | Boolean | Whether the table/view is a series/stream of events. A model that has a `timestamp` column is an event stream model. |\n| `with_entity_ids` | List | List of all entities with which the model is related. A model M1 is considered related to model M2 if there is an ID of model M2 in M1’s output columns. |\n| `with_columns` | List | List of all ID columns that this contract is applicable for. |\n\n##### `app_defaults`\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `table`/`view` | String | Name of the warehouse table/view containing the data. You can prefix the table/view with an external schema or database in the same warehouse, if applicable. Note that you can specify either a table or view but not both. |\n| `s3` | String | Name of the CSV file in your Amazon S3 bucket containing the data. |\n| `csv` | String | Name of the CSV file in your local storage containing the data. The file path should be relative to the project folder. |\n| `occurred_at_col` | String | Name of the column in table/view containing the timestamp. |\n| `row_identifier` | String | (Optional) List of all the identifiers whose combination acts as a primary key. If the unique row exists already during the run process while creating a copy of the input table, it is not copied again. |\n| [`ids`](#ids) | List | Specifies the list of all IDs present in the source table along with their column names (or column SQL expressions).<br><br>**Note**: Some input columns may contain IDs of associated entities. By their presence, such ID columns associate the row with the entity of the ID. The ID Stitcher may use these declarations to automatically discover ID-to-ID edges. |\n\n##### `ids`\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `select` | String | Specifies the column name to be used as the identifier. You can also specify a SQL expression if some transformation is required.<br><br>**Note**: You can also refer table from another Database/Schema in the same data warehouse. For example, `table: <database_name>.<schema_name>.<table_name>`. |\n| `type` | String | Type of identifier. All the ID types of a project are declared in [`pb_project.yaml`](#project-details). You can specify additional filters on the column expression.<br><br>**Note**: Each ID type is linked only with a single entity. |\n| `entity` | String | Entity name defined in the [`pb_project.yaml`](#project-details) file to which the ID belongs. |\n| `to_default_stitcher` | Boolean | Set this **optional** field to `false` for the ID to be excluded from the default ID stitcher. |\n\n### `profiles.yaml`\n\nThe `profiles.yaml` file lists `entity_vars`/`input_vars` used to create the output tables under `var_groups`.\n\nThe following `profiles.yaml` file defines a group of vars named `vars_list`. It also defines two models namely `user_profile`, and `user_python_model`:\n\n```\nvar_groups:\n  name: vars_list\n  entity_key: user # This is the name defined in project file. If we change that, we need to change the name here too.\n  vars:\n    - entity_var:\n        name: is_mql\n        select: max(case when salesForceLeadsTable.mql__c == 'True' then 1 else 0 end)\n        from: inputs/salesForceLeadsTable\n        description: Whether a domain is mql or not\n    - entity_var:\n        name: blacklistFlag\n        select: max(case when exclude_reason is not null then 1 else 0 end)\n        from: inputs/blacklistDomains\n        where: (context_sources_job_run_id = (select top 1 context_sources_job_run_id from blacklistDomains order by timestamp desc))\n        is_feature: false\n    - entity_var:\n        name: ignore_domain\n        select: case when {{user.Var(\"blacklistFlag\")}} = 1 or {{user.Var(\"domainSummary_account_type\")}} like '%free%' then 1 else 0 end\n        description: Whether a domain should be ignored for the analysis\n    - entity_var:\n        name: salesEvents\n        select: json_agg(activitydate, case when (type='Email' or tasksubtype = 'Email') then case when lower(subject) like '%[in]%' then 'sf_inbound_email' \\\n              else 'sf_outbound_email' end when macro(call_conversion) then 'sf_call' else null end as event)\n        from: inputs/salesforceTasks\n        description: Salesforce touches are converted to one of following events - sf_inbound_email, sf_outbound_email, sf_call, null\n        is_feature: false\n    - entity_var:\n        name: webhookFormSubmit\n        select:  min(timestamp)\n        from: inputs/webhookSource\n        where: variable_1 is null and timestamp < sales_conversion_timestamp and timestamp > var('start_date')\nmodels:\n  - name: user_profile\n    model_type: feature_table_model\n    model_spec:\n      validity_time: 24h # 1 day\n      entity_key: user\n```\n\n##### `var_groups`\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `name` | String | A unique name for the var\\_group. |\n| `entity_key` | String | The entity to which the var\\_group belongs to. |\n| `vars` | Object | This section is used to specify variables, with the help of `entity_var` and `input_var`. Aggregation on stitched ID type is done by default and is implicit. |\n\nOptionally, you can create models using the above vars. The following fields are common for all the model types:\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `name` | String | Name of the model. Note that a table with the same name is created in the data warehouse. For example, if you define the name as `user_table`, the output table will be named something like `Material_user_table_<rest-of-generated-hash>_<timestamp-number>`. |\n| `model_type` | String | Defines the type of model. Possible values are: `id_stitcher`, `feature_table_model`, `sql_template`, `entity_cohort`, `id_collator`, `python_model`, `feature_views`, etc. |\n| `model_spec` | Object | Creates a detailed configuration specification for the target model. Different schema is applicable for different model types as explained in each section below. |\n\nRudderStack supports the following model types:\n\n*   [Feature Views](https://www.rudderstack.com/docs/archive/profiles/0.14/example/feature-views/)\n*   [Feature Table (legacy)](https://www.rudderstack.com/docs/archive/profiles/0.14/example/feature-table/)\n*   [SQL Template](https://www.rudderstack.com/docs/archive/profiles/0.14/example/sql-model/)\n*   [ID Stitcher](https://www.rudderstack.com/docs/archive/profiles/0.14/example/id-stitcher/)\n*   [ID Collator](https://www.rudderstack.com/docs/archive/profiles/0.14/example/id-collator/)\n*   [Python model](https://www.rudderstack.com/docs/archive/profiles/0.14/predictions/#python-model)\n*   [Packages](https://www.rudderstack.com/docs/archive/profiles/0.14/example/packages/)\n\n### `README.md`\n\nThe `README.md` file provides a quick overview on how to use PB along with SQL queries for data analysis.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Project Structure | RudderStack Docs",
    "description": "Know the specifications of a site configuration file, PB project structure, configuration files, and their parameters.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.14/example/sql-model/",
    "markdown": "# SQL Models | RudderStack Docs\n\nStep-by-step tutorial on how to create a SQL Template model.\n\n* * *\n\n*     6 minute read  \n    \n\nThis guide provides a detailed walkthrough on how to use a PB project and create SQL Template models using custom SQL queries.\n\n## Prerequisites\n\n*   Familiarize yourself with:\n    \n    *   A basic Profile Builder project by following the [Profile Builder CLI](https://www.rudderstack.com/docs/archive/profiles/0.14/get-started/profile-builder/) steps.\n    *   [Structure](https://www.rudderstack.com/docs/archive/profiles/0.14/cli-user-guide/structure/) of a Profile Builder project and the parameters used in different files.\n\n## Sample project\n\nThe following sections describe how to define your PB project files:\n\n### Project detail\n\nThe [`pb_project.yaml`](https://www.rudderstack.com/docs/archive/profiles/0.14/cli-user-guide/structure/#project-details) file defines the project details such as name, schema version, connection name and the entities which represent different identifiers.\n\nYou can define all the identifiers from different input sources you want to stitch together as a single ID (`main_id` in this example):\n\n```\nname: sample_test\nschema_version: 67\nconnection: test\nmodel_folders:\n  - models\nentities:\n  - name: user\n    id_stitcher: models/test_id__\n    id_types:\n      - test_id\n      - exclude_id\nid_types:\n  - name: test_id\n    filters:\n      - type: include\n        regex: \"([0-9a-z])*\"\n      - type: exclude\n        value: \"\"\n  - name: exclude_id\n```\n\n### Input\n\nThe [input file](https://www.rudderstack.com/docs/archive/profiles/0.14/cli-user-guide/structure/#inputs) (`models/inputs.yaml`) file includes the input table references and corresponding SQL for the above-mentioned entities:\n\n```\ninputs:\n  - name: tbl_a\n    app_defaults:\n      table: Temp_tbl_a\n    occurred_at_col: insert_ts\n    ids:\n      - select: TRIM(COALESCE(NULL, id1))\n        type: test_id\n        entity: user\n        to_default_stitcher: true\n      - select: \"id2\"\n        type: test_id\n        entity: user\n        to_default_stitcher: true\n      - select: \"id3\"\n        type: exclude_id\n        entity: user\n        to_default_stitcher: true\n  - name: tbl_b\n    app_defaults:\n      view: Temp_view_b\n    occurred_at_col: timestamp\n    ids:\n      - select: \"id1\"\n        type: test_id\n        entity: user\n        to_default_stitcher: true\n      - select: \"id2\"\n        type: test_id\n        entity: user\n        to_default_stitcher: true\n      - select: \"id3\"\n        type: test_id\n        entity: user\n        to_default_stitcher: true\n  - name: tbl_c\n    app_defaults:\n      table: Temp_tbl_c\n    ids:\n      - select: \"id1\"\n        type: test_id\n        entity: user\n        to_default_stitcher: true\n      - select: \"id2\"\n        type: test_id\n        entity: user\n        to_default_stitcher: true\n```\n\n### Model\n\nProfiles **SQL model** lets you write custom SQL queries to achieve advanced use-cases to create desired output tables.\n\nA sample `profiles.yaml` file specifying a `single_sql` type SQL model:\n\n```\nmodels:\n- name: test_sql\n  model_type: sql_template\n  model_spec:\n    validity_time: 24h# 1 day\n    materialization:                 // optional\n      run_type: discrete             // optional [discrete, incremental]\n    single_sql: |\n        {%- with input1 = this.DeRef(\"inputs/tbl_a\") -%}\n          SELECT \n              id1 AS new_id1, \n              id2 AS new_id2, \n              {{input1}}.*\n          FROM {{input1}}\n        {%- endwith -%}        \n    occurred_at_col: insert_ts        // optional\n    ids:\n      - select: \"new_id1\"\n        type: test_id\n        entity: user\n        to_default_stitcher: true\n      - select: \"new_id2\"\n        type: test_id\n        entity: user\n        to_default_stitcher: true\n      - select: \"id3\"\n        type: test_id\n        entity: user\n        to_default_stitcher: true\n```\n\nA sample `profiles.yaml` file specifying a `multi_sql` type SQL model:\n\n```\nmodels:\n- name: test_sql\n    model_type: sql_template\n    model_spec:\n      validity_time: 24h # 1 day\n      materialization:\n        output_type: table\n        run_type: discrete\n      multi_sql: |\n        {% with input_material1 = this.DeRef(\"models/test_sql1\") input_material2 = this.DeRef(\"inputs/tbl_a\") input_material3 = this.DeRef(\"inputs/tbl_c\") %}\n          create {{this.GetMaterialization().OutputType.ToSql()}} {{this}} as (\n            select b.id1, b.id2, b.id3, b.insert_ts, a.new_id1, a.num_a, c.num_b, c.num_c\n            from {{ input_material1 }} a\n            full outer join {{ input_material2 }} b\n            on a.id2 = b.id2\n            full outer join {{ input_material3 }} c\n            on c.id2 = a.id2\n          );\n        {% endwith  %}        \n```\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> A `multi_sql` type SQL model only creates a table as an output type in a warehouse whereas `single_sql` type SQL model supports all the output types (deafult is `ephemeral`). See [materialization](https://www.rudderstack.com/docs/archive/profiles/0.14/resources/glossary/#materialization) for more information.\n\n##### Model specification fields\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `name` | String | Name of the SQL model. You can also refer this as an input as `models/test_sql`. |\n| `model_type` | String | Defines the type of model. |\n| `model_spec` | Object | Contains the specifications for the target model. |\n| `validity_time` | Time | Time Specifies the validity of the model with respect to its timestamp. For example, a model run as part of a scheduled nightly job for 2009-10-23 00:00:00 UTC with `validity_time`: `24h` would still be considered potentially valid and usable for any run requests, which do not require precise timestamps between 2009-10-23 00:00:00 UTC and 2009-10-24 00:00:00 UTC. This specifies the validity of generated feature table. Once the validity is expired, scheduling takes care of generating new tables. For example: 24h for 24 hours, 30m for 30 minutes, 3d for 3 days. |\n| `materialization` | List | Adds the key `run_type`: `incremental` to run the project in incremental mode. This mode considers row inserts and updates from the edge\\_sources input. These are inferred by checking the timestamp column for the next run. One can provide buffer time to consider any lag in data in the warehouse for the next incremental run like if new rows are added during the time of its run. If you do not specify this key then it’ll default to `run_type`: `discrete`. |\n| `single_sql` | List | Specifies the SQL template which must evaluate to a single SELECT SQL statement. After execution, it should produce a dataset which will materialize based on the provided materialization. |\n| `multi-sql` | List | Specifies the SQL template which can evaluate to multiple SQL statements. One of these SQL statements (typically the last one) must be a CREATE statement which shall be responsible for materializing the model into a table.<br><br>**Note**: You should set only one of `single_sql` or `multi_sql`. |\n| `occurred_at_col` | List | Name of the column which contains the timestamp value in the output of SQL template. |\n| `ids` | List | Specifies the list of all IDs present in the source table along with their column names (or column SQL expressions). It is required in case you want to use SQL models as an input to the `input_var` or `entity_var` fields. |\n\n## SQL template\n\nYou can pass custom SQL queries to the `single_sql` or `multi_sql` fields, which is also known as a **SQL template**. It provides the flexibility to write custom SQL by refering to any of the input sources listed in the `inputs.yaml` or any model listed in `models/profiles.yaml`.\n\nThe SQL templates follow a set query syntax which serves the purpose of creating a model. Follow the below rules to write SQL templates:\n\n*   Write SQL templates in the [pongo2 template engine](https://pkg.go.dev/github.com/flosch/pongo2#readme-first-impression-of-a-template) syntax.\n*   Avoid circular referencing while referencing the models. For example, `sql_model_a` references `sql_model_b` and `sql_model_b` references `sql_model_a`.\n*   Use `timestamp` variable (refers to the start time of the current run) to filter new events.\n*   `this` refers to the current model’s material. You can use the following methods to access the material properties available for `this`:\n    *   `DeRef(\"path/to/model\")`: Use this syntax `{{ this.DeRef(\"path/to/model\") }}` to refer to any model and return a database object corresponding to that model. The database object, in return, gives the actual name of the table/view in the warehouse. Then, generate the output, for example:\n\n```\n{% with input_table = this.DeRef(\"inputs/tbl_a\") %}\n    SELECT\n        t.a AS new_a,\n        t.b AS new_b,\n        t.*\n    FROM {{input_table}} AS t\n{% endwith %}\n```\n\n*   `GetMaterialization()`: Returns a structure with two fields: `MaterializationSpec{OutputType, RunType}`.\n    *   `OutputType`: You must use `OutputType` with `ToSQL()` method:  \n        For example, `CREATE OR REPLACE {{this.GetMaterialization().OutputType.ToSQL()}} {{this.GetSelectTargetSQL()}} AS ...`\n    *   `RunType`: For example, `this.GetMaterialization().RunType`\n\n## Refer SQL contents from another file\n\nIf you want to edit a SQL query in a text editor and not as a field in a YAML file, you can use the `ReadFile` method. It refers to the SQL contents stored in another file:\n\n```\nmodels:\n- name: example_sql_model\n  model_type: sql_template\n  model_spec:\n    validity_time: 24h # 1 day\n    materialization:\n      output_type: view\n      run_type: discrete\n    single_sql: \"{{this.ReadFile('models/compute.sql')}}\" # for a SQL file named compute.sql in the models folder\n    occurred_at_col: insert_ts\n```\n\n## See Also\n\nCreate user features using SQL models:\n\n*   [Profiles Stripe features project](https://github.com/rudderlabs/pb_sample_features)\n*   [Profiles Shopify features project](https://github.com/rudderlabs/profiles-shopify-features)\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "SQL Models | RudderStack Docs",
    "description": "Step-by-step tutorial on how to create a SQL Template model.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.14/activation-api/",
    "markdown": "# Activation API (Early Access) | RudderStack Docs\n\nExpose user profiles stored in your Redis instance over an API.\n\n* * *\n\n*     7 minute read  \n    \n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> The Activation API is part of our [Early Access Program](https://www.rudderstack.com/docs/resources/early-access-program/), where we work with users and customers to test new features and get feedback before making them generally available. These features are fully functional but can change as we improve them. We recommend connecting with our team before running them in production.\n> \n> [Contact us](https://www.rudderstack.com/contact/) if you would like access to this feature.\n\nWith RudderStack’s Activation API, you can fetch enriched user traits stored in your Redis instance and use them for near real-time personalization for your target audience.\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> You must have a working Redis instance in place before setting up the connection.\n\n[![Activation API](https://www.rudderstack.com/docs/images/profiles/activation-api.webp)](https://www.rudderstack.com/docs/images/profiles/activation-api.webp)\n\n## Overview\n\nA brief summary of how the Activation API works:\n\n1.  Sync all your customer 360 data from your Profiles project to your Redis store.\n2.  The Activation API sits on top of this Redis instance and provides endpoints for retrieving and using the enriched user data for personalization.\n\n## How to use the Activation API\n\n1.  In your Profiles project settings, scroll down to **Activation API** and turn on the **Enable sync to Redis** toggle.\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> Before you enable the Activation API toggle, make sure that:\n> \n> *   You have at least one successful Profiles run.\n> *   Your `pb_project.yaml` > `entities` defines a `feature_views` property.\n\n[![Enable Redis sync for using Activation API](https://www.rudderstack.com/docs/images/profiles/enable-redis.webp)](https://www.rudderstack.com/docs/images/profiles/enable-redis.webp)\n\n2.  Enter the [account credentials for your Redis instance](#redis-configuration) and click **Create**. This will also create a Redis destination in your dashboard.\n\n[![Enable Redis sync for using Activation API](https://www.rudderstack.com/docs/images/profiles/enable-redis-sync.webp)](https://www.rudderstack.com/docs/images/profiles/enable-redis-sync.webp)\n\n3.  [Generate a personal access token](#faq) with **Admin** role in your RudderStack dashboard. You will need this token for authenticating the Activation API.\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> Personal access token with an **Admin** role is only available to the **Org Admin** users. Refer [User management](https://www.rudderstack.com/docs/dashboard-guides/user-management/#org-admin) for more information.\n\nNote your Redis destination ID from the [**Settings**](https://www.rudderstack.com/docs/dashboard-guides/destinations/#destination-details) tab. Further, [use the Activation API endpoint](#get-user-profiles) to access your Redis instance and get user data.\n\nThis API uses [Bearer Authentication](https://swagger.io/docs/specification/authentication/bearer-authentication/) for authenticating all requests. Set the [personal access token](#faq) as the bearer token for authentication.\n\n## Base URL\n\n```\nhttps://profiles.rudderstack.com/v1/\n```\n\n## Get user profiles\n\n#### Request body\n\nString\n\nRedis destination ID.\n\nObject\n\nID containing `type` and `value`\n\n```\n{\n  \"entity\": <entity_type>,  // User, project, account, etc.\n  \"destinationId\": <redis_destination_id> , // Redis destination ID\n  \"id\": {\n    \"type\": <id_type>,\n    \"value\": <id_value>\n  }\n}\n```\n\n#### Example request\n\n```\nPOST /v1/activation HTTP/1.1\nHost: profiles.rudderstack.com\nContent-Type: application/json\nAuthorization: Bearer <personal_access_token>\nContent-Length: 90\n\n{\n \"entity\": <entity_type>,\n \"destinationId\": <redis_destination_id>, // Redis destination ID\n \"id\": {\n   \"type\": <id_type>,\n   \"value\": <id_value>\n }\n}\n```\n\n```\ncurl --location 'https://profiles.rudderstack.com/v1/activation' \\\n--header 'Content-Type: application/json' \\\n--header 'Authorization: Bearer <personal_access_token>' \\\n--data '{\n \"entity\": <entity_type>,\n \"destinationId\": <redis_destination_id>, // Redis destination ID\n \"id\": {\n   \"type\": <id_type>,\n   \"value\": <id_value>\n }\n}'\n```\n\n```\nconst axios = require('axios');\nlet data = JSON.stringify({\n  \"destinationId\": <redis_destination_id>,\n  \"entity\": <entity_type>,\n  \"id\": {\n    \"type\": <id_type>,\n    \"value\": <id_value>\n  }\n});\n\nlet config = {\n  method: 'post',\n  maxBodyLength: Infinity,\n  url: 'https://profiles.rudderstack.com/v1/activation',\n  headers: {\n    'Content-Type': 'application/json',\n    'authorization': 'Bearer <personal_access_token>'\n  },\n  data: data\n};\n\naxios.request(config)\n  .then((response) => {\n    console.log(JSON.stringify(response.data));\n  })\n  .catch((error) => {\n    console.log(error);\n  });\n```\n\n#### Responses\n\n*   If the personal access token is absent or trying to access a destination to which it does not have access:\n\n```\nstatusCode: 401\nResponse: {\n  \"error\": \"Unauthorized request. Please check your access token\"\n}\n```\n\n*   If the destination is not Redis or the destination ID is absent/blank:\n\n```\nstatusCode: 404\nResponse: {\n  \"error\": \"Invalid Destination. Please verify you are passing the right destination ID\"\n}\n```\n\n*   If ID is present:\n\n```\nstatusCode: 200\nResponse:\n{\n  \"entity\": <entity_type>,\n  \"id\": {\n    \"type\": <id_type>,\n    \"value\": <id_value>\n  },\n  \"data\": {\n    <traits_from_Redis>\n  }\n}\n```\n\n*   If ID is not present in Redis:\n\n```\nstatusCode: 200\nResponse:\n{\n  \"entity\": <entity_type>,\n  \"id\": {\n    \"type\": <id_type>,\n    \"value\": <id_value>\n  },\n  \"data\": {}\n}\n```\n\n## Use case\n\nYou can use the Activation API for real-time personalization. Once you fetch the user traits from your Redis instance via the API, you can pull them into your client application to alter the application behavior in real-time based on user interactions.\n\nYou can respond immediately with triggered, user-focused messaging based on actions like page views or app clicks and provide a better customer experience.\n\n[![Real time personalization use case](https://www.rudderstack.com/docs/images/profiles/activation-api-use-case.webp)](https://www.rudderstack.com/docs/images/profiles/activation-api-use-case.webp)\n\n## Redis configuration\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> You must have a working Redis instance in place before setting up the connection.\n\n*   **Address**: Enter the public endpoint of your Redis database. If you are using [Redis Cloud](https://app.redislabs.com/#/), you can find this endpoint by going to your Redis database and navigating to **Configuration** tab > **General**.\n\n[![Redis database public endpoint](https://www.rudderstack.com/docs/images/profiles/redis-public-endpoint.webp)](https://www.rudderstack.com/docs/images/profiles/redis-public-endpoint.webp)\n\n*   **Password**: Enter the database password. You can find it in the **Security** section of the **Configuration** tab:\n\n[![Redis database password](https://www.rudderstack.com/docs/images/profiles/redis-database-password.webp)](https://www.rudderstack.com/docs/images/profiles/redis-database-password.webp)\n\n*   **Cluster Mode**: Turn on this setting if you’re connecting to a Redis cluster.\n*   **Secure**: Enable this setting to secure the TLS communication between RudderStack Redis client and your Redis server.\n\n## Data mapping\n\nRudderStack creates multiple Reverse ETL sources automatically based on your Profiles project. You will see separate sources for different `id_served` connected to the same Redis destination.\n\nThe following `pb_project.yaml` snippet shows the sources to be created:\n\n```\nentities:\n  - name: user\n    id_types:\n      - main_id\n      - user_id\n    feature_views:\n      using_ids:\n        - id: email\n          name: features_by_email # Optional. Takes default view name, if not specified.\n        - id: salesforce_id\n          name: salesforce_id_stitched_features\n      features: # Optional\n        - from: models/cart_feature_table\n          include:\n            - \"*\"\n```\n\n## FAQ\n\n#### How do I generate a personal access token to use the Activation API?\n\n1.  Log in to your [RudderStack dashboard](https://app.rudderstack.com/).\n2.  Go to **Settings** > **Your Profile** > **Account** tab and scroll down to **Personal access tokens**. Then, click **Generate new token**:\n\n[![New personal access token in RudderStack dashboard](https://www.rudderstack.com/docs/images/rudderstack-api/personal-access-token-1.webp)](https://www.rudderstack.com/docs/images/rudderstack-api/personal-access-token-1.webp)\n\n3.  Enter the **Token name**. Set **Role** to **Admin** and click **Generate**.\n\n[![Personal access token name and role](https://www.rudderstack.com/docs/images/rudderstack-api/personal-access-token-2.webp)](https://www.rudderstack.com/docs/images/rudderstack-api/personal-access-token-2.webp)\n\n4.  Use the personal access token to authenticate to the Activation API.\n\n[![Personal access token details](https://www.rudderstack.com/docs/images/rudderstack-api/personal-access-token-3.webp)](https://www.rudderstack.com/docs/images/rudderstack-api/personal-access-token-3.webp)\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> Save the generated token securely as it will not be visible again once you close this window.\n\n#### How can I make Profiles work with the Activation API?\n\nTo use the Activation API with your Profiles project, you need a successful run of your Profiles project that is not past the retention period.\n\nTo enable the Activation API for your Profiles project, turn on the **Enable sync to Redis** setting. A Profile run will then sync automatically.\n\n[![Toggle API in Settings](https://www.rudderstack.com/docs/images/rudderstack-api/activation-api-toggle-settings.png)](https://www.rudderstack.com/docs/images/rudderstack-api/activation-api-toggle-settings.png)\n\n#### Why am I getting an error trying to enable API in my instance for a custom project hosted on GitHub?\n\nFor GitHub projects, you need to explicitly add the IDs of the custom project that need to be served.\n\nIn your `pb_project.yaml` file, you can specify them as shown:\n\n```\nentities:\n  - name: user\n    id_types:\n      - main_id\n      - user_id\n    feature_views:\n      name: user_feature_view\n      using_ids:\n        - id: email\n          name: features_by_email\n        - id: salesforce_id\n          name: salesforce_id_stitched_features\n      features:\n        - from: models/cart_feature_table\n          include:\n            - \"*\"\n```\n\n#### If I force a full resync, stop it, and then start a new sync, does RudderStack always perform a full sync the next time?\n\nIt depends on the state of the task when it was canceled.\n\n*   If the sync is cancelled while RudderStack is preparing a snapshot, then next run depends on the state of the previous successful run and any mapping changes.\n*   If it is cancelled after the sync data is prepared, the next run is incremental.\n\nGenerally if a sync is cancelled manually, it is recommended to trigger a full sync if the previous cancelled task was a full sync. If the previously cancelled sync was incremental, triggering an incremental sync is recommended.\n\n#### Does RudderStack perform a full sync if I add a new column?\n\nRudderStack does not change the sync mode if you make any column additions. It triggers a full sync only if you change/update the data mappings, for example, if the newly added column is sent to the destination via the [Visual Data Mapper](https://www.rudderstack.com/docs/sources/reverse-etl/visual-data-mapper/).\n\nFor Profiles activation syncs, RudderStack updates the mappings and automatically sends all columns from the customer 360 view by triggering a full sync.\n\n#### Suppose I’m running a full sync and the Profiles job is running in parallel and finishes eventually. What happens to the scheduled sync? Does it get queued?\n\nRudderStack first creates a temporary snapshot copy of any sync when it starts. So its syncing the created copy. Even if a Profiles job is running in parallel, the sync - if started - is not impacted by it.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Activation API (Early Access) | RudderStack Docs",
    "description": "Expose user profiles stored in your Redis instance over an API.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.14/example/id-stitcher/",
    "markdown": "# Identity Stitching | RudderStack Docs\n\nStep-by-step tutorial on how to stitch together different user identities.\n\n* * *\n\n*     7 minute read  \n    \n\nThis guide provides a detailed walkthrough on how to use a PB project and create output tables in a warehouse for a custom identity stitching model.\n\n## Prerequisites\n\n*   Familiarize yourself with:\n    \n    *   A basic Profile Builder project by following the [Profile Builder CLI](https://www.rudderstack.com/docs/archive/profiles/0.14/get-started/profile-builder/) steps.\n    *   [Structure](https://www.rudderstack.com/docs/archive/profiles/0.14/cli-user-guide/structure/) of a Profile Builder project and the parameters used in different files.\n\n## Output\n\nAfter [running the project](https://www.rudderstack.com/docs/archive/profiles/0.14/get-started/profile-builder/#7-generate-output-tables), you can view the generated material tables:\n\n1.  Log in to your Snowflake console.\n2.  Click **Worksheets** from the top navigation bar.\n3.  In the left sidebar, click **Database** and the corresponding **Schema** to view the list of all tables. You can hover over a table to see the full table name along with its creation date and time.\n4.  Write a SQL query like `select * from <table_name> limit 10;` and execute it to see the results:\n\n1.  Open [Postico2](https://eggerapps.at/postico2/). If required, create a new connection by entering the relevant details. Click **Test Connection** followed by **Connect**.\n2.  Click the **+** icon next to **Queries** in the left sidebar.\n3.  You can click **Database** and the corresponding schema to view the list of all tables/views.\n4.  Double click on the appropriate view name to paste the name on an empty worksheet.\n5.  You can prefix `SELECT *` from the view name pasted previously and suffix `LIMIT 10;` at the end.\n6.  Press Cmd+Enter keys, or click the **Run** button to execute the query.\n\n1.  Enter your Databricks workspace URL in the web browser and log in with your username and password.\n2.  Click the **Catalog** icon in left sidebar.\n3.  Choose the appropriate catalog from the list and click on it to view the contents.\n4.  You will see list of tables/views. Click on the appropriate table/view name to paste the name on the worksheet.\n5.  Then, you can prefix `SELECT * FROM` before the pasted view name and suffix `LIMIT 10;` at the end.\n6.  Select the query text. Press Cmd+Enter or click the **Run** button to execute the query.\n\n1.  Log in to your [Google Cloud Console](https://console.cloud.google.com/)\n2.  Search for Bigquery in the search bar.\n3.  Select Bigquery from **Product and Pages** to open the Bigquery **Explorer**.\n4.  Select the correct project from top left drop down menu.\n5.  In the left sidebar, click the project ID, then the corresponding dataset view list of all the tables and views.\n6.  Write a SQL query like `select * from <table_name> limit 10;` and execute it to see the results.\n\nA sample output containing the results in Snowflake:\n\n![Generated tables (Snowflake)](https://www.rudderstack.com/docs/images/profiles/snowflake-console.webp)\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> Profiles project includes an ID stitcher model (`default_id_stitcher`) by default even if you do not define any specs for creating one. It takes all the input sources and ID types defined in the file `inputs.yaml` file. Also, it creates a custom ID stitcher when you define an ID stitcher model explicitly along with the specs.\n\n## Sample project for Custom ID Stitcher\n\nThis sample project considers multiple user identifiers in different warehouse tables to ties them together to create a unified user profile. The following sections describe how to define your PB project files:\n\n### Project detail\n\nThe [`pb_project.yaml`](https://www.rudderstack.com/docs/archive/profiles/0.14/cli-user-guide/structure/#project-details) file defines the project details such as name, schema version, connection name and the entities which represent different identifiers.\n\nThere can be different ID types for an entity. You can include all such identifiers in the `id_types` field under `entities`. `main_id` specified under `id_types` is not an ID type but a placeholder for the column which serves as the primary identifier for that entity.\n\nIn case of `id_stitcher` model, the `main_id` for the entity is `rudder_id` (predefined ID type) by default. For other models, any other ID type can be the `main_id`, for example `session_id`. Hence, if you want to specify the ID type of a column as a primary identifier, you can specify `main_id`.\n\n```\n# Project name\nname: sample_id_stitching\n# Project's yaml schema version\nschema_version: 67\n# Warehouse connection\nconnection: test\n# Folder containing models\nmodel_folders:\n  - models\n# Entities in this project and their ids.\nentities:\n  - name: user\n    id_stitcher: models/user_id_stitcher # modelRef of custom ID stitcher model\n    id_types:\n      - main_id # You need to add ``main_id`` to the list only if you have defined ``main_id_type: main_id`` in the id stitcher buildspec.\n      - user_id # one of the identifier from your data source.\n      - email\n# lib packages can be imported in project signifying that this project inherits its properties from there\npackages:\n  - name: corelib\n    url: \"https://github.com/rudderlabs/profiles-corelib/tag/schema_{{best_schema_version}}\"\n    # if required then you can extend the package definition such as for ID types.\n```\n\n### Input\n\nThe [input file](https://www.rudderstack.com/docs/archive/profiles/0.14/cli-user-guide/structure/#inputs) (`models/inputs.yaml`) file includes the input table references and corresponding SQL for the above-mentioned entities:\n\n```\ninputs:\n- name: rsIdentifies\n  contract: # constraints that a model adheres to\n    is_optional: false\n    is_event_stream: true\n    with_entity_ids:\n      - user\n    with_columns:\n      - name: timestamp\n      - name: user_id\n      - name: anonymous_id\n      - name: email\n  app_defaults:\n    table: rudder_events_production.web.identifies # one of the WH table RudderStack generates when processing identify or track events.\n    occurred_at_col: timestamp\n    ids:\n      - select: \"user_id\" # kind of identity sql to pick this column from above table.\n        type: user_id\n        entity: user # as defined in project file\n        to_default_stitcher: true # default value\n      - select: \"anonymous_id\"\n        type: anonymous_id\n        entity: user\n        to_default_stitcher: true\n      - select: \"lower(email)\" # can use sql.\n        type: email\n        entity: user\n        to_default_stitcher: true\n- name: rsTracks\n  contract:\n    is_optional: false\n    is_event_stream: true\n    with_entity_ids:\n      - user\n    with_columns:\n      - name: timestamp\n      - name: user_id\n      - name: anonymous_id\n  app_defaults:\n    table: rudder_events_production.web.tracks # another table in WH maintained by RudderStack processing track events.\n    occurred_at_col: timestamp\n    ids:\n      - select: \"user_id\"\n        type: user_id\n        entity: user\n        to_default_stitcher: true\n      - select: \"anonymous_id\"\n        type: anonymous_id\n        entity: user\n        to_default_stitcher: true\n```\n\nColumns specified under `ids` field are automatically sent for identity stitching unless you specify `to_default_stitcher` as `false`.\n\n### Model\n\nProfiles **Identity stitching** model maps and unifies all the specified identifiers (in `pb_project.yaml` file) across different platforms. It tracks the user journey uniquely across all the data sources and stitches them together to a `rudder_id`.\n\nA sample `profiles.yaml` file specifying an identity stitching model (`user_id_stitcher`) with relevant inputs:\n\n```\nmodels:\n  - name: user_id_stitcher\n    model_type: id_stitcher\n    model_spec:\n      validity_time: 24h\n      entity_key: user\n      materialization:\n        run_type: incremental # default value is `discrete` for a custom ID stitcher and `incremental` for the default ID stitcher.\n      incremental_timedelta: 12h\n      main_id_type: main_id\n      edge_sources:\n        - from: inputs/rsIdentifies\n        - from: inputs/rsTracks\n```\n\n##### Model specification fields\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `validity_time` | Time | Specifies the validity of the model with respect to its timestamp. For example, a model run as part of a scheduled nightly job for 2009-10-23 00:00:00 UTC with `validity_time`: `24h` would still be considered potentially valid and usable for any run requests, which do not require precise timestamps between 2009-10-23 00:00:00 UTC and 2009-10-24 00:00:00 UTC. This specifies the validity of generated feature table. Once the validity is expired, scheduling takes care of generating new tables. For example: 24h for 24 hours, 30m for 30 minutes, 3d for 3 days |\n| `entity_key` | String | Specifies the relevant entity from your `input.yaml` file. For example, here it should be set to `user`. |\n| `materialization` | List | Adds the key `run_type`: `incremental` to run the project in incremental mode. This mode considers row inserts and updates from the `edge_sources` input. These are inferred by checking the timestamp column for the next run. One can provide buffer time to consider any lag in data in the warehouse for the next incremental run like if new rows are added during the time of its run. If you do not specify this key then it’ll default to `run_type`: `discrete`. |\n| `incremental_timedelta` | List | (Optional )If materialization key is set to `run_type`: `incremental`, then this field sets how far back data should be fetched prior to the previous material for a model (to handle data lag, for example). The default value is 4 days. |\n| `main_id_type` | ProjectRef | (Optional) ID type reserved for the output of the identity stitching model, often set to `main_id`. It must not be used in any of the inputs and must be listed as an id type for the entity being stitched. If you do not set it, it defaults to `rudder_id`. Do not add this key unless it’s explicitly required, like if you want your identity stitcher table’s `main_id` column to be called `main_id`. For more information, see below. |\n| `edge_sources` | List | Specifies inputs for the identity stitching model as mentioned in the `inputs.yaml` file. |\n\n## Use cases\n\nThis section describes some common identity stitching use cases:\n\n*   **Identifiers from multiple data sources**: You can consider multiple identifiers and tables by:\n    \n    *   Adding entities in `pb_project.yaml` representing identifiers.\n    *   Adding references to table and corresponding sql in `models/inputs.yaml`\n    *   Adding table reference names defined in `models/inputs.yaml` as `edge_sources` in your model definition.\n*   **Leverage Sql Support**: You can use SQL in your `models/inputs.yaml` to achieve different scenarios. For example, you want to tag all the internal users in your organization as one entity. You can use the email domain as the identifier by adding a SQL query to extract the email domain as the identifier value: `lower(split_part({{email_col}}, '@', 2))`\n    \n*   **Custom ID Stitcher**: You can define a custom ID stitcher by defining the required id stitching model in `models/profiles.yaml`.\n    \n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Identity Stitching | RudderStack Docs",
    "description": "Step-by-step tutorial on how to stitch together different user identities.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.14/example/id-collator/",
    "markdown": "# ID Collator | RudderStack Docs\n\nStep-by-step tutorial stitching different user identities together.\n\n* * *\n\n*     3 minute read  \n    \n\nID Stitching is one of the most important features of Profiles. Being able to perform ID stitching to determine the accounts belonging to the same customer/user is very important to get a 360-degree view of that user.\n\nHowever many a times, we may not require ID stitching for a particular entity, especially if there are no edges in the ID graph of an entity. To build a feature table on such an entity, you will still need to perform ID stitching. Although this approach is not wrong, it is computationally redundant.\n\nProfiles provides the ID Collator is to get all IDs of that particular entity from various input tables and create one collated list of IDs.\n\n## Sample project\n\nLet’s take a case where we have defined two entities in our project - one is `user` and the other is `session`.\n\nIf `user` entity has multiple IDs defined, there are basically edges which make the use of an ID stitcher logical. On the other hand, `session` may have only one ID, `ssn_id`, there won’t be any possibility of edges. In such a case, all we need is a complete list of `ssn_id`.\n\nHere is the corresponding inputs and entities definition.\n\n```\nentities:\n  - name: user\n    id_column_name: user_rud_id\n    id_types:\n      - user_id\n      - anonymous_id\n  - name: session\n    id_column_name: session_id\n    id_types:\n      - ssn_id\n```\n\nProject file:\n\n```\ninputs:\n  - name: user_accounts\n    table: tbl_user_accounts\n    occurred_at_col: insert_ts\n    ids:\n      - select: \"user_id\"\n        type: user_id\n        entity: user\n  - name: sign_in\n    table: tbl_sign_in\n    occurred_at_col: insert_ts\n    ids:\n      - select: \"user_id\"\n        type: user_id\n        entity: user\n      - select: \"ssn_id\"\n        type: ssn_id\n        entity: session\n      - select: \"anonymous_id\"\n        type: anonymous_id\n        entity: user\n  - name: sign_up\n    table: tbl_sign_up\n    occurred_at_col: insert_ts\n    ids:\n      - select: \"user_id\"\n        type: user_id\n        entity: user\n      - select: \"ssn_id\"\n        type: ssn_id\n        entity: session\n      - select: \"anonymous_id\"\n        type: anonymous_id\n        entity: user\n```\n\nHere, the `entity: session` has only one ID type. Creating an ID stitcher for such an entity is possible but unnecessary.\n\nUsing all the models having `ssn_id`, we can just make a union of all `ssn_id` and get all distinct values of it and obtain the final list of sessions.\n\nThe underlying SQL will look as follows:\n\n```\nSELECT ssn_id AS session_id FROM sign_in\nUNION\nSELECT ssn_id AS session_id FROM sign_up;\n```\n\n## YAML Changes\n\nThe YAML writer cannot define a custom ID collator the way they define a custom ID stitcher. If an entity has no edges, the PB project will automatically figure out if an ID collator is needed. To exclude certain inputs (having the required ID) from being used in the collation, we can just set `to_id_stitcher: false` in the input.\n\n```\nentities:\n  - name: session\n    id_column_name: session_id\n    id_types:\n      - ssn_id\n```\n\nThe `id_column_name` is a new field added in the entity definition which will be the name of the ID column and it applies to both ID stitcher and ID collator.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> In the ID collator, you won’t generate a UUID like in ID stitcher.\n\n## Comparing ID Collator and ID Stitcher\n\n| ID Stitcher | ID Collator |\n| --- | --- |\n| Uses edges to converge the ID graph. | Collates all distinct IDs as there is only one ID Type and no edges are present. |\n| Higher cost of computation. | Lower cost of computation. |\n| A UUID is generated and used as the unique identifier for the entity. | Collates the existing IDs only. |\n| The generated ID is always of the type: `rudder_id` | The ID column of the generated ID collator table/view will be of the ID type of the corresponding ID. |\n| User may override the default ID stitcher with custom one. | You cannot override the default ID collator, though you can define a custom ID stitcher to override default ID collator. |\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "ID Collator | RudderStack Docs",
    "description": "Step-by-step tutorial stitching different user identities together.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.14/example/feature-table/",
    "markdown": "# Feature Table | RudderStack Docs\n\nStep-by-step tutorial on creating a feature table model.\n\n* * *\n\n*     9 minute read  \n    \n\nOnce you have done [identity stitching](https://www.rudderstack.com/docs/archive/profiles/0.14/core-concepts/identity-stitching/) to unify the identity of your users across all the cross-platforms, you can evaluate and maintain the required features/traits for each identified user in a feature table.\n\nThis guide provides a detailed walkthrough on how to use a PB project and create output tables in a warehouse for a feature table model.\n\n## Prerequisites\n\nFamiliarize yourself with:\n\n*   A basic Profile Builder project by following the [Profile Builder CLI](https://www.rudderstack.com/docs/archive/profiles/0.14/get-started/profile-builder/) steps.\n*   [Structure](https://www.rudderstack.com/docs/archive/profiles/0.14/cli-user-guide/structure/) of a Profile Builder project and the parameters used in different files.\n*   [Identity Stitching](https://www.rudderstack.com/docs/archive/profiles/0.14/example/id-stitcher/) model as feature table reuses its output to extract the required features/traits.\n\n## Sample GitHub projects\n\n*   [Basic features](https://github.com/rudderlabs/profiles-base-features) like active days, session length, last seen date, name, etc.\n*   [Ecommerce features](https://github.com/rudderlabs/profiles-ecommerce-features) like highest transaction value, days since first purchase, items purchased ever, total products added, etc.\n*   [Features using Shopify tables](https://github.com/rudderlabs/profiles-shopify-features) like products added in past 1 day, transactions in past 90 days, etc.\n*   [Features using Stripe tables](https://github.com/rudderlabs/profiles-stripe-features) like total fees, days since first sale, sales i n past 365 days, has credit card, etc.\n\n## Sample project\n\nThis sample project uses the output of an identity stitching model as an input to create a feature table. The following sections describe how to define your PB project files:\n\n### Project detail\n\nThe [`pb_project.yaml`](https://www.rudderstack.com/docs/archive/profiles/0.14/cli-user-guide/structure/#project-details) file defines the project details such as name, schema version, connection name and the entities which represent different identifiers.\n\nYou can define all the identifiers from different input sources you want to stitch together as a `user_main_id`:\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> You need to add `main_id` to the list only if you have defined `main_id_type: main_id` in the ID stitcher buildspec.\n\n```\n# Project name\nname: sample_id_stitching\n# Project's yaml schema version\nschema_version: 67\n# Warehouse connection\nconnection: test\n# Folder containing models\nmodel_folders:\n  - models\n# Entities in this project and their ids.\nentities:\n  - name: user\n    id_types:\n      - main_id # You need to add ``main_id`` to the list only if you have defined ``main_id_type: main_id`` in the id stitcher buildspec.\n      - user_id # one of the identifier from your data source.\n      - email\n# lib packages can be imported in project signifying that this project inherits its properties from there\npackages:\n  - name: corelib\n    url: \"https://github.com/rudderlabs/profiles-corelib/tag/schema_{{best_schema_version}}\"\n    # if required then you can extend the package definition such as for ID types.\n```\n\n### Input\n\nThe [input file](https://www.rudderstack.com/docs/archive/profiles/0.14/cli-user-guide/structure/#inputs) file includes the input table references and corresponding SQL for the above-mentioned entities:\n\n```\ninputs:\n- name: rsIdentifies\n  contract: # constraints that a model adheres to\n    is_optional: false\n    is_event_stream: true\n    with_entity_ids:\n      - user\n    with_columns:\n      - name: timestamp\n      - name: user_id\n      - name: anonymous_id\n      - name: email\n  app_defaults:\n    table: rudder_events_production.web.identifies # one of the WH table RudderStack generates when processing identify or track events.\n    occurred_at_col: timestamp\n    ids:\n      - select: \"user_id\" # kind of identity sql to pick this column from above table.\n        type: user_id\n        entity: user # as defined in project file\n        to_default_stitcher: true\n      - select: \"anonymous_id\"\n        type: anonymous_id\n        entity: user\n        to_default_stitcher: true\n      - select: \"lower(email)\" # can use sql.\n        type: email\n        entity: user\n        to_default_stitcher: true\n- name: rsTracks\n  contract:\n    is_optional: false\n    is_event_stream: true\n    with_entity_ids:\n      - user\n    with_columns:\n      - name: timestamp\n      - name: user_id\n      - name: anonymous_id\n  app_defaults:\n    table: rudder_events_production.web.tracks # another table in WH maintained by RudderStack processing track events.\n    occurred_at_col: timestamp\n    ids:\n      - select: \"user_id\"\n        type: user_id\n        entity: user\n        to_default_stitcher: true\n      - select: \"anonymous_id\"\n        type: anonymous_id\n        entity: user\n        to_default_stitcher: true\n```\n\n### Model\n\nProfiles **Feature Table** model lets you define the specific features/traits you want to evaluate from the huge spread of scattered data in your warehouse tables.\n\nA sample `profiles.yaml` file specifying a feature table model (`user_profile`):\n\n```\nmodels:\n  - name: user_profile\n    model_type: feature_table_model\n    model_spec:\n      validity_time: 24h\n      entity_key: user\n      features:\n        - user_lifespan\n        - days_active\n        - min_num_c_rank_num_b_partition\nvar_groups:\n  - name: user_vars\n    entity_key: user\n    vars:\n      - entity_var:\n          name: first_seen\n          select: min(timestamp::date)\n          from: inputs/rsTracks\n          where: properties_country is not null and properties_country != ''\n      - entity_var:\n          name: last_seen\n          select: max(timestamp::date)\n          from: inputs/rsTracks\n      - entity_var:\n          name: user_lifespan\n          select: '{{user.Var(\"last_seen\")}} - {{user.Var(\"first_seen\")}}'\n          description: Life Time Value of a customer\n      - entity_var:\n          name: days_active\n          select: count(distinct timestamp::date)\n          from: inputs/rsTracks\n          description: No. of days a customer was active\n      - entity_var:\n          name: campaign_source\n          default: \"'organic'\"\n      - entity_var:\n          name: user_rank\n          default: -1\n      - entity_var:\n          name: campaign_source_first_touch\n          select: first_value(context_campaign_source)\n          window:\n            order_by:\n              - timestamp asc\n          from: inputs/rsIdentifies\n          where: context_campaign_source is not null and context_campaign_source != ''\n      - input_var:\n          name: num_c_rank_num_b_partition\n          select: rank()\n          from: inputs/tbl_c\n          default: -1\n          window:\n            partition_by:\n              - \"{{tbl_c}}.num_b\"\n            order_by:\n              - \"{{tbl_c}}.num_c asc\"\n          where: \"{{tbl_c}}.num_b >= 10\"\n      - entity_var:\n          name: min_num_c_rank_num_b_partition\n          select: min(num_c_rank_num_b_partition)\n          from: inputs/tbl_c\n      - entity_var:\n          name: first_bill\n          select: min({{tbl_billing.Var(\"payment\")}})\n          from: inputs/tbl_billing\n          column_data_type: '{{warehouse.DataType(\"float\")}}'\n```\n\n##### Model specification fields\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `validity_time` | Time | Specifies the validity of the model with respect to its timestamp. For example, a model run as part of a scheduled nightly job for 2009-10-23 00:00:00 UTC with `validity_time`: `24h` would still be considered potentially valid and usable for any run requests, which do not require precise timestamps between 2009-10-23 00:00:00 UTC and 2009-10-24 00:00:00 UTC. This specifies the validity of generated feature table. Once the validity is expired, scheduling takes care of generating new tables. For example: `24h` for 24 hours, `30m` for 30 minutes, `3d` for 3 days, and so on. |\n| `entity_key` | String | Specifies the relevant entity from your `input.yaml` file. |\n| `features` | String | Specifies the list of `name` in `entity_var`, that must act as a feature. |\n\n**`entity_var`**\n\nThe `entity_var` field defines the features which act as an input for the feature table model. This variable stores the data temporarily, however, you can choose to store its data permanently by specifying the `name` in it as a feature in the `features` key.\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `name` | String | Name of the `entity_var` to identify it uniquely. |\n| `select` | String | Column name/value you want to select from the table. This defines the actual value that will be stored in the variable. You can use simple SQL expressions or select an `entity_var` as `{{entityName.Var(\\\"entity_var\\\")}}`. It has to be an aggregate operation that ensures the output is a unique value for a given `main_id`. For example: min(timestamp), count(\\*), sum(amount) etc. This holds true even when a window function (optional) is used. For example:: first\\_value(), last\\_value() etc are valid while rank(), row\\_number(), etc. are not valid and give unpredictable results. |\n| `from` | List | Reference to the source table from where data is to be fetched. You can either refer to another model from the same YAML or some other table specified in input YAML. |\n| `where` | String | Any filters you want to apply on the input table before selecting a value. This must be SQL compatible and should consider the data type of the table. |\n| `default` | String | Default value in case no data matches the filter. When defining default values, make sure you enclose the string values in single quotes followed by double quotes to avoid SQL failure. However, you can use the non-string values without any quotes. |\n| `description` | String | Textual description of the `entity_var`. |\n| `window` | Object | Specifies the window function. Window functions in SQL usually have both `partition_by` and `order_by` properties. But for `entity_var`, `partition_by` is added with `main_id` as default; so, adding `partition_by` manually is not supported. If you need partitioning on other columns too, check out `input_var` where `partition_by` on arbitrary and multiple columns is supported. |\n| `column_data_type` | String | (Optional) Data type for the `entity_var`. Supported data types are: `integer`, `variant`, `float`, `varchar`, `text`, and `timestamp`. |\n\n**`input_var`**\n\nThe syntax of `input_var` is similar to `entity_var`, with the only difference that instead of each value being associated to a row of the feature table, it’s associated with a row of the specified input. While you can think of an `entity_var` as adding a helper column to the feature table, you can consider an `input_var` as adding a helper column to the input.\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `name` | String | Name to store the retrieved data. |\n| `select` | String | Data to be stored in the name. |\n| `from` | List | Reference to the source table from where data is to be fetched. |\n| `where` | String | (Optional) Applies conditions for fetching data. |\n| `default` | String | (Optional) Default value for any entity for which the calculated value would otherwise be NULL. |\n| `description` | String | (Optional) Textual description. |\n| `column_data_type` | String | (Optional) Data type for the `input_var`. Supported data types are: `integer`, `variant`, `float`, `varchar`, `text`, and `timestamp`. |\n| `window` | Object | (Optional) Specifies a window over which the value should be calculated. |\n\n**`window`**\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `partition_by` | String | (Optional) List of SQL expressions to use in partitioning the data. |\n| `order_by` | String | (Optional) List of SQL expressions to use in ordering the data. |\n\nIn window option, `main_id` is not added by default, it can be any arbitrary list of columns from the input table. So if a feature should be partitioned by `main_id`, you must add it in the `partition_by` key.\n\n### Output\n\nAfter [running the project](https://www.rudderstack.com/docs/archive/profiles/0.14/get-started/profile-builder/#7-generate-output-tables), you can view the generated material tables.\n\n1.  Log in to your Snowflake console.\n2.  Click **Worksheets** from the top navigation bar.\n3.  In the left sidebar, click **Database** and the corresponding **Schema** to view the list of all tables. You can hover over a table to see the full table name along with its creation date and time.\n4.  Write a SQL query like `select * from <table_name>` and execute it to see the results:\n\n1.  Open [Postico2](https://eggerapps.at/postico2/). If required, create a new connection by entering the relevant details. Click **Test Connection** followed by **Connect**.\n2.  Click the **+** icon next to **Queries** in the left sidebar.\n3.  You can click **Database** and the corresponding schema to view the list of all tables/views.\n4.  Double click on the appropriate view name to paste the name on an empty worksheet.\n5.  You can prefix `SELECT *` from the view name pasted previously and suffix `LIMIT 10;` at the end.\n6.  Press Cmd+Enter keys, or click the **Run** button to execute the query.\n\n1.  Enter your Databricks workspace URL in the web browser and log in with your username and password.\n2.  Click the **Catalog** icon in left sidebar.\n3.  Choose the appropriate catalog from the list and click on it to view contents.\n4.  You will see list of tables/views. Click the appropriate table/view name to paste the name on worksheet.\n5.  You can prefix `SELECT * FROM` before the pasted view name and suffix `LIMIT 10;` at the end.\n6.  Select the query text. Press Cmd+Enter, or click the **Run** button to execute the query.\n\n1.  Log in to your [Google Cloud Console](https://console.cloud.google.com/)\n2.  Search for Bigquery in the search bar.\n3.  Select Bigquery from **Product and Pages** to open the Bigquery **Explorer**.\n4.  Select the correct project from top left drop down menu.\n5.  In the left sidebar, click the project ID, then the corresponding dataset view list of all the tables and views.\n6.  Write a SQL query like `select * from <table_name> limit 10;` and execute it to see the results.\n\nA sample output containing the results in Snowflake:\n\n![Generated table (Snowflake)](https://www.rudderstack.com/docs/images/profiles/profiles-feature-table.webp)\n\n## Feature table for cohort\n\nTo create feature table for a specific cohort, you can pass the cohort’s path in the `entity_cohort` field:\n\n```\n- name: us_users_features\n  model_type: feature_table_model\n  model_spec:\n    entity_cohort: models/knownUsUsers\n    time_grain: \"day\"\n    validity_time: 24h # 1 day\n    features:\n      - has_credit_card\n```\n\nTo create feature tables for the entire set of an entity’s instance, specify the `entity_key`:\n\n```\n- name: all_users_features\n  model_type: feature_table_model\n  model_spec:\n    entity_key: user\n    time_grain: \"day\"\n    validity_time: 24h # 1 day\n    features:\n      - max_timestamp\n```\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Feature Table | RudderStack Docs",
    "description": "Step-by-step tutorial on creating a feature table model.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.14/example/predictive-features-snowflake/",
    "markdown": "# Predictive features | RudderStack Docs\n\nCreate predictive features with RudderStack and Snowflake without using MLOps\n\n* * *\n\n*     2 minute read  \n    \n\nPredictive features like future LTV and churn propensity can be game changing for a business. If your marketing, customer success, and other teams want to use them, though, your company often faces a binary choice: use a one-size-fits-all solution within an existing SaaS platform (i.e., marketing automation tool), or build out ML and MLOps capabilities internally.\n\nBoth options have significant drawbacks. First, templated SaaS-based solutions can’t leverage all of your customer data and aren’t configurable, which results in low accuracy and impact. On the other hand, hiring data scientists and setting up MLOps is expensive and complex.\n\nModern data teams need an option in the middle: the ability to deploy model templates on all of their customer data, but without additional tooling, processes and headcount.\n\nWith RudderStack Predictions and Snowflake, you can create predictive features directly in your warehouse, without the need to set up MLOps processes and infrastructure. Predictions leverages the full power of Snowpark to run ML models within your existing data engineering workflow.\n\nIn this section, you will learn about two ways to build predictive features in RudderStack Predictions:\n\n1.  [Set up automated features in the RudderStack UI](https://www.rudderstack.com/docs/archive/profiles/0.14/example/predictive-features-snowflake/setup-automated-features/) - You can setup and run the jobs within the RudderStack UI. This process makes it easy for less technical users to implement basic predictive features.\n2.  [Code your own custom predictions](https://www.rudderstack.com/docs/archive/profiles/0.14/example/predictive-features-snowflake/custom-code/) - Predictions also supports a code-based approach that gives technical users full control to define custom predictive features that match their unique business logic.\n\nIt’s important to note that Predictions runs on top of RudderStack [Profiles](https://www.rudderstack.com/docs/profiles/overview/), a product that automates identity resolution and user feature development in Snowflake.\n\nPredictions leverages the Profiles identity graph to train and run ML models. Because Predictions is part of Profiles, project outputs include an identity graph, standard user featuers (i.e., `last_seen`) and predictive user features (i.e., `percentile_churn_score_30_days`). Both types of features are built using RudderStack data sources and standardized feature definitions.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Predictive features | RudderStack Docs",
    "description": "Create predictive features with RudderStack and Snowflake without using MLOps",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.14/example/feature-views/",
    "markdown": "# Feature Views | RudderStack Docs\n\nStep-by-step tutorial on creating an feature view models.\n\n* * *\n\n*     9 minute read  \n    \n\nOnce you have done [identity stitching](https://www.rudderstack.com/docs/archive/profiles/0.14/core-concepts/identity-stitching/) to unify the identity of your users across all the cross-platforms, you can evaluate and maintain the required features/traits for each identified user using a feature views model.\n\n## Prerequisites\n\n*   A basic Profile Builder project by following the [Profile Builder CLI](https://www.rudderstack.com/docs/archive/profiles/0.14/get-started/profile-builder/) steps.\n*   [Structure](https://www.rudderstack.com/docs/archive/profiles/0.14/cli-user-guide/structure/) of a Profile Builder project and the parameters used in different files.\n*   [Identity Stitching](https://www.rudderstack.com/docs/archive/profiles/0.14/example/id-stitcher/) model as Feature Views reuses its output to extract the required features/traits.\n\n## Feature Views model\n\nYou can define and extract the required features/traits for an entity from your data warehouse using the feature views model. Once done, you can send them to the downstream destinations. A destination could either be the [Activation API](https://www.rudderstack.com/docs/archive/profiles/0.14/activation-api/) or any [Reverse ETL destination](https://www.rudderstack.com/docs/destinations/warehouse-destinations/) that RudderStack supports. Each such destination requires data in the form of a table with an ID column and one or more feature columns.\n\nYou can use the Feature Views model to access the entity features based on any ID type and create a view having all or a specified set of entity features across the project. It also lets you unify the traits/features (defined using `entity_vars`) and ML models to generate a comprehensive customer 360 table.\n\nTo create a feature views model, you can add `feature_views` section under `entities` and provide a list of ID types under the `id_served` field. RudderStack assigns a default name to the model, if not provided, and adds all the available features on the entity into the view by default.\n\n### Default feature views model\n\nThe `pb_project.yaml` file for a default feature views model:\n\n```\n...\nentities:\n  - name: user\n    id_types:\n      - main_id\n      - user_id\n    feature_views:\n      using_ids:\n        - id: email\n          name: features_by_email\n        - id: salesforce_id\n          name: salesforce_id_stitched_features\n      features:\n        - from: models/feature_table_1 #include everything from `feature_table_1` (default behaviour if `include` is not specified)\n          include:\t[\"*\"]\n        - from: models/feature_table_2 #exclude 'middle_name' feature from `feature_table_2`\n          exclude:\t[\"middle_name\"] \n```\n\n### Custom feature views model\n\nYou can also define a custom feature views model by including/excluding features from any other model and adding their references to the `feature_views` section.\n\nThe `models/profiles.yaml` file for a custom feature views model:\n\n```\nmodels:\n  - name: cart_feature_views\n    model_type: feature_views\n    model_spec:\n      validity_time: 24h # 1 day\n      entity_key: user\n      id_served: user_id\n      feature_list:\n        - from: packages/pkg/models/cart_table # a table created by package\n          include: [\"*\"] # will include all the traits\n        - from: models/user_var_table\n          include: [\"*\"]\n          exclude: [cart_quantity, purchase_status] # except two, all the other traits will be included\n        - from: models/sql_model\n          include: [lifetime_value] # will include only one trait\n```\n\n## Sample project\n\nThis sample project uses the output of an identity stitching model as an input to create a feature views. The following sections describe how to define your PB project files:\n\n### Project detail\n\nThe [`pb_project.yaml`](https://www.rudderstack.com/docs/archive/profiles/0.14/cli-user-guide/structure/#project-details) file defines the project details such as name, schema version, connection name and the entities which represent different identifiers.\n\nYou can define all the identifiers from different input sources you want to stitch together as a `user_main_id`:\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> You need to add `main_id` to the list only if you have defined `main_id_type: main_id` in the ID stitcher spec.\n\n```\n# Project name\nname: sample_id_stitching\n# Project's yaml schema version\nschema_version: 67\n# Warehouse connection\nconnection: test\n# Folder containing models\nmodel_folders:\n  - models\n# Entities in this project and their ids.\nentities:\n  - name: user\n    id_types:\n      - main_id # You need to add `main_id` to the list only if you have defined `main_id_type: main_id` in the id stitcher spec.\n      - user_id # one of the identifier from your data source.\n      - email\n# lib packages can be imported in project signifying that this project inherits its properties from there\npackages:\n  - name: corelib\n    url: \"https://github.com/rudderlabs/profiles-corelib/tag/schema_{{best_schema_version}}\"\n    # if required then you can extend the package definition such as for ID types.\n```\n\n### Input\n\nThe [input file](https://www.rudderstack.com/docs/archive/profiles/0.14/cli-user-guide/structure/#inputs) file includes the input table references and corresponding SQL for the above-mentioned entities:\n\n```\ninputs:\n- name: rsIdentifies\n  contract: # constraints that a model adheres to\n    is_optional: false\n    is_event_stream: true\n    with_entity_ids:\n      - user\n    with_columns:\n      - name: timestamp\n      - name: user_id\n      - name: anonymous_id\n      - name: email\n  app_defaults:\n    table: rudder_events_production.web.identifies # one of the WH table RudderStack generates when processing identify or track events.\n    occurred_at_col: timestamp\n    ids:\n      - select: \"user_id\" # kind of identity sql to pick this column from above table.\n        type: user_id\n        entity: user # as defined in project file\n        to_default_stitcher: true\n      - select: \"anonymous_id\"\n        type: anonymous_id\n        entity: user\n        to_default_stitcher: true\n      - select: \"lower(email)\" # can use sql.\n        type: email\n        entity: user\n        to_default_stitcher: true\n- name: rsTracks\n  contract:\n    is_optional: false\n    is_event_stream: true\n    with_entity_ids:\n      - user\n    with_columns:\n      - name: timestamp\n      - name: user_id\n      - name: anonymous_id\n  app_defaults:\n    table: rudder_events_production.web.tracks # another table in WH maintained by RudderStack processing track events.\n    occurred_at_col: timestamp\n    ids:\n      - select: \"user_id\"\n        type: user_id\n        entity: user\n        to_default_stitcher: true\n      - select: \"anonymous_id\"\n        type: anonymous_id\n        entity: user\n        to_default_stitcher: true\n```\n\n### Model\n\nThe **feature views** model lets you define and extract the features/traits from your warehouse tables. Each feature is defined using an `entity_var`.\n\nA sample `profiles.yaml` file specifying a feature views model:\n\n```\nvar_groups:\n  - name: first_group\n    entity_key: user\n    vars:\n      - entity_var:\n          name: first_seen\n          select: min(timestamp::date)\n          from: inputs/rsTracks\n          where: properties_country is not null and properties_country != ''\n      - entity_var:\n          name: last_seen\n          select: max(timestamp::date)\n          from: inputs/rsTracks\n          is_feature: false # Specifies the entity_var is not a feature\n      - entity_var:\n          name: user_lifespan\n          select: '{{user.Var(\"last_seen\")}} - {{user.Var(\"first_seen\")}}'\n          description: Life Time Value of a customer\n      - entity_var:\n          name: days_active\n          select: count(distinct timestamp::date)\n          from: inputs/rsTracks\n          description: No. of days a customer was active\n      - entity_var:\n          name: campaign_source\n          default: \"'organic'\"\n      - entity_var:\n          name: user_rank\n          default: -1\n      - entity_var:\n          name: campaign_source_first_touch\n          select: first_value(context_campaign_source)\n          window:\n              order_by:\n                  - timestamp asc\n              partition_by:\n                  - main_id\n          from: inputs/rsIdentifies\n          where: context_campaign_source is not null and context_campaign_source != ''\n      - input_var:\n          name: num_c_rank_num_b_partition\n          select: rank()\n          from: inputs/tbl_c\n          default: -1\n          window:\n            partition_by:\n              - '{{tbl_c}}.num_b'\n            order_by:\n              - '{{tbl_c}}.num_c asc'\n          where: '{{tbl_c}}.num_b >= 10'\n      - entity_var:\n          name: min_num_c_rank_num_b_partition\n          select: min(num_c_rank_num_b_partition)\n          from: inputs/tbl_c\n```\n\n**`var_groups`**\n\nThe `var_groups` field groups all the `vars` under it and provides the provision to define any configuration keys that need to be shared across `vars`.\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `name` | String | Name to identify the `var_groups` uniquely. |\n| `entity_key` | String | Specifies the entity to be used. |\n| `vars` | List | Specifies the `entity_var` and `input_var` variables. |\n\n**`entity_var`**\n\nThe `entity_var` field provides inputs for the feature views model. This variable stores the data temporarily, however, you can choose to store its data permanently by specifying the `name` in it as a feature in the `features` key.\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `name` | String | Name of the `entity_var` to identify it uniquely. |\n| `select` | String | Column name/value you want to select from the table. This defines the actual value that will be stored in the variable. You can use simple SQL expressions or select an `entity_var` as `{{entityName.Var(\\\"entity_var\\\")}}`. It has to be an aggregate operation that ensures the output is a unique value for a given `main_id`. For example: min(timestamp), count(\\*), sum(amount) etc. This holds true even when a window function (optional) is used. For example:: first\\_value(), last\\_value() etc are valid while rank(), row\\_number(), etc. are not valid and give unpredictable results. |\n| `from` | List | Reference to the source table from where data is to be fetched. You can either refer to another model from the same YAML or some other table specified in input YAML. |\n| `where` | String | Any filters you want to apply on the input table before selecting a value. This must be SQL compatible and should consider the data type of the table. |\n| `default` | String | Default value in case no data matches the filter. When defining default values, make sure you enclose the string values in single quotes followed by double quotes to avoid SQL failure. However, you can use the non-string values without any quotes. |\n| `description` | String | Textual description of the `entity_var`. |\n| `is_feature` | Boolean | Determines whether the `entity_var` is a feature. The default value is true. |\n| `window` | Object | Specifies the window function. Window functions in SQL usually have both `partition_by` and `order_by` properties. But for `entity_var`, `partition_by` is added with `main_id` as default; so, adding `partition_by` manually is not supported. If you need partitioning on other columns too, check out `input_var` where `partition_by` on arbitrary and multiple columns is supported. |\n\n**`input_var`**\n\nThe syntax of `input_var` is similar to `entity_var`, with the only difference that instead of each value being associated to a row of the feature views, it’s associated with a row of the specified input. While you can think of an `entity_var` as adding a helper column to the feature views, you can consider an `input_var` as adding a helper column to the input.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> If more than one `input_var` are required to derive an `entity_var`, then all the `input_var` must be defined on the same table.\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `name` | String | Name to store the retrieved data. |\n| `select` | String | Data to be stored in the name. |\n| `from` | List | Reference to the source table from where data is to be fetched. |\n| `where` | String | (Optional) Applies conditions for fetching data. |\n| `default` | String | (Optional) Default value for any entity for which the calculated value would otherwise be NULL. |\n| `description` | String | (Optional) Textual description. |\n| `window` | Object | (Optional) Specifies a window over which the value should be calculated. |\n\n**`window`**\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `partition_by` | String | (Optional) List of SQL expressions to use in partitioning the data. |\n| `order_by` | String | (Optional) List of SQL expressions to use in ordering the data. |\n\nIn window option, `main_id` is not added by default, it can be any arbitrary list of columns from the input table. So if a feature should be partitioned by `main_id`, you must add it in the `partition_by` key.\n\n### Output\n\nAfter [running the project](https://www.rudderstack.com/docs/archive/profiles/0.14/get-started/profile-builder/#7-generate-output-tables), you can view the generated material tables.\n\n1.  Log in to your Snowflake console.\n2.  Click **Worksheets** from the top navigation bar.\n3.  In the left sidebar, click **Database** and the corresponding **Schema** to view the list of all tables. You can hover over a table to see the full table name along with its creation date and time.\n4.  Write a SQL query like `select * from <table_name>` and execute it to see the results:\n\n1.  Open [Postico2](https://eggerapps.at/postico2/). If required, create a new connection by entering the relevant details. Click **Test Connection** followed by **Connect**.\n2.  Click the **+** icon next to **Queries** in the left sidebar.\n3.  You can click **Database** and the corresponding schema to view the list of all tables/views.\n4.  Double click on the appropriate view name to paste the name on an empty worksheet.\n5.  You can prefix `SELECT *` from the view name pasted previously and suffix `LIMIT 10;` at the end.\n6.  Press Cmd+Enter keys, or click the **Run** button to execute the query.\n\n1.  Enter your Databricks workspace URL in the web browser and log in with your username and password.\n2.  Click the **Catalog** icon in left sidebar.\n3.  Choose the appropriate catalog from the list and click on it to view contents.\n4.  You will see list of tables/views. Click the appropriate table/view name to paste the name on worksheet.\n5.  You can prefix `SELECT * FROM` before the pasted view name and suffix `LIMIT 10;` at the end.\n6.  Select the query text. Press Cmd+Enter, or click the **Run** button to execute the query.\n\n1.  Log in to your [Google Cloud Console](https://console.cloud.google.com/)\n2.  Search for Bigquery in the search bar.\n3.  Select Bigquery from **Product and Pages** to open the Bigquery **Explorer**.\n4.  Select the correct project from top left drop down menu.\n5.  In the left sidebar, click the project ID, then the corresponding dataset view list of all the tables and views.\n6.  Write a SQL query like `select * from <table_name> limit 10;` and execute it to see the results.\n\nA sample output containing the results in Snowflake:\n\n![Generated table (Snowflake)](https://www.rudderstack.com/docs/images/profiles/profiles-feature-table.webp)\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Feature Views | RudderStack Docs",
    "description": "Step-by-step tutorial on creating an feature view models.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.14/example/predictive-features-snowflake/prerequisite/",
    "markdown": "# Prerequisites | RudderStack Docs\n\nPrerequisites to generate predictive features in Snowflake using RudderStack Predictions\n\n* * *\n\n*     8 minute read  \n    \n\nTo follow this guide, you will need access to both RudderStack and Snowflake. If you do not have access, follow these links to create a free [RudderStack account](https://app.rudderstack.com/signup?type=freetrial) and [Snowflake account](https://signup.snowflake.com/).\n\nOnce you set up your RudderStack account, [reach out to our support team](mailto:support@rudderstack.com?subject=I%20would%20like%20access%20to%20your%20Predictions%20feature) to request access to our Predictions feature.\n\n## Set up Snowflake for Event Stream data\n\nBecause Predictions is designed to run in a production environment, you need to perform some basic set up in Snowflake (and later, your RudderStack workspace) to simulate the pipelines you would run when collecting user event data.\n\n### Create a new role and user in Snowflake\n\nIn your Snowflake console, run the following commands to create the role `QUICKSTART`.\n\nVerify the role `QUICKSTART` was successfully created.\n\nCreate a new user QUICKSTART\\_USER with a password `<strong_unique_password>`.\n\n```\nCREATE USER QUICKSTART_USER PASSWORD = '<strong_unique_password>' DEFAULT_ROLE = 'QUICKSTART';\n```\n\nVerify the user `QUICKSTART_USER` was successfully created.\n\n### Create RudderStack schema and grant permissions to role\n\nCreate a dedicated schema `_RUDDERSTACK` in your database.\n\n**Replace `<YOUR_DATABASE>` in all queries with your actual database name.**\n\n```\nCREATE SCHEMA \"<YOUR_DATABASE>\".\"_RUDDERSTACK\";\n```\n\nGrant full access to the schema \\_RUDDERSTACK for the previously created role `QUICKSTART`.\n\n```\nGRANT ALL PRIVILEGES ON SCHEMA \"<YOUR_DATABASE>\".\"_RUDDERSTACK\" TO ROLE QUICKSTART;\n```\n\n### Grant permissions on the warehouse, database, schema, and table\n\nEnable the user `QUICKSTART_USER` to perform all operations allowed for the role `QUICKSTART` (via the privileges granted to it).\n\n```\nGRANT ROLE QUICKSTART TO USER QUICKSTART_USER;\n```\n\nRun the following commands to allow the role `QUICKSTART` to look up the objects within your warehouse, database, schema, and the specific table or view:\n\n```\nGRANT USAGE ON WAREHOUSE \"<YOUR_WAREHOUSE>\" TO ROLE QUICKSTART;\nGRANT USAGE ON DATABASE \"<YOUR_DATABASE>\" TO ROLE QUICKSTART;\nGRANT USAGE ON SCHEMA \"<YOUR_DATABASE>\".\"_RUDDERSTACK\" TO ROLE QUICKSTART;\nGRANT SELECT ON ALL TABLES IN SCHEMA \"<YOUR_DATABASE>\".\"_RUDDERSTACK\" TO ROLE  QUICKSTART;\nGRANT SELECT ON FUTURE TABLES IN SCHEMA \"<YOUR_DATABASE>\".\"_RUDDERSTACK\" TO ROLE QUICKSTART;\nGRANT SELECT ON ALL VIEWS IN SCHEMA \"<YOUR_DATABASE>\".\"_RUDDERSTACK\" TO ROLE QUICKSTART;\nGRANT SELECT ON FUTURE VIEWS IN SCHEMA \"<YOUR_DATABASE>\".\"_RUDDERSTACK\" TO ROLE QUICKSTART;\n```\n\n**Replace `<YOUR_DATABASE>` with the exact Snowflake database name.**\n\n## Import RudderStack event data from the Snowflake marketplace\n\nTo set up automated features, you will need the RudderStack event data in your Snowflake warehouse. If you already use RudderStack and have the following tables and fields (see below), skip to the [Profiles Schema and Permissions](#profiles-schema-and-permissions) section. For this guide, using the provided sample data is recommended.\n\n*   `TRACKS`\n*   `IDENTIFIES`\n    *   `user_id`\n    *   `anonymous_id`\n    *   `email`\n*   `PAGES`\n*   `ORDER_COMPLETED`\n\n**NOTE:** You must have all the three identity types in your `INDENTIFIES` table. If you are using your own data and don’t normally track email, you can send the following `identify` call to add the column:\n\n```\nrudderanalytics.identify('userId', {\n    email:'email@address.com',\n    name:'name'\n})\n```\n\n### Get sample data\n\nIf you are setting up RudderStack for the first time go to the [Snowflake Marketplace](https://app.snowflake.com/marketplace/listing/GZT0Z856CMJ/rudderstack-inc-rudderstack-event-data-for-quickstart) and add RudderStack Event Data for Quickstart to your Snowflake account for free. This will add a database with the needed tables to your Snowflake warehouse with no additional storage cost for you.\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/Snowflake-marketplace.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/Snowflake-marketplace.webp)\n\nAt the next screen, open **Options** and add role `QUICKSTART` to have access to this database.\n\n### Create schema for sample data\n\nThe database with the sample data is read-only so you will need to copy it to a new schema to be able to create a valid event stream pipeline (and run a Predictions job on the data).\n\nCreate a new schema in the database you already set up. Name the schema “EVENTS”.\n\n```\nCREATE SCHEMA \"<YOUR_DATABASE>\".\"EVENTS\";\n```\n\nGive permission to the `QUICKSTART` role to create new tables in the above schema.\n\n```\nGRANT ALL PRIVILEGES ON SCHEMA \"<YOUR_DATABASE>\".\"EVENTS\" FOR ROLE QUICKSTART;\n```\n\nCopy the sample data into the newly create schema.\n\n```\nCREATE TABLE \"<YOUR_DATABASE>\".\"EVENTS\".\"TRACKS\" AS SELECT * FROM \"SNOWFLAKE_QUICKSTART\".\"PUBLIC\".\"TRACKS\";\nCREATE TABLE \"<YOUR_DATABASE>\".\"EVENTS\".\"IDENTIFIES\" AS SELECT * FROM \"SNOWFLAKE_QUICKSTART\".\"PUBLIC\".\"IDENTIFIES\";\nCREATE TABLE \"<YOUR_DATABASE>\".\"EVENTS\".\"PAGES\" AS SELECT * FROM \"SNOWFLAKE_QUICKSTART\".\"PUBLIC\".\"PAGES\";\nCREATE TABLE \"<YOUR_DATABASE>\".\"EVENTS\".\"ORDER_COMPLETED\" AS SELECT * FROM \"SNOWFLAKE_QUICKSTART\".\"PUBLIC\".\"ORDER_COMPLETED\";\n```\n\nNow you are ready to create a pipeline connection in RudderStack.\n\n## Create JavaScript source\n\nRudderStack’s Profiles and Predictions products require a warehouse destination with an active sync from a source (a data pipeline). Therefore we will create a JavaScript source that can send a test event to Snowflake.\n\nAfter logging into RudderStack, navigate to the **Directory** from the sidebar on the left, then select the JavaScript source from the list of sources.\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/select-js-source.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/select-js-source.webp)\n\nEnter “QuickStart Test Site” for the source name and click `Continue`. You have successfully added a source!\n\nNote at the bottom of the JavaScript Source page is a `Write Key`. You will need this for sending a test event after connecting the Snowflake destination.\n\n## Create Snowflake destination\n\nNavigate to the **Overview** tab in the JavaScript source view and click on **Add Destination**.\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/add-destination.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/add-destination.webp)\n\nSelect the Snowflake destination from the list, then on the next page give it the name “Snowflake QuickStart” and click **Continue**.\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/select-snowflake.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/select-snowflake.webp)\n\nAdd in your Snowflake connection credentials:\n\n*   **Account**: Your account name.\n*   **Database**: Your database name that you used in the previous steps for `QUICKSTART`.\n*   **Warehouse**: Your warehouse that you granted usage to `QUICKSTART`.\n*   **User**: `QUICKSTART_USER`\n*   **Role**: `QUICKSTART`\n*   **Password**: Password for `QUICKSTART_USER`.\n*   **Namespace**: `EVENTS`\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/snowflake-config.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/snowflake-config.webp)\n\nAt the bottom under **Object Storage Configuration** toggle **Use RudderStack managed object storage** ON.\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/object-storage-toggle.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/object-storage-toggle.webp)\n\nLeave the defaults for all other settings and click **Continue**. RudderStack will verify credentials and that it has the needed permissions.\n\nYou have now created a pipeline connection in RudderStack!\n\n## Send test event\n\nYou can use a test site to send a `connection_setup` event. This will not effect the sample data tables. But first, get the following configuration data from RudderStack:\n\n*   RudderStack Data Plane URL\n*   JavaScript Source Write Key\n\n### Data Plane URL\n\nGo to the **Connections** page in the RudderStack app and copy the **Data Plane** URL from the top of the page.\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/data-plane-url.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/data-plane-url.webp)\n\n### Write key\n\nGo to your JavaScript source in RudderStack and in the **Setup** tab scroll down and copy the **Write key**.\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/write-key.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/write-key.webp)\n\n### Test event\n\nGo to RudderStack’s [test website](https://ryanmccrary.github.io/rudderstackdemo/) and copy your Data Plane URL and Write Key into the top fields and press **Submit**.\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/test-site-setup.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/test-site-setup.webp)\n\nEnter `connection_setup` into the `event_name` field next to **Send Custom Event** and then click on **Send Custom Event**.\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/test-site-event.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/test-site-event.webp)\n\nYou can check the event using RudderStack’s [**Live events**](https://www.rudderstack.com/docs/dashboard-guides/live-events/) view or check the **Syncs** tab in the Snowflake destination.\n\n**Note that the test event needs to be delivered to Snowflake to validate the pipeline.** If needed, you can run a manual sync by clicking **Sync now** in the **Syncs** tab of the Snowflake destination view in RudderStack.\n\n## Profiles schema and permissions\n\nRemember that Predictions automatically runs a Profiles job to create an identity graph. In this step, create a new schema where the identity graph and the related tables and views will be generated.\n\n```\nCREATE SCHEMA \"<YOUR_DATABASE>\".\"PROFILES\";\n```\n\nNow we need to grant permissions to the `QUICKSTART` role.\n\nProfiles will need the following permissions to run:\n\n*   Read access to all input tables to the model (already complete if you followed the previous setup steps)\n*   Write access to the schemas and common tables that the Profiles project creates.\n\nFor the write access run the following statements:\n\n```\nGRANT ALL PRIVILEGES ON SCHEMA PROFILES_QUICKSTART.PROFILES TO ROLE QUICKSTART;\nGRANT SELECT ON ALL TABLES IN SCHEMA PROFILES_QUICKSTART.PROFILES TO ROLE QUICKSTART;\nGRANT SELECT ON FUTURE TABLES IN SCHEMA PROFILES_QUICKSTART.PROFILES TO ROLE QUICKSTART;\nGRANT SELECT ON ALL VIEWS IN SCHEMA PROFILES_QUICKSTART.PROFILES TO ROLE QUICKSTART;\nGRANT SELECT ON FUTURE VIEWS IN SCHEMA PROFILES_QUICKSTART.PROFILES TO ROLE QUICKSTART;\n```\n\nYou are now ready to run Profiles and Predictions projects in the RudderStack UI!\n\n## Profiles CLI setup\n\nBefore you start building automated features, you need to perform some additional setup steps so that you can transition seamlessly from the UI-based workflow to the code-based workflow in the [code your own custom predictions](https://www.rudderstack.com/docs/archive/profiles/0.14/example/predictive-features-snowflake/custom-code/) section.\n\nTo build custom features with code, you will need Python3 and the RudderStack Profiles CLI tool (`PB`, for Profiles Builder) installed on your machine. If you do not have `PB` installed, follow the instructions below. This includes authentication for your Snowflake environment. **Use the warehouse, database, and schema setup in the previous steps.** This authentication will be used for accessing your Snowflake warehouse and running Snowpark. For more information about Profiles CLI tool, see [documentation](https://www.rudderstack.com/docs/profiles/get-started/profile-builder/).\n\n### Install Profile Builder tool\n\nOpen a console window and install the Profile Builder `PB` tool.\n\n```\npip3 install profiles-rudderstack\n```\n\nCheck the version to make sure it is at least `0.10.5`\n\n### Install ML dependency\n\nIn order to run ML models you will need to install the python package `profiles-multieventstream-features`. Run the following command to install it.\n\n```\npip install git+https://github.com/rudderlabs/profiles-pycorelib\n```\n\nEnsure you have the following python packages installed. These are required to use the `rudderstack-profiles-classifier` package to train classification models for predictive features.\n\n```\ncachetools>=4.2.2\nhyperopt>=0.2.7\njoblib>=1.2.0\nmatplotlib>=3.7.1\nseaborn>=0.12.0\nnumpy>=1.23.1\npandas>=1.4.3\nPyYAML>=6.0.1\nsnowflake_connector_python>=3.1.0\nsnowflake-snowpark-python[pandas]>=0.10.0\nscikit_learn>=1.1.1\nscikit_plot>=0.3.7\nshap>=0.41.0\nplatformdirs>=3.8.1\nxgboost>=1.5.0\nredshift-connector\n```\n\n### Create warehouse connection\n\nInitiate a warehouse connection:\n\nFollow the prompts and enter the details for your Snowflake warehouse/database/schema/user.\n\n```\nEnter Connection Name: quickstart\nEnter target:  (default:dev)  # Press enter, leaving it to default\nEnter account: <YOUR_ACCOUNT>\nEnter warehouse: <YOUR_WAREHOUSE>\nEnter dbname: <YOUR_DATABASE>\nEnter schema: PROFILES\nEnter user: QUICKSTART_USER\nEnter password: <password>\nEnter role: QUICKSTART\nAppend to /Users/<user_name>/.pb/siteconfig.yaml? [y/N]\ny\n```\n\n### Enable ML models\n\nFinally, enable ML models within `siteconfig.yaml`.\n\nOpen the file `/Users/<user_name>/.pb/siteconfig.yaml` in a text editor.\n\nAt the bottom of the file there is a `py_models` section. Update it to look like this:\n\n```\npy_models:\n    enabled: true\n    python_path: $(which python3)\n    credentials_presets: null\n    allowed_git_urls_regex: \"\"\n```\n\n## Snowpark\n\nPredictive features utilizes Snowpark within your Snowflake environment. It uses the same authentication as Snowflake and is able to run jobs within Snowflake.\n\nThis will run python code in a virtual warehouse in Snowflake and will incur compute costs. These costs vary depending on the type of model and the quantity of data used in training and prediction. For more general information on Snowflake compute costs, see [Understanding Compute Costs](https://docs.snowflake.com/en/user-guide/cost-understanding-compute).\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Prerequisites | RudderStack Docs",
    "description": "Prerequisites to generate predictive features in Snowflake using RudderStack Predictions",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.14/example/predictive-features-snowflake/setup-automated-features/",
    "markdown": "# Setup Automated Features | RudderStack Docs\n\nSet up RudderStack to build automated features in the RudderStack UI\n\n* * *\n\n*     6 minute read  \n    \n\nSetting up automated features in the RudderStack UI is a straight-forward process. Predictive features are configured within a Profiles project and automatically added to the feature table output when the project is run.\n\n## Project setup\n\nFollow the steps below to set up a project and build predictive features:\n\n### Log into RudderStack\n\nYou can log-in [here](https://app.rudderstack.com/login).\n\n### Navigate to Profiles screen\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/Navigation.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/Navigation.webp)\n\n### Enter a name and description\n\nEnter a unique name and description for the Profiles Project where you want to build the predictive features.\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/Profiles-Name.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/Profiles-Name.webp)\n\n### Select sources\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/Sources.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/Sources.webp)\n\nSelect your Snowflake warehouse. If you have not configured the Snowflake warehouse, set up an event stream connection to Snowflake in RudderStack ([see details here](https://www.rudderstack.com/docs/destinations/warehouse-destinations/snowflake/)) and refer to the setup steps above.\n\nOnce you select the warehouse, you will be able to choose from RudderStack event sources that are connected to Snowflake. In this example, the JavaScript source created above is used to write to the same schema as the sample data. Profiles will use the `PAGES`, `TRACKS`, `IDENTIFIES`, and `ORDER_COMPLETED` tables from that schema to build automated and predictive features.\n\n## Map ID fields\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/Map-ID.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/Map-ID.webp)\n\nMap the fields from the source table(s) to the correct type of ID. The standard ID types are:\n\n*   `user_id`\n*   `anonymous_id`\n*   `email`\n\n**Note that for RudderStack event sources, standard ID column names will be mapped for you automatically**. If you have included additional identifiers in your payloads, you can map those custom column names to standard identifiers by clicking **Add mapping** at the bottom of the table.\n\n### Map `Order_Completed` table\n\nClick on **Add mapping** and map the `USER_ID` and `ANONYMOUS_ID` columns to standard identifiers to include the `ORDER_COMPLETED` table as a source for the identity graph and user features.\n\n| Source | Event | Property | ID Type |\n| --- | --- | --- | --- |\n| QuickStart Test Site | ORDER\\_COMPLETED | USER\\_ID | user\\_id |\n| QuickStart Test Site | ORDER\\_COMPLETED | ANONYMOUS\\_ID | anonymous\\_id |\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/map-id-orders.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/map-id-orders.webp)\n\n## Create default features in the UI\n\nThere are two types of automated features you can define in the UI:\n\n*   Default features\n*   Custom features\n\nThis guide focuses on the default features that are automatically generated.\n\n### Set up default features\n\nDefault features are features commonly used in Profiles projects. RudderStack provides a template library for these features to make them easy to add to your project. Templated features give you access to over 40 different standard and predictive features, which are generated in Snowflake automatically.\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/feature-categories.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/feature-categories.webp)\n\nDefault features are divided into 4 categories:\n\n*   **Attribution** - campaign, source, and churn features\n*   **Demographics** - user trait features\n*   **Engagement** - user activity features\n*   **Predictive ML Features** - predictive features\n\nYou can open the drop down menu for each category and select as many as you would like for your project.\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/feature-attribution.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/feature-attribution.webp)\n\nFor this guide, select:\n\n*   Attribution\n    *   `first_source_name`\n    *   `is_churned_30_days`\n    *   `is_churned_90_days`\n*   Demographics\n    *   `first_name`\n    *   `last_name`\n    *   `state`\n*   Engagement\n    *   `first_date_seen`\n    *   `last_date_seen`\n    *   `total_sessions_90_days`\n    *   `total_sessions_last_week`\n*   Predictive ML Features\n    *   `percentile_churn_score_30_days`\n\nIt is important to remember that RudderStack runs all of the feature-generation code transparently in Snowflake. For any of the default features, other than Predictive ML Features, you can click on **Preview Code** and get a yaml code snippet defining that feature (the yaml definition is used to generate SQL). This is helpful for technical users who want a deeper understanding of feature logic (and a running start for coding their own features).\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/churn-code-snippet.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/churn-code-snippet.webp)\n\n#### Churn definition\n\nRudderStack Predictions automatically generates a binary churn value for every user based on inactivity over a 7, 30, or 90-day period.\n\nFor example, to calculate the `is_churned_7_days` value, RudderStack looks for any activity timestamp for a particular user in the `TRACKS` and `PAGES` tables over the previous 7 day period. Practically, this means that RudderStack executes a ‘max timestamp’ query against those tables to see if users have viewed a page or performed other tracked actions (like clicks, form submits, add to carts, etc.) and then calculates the difference from today. If the query returns 7 or more, that means they haven’t performed any activity over the last 7 days and their `is_churned_7_days` trait is set to `1`.\n\n#### How Predictions models percentile churn scores\n\nUsing the standard definition (no activity over a defined period), RudderStack Predictions automatically runs a python-based churn model in Snowpark that predicts whether users will become inactive (churn) over the next 7, 30, or 90-day period. This model is trained on existing user data, using the Profiles identity graph, so it is recommended that you have a minimum of 5,000-10,000 unique users to achieve accurate output for business use cases.\n\n**How Predictions automates ML with Snowpark**\n\nPredictions streamlines integration with Snowpark by using the authentication from your existing Snowflake integration in RudderStack.\n\nIn order to run models in Snowpark, there is one additional set of permissions required. To run Predictions jobs, you must have permission to create stages within your schema. For more information see the **CREATE STAGE** [documentation](https://docs.snowflake.com/en/sql-reference/sql/create-stage#access-control-requirements).\n\nOnce permissions are granted, you will be able to run jobs that produce predictive features. **If you have followed the steps in [Prerequisite](https://www.rudderstack.com/docs/archive/profiles/0.14/example/predictive-features-snowflake/prerequisite/) guide, that permission has already been granted.**\n\n## Create custom features in the UI\n\nIf a needed feature is not in the template library, you can define a custom feature in the UI. Custom features can be standard or predictive features.\n\n### Add Custom Features\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/custom-feature.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/custom-feature.webp)\n\nClick on **Add a custom feature** at the top of the page and build an `average_order` feature with the following values:\n\n| Field | Value |\n| --- | --- |\n| **Name** | average\\_order |\n| **Description** | Average Order Size including shipping, taxes, and discounts |\n| **Function Type** | AGGREGATE |\n| **Function** | AVG |\n| **Event** | EVENTS.ORDER\\_COMPLETED |\n| **Property or Trait** | TOTAL |\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/custom-feature-define.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/custom-feature-define.webp)\n\nOnce complete click **Save**. The custom feature will be added to the top of the page.\n\n## Set Schedule\n\nThere are three options to set a schedule for how often the feature generation job runs:\n\n*   Basic\n*   Cron\n*   Manual\n\n### Basic\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/schedule-basic.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/schedule-basic.webp)\n\nSchedule on a predetermined interval.\n\nThe frequency can be every:\n\n*   30 minutes\n*   1 hour\n*   3 hours\n*   6 hours\n*   12 hours\n*   24 hours\n\nThen select a starting time for the initial sync.\n\n### Cron\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/schedule-cron.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/schedule-cron.webp)\n\nSchedule using cron expressions for more specific scheduling (i.e. Daily on Tuesdays and Thursdays).\n\nIf you are not familiar with cron expressions, you can use the builder in the UI.\n\n### Manual\n\nOnly runs when manually triggered within the UI. For this guide, select **Manual**.\n\n## Save, review, and create project\n\n### Save project\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/save-project.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/save-project.webp)\n\nFill in the **Schema** field with `PROFILES` (to match what we created earlier). This is where the feature table will be written to in Snowflake.\n\n### Review and create project\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/review-create.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/review-create.webp)\n\nFinally, review all the settings and when ready, click `Create user 360`.\n\n## Review created features\n\nOnce the initial project run is initiated, it may take up to 25-30 minutes to complete. Once the job is done, you are able to explore the data in RudderStack’s UI, including model fit charts for predictive features and individual user records with all features.\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/ui-predictive.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/ui-predictive.webp)\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/ui-predictive-graphs.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/ui-predictive-graphs.webp)\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/ui-explorer.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/ui-explorer.webp)\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Setup Automated Features | RudderStack Docs",
    "description": "Set up RudderStack to build automated features in the RudderStack UI",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.14/example/predictive-features-snowflake/custom-code/",
    "markdown": "# Custom Predictive Features | RudderStack Docs\n\nCode your own custom predictive features\n\n* * *\n\n*     8 minute read  \n    \n\nWhile automated features are incredibly useful for quickly deploying activity-based churn scores, data teams inevitably want to go deeper and define custom predictions that match their unique business logic and KPIs.\n\nBasic customization is possible in the UI as we covered above, but Predictions also supports a code-based workflow that gives technical users full control and complete customizability, as well as the ability to integrate the process into their existing development workflow.\n\nFor example, if you are an eCommerce company, it can be helpful to predict whether or not a user will make a purchase over a certain dollar amount, over the next `n` days.\n\nRudderStack makes it easy to migrate from the UI-based workflow to the code-based workflow to build these more complex use cases.\n\n## Download project files\n\nOn the Profiles screen, find your project and click the **Download this Project** button on the top right side. This will download all the files for that Profiles project in a compressed (zip) file including the modeling files.\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/download-project.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/download-project.webp)\n\nInside the Profiles folder you will find `pb_project.yaml` and a `models` folder with `resources.yaml`.\n\n```\nRudderStack QuickStart\n├── pb_project.yaml\n├── models\n│   ├── resources.yaml\n```\n\n### pb\\_project.yaml\n\n`pb_project.yaml` is the main configuration file for the Profiles project. The top section defines the `name`, `schema_version`, `connection`, and `model_folder` (where the files that define the details of the Profiles project can be found).\n\nUpdate the following values:\n\n*   `name` to `Profile-Quickstart` to match the name in the UI.\n*   `connection` to `QUICKSTART` to match the database connection we made in the Prerequisites section.\n\n```\nname: Profile-QuickStart\nschema_version: 67 # Or most recent version\nconnection: QUICKSTART\nmodel_folders:\n    - models\n```\n\nBelow there is an `entities` section that defines the entities and the kinds of ID’s make up that entity. An entity is a business concept or unit that will be used to build the identity graph and features. Projects can contain multiple entities like user, household, and organization.\n\nFor this project, there is one entity called `user` with 5 different types of IDs. An ID type maps which ID fields can be joined together. For example, if you have two tables with `user_id` columns called `id` and `userid`, by giving each the type `user_id` Profiles knows to join those tables on those columns.\n\nThe ID fields are already mapped to these types in the UI. Nothing needs to be updated in this section.\n\n```\nentities:\n  - name: user\n    id_types:\n      - main_id\n      - user_id\n    feature_views:\n      using_ids:\n        - id: email\n          name: features_by_email\n        - id: salesforce_id\n          name: salesforce_id_stitched_features\n      features:\n        - from: models/cart_feature_table\n          include:\n            - \"*\"\n```\n\nFinally there is a `packages` section. This section allows you to import a Profiles project from GitHub and use the feature definitions from that project in this one. The imported project provides the definitions for the standard features selected in the UI. Nothing needs to be updated in this section.\n\n```\npackages:\n    - name: base_features\n      url: https://github.com/rudderlabs/rudderstack-profiles-multieventstream-features\n      inputsMap: # These are the tables automatically mapping in the UI (TRACKS, PAGES, IDENTIFIES)\n        rsIdentifies_1: inputs/rsIdentifies_1\n        rsIdentifies_2: nil\n        rsIdentifies_3: nil\n        rsPages_1: inputs/rsPages_1\n        rsPages_2: nil\n        rsPages_3: nil\n        rsTracks_1: inputs/rsTracks_1\n        rsTracks_2: nil\n        rsTracks_3: nil\n      overrides: # By default all features are imported from the project, therefore the features we did not select need to be disabled\n        - requested_enable_status: disabled\n          models:\n            - entity/user/active_days_in_past_365_days\n            - entity/user/active_days_in_past_7_days\n            - entity/user/avg_session_length_in_sec_365_days\n            - entity/user/avg_session_length_in_sec_last_week\n            - entity/user/avg_session_length_in_sec_overall\n            - entity/user/campaign_sources\n            - entity/user/campaigns_list\n            - models/churn_7_days_model\n            - models/churn_90_days_model\n            - entity/user/country\n            - entity/user/currency\n            - entity/user/days_since_account_creation\n            - entity/user/days_since_last_seen\n            - entity/user/first_campaign_name\n            - entity/user/is_churned_7_days\n            - entity/user/last_campaign_name\n            - entity/user/last_source_name\n            - entity/user/max_timestamp_bw_tracks_pages\n            - entity/user/mediums_list\n            - entity/user/sources_list\n            - entity/user/total_sessions_365_days\n            - entity/user/total_sessions_till_date\n```\n\n### resource.yaml\n\n`resources.yaml` contains two main sections: `inputs` and `var_groups`.\n\nThe `inputs` section defines what ID’s are in each table and their mapping. Currently these are all the tables and mappings that were defined in the UI. These tables are used for creating an identity graph and all features related to it.\n\nIf you want to add another table in the future, the table and ID mappings would be added here. Below is an example of the `ORDER_COMPLETED` table we manually mapped in the UI. It consists of the following fields:\n\n| Field | Description |\n| --- | --- |\n| name | alias for the table; the primary reference in the rest of the yaml files |\n| table | `<SCHEMA>.<TABLE_NAME>` |\n| select | column with ID |\n| type | kind of ID |\n| entity | what entity the ID should be mapped to |\n| to\\_default\\_stitcher | `true` unless you decide to use a different ID stitcher |\n| remapping | leave as `null` |\n\n```\n- name: rs_EVENTS_ORDER_COMPLETED\n  app_defaults:\n    table: EVENTS.ORDER_COMPLETED\n    ids:\n        - select: USER_ID\n          type: user_id\n          entity: user\n          to_default_stitcher: true\n        - select: ANONYMOUS_ID\n          type: anonymous_id\n          entity: user\n          to_default_stitcher: true\n  remapping: null\n```\n\nThe `var_groups` section is where custom features are defined, both custom features created in the UI and those added via code in this file. Custom features are organized into groups by entity (in our case only `user`). The entity is like the `group by` variable in a SQL query.\n\nBelow that custom features are defined in the `vars` subsection. Here is the `average_order` feature we created in the UI.\n\n```\n- entity_var:\n    is_feature: true\n    name: average_order\n    description: Average Order Size including shipping, taxes, and discounts\n    select: AVG(TOTAL)\n    from: inputs/rs_EVENTS_ORDER_COMPLETED\n```\n\nA name and description are required for the custom feature and then it is defined using declarative SQL syntax. This allows you to define the custom feature the same way you would if creating a new table with SQL.\n\n## Create a custom predictive feature\n\nJust like in the UI workflow, you must already have defined the feature you want to predict. Therefore we are going to add a new custom feature for large purchases in the last 90 days. **NOTE: Currently predictive features can only be binary (i.e. 1/0)**\n\nA large order is defined here as any order with a `TOTAL` of > $100.\n\nAt the bottom of the `resources.yaml`, add the name and definition for `large_purchase_last_90`.\n\n```\n- entity_var:\n  name: large_purchase_last_90\n  description: Customer that made a purchase of >$100 in the last 90 days.\n  select: CASE WHEN MAX(TOTAL) > 100 THEN 1 ELSE 0 END\n  from: inputs/re_EVENTS_ORDER_COMPLETED\n  where: DATEDIFF(days, TIMESTAMP, CURRENT_DATE) <= 90\n```\n\nYou can use SQL functions and keywords in the definition. FOr example, a `CASE` statement in the SELECT statement and add a `where` statement and use the `DATEDIFF` function. You can also use the alias for the `ORDER_COMPLETED` table in the `from` statement.\n\nFor more details on Profiles and project file structure, you can review the Profiles [documentation](https://www.rudderstack.com/docs/profiles/overview/).\n\n## Organize the project in two files (**OPTIONAL**)\n\nProfiles does not need specific yaml files in the `models` folder in order to run. That allows you to organize your code as you feel is best. You can keep it all in one file or can split it over multiple files.\n\nYou can split the `resources.yaml` file into `inputs.yaml` and `profiles.yaml` by creating the two yaml files. Then copy everything from the `inputs` section into `inputs.yaml` and `var_groups` into `profiles.yaml`.\n\nOnce done, you can delete the `resources.yaml`.\n\n## Add a custom predictive feature\n\nThis section explains how to create 2 new custom predictive features from `large_purchase_last_90` called `likelihood_large_purchase_90` (raw score) and `percentile_large_purchase_90`(percentile score).\n\n#### Add Python ML requirement\n\nIn order to add custom predictive features, add the `profiles-pycorelib` package to the project requirements. At the bottom of `pb_project.yaml` add the following code to `pb_project.yaml`.\n\n```\npython_requirements:\n  - profiles-pycorelib==0.2.1\n```\n\n#### Create ml\\_models.yaml\n\nNow, create a new file and name it `ml_models.yaml`. This file is where you can define 2 new custom predictive features and how to train the ML model. The code for these new predictive features is discussed below.\n\nThis file is organized by the predictive model created for predictive features, not the individual features. The top level consists of:\n\n| Field/Section | Description |\n| --- | --- |\n| `name` | Name of the model (not feature) |\n| `model_type` | `python_model` |\n| `model_spec` | All of the model specifications |\n\n* * *\n\n`model_spec` section:\n\n| Section | Description |\n| --- | --- |\n| `train` | Training configuration |\n| `predict` | Scoring configuration |\n\n```\nmodels:\n    - name: &model_name large_purchase_90_model\n      model_type: python_model\n      model_spec:\n        occurred_at_col: insert_ts\n        entity_key: user\n        validity_time: 24h\n        py_repo_url: git@github.com:rudderlabs/rudderstack-profiles-classifier.git # Model training and scoring repo\n\n        train:\n          file_extension: .json\n          file_validity: 2160h # 90 days; when the model will be retrained\n          inputs: &inputs\n            - packages/base_features/models/rudder_user_base_features # location of the base features created in the UI\n            - packages/large_purchase_last_90 # custom feature created in var_groups\n            - models/average_order # custom feature we created in the UI\n          config:\n            data: &model_data_config\n              package_name: feature_table\n              label_column: large_purchase_last_90 # target feature\n              label_value: 1 # target feature value predicting\n              prediction_horizon_days: 90 # how far into the future\n              features_profiles_model:  'rudder_user_base_features' #taken from inputs\n              output_profiles_ml_model: *model_name\n              eligible_users: 'large_purchase_last_90 is not null' # limit training data to those with non-null values\n              inputs: *inputs\n            preprocessing: &model_prep_config\n              ignore_features: [first_name, last_name, state] # features we do not used in a model\n\n        predict:\n          inputs: *inputs # copy from train\n          config:\n            data: *model_data_config # copy from train\n            preprocessing: *model_prep_config # copy from train\n            outputs:\n              column_names:\n                percentile: &percentile percentile_large_purchase_90 # name of percentile feature\n                score: &raw_score likelihood_large_purchase_90 # name of raw likelihood feature\n              feature_meta_data: &feature_meta_data\n                features:\n                  - name: *percentile\n                    description: 'Percentile of likelihood score. Higher the score the more likely to make a larger purchase'\n                  - name: *raw_score\n                    description: 'Raw likelihood score. Higher the score the more likely to make a larger purchase'\n\n        <<: *feature_meta_data\n```\n\n## Compile and run\n\nSave all files. Now compile the project, this will make sure all SQL and python files are able to be created.\n\nFinally, run the project. This will generate the same files as `compile` and then execute them in Snowflake. The first run can take at least 30 minutes because of training ML models.\n\n## Final table\n\nThe final predictive features can be found in your Snowflake environment together in the same table. The table will provide you with the unified user ID, created by RudderStack, when the features are valid as of (i.e. when the model was last run to create these features), and model ID, and your predictive features.\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/final-table.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/final-table.webp)\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Custom Predictive Features | RudderStack Docs",
    "description": "Code your own custom predictive features",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.14/example/packages/",
    "markdown": "# Additional Concepts | RudderStack Docs\n\nAdditional concepts related to Profiles like packages, best practices, partial feature tables, etc.\n\n* * *\n\n*     19 minute read  \n    \n\nThis guide explains some of the advanced concepts related to Profiles.\n\n## Packages\n\nProfiles gives you the flexibility to utilize models from existing library projects while defining your own models and inputs within the PB project. This approach allows for a seamless integration of library of pre-existing features, which are readily available and can be applied directly to data streamed into your warehouse.\n\nIn the absence of any explicitly defined models, the PB project is capable of compiling and running models from the library package given that inputs are present in the warehouse as assumed in the lib package.\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> Packages currently work only on Snowflake.\n\nThe following list of packages are currently available in Profiles. You can [contact the RudderStack team](mailto:support@rudderstack.com) to access these:\n\n*   [profiles-corelib](https://github.com/rudderlabs/profiles-corelib)\n*   [profiles-base-features](https://github.com/rudderlabs/profiles-base-features)\n*   [profiles-shopify-features](https://github.com/rudderlabs/profiles-shopify-features)\n*   [profiles-ecommerce-features](https://github.com/rudderlabs/profiles-ecommerce-features)\n*   [profiles-stripe-features](https://github.com/rudderlabs/profiles-stripe-features)\n*   [profiles-multieventstream-features](https://github.com/rudderlabs/profiles-multieventstream-features)\n\nGenerally, there will be some deviations in terms of the database name and schema name of input models - however, you can easily handle this by remapping inputs.\n\nA sample `pb_project.yaml` file may look as follows:\n\n```\nname: app_project\nschema_version: 67\nprofile: test\npackages:\n  - name: test_ft\n    gitUrl: \"https://github.com/rudderlabs/librs360-shopify-features/tree/main\"\n```\n\nIn this case, the PB project imports a single package. It does not require a separate `models` folder or entities as the input and output models will be sourced from the imported packages.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> Note that:\n> \n> *   If non-mandatory inputs required by the model are not present in the warehouse, you can still run the model.\n> *   If there is any deviation in the table/view name for input models, that is, if the inputs assumed in library package are present under some other name, make sure to do the remapping.\n> *   If some of the assumed inputs are not present at all, they should be remapped to `nil`. This way you can create and run imported packages with minimal set of inputs present.\n\nFor example, to import a library package with the name of `shopify_features`:\n\n```\npackages: \n  - name: shopify_features\n    url: https://github.com/rudderlabs/librs360-shopify-features/tree/main\n    inputsMap: \n      rsCartCreate: inputs/rsWarehouseCartCreate\n      rsCartUpdate: inputs/rsCartUpdate\n      rsIdentifies: inputs/rsIdentifies\n      rsOrderCancelled: inputs/rsOrderCancelled\n      rsOrderCreated: inputs/rsOrderCreated\n      rsPages: nil\n      rsTracks: nil\n```\n\nIn `models`/`inputs.yaml`, these inputs need to be defined with table names present in the warehouse.\n\n```\ninputs:\n  - name: rsWarehouseCartCreate\n    table: YOUR_DB.YOUR_SCHEMA.CART_CREATE_TABLE_NAME_IN_YOUR_WH\n    occurred_at_col: timestamp\n    ids:\n      - select: \"anonymous_id\"\n        type: anonymous_id\n        entity: user\n    source_metadata:\n      role: shopify\n      category: webhook\n  - name: rsIdentifies\n    table: YOUR_DB.YOUR_SCHEMA.IDENTIFIES\n    occurred_at_col: timestamp\n    ids:\n      - select: \"user_id\"\n        type: user_id\n        entity: user\n      - select: \"anonymous_id\"\n        type: anonymous_id\n        entity: user\n      - select: \"lower(email)\"\n        type: email\n        entity: user\n    source_metadata:\n      role: shopify\n      category: webhook\n```\n\nNote that the name of the table/view is changed to the appropriate name in your warehouse. If tables are present with the same name (including database name and schema name) then no remapping is required.\n\n### Modify ID types\n\n#### Extend existing package\n\nYou can add custom ID types to the default list or modify an existing one by extending the package to include your specifications.\n\nFor the corresponding `id_type`, add the key `extends:` followed by name of the same/different `id_type` that you wish to extend and the `filters` with `include`/`exclude` values.\n\n```\n---pb_project.yaml---\npackages:\n  - name: corelib\n    url: \"https://github.com/rudderlabs/rudderstack-profiles-corelib/tag/schema_{{best_schema_version}}\"\nid_types:\n  - name: user_id\n    extends: user_id\n    filters:\n      - type: exclude\n        value: 123456\nid_types:\n  - name: customer_id\n    extends: user_id\n    filters:\n      - type: include\n        regex: sample\n```\n\n*   **id\\_types**\n\nEnlists the type of identifiers to be used for creating ID stitcher/`entity_var`/`input_var`. For example, you can define anonymous IDs that do not include the value `undefined` or email addresses in proper format.\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `name` | String | Name of the ID type like email, user ID, etc. |\n| `extends` | List | (Optional) Name of the ID type you wish to extend. |\n| `filters` | List | Filter(s) the ID types to include/exclude specific values. The filters are processed in the defined order. |\n\n*   **filters**\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `type` | String | Type of filter. Allowed values are `include` or `exclude`. |\n| `value` | String | Value to match, for example, you can reject certain invalid ID values like `NaN`, `unknown`, `test@domain.com`, etc. |\n| `regex` | String | Regular expression with which to match the values. |\n| `sql` | List | SQL statement with `select` and `from` keys. |\n\n#### Custom list of ID types\n\nTo have custom list of ID types other than the provisions in the default package, you can remove and add your list as follows:\n\n```\nentities:\n  - name: user\n    id_types:\n      - user_id\n      - anonymous_id\n      - email\n\nid_types:\n  - name: user_id\n  - name: anonymous_id\n    filters:\n      - type: exclude\n        value: \"\"\n      - type: exclude\n        value: \"unknown\"\n      - type: exclude\n        value: \"NaN\"\n  - name: email\n    filters:\n    - type: include\n      regex: \"[A-Za-z0-9+_.-]+@(.+)\"\n```\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> Make sure that the ID types are also defined in the entity definition.\n\n## Model Contracts\n\nYou can use the `contract` field to specify the constraints your model should adhere to while using the warehouse data.\n\nSuppose a model (`M1`) is dependent on model (`M2`). Now, `M1` can specify a contract defining the columns and entities that it needs from `M2` to be executed successfully. Also, it becomes mandatory for `M2` to provide the required columns and entities for contract validation.\n\n**Example 1**\n\nThe following `inputs.yaml` file defines a contract:\n\n```\n- name: rsIdentifies\n  contract:\n    is_optional: false\n    is_event_stream: true\n    with_entity_ids:\n      - user\n    with_columns:\n      - name: timestamp\n      - name: user_id\n      - name: anonymous_id\n```\n\nHere, the contract specifies that:\n\n*   The input table (`rsIdentifies`) must exist in the warehouse.\n*   The model is an event stream model where every row in the table must be an event.\n*   There must be a column in the inputs table (`rsIdentifies`) which represents the user identifier for `user` entity.\n*   The input table (`rsIdentifies`) must have the `timestamp`, `user_id`, and `anonymous_id` columns.\n\n**Example 2**\n\nLet’s consider a SQL model, `rsSessionTable` which takes `shopify_session_features` as an input:\n\n```\nmodels:\n- name: rsSessionTable\n  model_type: sql_template\n  model_spec:\n    ... # model specifications\n    single_sql: |\n      {% set contract = BuildContract('{\"with_columns\":[{\"name\":\"user_id\"}, {\"name\":\"anonymous_id\"}]}') %}\n      {% with SessionFeature = this.DeRef(\"models/shopify_session_features\",contract)%}\n          select user_id as id1, anonymous_id as id2 from {{SessionFeature}}      \n    contract:\n      with_entity_ids:\n        - user\n      with_columns:\n        - name: user_id\n          type: string\n        - name: anonymous_id\n          type: string\n```\n\nThere are two contracts defined in the above example:\n\n*   Contract for `shopify_session_features` model which dictates that the `user_id`, and `anonymous_id` columns must be present.\n*   Contract for `rsSessionTable` model which dictates that it must have a column representing the user identifier for `user` entity. Also, the `user_id`, and `anonymous_id` columns must be present.\n\nThis helps in improving the data quality, error handling, and enables static and dynamic validation of the project.\n\n## Partial feature tables\n\nPartial feature tables are created when only a few input sources are available.\n\nFor example, lets say that you import a library package and some of the input models assumed in the package are not present in your warehouse.\n\nWhen you remap some of these input models to nil, those inputs and the features directly or indirectly dependent upon those inputs are disabled. In such cases, a partial feature table is created from the rest of the available inputs. Similarly, ID stitcher also runs even if few of the edge sources are not present in the warehouse or remapped to nil.\n\n## Pre and post hooks\n\nA pre hook enables you to execute an SQL before running a model, for example, if you want to change DB access, create a DB object, etc. Likewise, a post hook enables you to execute an SQL after running a model. The SQL can also be templatized. Here’s an example code snippet:\n\n```\nmodels:\n  - name: test_id_stitcher\n    model_type: id_stitcher\n    hooks:\n      pre_run: \"CREATE OR REPLACE VIEW {{warehouse.ObjRef('V1')}} AS (SELECT * from {{warehouse.ObjRef('Temp_tbl_a')}});\"\n      post_run: 'CREATE OR REPLACE VIEW {{warehouse.ObjRef(\"V2\")}} AS (SELECT * from {{warehouse.ObjRef(\"Temp_tbl_a\")}});'\n    model_spec:\n      - # rest of model specs go here\n```\n\n## Use Amazon S3 bucket as input\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> This is an experimental feature.\n\nIf you store data in your Amazon S3 bucket in a CSV file format, you can use it as an input for the Profiles models. The S3 URI path must be specified in the `app_defaults.s3`:\n\n```\nname: s3_table\ncontract:\n  is_optional: false\n  is_event_stream: true\n  with_entity_ids:\n    - user\n  with_columns:\n    - name: insert_ts\n      datatype: timestamp\n    - name: num_a\n      datatype: integer\napp_defaults:\n  s3: \"s3://bucket-name/prefix/example.csv\"\n  occurred_at_col: insert_ts\n  ids:\n    - select: \"id1\"\n      type: test_id\n      entity: user\n    - select: \"id2\"\n      type: test_id\n      entity: user\n```\n\nEnsure that the CSV file follows the standard format with the first row as the header containing column names, for example:\n\n```\nID1,ID2,ID3,INSERT_TS,NUM_A\na,b,ex,2000-01-01T00:00:01Z,1\nD,e,ex,2000-01-01T00:00:01Z,3\nb,c,ex,2000-01-01T00:00:01Z,2\nNULL,d,ex,2000-01-01T00:00:01Z,4\n```\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> Note that:\n> \n> *   To escape comma (`,`) from any cell of the CSV file, enclose that cell with double quotes `\" \"` .\n> *   Double quotes (`\" \"`) enclosing a cell are ignored.\n\nFollow the below steps to grant PB the required permissions to access the file in S3 Bucket:\n\n### Private S3 bucket\n\nAdd `region`, [`access key id`](#generate-access-key-id-and-secret-access-key), [`secret access key`](#generate-access-key-id-and-secret-access-key), and [`session token`](#generate-session-token) in your `siteconfig` file so that PB can access the private bucket. By default, the region is set to `us-east-1` unless specified otherwise.\n\n```\naws_credential:\n    region: us-east-1\n    access_key: **********\n    secret_access_key: **********\n    session_token: **********\n```\n\n#### Generate `access key id` and `secret access key`\n\n1.  Open the AWS IAM console in your AWS account.\n2.  Click **Policies**.\n3.  Click **Create policy**.\n4.  In the Policy editor section, click the JSON option.\n5.  Replace the existing JSON policy with the following policy and replace the <bucket\\_name> with your actual bucket name:\n\n```\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:GetObjectVersion\"\n            ],\n            \"Resource\": \"arn:aws:s3:::<bucket_name>/*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:ListBucket\",\n                \"s3:GetBucketLocation\"\n            ],\n            \"Resource\": \"arn:aws:s3:::<bucket_name>\",\n            \"Condition\": {\n                \"StringLike\": {\n                    \"s3:prefix\": [\n                        \"*\"\n                    ]\n                }\n            }\n        }\n    ]\n}\n```\n\n6.  Click **Review policy**.\n7.  Enter the policy name. Then, click **Create policy** to create the policy.\n\nFurther, create an IAM user by following the below steps:\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> An IAM user requires the following permissions on an S3 bucket and folder to access files in the folder (and sub-folders):\n> \n> *   s3:GetBucketLocation\n> *   s3:GetObject\n> *   s3:GetObjectVersion\n> *   s3:ListBucket\n\n1.  In AWS IAM console, Click **Users**.\n2.  Click **Create user**.\n3.  Enter a name for the user.\n4.  Select Programmatic access as the access type, then click **Next: Permissions**.\n5.  Click **Attach existing policies directly**, and select the policy you created earlier. Then click **Next**.\n6.  Review the user details, then click **Create user**.\n7.  Copy the access key ID and secret access key values.\n\n#### Generate `session token`\n\n1.  Use the AWS CLI to create a named profile with the AWS credentials that you copied in the previous step.\n2.  To get the session token, run the following command:\n\n```\n $ aws sts get-session-token --profile <named-profile>\n```\n\nSee [Snowflake](https://docs.snowflake.com/en/user-guide/data-load-s3-config-aws-iam-user), [Redshift](https://docs.aws.amazon.com/redshift/latest/dg/copy-usage_notes-access-permissions.html), and [Databricks](https://docs.databricks.com/en/ingestion/copy-into/generate-temporary-credentials.html) for more information.\n\n### Public S3 Bucket\n\nYou **must** have the following permissions on the S3 bucket and folder to access files in the folder (and sub-folders):\n\n*   s3:GetBucketLocation\n*   s3:GetObject\n*   s3:GetObjectVersion\n*   s3:ListBucket\n\nYou can use the following policy in your bucket to grant the above permissions:\n\n1.  Go to the **Permissions** tab of your S3 bucket.\n2.  Edit bucket policy in **Permissions** tab and add the following policy. Replace the <bucket\\_name> with your actual bucket name:\n\n```\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": \"*\",\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:GetObjectVersion\"\n            ],\n            \"Resource\": \"arn:aws:s3:::<bucket_name>/*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": \"*\",\n            \"Action\": [\n                \"s3:ListBucket\",\n                \"s3:GetBucketLocation\"\n            ],\n            \"Resource\": \"arn:aws:s3:::<bucket_name>\"\n        }\n    ]\n}\n```\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> In Redshift, you additionally need to set an IAM role as **default** for your cluster, unless access keys are provided. It is necessary because more than one IAM role can be associated with the cluster, and Redshift needs explicit permission granted through an IAM role to access the S3 bucket (Public or Private).\n> \n> Follow [Redshift Documentation](https://docs.aws.amazon.com/redshift/latest/mgmt/default-iam-role.html#set-default-iam) for setting an IAM role as default.\n\n## Use CSV file as input\n\nAn input file (`models/inputs.yaml`) contains details of input sources such as tables, views, or CSV files along with column name and SQL expression for retrieving values.\n\nYou can read data from a CSV file by using `csv: <path_to_filename>` under `app_defaults` in the input specs. CSV data is loaded internally as a single SQL select query, making it useful for seeding tests.\n\nA sample code is as shown:\n\n```\n    app_defaults:\n      csv: \"../common.xtra/Temp_tbl_a.csv\"\n      # remaining syntax is same for all input sources\n```\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> RudderStack does not support CSV files with more than a few hundred rows.\n\n## Filter data\n\nYou can filter out any data by using the `filters` field in your projects file:\n\nFor example, if you want to exclude all the blacklisted email addresses, you can create an input model (for example, `csv_email_blacklist`) with CSV file as a source, that contains all such email addresses:\n\n```\nid_types:\n  - name: email\n    filters:\n      - type: exclude\n        sql:\n          select: email\n          from: inputs/csv_email_blacklist\n```\n\nAnother example, if you want to exclude all the user\\_ids, you can create an SQL model (for example, `sql_exclusion_model`) that contains a specific logic to enlist all such IDs:\n\n```\nid_types:\n  - name: user_id\n    filters:\n      - type: exclude\n        sql:\n          select: user_id\n          from: inputs/models/sql_exclusion_model\n```\n\n## Associate SSH key to Git project\n\nTo add public SSH key to your Git project:\n\n1.  Open your Profile project’s Git repository in the browser and click **Settings**.\n2.  Click **SSH Keys**.\n3.  Assign a name (say `Sample Profiles Key`) and paste the key generated from the RudderStack dashboard or a public-key generated using CLI.\n4.  Click **Add Key**.\n\nFor more information, see:\n\n*   [GitHub documentation](https://docs.github.com/en/authentication/connecting-to-github-with-ssh/managing-deploy-keys#deploy-keys)\n*   [GitLab documentation](https://docs.gitlab.com/ee/user/project/deploy_keys/#create-a-project-deploy-key)\n*   [BitBucket documentation](https://confluence.atlassian.com/bitbucketserver/ssh-access-keys-for-system-use-776639781.html)\n\n## Use private Git repos via CLI\n\nFollow these steps:\n\n1.  [Generate the SSH Key](https://git-scm.com/book/en/v2/Git-on-the-Server-Generating-Your-SSH-Public-Key).\n2.  [Associate the SSH Key to your Git Project](#associate-ssh-key-to-git-project).\n3.  Add private keys as credentials in the `siteconfig.yaml` file:\n\n```\ngitcreds:\n  - reporegex: git@<provider-host>:<org-name>/*\n    key: |\n    -----BEGIN OPENSSH PRIVATE KEY-----\n    b3BlbnNzaC1rZXktdjEAAAAABG5vbmUAAAAEb..........\n    -----END OPENSSH PRIVATE KEY-----    \n```\n\n## Supported Git URLs\n\nProfiles supports Git URLs for packages and scheduling via UI. You can host the repos at:\n\n*   GitHub\n*   GitLab\n*   BitBucket\n\n[Contact the RudderStack team](mailto:support@rudderstack.com) if your preferred host isn’t included.\n\nFor private repos, RudderStack only supports SSH Git URLs. You need to add credentials to the `siteconfig.yaml` and public ssh key manually to the platforms. See [Use private Git repos via CLI](#use-private-git-repos-via-cli).\n\nThe URL scheme doesn’t depend on individual Git provider host. You can use the below-mentioned Git URLs:\n\n**1\\. URL for the default branch of a repository**\n\n*   **Syntax:**\n    \n    `https://<provider-host>/<org-name>/<repo-name>/path/to/project`\n    \n*   **Example:**\n    \n    `https://github.com/rudderlabs/librs360-shopify-features/shopify-features` `https://gitlab.com/rudderlabs/librs360-shopify-features/shopify-features` `https://bitbucket.org/rudderlabs/librs360-shopify-features/shopify-features`\n    \n\n**For private repos, RudderStack support SSH URLs:**\n\n*   **Syntax:**\n    \n    `git@<provider-host>:<org-name>/<repo-name>/path/to/project`\n    \n*   **Example:**\n    \n    `git@github.com:rudderlabs/librs360-shopify-features/shopify-features` `git@gitlab.com:rudderlabs/librs360-shopify-features/shopify-features` `git@gbitbucket.org:rudderlabs/librs360-shopify-features/shopify-features`\n    \n\n**2\\. URL for a specific branch of a repository**\n\n*   **Syntax:**\n    \n    `https://<provider-host>/<org-name>/<repo-name>/tree/<branch-name>/path/to/project`\n    \n*   **Example:**\n    \n    `https://github.com/rudderlabs/librs360-shopify-features/tree/main/shopify-features` `https://gitlab.com/rudderlabs/librs360-shopify-features/tree/main/shopify-features` `https://bitbucket.org/rudderlabs/librs360-shopify-features/tree/main/shopify-features`\n    \n\n**3\\. URL for a specific tag within the repository**\n\n*   **Syntax:**\n    \n    `https://<provider-host>/<org-name>/<repo-name>/tag/<tag-name>/path/to/project`\n    \n*   **Example:**\n    \n    `https://github.com/rudderlabs/librs360-shopify-features/tag/wht_test/shopify-features` `https://gitlab.com/rudderlabs/librs360-shopify-features/tag/wht_test/shopify-features` `https://bitbucket.org/rudderlabs/librs360-shopify-features/tag/wht_test/shopify-features`\n    \n\n**4\\. URL for a specific commit within the repository**\n\n*   **Syntax:**\n    \n    `https://<provider-host>/<org-name>/<repo-name>/commit/<commit-hash>/path/to/project`\n    \n*   **Example:**\n    \n    `https://github.com/rudderlabs/librs360-shopify-features/commit/b8d49/shopify-features` `https://gitlab.com/rudderlabs/librs360-shopify-features/commit/b8d49/shopify-features` `https://bitbucket.org/rudderlabs/librs360-shopify-features/commit/b8d49/shopify-features`\n    \n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> RudderStack supports the git SSH URL with following pattern only in the dashboard:\n> \n> *   `git@<provider-host>:<org-name>/<repo-name>/tree/<branch-name>`\n> *   `git@<provider-host>:<org-name>/<repo-name>`\n> *   `git@<provider-host>:<org-name>/<repo-name>/tree/main/path/to/project`\n> \n> RudderStack supports any subfolder in git project without .git extension.\n\nIt is recommended to use git-tags instead of the latest commit on main branch of your library projects. Also, you can use a specific tag, for example: `https://github.com/org-name/lib-name/tag/schema_<n>`.\n\nIf you want Profile Builder to figure out the best schema version for every run, you can use the placeholder {{best\\_schema\\_version}}, for example, `https://github.com/org-name/lib-name/tag/schema_{{best_schema_version}}`. The selection of compatible git tags is done by PB, that is, it will figure out the best compatible version for the lib package.\n\nA sample project file:\n\n```\npackages:\n  - name: shopify_features\n    url: https://github.com/org-name/lib-names/tag/schema_{{best_schema_version}}\n    inputsMap:\n      rsCartUpdate: inputs/rsCartUpdate\n      rsIdentifies: inputs/rsIdentifies\n```\n\nUsing this will make Profiles use the best compatible version of the library project in case of any schema updates.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> You don’t have to replace the placeholder `{{best_schema_version}}`. For instance, if `https://github.com/org-name/lib-names/tags/` has a tag for schema\\_44, then `https://github.com/org-name/lib-names/tag/schema_44` will be automatically used. In any case, if you replace the placeholder with actual tag name, the project will work without any issues.\n\n## View model dependencies\n\nYou can create a DAG to see all the model dependencies, that is, how a model is dependent on other models by using any one of the following commands:\n\n`pb show dataflow`  \nOR  \n`pb show dependencies`\n\nFurther, you can use the `pb show models` command to view information about the models in your project. See [show](https://www.rudderstack.com/docs/archive/profiles/0.14/cli-user-guide/commands/#show) command for more information.\n\n## Multi-version support\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> This feature is supported for the Profiles versions 0.10.8, 0.11.5, and 0.12.0 onwards.\n\nYou can constraint your Profiles project to run only on specific version(s) by specifying it in the `pb_project.yaml` file, under `python_requirements` key. For example, use the below snippet to run your project on v0.10.8:\n\n```\npython_requirements:\n  - profiles-rudderstack==0.10.8\n```\n\nUse the below snippet to stay on any minor version between 0.12.0 and 0.13.0. If a new minor version is released, your project will be auto-migrated to that version:\n\n```\npython_requirements:\n  - profiles-rudderstack>=0.12.0,<0.13.0\n```\n\nIf you do not specify any version in `pb_project.yaml`, the latest Profiles version is used by default. The version constraints follow the same syntax as those of [Python dependency specifiers](https://packaging.python.org/en/latest/specifications/dependency-specifiers/).\n\nMake sure that the version of Profiles project is the same in your environment and the `pb_project.yaml` file. Otherwise, RudderStack will throw an error.\n\n## Reuse models output\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> This is an experimental feature.\n\nYou can define the `time_grain` parameter for a model to ensure the model runs only once in that time period. It lets you reuse the output material of that model within the time period, preventing unnecessary recalculations⁠.\n\nFor example, if you set the `time_grain` value for an ID stitcher model to a `day` and run the feature table model (based on the ID stitcher) multiple times during a day, the feature table model will reuse the ID stitcher’s output to compute the features. This will save a large amount of time and computations whenever you run the feature table model within that day.\n\nSimilarly, you can choose to run a feature such as `weekly_spends` only once a week, or `last_active_day` on a daily basis.\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> Setting `time_grain` parameter does not mean that the model will run automatically at the specified time period. It just ensures that the model runs only once during that time period and its outputs are reused within that duration. To schedule your project run, you must use the RudderStack dashboard.\n> \n> For example, if `time_grain` for a model is set to a `week`, its outputs will be reused throughout the week and running it twice within the week won’t change its results.\n\nYou can define the `time_grain` parameter in the `profiles.yaml` file of your project:\n\n```\nmodels:\n  - name: user_id_graph\n    model_type: id_stitcher\n    model_spec:\n      entity_key: user\n      time_grain: \"hour\"\n      materialization:\n        run_type: incremental\n      edge_sources:\n        - from: inputs/rsIdentifies\n        - from: inputs/rsTracks\nvar_groups:\n  - name: user_daily_vars\n    time_grain: \"day\"\n    entity_key: user\n    vars:\n      - entity_var:\n          name: days_since_last_seen\n          select: \"{{macro_datediff('{{user.Var(\\\"max_timestamp\\\")}}')}}\"\n  - name: user_weekly_vars\n    time_grain: \"week\"\n    entity_key: user\n    vars:\n      - entity_var:\n         name: weekly_amt_spent\n         select: sum(total_price_usd) - coalesce(sum(total_price_usd_order_cancelled), 0)\n         from: models/rsOrderCreatedOrderCancelled\n         where: \"{{macro_datediff_n('timestamp','7')}}\"\n```\n\nIn the above example, suppose you schedule your Profiles project to run every hour. The output for `user_id_graph` model will be computed every hour, the output for `user_daily_vars` will be computed once a day, and the output for `user_weekly_vars` will be computed once a week.\n\nFor a default ID stitcher model, you can define the `time_grain` value in the `entities` section as shown below:\n\n```\nentities:\n  - name: user\n    id_types:\n      - test_id\n      - exclude_id\n    default_id_stitcher:\n      time_grain: \"day\"\n```\n\n#### Supported values\n\nYou can set the following values for the `time_grain` field:\n\n*   `tick`: Considers all the data up to the current moment (default value).\n*   `10minutes`: Considers data up to the last 10-minute interval.\n*   `hour`: Considers data up to the end of the previous hour.\n*   `day`: Considers data up to the end of the previous day.\n*   `week`: Considers data up to the end of the previous week.\n*   `month`: Considers data up to the end of the previous month.\n*   `year`: Considers data up to the end of the previous year.\n\n## Enable/disable model run\n\nWhen you have various interdependent models, you might want to run only the required ones for a specific output.\n\nRudderStack provides the `enable_status` parameter which lets you specify whether to run a model or not. Using it, you can exclude the unnecessary models from the execution process. You can assign the following values to the `enable_status` field in your `pb_project.yaml` file:\n\n*   `good_to_have` (default): It will not execute or cause a failure when it is not possible to execute the model.\n*   `must_have`: It will cause a failure when is not possible to execute the model.\n*   `only_if_necessary`: The model gets disabled if it has no dependency on the final output. It is the default value for a default ID stitcher model and ensures that the model is not executed if it is not required.\n*   `disabled`: The model gets disabled and is not executed.\n\nA sample `pb_project.yaml` file with `enable_status` parameter:\n\n```\nname: app_project\nschema_version: 67\nconnection: test\nmodel_folders:\n  - models\nentities:\n  - name: user\n    id_types:\n      - user_id\n      - anonymous_id\n    default_id_stitcher:\n      validity_time: 24h # 1 day\n      materialization:\n        run_type: incremental\n        enable_status: good_to_have\n      incremental_timedelta: 12h # half a day\n\nid_types:\n  - name: user_id\n    filters:\n      - type: include\n        regex: \"([0-9a-z])*\"\n      - type: exclude\n        value: \"\"\n  - name: anonymous_id\n```\n\n**Use-case**\n\nConsider a scenario where an ID stitcher model (`ids`) is dependent on `tbl_a` and `tbl_b`, and a feature table model (`ft1`) depends on the ID Stitcher (`ids`) and an input model `tbl_a`.\n\n*   If the ID stitcher and feature table models are marked as `only_if_necessary`, there is no need to execute either of them. However, if the feature table is marked as `good_to_have`/`must_have`, then all the models must run to create final output.\n*   If `tbl_a` is set to `disabled`, the ID stitcher and feature table will not run. If either of them is marked as `must_have`, the project will run into an error.\n\n## Window functions\n\nA window function operates on a window (group) of related rows. It performs calculation on a subset of table rows that are connected to the current row in some way. The window function has the ability to access more than just the current row in the query result.\n\nThe window function returns one output row for each input row. The values returned are calculated by using values from the sets of rows in that window. A window is defined using a window specification, and is based on three main concepts:\n\n*   Window partitioning, which forms the groups of rows (`PARTITION BY` clause)\n*   Window ordering, which defines an order or sequence of rows within each partition (`ORDER BY` clause)\n*   Window frames, which are defined relative to each row to further restrict the set of rows (`ROWS` specification). It is also known as the frame clause.\n\n**Snowflake** does not enforces users to define the cumulative or sliding frames, and considers `ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING` as the default cumulative window frame. However, you can override this by defining the frame manually.\n\nOn the **Redshift** aggregate window function list given below, specify the `frame_clause` while using any function from the list:\n\n*   `AVG`\n*   `COUNT`\n*   `CUME_DIST`\n*   `DENSE_RANK`\n*   `FIRST_VALUE`\n*   `LAG`\n*   `LAST_VALUE`\n*   `LEAD`\n*   `LISTAGG`\n*   `MAX`\n*   `MEDIAN`\n*   `MIN`\n*   `NTH_VALUE`\n*   `PERCENTILE_CONT`\n*   `PERCENTILE_DISC`\n*   `RATIO_TO_REPORT`\n*   `STDDEV_POP`\n*   `STDDEV_SAMP` (synonym for `STDDEV`)\n*   `SUM`\n*   `VAR_POP`\n*   `VAR_SAMP` (synonym for `VARIANCE`)\n\nIn the Redshift ranking window functions given below, **do not** specify the `frame_clause` while using any function from the list:\n\n*   `DENSE_RANK`\n*   `NTILE`\n*   `PERCENT_RANK`\n*   `RANK`\n*   `ROW_NUMBER`\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> Use `frame_clause` carefully when using a window function. While It is not very critical for Snowflake, using it incorrectly in Redshift can lead to errors.\n\nExample of using `frame_clause`:\n\n```\n- entity_var:\n    name: first_num_b_order_num_b\n    select: first_value(tbl_c.num_b) # Specify frame clause as aggregate window function is used\n    from: inputs/tbl_c\n    default: -1\n    where: tbl_c.num_b >= 10\n    window:\n        order_by:\n        - tbl_c.num_b desc\n        frame_clause: ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING\n- entity_var:\n    name: first_num_b_order_num_b_rank\n    select: rank() # DO NOT specify frame clause as ranking window function is used\n    window:\n        order_by:\n        - first_num_b_order_num_b asc\n```\n\nNote how `frame_clause` is specified in first `entity_var` and not in the second one.\n\n## Macros\n\nMacros are the reusable functions that encapsulate complex processing logic directly within the SQL expression. You can create macros to perform computations and use them in multiple models. For example:\n\n```\nmacros:\n  - name: subtract_range\n    inputs:\n      - first_date\n      - second_date\n    value: \"{{first_date}} - {{second_date}}\"\n  - name: multiplyBy10_add\n    inputs:\n      - first_number\n      - second_number\n    value: \"{{first_number}} * 10 + {{second_number}}\"\n```\n\nYou can create a file (say `macros.yaml`) file under the `models` folder and refer them using the macro name in the `profiles.yaml` file:\n\n```\n- entity_var:\n    name: days_since_first_sale\n    select: \"{{ subtract_range('{{user.Var(\\\"first_sale_time\\\")}}') }}\"\n```\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `name` | String | Name to uniquely identify the macro. |\n| `inputs` | List | Defines the input variables to be used by the macro function. |\n| `value` | String | Contains the reusable function in the form of a SQL expression. Any reference to the inputs must be encapsulated within double curly brackets. |\n\nSee a sample [Profiles library project](https://github.com/rudderlabs/profiles-stripe-features) that creates user features from Stripe tables using Macros.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Additional Concepts | RudderStack Docs",
    "description": "Additional concepts related to Profiles like packages, best practices, partial feature tables, etc.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.14/changelog/",
    "markdown": "# Changelog | RudderStack Docs\n\nChangelog for all the Profiles versions.\n\n* * *\n\n*     47 minute read  \n    \n\n## Version 0.14.2\n\n_28 June 2024_\n\n**Bug Fixes**\n\n*   Resolved “authentication token has expired” issue on Snowflake.\n\n## Version 0.14.1\n\n_26 June 2024_\n\n**Bug Fixes**\n\n*   Resolved few migration issues.\n*   Fixed BigQuery issues where the project name had special characters.\n\n## Version 0.14.0\n\n_19 June 2024_\n\n**What’s New**\n\n*   You can now refer to `entity_vars` defined on cohorts using the `<cohort_name>.Var(\"<var_name>\")` syntax along with the original `entity.Var(\"v1\")` syntax.\n*   Vars from ancestor vars are now inherited in derived cohorts. If a derived cohort has vars with the same name as ancestors, they are overridden.\n*   You can specify `timegrains` on a specific `entity_var` and not the entire `var_group`.\n*   If a column is specified as mandatory in a model contract, it must be present in the warehouse. Otherwise, the project will fail. Also, contract validation is now enabled for SQL models and Python models.\n*   `pb show models` command now displays additional information such as the `enable_status` of models, warehouse name, namespace, and timegrains.\n*   In materialization status, the flag `not_needed` has been renamed to `only_if_necessary`.\n*   Some experimental features are added to optimize performance by partitioning var tables.\n*   A new key `row_identifier` _(experimental)_ has been added under `app_defaults` in the `inputs.yaml` file.\n*   Schema has been updated from 63 -> 67 in the `pb_project.yaml` file.\n\n**Improvements**\n\n*   Few optimizations in the RudderStack dashboard to make it leaner and faster.\n\n**Bug Fixes**\n\n*   Fixed the issue where features list wasn’t getting added to input models build specification.\n*   Resolved a bug where <`entity`\\>`/feature_views` model wasn’t getting created, in case the `feature_views` key didn’t have any value.\n*   Resolved the error where some BigQuery projects were failing with the message `Dataset <> not found in location`.\n*   Fixed the symlink issue where the `pb run`/`pb compile` commands were failing when the relative path of the project is passed.\n*   Resolved the bug where input models were getting enabled if they did not exist earlier, even if their status wasn’t set.\n*   Error is now being thrown in case all the inputs of a model are disabled but they are mandatory.\n*   Fixed the error `nil pointer dereference` when `other_id` is not defined in the feature views.\n*   Resolved the issue where different cohorts with same feature names were throwing an error.\n*   Fixed the bug where a derived cohort was not able to refer other cohort’s vars.\n*   Fixed `column name is ambiguous` error in feature views when there are vars with the same name in different timegrains.\n*   Resolved the issue where some projects with concurrency were failing with error `concurrent map read and map write`.\n*   Fixed the issue with force runs on models with begin time.\n\n**Known Issues**\n\n*   RudderStack does not support accessing input sources in a different project for the BigQuery warehouse.\n*   Linux users might see this warning for all command runs - you can ignore it: `WARN[0000]log.go:228 gosnowflake.(*defaultLogger).Warn DBUS_SESSION_BUS_ADDRESS envvar looks to be not set, this can lead to runaway dbus-daemon processes. To avoid this, set envvar DBUS_SESSION_BUS_ADDRESS=$XDG_RUNTIME_DIR/bus (if it exists) or DBUS_SESSION_BUS_ADDRESS=/dev/null`.\n*   Redshift: If two different users create material objects on the same schema, RudderStack gives an error during cleanup when trying to drop views created by the other user, like `user_var_table`.\n*   `pb validate access` command does not work for BigQuery.\n*   `pb insert` does not work for Redshift, Databricks, and BigQuery.\n*   Cross database references can fail on Redshift for a few clusters.\n*   If you are referring a public package in the project and get `ssh: handshake failed error`, then you’ll have to manually remove the entire folder from _WhtGitCache_ to make it work.\n*   The code for `validity_time` is redundant and should be removed.\n*   In some cases, you may need to install the `profiles-rudderstack` and `profiles-rudderstack-bin` pip packages separately.\n*   You may have to execute the compile command once before executing validate access. Otherwise, you will get a `seq_no` error.\n*   Timegrains is an experimental feature. There might be some undiscovered issues.\n*   Activations does not work with Redshift warehouse.\n\n## Version 0.13.2\n\n_11 June 2024_\n\n**Bug Fixes**\n\n*   Fixed issues with auto-migrate on Windows.\n*   Resolved insufficient memory error encountered while adding `main_id` to `input_var` tables in Redshift.\n\n## Version 0.13.1\n\n_23 May 2024_\n\n**Bug Fixes**\n\n*   Resolved the issue with remapped models in entity projects in the RudderStack dashboard.\n*   Fixed timestamp casting issues on BigQuery.\n\n**Known Issues**\n\n*   RudderStack does not support accessing input sources in a different project for the BigQuery warehouse.\n*   Linux users might see this warning for all command runs - you can ignore it: `WARN[0000]log.go:228 gosnowflake.(*defaultLogger).Warn DBUS_SESSION_BUS_ADDRESS envvar looks to be not set, this can lead to runaway dbus-daemon processes. To avoid this, set envvar DBUS_SESSION_BUS_ADDRESS=$XDG_RUNTIME_DIR/bus (if it exists) or DBUS_SESSION_BUS_ADDRESS=/dev/null`.\n*   Redshift: If two different users create material objects on the same schema, RudderStack gives an error during cleanup when trying to drop views created by the other user, like `user_var_table`.\n*   `pb validate access` command does not work for BigQuery.\n*   `pb insert` does not work for Redshift, Databricks, and BigQuery.\n*   Cross database references can fail on Redshift for a few clusters.\n*   If you are referring a public package in the project and get `ssh: handshake failed error`, then you’ll have to manually remove the entire folder from _WhtGitCache_ to make it work.\n*   The code for `validity_time` is redundant and should be removed.\n*   In some cases, you may need to install the `profiles-rudderstack` and `profiles-rudderstack-bin` pip packages separately.\n*   You may have to execute the compile command once before executing validate access. Otherwise, you will get a `seq_no` error.\n*   Cohort features do not inherit the parent’s features.\n*   RudderStack dashboard lists a cohort only if it has features.\n*   Timegrains is an experimental feature. There might be some undiscovered issues.\n*   Activations does not work with Redshift warehouse.\n\n## Version 0.13\n\n_16 May 2024_\n\n**What’s New**\n\n*   Timegrains feature _(experimental)_ lets you reuse the output material of a model within the specified time period, preventing unnecessary recalculations⁠. You can define its value as a day, week, month etc. to compute features at the end of that particular period.\n*   Cohorts feature _(experimental)_ lets you define core customer segments within an entity based on some characteristics.\n*   Removed support for `rebase_incremental` in project\\_spec. However, you can specify it using the command-line tool.\n*   The `output` folder now generates artifacts for `run` and `compile` commands in separate subfolders with the same name.\n*   `pb init pb-project` now comes with a sample SQL model.\n*   `ReadFile` support introduced in YAML which lets you highlight syntax. In a SQL model, you can use `{{this.ReadFile(\"models/sql_file.sql\")}}` to refer SQL content from a file.\n*   Material registry is now at v5. It includes two new columns - `model_ref` and `registry_version`. The registry will be automatically migrated once you execute `pb run` command.\n\n**Improvements**\n\n*   Improved performance in compilation and ID stitching processes, resulting in faster operations.\n*   Improved error handling at some places.\n*   Few internal refactorings for enhanced working.\n\n**Bug Fixes**\n\n*   Removed `include_untimed` key from the `pb_project.yaml` file as it was redundant.\n*   Fixed sporadic lengthening of project runs in RudderStack dashboard.\n\n**Known Issues**\n\n*   Linux users might see this warning for all command runs - you can ignore it: `WARN[0000]log.go:228 gosnowflake.(*defaultLogger).Warn DBUS_SESSION_BUS_ADDRESS envvar looks to be not set, this can lead to runaway dbus-daemon processes. To avoid this, set envvar DBUS_SESSION_BUS_ADDRESS=$XDG_RUNTIME_DIR/bus (if it exists) or DBUS_SESSION_BUS_ADDRESS=/dev/null`.\n*   Redshift: If two different users create material objects on the same schema, RudderStack will throw error during cleanup when trying to drop views created by the other user, such as `user_var_table`.\n*   `pb validate access` command does not work for BigQuery.\n*   Some commands such as `pb insert` does not work for Redshift, Databricks, and BigQuery.\n*   For a few clusters, cross database references can fail on Redshift.\n*   If you are referring a public package in the project and get `ssh: handshake failed error`, then you’ll have to manually clear _WhtGitCache_ folder to make it work.\n*   The code for `validity_time` is redundant and should be removed.\n*   In some cases, you may need to install the `profiles-rudderstack` and `profiles-rudderstack-bin` pip packages separately.\n*   You may have to execute the compile command once before executing validate access. Otherwise, you will get a `seq_no` error.\n*   Cohort features do not inherit parent’s features.\n*   RudderStack dashboard lists a cohort only if it has features.\n*   Timegrains is an experimental feature. There might be some undiscovered issues.\n*   Activations does not work with Redshift warehouse.\n\n## Version 0.12.1\n\n_2 May 2024_\n\n**Improvements**\n\n*   If all the inputs of a model are disabled, the model is disabled by default.\n\n**Bug Fixes**\n\n*   Resolved bug where some projects were failing in case nested columns were missing.\n*   Updated the migration logic to preserve old view names. The reason being that some existing Activation API projects were failing due to the renaming of `serve_traits` to `feature_views`.\n\n**Known Issues**\n\n*   Linux users might see this warning for all command runs - you can ignore it: `WARN[0000]log.go:228 gosnowflake.(*defaultLogger).Warn DBUS_SESSION_BUS_ADDRESS envvar looks to be not set, this can lead to runaway dbus-daemon processes. To avoid this, set envvar DBUS_SESSION_BUS_ADDRESS=$XDG_RUNTIME_DIR/bus (if it exists) or DBUS_SESSION_BUS_ADDRESS=/dev/null`.\n*   Redshift: If two different users create material objects on the same schema, RudderStack will throw error during cleanup when trying to drop views created by the other user, such as `user_var_table`.\n*   `pb validate access` command does not work for BigQuery.\n*   Some commands such as `pb insert` does not work for Redshift, Databricks, and BigQuery.\n*   For a few clusters, cross database references can fail on Redshift.\n*   If you are referring a public package in the project and get `ssh: handshake failed error`, then you’ll have to manually clear _WhtGitCache_ folder to make it work.\n*   The code for `validity_time` is redundant and should be removed.\n*   In some cases, you may need to install the `profiles-rudderstack` and `profiles-rudderstack-bin` pip packages separately.\n*   You may have to execute the compile command once before executing validate access. Otherwise, you will get a `seq_no` error.\n\n## Version 0.12.0\n\n_25 April 2024_\n\n**What’s New**\n\n*   Support for external tables on Redshift Serverless.\n    \n*   Redshift users can now also connect via SSH Tunnel.\n    \n*   Support to specify the version on which you want to run your Profiles project. It is also backported to v0.10.8 and v0.11.5 onwards.\n    \n*   Feature views path refs look like below:\n    \n    *   user/all/feature\\_view _\\# By default, the id served is user `main_id`._\n    *   user/all/feature\\_view/using\\_email _\\# For non default IDs, the path ref has using\\_<idname>_\n    *   user/all/feature\\_view/salesforce\\_id\\_stitched\\_features _\\# The names can still be overridden._\n*   Ability to add description (optional) to feature tags in the project file.\n    \n    ```\n    available_tags:\n        - name: demographics\n          description: all tags related to user demographics\n        - name: billing\n    ```\n    \n*   Reorganized the overall flow when defining a Redshift connection. You can specify the region while using Redshift.\n    \n*   Creating a new project using `pb init pb-project` now comes with sample data in CSV format, in a folder named `csvs`. These are referenced in the input file.\n    \n*   Renamed `Entity Traits 360` model to `Feature Views`. Also, in the `pb_project.yaml` file, a Feature Views model is included by default under the entities definition.\n    \n*   Schema has been migrated from 54 -> 61 in the project file.\n    \n\n**Improvements**\n\n*   `pb init connection` now fetches connection name from the site configuration file, and asks you to specify one as well. If you don’t enter any value and there’s only one entry in the file, then it picks that value.\n*   Default ID stitcher now uses all package models for ID sources as well, in addition to local ones.\n*   Relevant error is now thrown, if ID types for an entity has not been specified in the `pb_project.yaml` file.\n*   An entity project in the RudderStack dashboard no longer fails in case a few columns are missing in the source tables.\n*   Debugging is improved and the correct line number is displayed.\n*   `pb show models` command now includes model dependencies along with some more statistics.\n*   Few retoolings from the ground up.\n\n**Bug Fixes**\n\n*   Resolved bug related to failure when a git repo couldn’t be found in the RudderStack dashboard.\n*   Resolved bug where default ID stitcher was getting created even if there were no ID edges.\n*   Resolved bug where Redshift Profiles projects were failing if names of `entity_var`/`input_var` were longer than 47 characters.\n*   Resolved bug where `pb run --rebase_incremental` command was taking edges from previous runs.\n*   Resolved the issue where a project wasn’t compiling in case it referenced a package having inputs that weren’t defined in the application with the same name.\n\n## Version 0.11.5\n\n_16 April 2024_\n\n**Bug Fixes**\n\n*   Fixed issue for Redshift where the driver version wasn’t getting populated correctly.\n*   Improved cleanup functionality for Redshift by dropping procedures that were used for creating `entity_vars`.\n*   Resolved the issue where migrated project folder’s files weren’t getting deleted.\n*   Fixed the bug where `pb run --rebase_incremental` command was taking edges from previous runs.\n*   Few internal refactorings while returning data types of columns.\n\n## Version 0.11.3\n\n_1 April 2024_\n\n**What’s New**\n\n*   An optional parameter `column_data_type` to specify the data type for an `entity_var`/`input_var`.\n*   Support for programmatic credentials for Redshift.\n*   Schema update in the project yaml file from 53 to 54.\n\n**Improvements**\n\n*   Better error propagation in case of concurrency.\n*   Few internal refactorings for improved overall working.\n\n**Bug Fixes**\n\n*   Resolved `relation still open` error when accessing external tables in Redshift.\n*   Fixed some bugs when getting the `latest seq_no` for a material in BigQuery.\n*   Resolved the issue of conflict in row-ID in case of very large datasets in BigQuery.\n*   Begin and end time of all models are now in UTC timezone. This fixes a few inconsistency issues in models.\n*   Resolved a concurrency issue which occurred on two different root models with the same name.\n\n## Version 0.11.2\n\n_15 March 2024_\n\n**What’s New**\n\n*   You can now do parallel processing while running a project using the `--concurrency` flag. Currently, this is supported only for Snowflake warehouse. It is recommended to use this option judiciously as applying a very large value can impact your system resources.\n*   RedShift users can now access external tables in their data catalog(s).\n\n**Improvements**\n\n*   Project created using `pb init pb-project` now works for all warehouses.\n\n**Bug Fixes**\n\n*   Fixed issues encountered while running BigQuery projects on Windows.\n*   Resolved errors for entity var names in case they match with input column name.\n*   Resolved bugs related to inserting `seq_no`.\n\n## Version 0.11.1\n\n_7 March 2024_\n\n*   Includes bug fixes related to creating vars on ID models and nil model remapping.\n\n## Version 0.11.0\n\n_1 March 2024_\n\n**What’s New**\n\n*   RudderStack now supports BigQuery (beta), offering the same seamless experience as on other data warehouses.\n*   CSV models _(Experimental)_: In the inputs specs, RudderStack has added the ability to read data from a CSV file, instead of a Database table/view. You can use files from local storage, or kept on S3. Under `app_defaults`, instead of `table`/`view`, use `csv` (local storage) or `s3` (kept on S3) followed by the path where the CSV file is kept. Note that this feature is experimental, and RudderStack currently supports S3 on Snowflake and Redshift. A sample code is as follows:\n\n```\n    app_defaults:\n      csv: \"../common.xtra/Temp_tbl_a.csv\"\n…\n    app_defaults:\n      s3: \"s3://s3-wht-input-test-bucket/test/Temp_tbl_d.csv\"\n```\n\n*   Filter IDs: You can now filter out a vast number of ID’s using SQL. For example, if you wish to exclude all blacklisted ID’s that are listed in an input model named `csv_email_blacklist` and user ID’s from an SQL model named `sql_exclusion_model`, then, you may edit your project file as:\n\n```\nid_types:\n  - name: email\n    filters:\n      - type: exclude\n        sql:\n          select: email\n          from: inputs/csv_email_blacklist\n  - name: user_id\n    filters:\n      - type: exclude\n        sql:\n          select: user_id\n          from: models/sql_exclusion_model\n```\n\n*   Pre and Post Hooks: A pre hook enables you to execute an SQL, before running a model, for example, if you want to change DB access, create a DB object, etc. Likewise, a post hook enables you to execute an SQL after running a model. The SQL can also be templatized. Here’s an example code snippet:\n\n```\nmodels:\n  - name: test_id_stitcher\n    model_type: id_stitcher\n    hooks:\n      pre_run: \"CREATE OR REPLACE VIEW {{warehouse.ObjRef('V1')}} AS (SELECT * from {{warehouse.ObjRef('Temp_tbl_a')}});\"\n      post_run: 'CREATE OR REPLACE VIEW {{warehouse.ObjRef(\"V2\")}} AS (SELECT * from {{warehouse.ObjRef(\"Temp_tbl_a\")}});'\n    model_spec:\n```\n\n*   `pb show models` - You can now view in JSON format by passing the flag –json.\n*   For Databricks, RudderStack now supports the `pb validate access` command.\n*   RudderStack has reverted to having a custom ID stitcher in a new project created using `pb init pb-project`.\n*   When creating a new connection in Redshift, you’ll now be asked to input sslmode. You can enter either `disable` (default) or `require`. This will help RudderStack’s tool to work with Redshift DB’s that require SSL mode to be enabled.\n*   RudderStack supports triggering tasks by using URL and also read the status back.\n*   RudderStack dashboard now supports Git Projects hosted on BitBucket and GitLab.\n*   In model specs, a materialization’s `enable_status` is changed to `snake_case`. That is, `enable_status`: `mustHave` -> `enable_status`: `must_have`.\n*   Schema version in the project file has been updated from 49 to 53.\n\n**Improvements**\n\n*   Better error messages are shown in case of incorrect/missing/duplicate entity-vars.\n*   Error handling has been improved at a few places in Python models.\n*   Model path refs are now case insensitive.\n*   The command `pb migrate auto` can now handle the case where model folders aren’t present.\n*   Specific messages are now shown, in case of errors in the material registry.\n*   Due to limitations of Databricks SQL, RudderStack has added restrictions on using catalog `hive_metastore`. So, in case a user on that catalog tries to use RudderStack’s tool, an error is thrown.\n\n**Bug Fixes**\n\n*   Resolved the intermittent issue in Redshift where it throws an error `ptr_to_latest_seqno_cache does not exist`.\n*   Bugs in `pb show idstitcher-report`, `pb show user-lookup`, and `pb cleanup materials` commands have been rectified. `pb show idstitcher-report` is still flaky, however, RudderStack team is working on improving it.\n*   Fixed bug in packages wherein `entityvars`/`inputvars` weren’t able to refer SQL models.\n*   Resolved erroneous queries for validate access command in case of missing privileges.\n*   Recsolved the issue where git repo wasn’t getting cloned in case `cache_dir` in siteconfig was written using tilde notation.\n*   Fixed some bugs related to `begin_time` of models.\n*   Resolved a few issues when cloning Git projects in the web app.\n*   Several fixes in gRPC, making it more stable.\n*   The remapping: key is removed (if exists) in `models/inputs.yaml` as it was redundant.\n*   Resolved some bugs in incremental ID stitcher.\n\n**Known Issues**\n\n*   `pb validate access` command does not work for BigQuery.\n\n## Version 0.10.6\n\n_19 January 2024_\n\nAn internal fix to address issues that arose from a recent update by Snowflake.\n\n## Version 0.10.5\n\n_9 January 2024_\n\nSome internal fixes to make py-native models more robust.\n\n## Version 0.10.4\n\n_15 December 2023_\n\nOur latest version has a plethora of features that makes our product more feature-rich and impactful.\n\n**What’s New**\n\n*   **Vars as models**: Earlier, Vars could only be defined inside the feature table under `vars:` section. Now, Vars are defined independent of feature tables. In the model specs file, we have created a new top level key called `var_groups`. We can create multiple groups of vars that can then be used in various models (eg. in feature table). All vars in a var-group need to have the same entity. So if you have 2 entities, you need at least 2 var groups. However, you can create multiple var\\_groups for every entity. For example, you can create churn\\_vars, revenue\\_vars, engagement\\_vars etc. So that it is easier to navigate and maintain the vars that you need. Each such model shall have name, entity\\_key and vars (list of objects). This is in line with Profiles design philosophy to see everything as a model.\n*   **User defined model types via Python \\[Experimental feature\\]**: Ever wondered what it would take to implement a new model type yourself? Custom model types can now be implemented in Python. Check out [this library](https://github.com/rudderlabs/profiles-pycorelib) for some officially supported model types with their Python source. Note that this is an experimental feature, so the implementation and interfaces can change significantly in the upcoming versions. To use a python package model in your project, simply specify it as a `python_requirement` in `pb_project.yaml`, similar to requirements.txt. The BuildSpec structure is defined using JSON schema within the Python package. Below code snippet shows how the requirements such as for training and config can be specified in the project:\n\n```\n    entities:\n      - name: user\n      python_requirements:\n      - profiles_rudderstack_pysql==0.2.0 #registers py_sql_model model type\n```\n\n```\n    models:\n      - name: test_py_native_model\n        model_type: py_sql_model\n        model_spec:\n          occurred_at_col: insert_ts\n          validity_time: 24h\n          train_config:\n            prop1: \"prop1\"\n            prop2: \"prop2\"\n```\n\n*   **Default ID stitcher**: Until now, when a new project was created using `pb init pb-project`, the file `profiles.yaml` had specifications for creating a custom ID stitcher. That has a few limitations, when edge sources are spanning across packages. Also, we observed that several of our users weren’t doing much changes to the ID stitcher, except for making it `incremental`. As a solution, we have a “default ID stitcher”, that is created by default for all projects. It runs on all the input sources and ID types defined. For quickstart purposes, users needn’t make any changes to the project, to get the ID stitcher working. In case any changes are to be made, then a user can create a custom ID stitcher, as was done in earlier versions.\n    \n*   **Default ID types**: Now, common concepts like ID types can be loaded from packages. So we needn’t define them in all new projects. Hence, we have moved the common ID type definitions into a library project called [`profiles-corelib`](https://github.com/rudderlabs/rudderstack-profiles-corelib). So when you create a new project, the key `id_types` is not created by default. In case you wish to create a custom list of ID types that is different from the default one, then you may do it as was the case in earlier versions.\n    \n*   **Override packages**: Continuing from previous point: packages now have `overrides` materialization spec. In case you wish to add custom ID types to the default list or modify an existing one, then you may extend the package to include your specifications. For the corresponding id\\_type, add the key `extends:` followed by name of the same/different id\\_type that you wish to extend, and corresponding `filters` with include/exclude values. Below is an example of the same:\n    \n\n```\n    packages:\n        - name: foo-bar\n        url: \"https://github.com/rudderlabs/package-555\"\n    id_types:\n        - name: user_id\n        extends: user_id\n        filters:\n            - type: exclude\n              value: 123456\n    id_types:\n        - name: customer_id\n        extends: user_id\n        filters:\n            - type: include\n              regex: sample\n```\n\n*   **entity\\_var tags**: You can now define a list of tags in the project file under `tags:` key. Then, you can add a tag to each entity\\_var.\n*   **Redshift**: We have added support for the RA3 node type. So now our users on that cluster can cross-reference objects in another database/schema.\n*   Schema version in the project file has been updated from 44 -> 49.\n\n**Improvements**\n\n*   Generated ID’s are now more stable. This means that they are unlikely to adapt to merging of ID Clusters, thereby creating a more accurate profile of your users.\n*   By default, every entity\\_var is a feature, unless specified otherwise using `is_feature: false`. So now, you need not explicitly add them to the `features:` list.\n*   You can now add escape characters to an entity\\_var’s description.\n*   Several internal refactorings to improve overall working of the application.\n\n**Bug Fixes**\n\n*   An entity\\_var having a description with special characters was failing during project re-runs. This has now been resolved.\n*   We have fixed the bug where two entity\\_vars across different entities in the same project couldn’t have the same name.\n*   Fixed some bugs related to vars as models, auto migration of projects, and ID lookup.\n\n**Known Issues**\n\n*   Redshift: If two different users create material objects on the same schema, then our tool will throw error when trying to drop views created by the other user, such as `user_var_table`.\n*   Some commands such as `insert` do not work on Redshift and Databricks.\n*   For a few clusters, cross DB references can fail on Redshift.\n*   If you are referring a public package in the project and get `ssh: handshake failed` error, then you’ll have to manually clear `WhtGitCache` folder to make it work.\n*   The code for `validity_time` is redundant and should be removed.\n*   Sometimes you may have sometimes install both the pip packages separately (`profiles-rudderstack` and `profiles-rudderstack-bin`).\n*   You may have to execute the `compile` command once, before executing `validate access`. Otherwise, you can get a `seq_no` error.\n\n## Version 0.9.4\n\n_8 November 2023_\n\nThis release includes the following bug fixes and improvements:\n\n*   `pb run --grep_var_dependencies` - we are now setting default values using the rule “if a project is migrated on load from a version older than 43, then grep\\_var\\_dependencies will default to true otherwise false”. Also, handled a null pointer case for non existent vars listed in dependencies.\n*   `pb migrate_on_load` / `migrate auto` - we have made the message clearer on curly braches in dot syntax message.\n*   `pb migrate manual` - we have removed compatibility-mode as it was no longer required.\n*   A few internal refactorings.\n\n## Version 0.9.3\n\n_2 November 2023_\n\nThis release addresses a few vulnerability fixes.\n\n## Version 0.9.2\n\n_26 October 2023_\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> In case you are unable to install then we recommend having Python3 versions from 3.8 to 3.10.\n\nThis release includes a bug fix on self dependency of vars, in case column has same name as entity-var.\n\n## Version 0.9.1\n\n_19 October 2023_\n\nOur latest release contains some useful features and improvements, as asked by our users.\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> After the auto-migration to v44, you might be shown some warnings to do changes in the YAML. Please check the Tutorials section. Or, you may contact our team and we will assist you with the same.\n\n**What’s New**\n\n*   We have added support for Databricks (beta). Now Databricks users can seamlessly create ID stitcher and feature table models, without writing complex SQL queries! If you’re using Databricks and want to try out Profiles, kindly get in touch with our team.\n*   **Vars as models** : Now, entity\\_vars and input\\_vars can be treated as independent models. Presently, they are tied to a feature table model. In SQL template text, for example in SQL model templates, please use `{{entity-name.Var(var-name)}}` going forward to refer to an entity-var or an input-var. For example, for entity\\_var `user_lifespan` in HelloPbProject, change `select: last_seen - first_seen` to `select: '{{user.Var(\"last_seen\")}} - {{user.Var(\"first_seen\")}}'`.\n*   `pb show dataflow` and `pb show dependencies` commands - A new flag `--include_disabled` flag is added to let disabled models be part of the generated image. Also, we now show the relative path from local root, instead of the full path.\n*   `pb run` command - Added flag `--ignore-model-errors` to let the project continue running in case of an erroneous model. So, the execution wouldn’t stop due to 1 bad model.\n*   `pb run` - Added flag `--grep_var_dependencies` (default: true) which searches for vars dependencies by using `grep` over fields from vars definition.\n*   `pb show idstitcher-report` - Added flag `--seq_no`, using which a specific run for an ID stitcher model can be specified.\n*   **Best schema version** - For a library project, in the `url` key of `packages`, we have introduced the concept of “best version tag”. That is, instead of specifying the specific Git URL of the library project, we give a URL with GIT tag `url: https://github.com/rudderlabs/librs360-shopify-features/tag/schema_{{best_schema_version}}`. Using this will make our tool use the best compatible version of the library project, in case of any schema updates.\n*   Schema has been migrated from version 42 -> 44.\n\n**Improvements**\n\n*   The command `pb show user-lookup` now includes more details including the count of rows created and total number of features.\n*   Commenting out features will ensure that the corresponding entity-var and any related entity-var/input-var being used only for computation of this commented feature wont run\n*   Several improvements done beneath the surface.\n\n**Bug Fixes**\n\n*   The flag `-- force` was having issues in dropping priorly created materialization models. This has now been resolved.\n*   Fixed bug where project was unable to run due to giving a custom name to the ID stitcher.\n*   Resolved an issue in the command `pb show idstitcher-report`, in the case if the hash of the ID Stitcher model has changed from that of the last run, rerunning the ID Stitcher model.\n*   Removed flag `-l` from the command `pb show idstitcher-report` as it was redundant.\n\n**Known Issues**\n\n*   Redshift: If two different users create material objects on the same schema, then our tool will throw error when trying to drop views created by the other user, such as `user_var_table`.\n*   Some commands such as `insert` do not work on Redshift and Databricks.\n*   For a few clusters, cross DB references can fail on Redshift.\n\n## Version 0.8.0\n\n_25 August 2023_\n\n**What’s New**\n\n*   Model Contracts - We have added support for model contracts and their validation. For every input or SQL model, there’s a new key `contract:` which contains the following keys: `is_optional` (boolean, to indicate if the model is optional), `is_event_stream` (boolean, in case the data is event stream and has timestamp), `with_entity_ids` (list of all entities model contains), `with_columns` (list of all column names model have). A contract can be passed along with the model path in `this.DeRef`. For more information, check out [Model Contracts](https://www.rudderstack.com/docs/archive/profiles/0.14/example/packages/#model-contracts).\n*   Inputs model - The keys `occurred_at_col` and `ids` are now a part of `app_defaults`, to reinforce that they can also be overridden.\n*   Schema has been migrated from 40 -> 42 in the project file.\n\n**Improvements**\n\n*   The command `pb cleanup materials` now removes tables generated by Python models also.\n*   `pb show user-lookup` now includes user traits from Python models as well.\n*   A few changes under the hood, for more efficient processing and execution.\n\n**Bug Fixes**\n\n*   Fixed issue in Python models where validity of the train file wasn’t working and it so was retraining the model(s) on every run.\n*   Resolved the bug where wrong credentials in siteconfig file was not printing the exact error.\n*   Queries for checking warehouse access (grant) were duplicated and therefore recursively checking grants on the same models again and again. This resulted in taking more time than what was required. It has now been fixed.\n*   `pb migrate auto` - There was an issue in migration of multi-line strings of SQL models, that has now been resolved.\n\n## Version 0.7.3\n\n_14 August 2023_\n\n**What’s New**\n\n*   `pb show idstitcher-report`:`pb show idstitcher-report`: By passing flag `--id_stitcher_model`, you can now create an HTML report with relevant results and graphics including largest cluster, ID graph, etc.\n*   Material Registry has been updated to version 4, as additional information is now stored for target (as defined in siteconfig), system username, and invocation metadata (hostname and the project’s invocation folder). So now, if anyone logs into the system and creates material objects using PB, then these details will be stored. This is based on a feature request from one of our customers. Note: make sure to execute `pb validate access` for migrating the registry.\n*   `pb discover materials`:`pb discover materials` - This command now shows a few additional columns - target, username, hostname, invocation folder.\n*   Default ID stitcher: In the inputs file, the key `to_default_stitcher` needs to be set to `true` explicitly for an ID to get picked in the default ID stitcher. This field is optional and by default set to false, without impacting if the project is using a custom ID stitcher. In your project file, if you remove the key `id_stitcher: models/<name of ID stitcher model>`, then it’ll use the default ID stitcher and create a material view of the name `<entity_name>_default_id_stitcher`.\n*   In the inputs.yaml file, table or view names now appear under a key named `app_defaults:`. This signifies that these are values that input defaults to, when the project is run directly. For library projects, inputs can be remapped and appdefaults overridden. when library projects are imported.\n*   Schema has been migrated from 38 -> 40 in the project file.\n\n**Improvements**\n\n*   `pb init pb-project`:`pb init pb-project`: Added keys on default ID stitcher.\n*   A few improvements behind the scenes, for enhancing the overall functionality.\n\n**Bug Fixes**\n\n*   Resolved the issue where projects migrated using `migrate_on_load` were referring to the location of the migrated project in the material registry. This was affecting the count of ID’s before and after stitching.\n*   Fixed bug where ID stitcher wouldn’t check whether a material was actually existing in the database, before running in incremental mode.\n*   When the material registry was on an unsupported common tables version, then the project environment loading would fail, thereby crashing the application. This has now been resolved.\n*   Features defined in Python models, now do appear in the list of features.\n*   Vars can still be specified in specs of a feature table model. However, the app ignores them. This is a bug and would be fixed in subsequent releases.\n\n## Version 0.7.2\n\n_24 July 2023_\n\nOur newest release brings enhanced functionality and a more efficient experience.\n\n**What’s New**\n\n*   **Model Enable/Disable**: You can now enable or disable specific models using the `materialization` key in model specifications. Use the `status` key to set values. For more information, refer to [Models enabling themselves](https://rudderlabs.github.io/pywht/source/120_tutorials.html#models-enabling-themselves).\n*   **Migrate Auto**: When migrating a project, the ordering of elements now remains the same as in the original files, preserving comments.\n*   **Graceful Application Exit**: You can now exit the application gracefully while it’s running. For example, if you’re generating material tables using the run command, you can exit using Ctrl+C.\n*   **Schema Migration**: The schema version in the project file has been updated from 37 to 38.\n\n**Improvements**\n\n*   Projects created using `init pb-project` now include dependencies.\n*   Instead of generating one big SQL file, we now create multiple files in a folder during SQL generation of a feature table model. This reduces the disk space requirements.\n*   Internal optimizations have been implemented to improve overall performance and efficiency.\n\n**Bug Fixes**\n\n*   An issue has been fixed where insufficient grants for accessing the warehouse would lead to duplicate suggested queries. Also, in some cases, incorrect queries were displayed, such as when a Redshift user was asked to grant a role.\n*   The project URL is now being stored in the material registry, instead of GitHub passkey.\n*   Fixed a bug where macros defined in a separate file as global macros were unable to access a common context.\n*   Resolved a bug where Python models were not appearing in the dependency graph.\n\n## Version 0.7.1\n\n_23 June 2023_\n\nOur latest release addresses some critical issues in the previous release. Therefore, if you’re on v0.7.0, then it’s highly recommended to update to the latest version.\n\n## Version 0.7.0\n\n_22 June 2023_\n\nOur newest release is quite significant in terms of new features and improvements offered. Be sure to try it out and share your feedback with us.\n\n**What’s New**\n\n*   `query` - A new command which displays output of tables/views from the warehouse. So you can view generated material tables from the CLI itself. For example, `pb query \"select * from {{this.DeRef(\"models/user_id_stitcher\")}}\"`.\n*   `show idstitcher-report` - A new sub command that creates report on an ID stitcher run. Such as, whether it converged, count of Pre-stitched ID’s before run, Post-stitched ID’s after run, etc. Usage: `pb show idstitcher-report` .\n*   `show user-lookup` - A new sub command that allows you to search a user by using any of the traits as ID types. E.g., `pb show user-lookup -v <trait value>`.\n*   If non-mandatory inputs required by the model are not present in the warehouse, you can still run the model. Applicable to packages and feature tables.\n*   Schema updated from 33 -> 37 in the project file. Please note that the material registry has been migrated to version 3, so you’ll have to execute `pb validate access` once in order to execute the `run` command.\n\n**Improvements**\n\n*   Added an optional field `source_metadata` in the model file inputs.yaml.\n*   Added EnableStatus field in materialization so that models can be enabled and disabled automatically based on whether it is required or not.\n*   Default ID stitcher now supports incremental mode as well.\n*   In macros, you can now specify timestamps in any format.\n\n**Bug Fixes**\n\n*   In case a project is migrated using flag `migrate_on_load`, then src\\_url in the material registry was pointing to the new folder. Now, that is fixed.\n*   Resolved bugs in generating edges for dependency graphs.\n*   Tons of several other improvements and bug fixes under the hood.\n\n## Version 0.6.0\n\n_26 May 2023_\n\nWe are excited to announce the release of PB Version 0.6.0: packed with new features, improvements, and enhanced user experience.\n\n**What’s New**\n\n*   **Support for begin time and end time**: Our latest release introduces the ability to specify time range for your operations, using two new flags `--begin_time` and `--end_time`. By default, the `--end_time` flag is set to `now`. For example, you can use the command `pb run --begin_time 2023-05-01T12:00:00Z` to fetch all data loaded after 1st May 2023. Note that the flag `--timestamp` is now deprecated.\n*   A new flag, `model_refs`, has been introduced which restricts the operation to a specified model. You can specify model references, such as `pb run --model_refs models/user_id_stitcher`.\n*   `seq_no` - Another new flag, using which you can Continue a previous run by specifying its sequence number. Models that already exist would not be rebuilt, unless `--force` is also specified.\n*   **Show command** - `pb show dependencies` has been added to generate a graph showcasing model dependencies. This visual representation will help you understand the relationships between various models in your project.\n*   **Show command** - `pb show dataflow`: Another new command which generates a graph with reversed edges, illustrating the dataflow within your project.\n*   **migrate\\_on\\_load** - A new flag, `migrate_on_load`, has been introduced. When executing the command `pb run --migrate_on_load`, by default this flag creates a `migrations` folder inside the project folder that has migrated version of your project to the latest schema version and runs it on the warehouse. This simplifies the process of upgrading your project to the latest schema version, without changing source files.\n*   **migrated\\_folder\\_path** - Continuing from previous command, you can use this flag to change folder location of the migrated project.\n*   Schema in the project file has been updated to version 33.\n\n**Improvements**\n\n*   SQL Models now provide more relevant and informative error messages instead of generic “not found” errors. This simplifies troubleshooting and debugging processes.\n*   Numerous improvements and optimizations have been made behind the scenes, enhancing the overall performance and stability of PB.\n\n## Version 0.5.2\n\n_5 May 2023_\n\nOur latest release offers significant performance improvements, enhancements, and bug fixes to provide a better experience.\n\n**What’s New:**\n\n*   A new command, `pb show models`, which displays various models and their specifications in the project.\n*   Ability to exit the application while the run command is being executed.\n*   Project schema version has been migrated to 30.\n\n**Improvements:**\n\n*   Major performance improvements for Redshift. In large data sets, it will reduce the time taken to create ID stitcher table to less than 1/4th of the time taken earlier.\n*   `insert` command now picks the connection specified in the current project folder. If not available, it picks “test” in the connection file.\n*   Siteconfig is now validated when project is loaded.\n*   The `cleanup materials` command now removes SQL models as well.\n\n**Bug Fixes:**\n\n*   Resolved the problem where values with null timestamps were excluded from incremental ID stitcher.\n*   The `insert` command was showing a success message even if no tables were inserted in the warehouse. This has been fixed.\n\n## Version 0.5.1\n\n_11 April 2023_\n\n**What’s New**\n\n*   Updated schema to version 28 in the project file.\n\n**Improvements**\n\n*   Changed project path parameter from `-w` to `-p` for improved usability.\n\n**Bug Fixes**\n\n*   Addressed a few reported bugs for an improved user experience.\n*   Implemented performance enhancements to optimize overall system performance.\n\n## Version 0.5.0\n\n_28 March 2023_\n\nThis release offers significant new additions and improvements, as well as bug fixes.\n\n**What’s New**\n\n*   **Cleanup materials** - You can now use the command `pb cleanup materials` to delete materials in the warehouse automatically, without the need for manual deletion. Just specify the retention time period in the number of days (default 180) and all tables/views created prior to that date will be deleted.\n    \n*   Schema has been migrated to 27. This includes the following changes:\n    \n    *   **pb\\_project.yaml** - The schema version has been updated from 25 → 27. Also, `main_id` is removed from `id_types` as main\\_id\\_type is now optional, rudder\\_id is the main\\_id\\_type by default.\n    *   **models/profiles.yaml** - To explicitly declare edge source ids, each value in `edge_sources` now requires a `from:` key to be appended. Also, if you didn’t define `main_id` in the project file, then no need to specify here.\n\n**Improvements**\n\n*   In the backend code we’ve enabled registry migration which flattens the registry, enabling incremental ID stitcher to operate on incomplete materials. It also introduces a mechanism for migrating common tables.\n*   We have implemented better error handling for cases where an incorrect model name is passed. Any errors related to incorrect model names are now properly identified and handled by the system.\n*   Based on feedback from our users, we have renamed default models from `domain_profile<>` to `user_profile<>`.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> Due to changes in registry, we will be depricating older versions of PB.\n\n**Bug Fixes**\n\n*   Fixed the bug where some experimental features, such as Discover, were not working for Redshift.\n*   Addressed the problem where validation errors were incorrectly being triggered when a connection had multiple targets, one of which was invalid. The system now only generates an error if the warehouse target that is being passed has errors.\n*   In addition to previous one, a few more bugs were fixed that were related to validation.\n*   Errors were coming for users who had initialized the GIT repository but had not added the remote origin. This issue has now been fixed.\n\n**Known Issues**\n\n*   Warning: While the run command is being executed, canceling it by pressing Ctrl+C doesn’t work as expected. Though it will stop the program’s execution on the CLI, the query will keep running on the data warehouse. This is a documented Snowflake behavior.\n*   In a model, an input can’t use columns named “MAIN\\_ID”, “OTHER\\_ID”, “OTHER\\_ID\\_TYPE”, or “VALID\\_AT” in its ID SQL.\n*   When creating a connection via `init` command, pressing the Ctrl+C command doesn’t exit the application.\n*   `migrate auto` jumbles up the order and removes comments.\n*   On Redshift, validate access passes all tests, but `run` command sometimes fail giving error “permission denied for language plpythonu”.\n*   Some commands such as `insert` do not work on Redshift.\n*   For a few clusters, cross DB references can fail on Redshift.\n*   The command `migrate auto` migrates siteconfig in your home directory but not any local one.\n*   While working with same type of data in Snowflake and Redshift you might encounter errors where it works on Snowflake but not on Redshift. This is due to the fact that implicit casting of different data types for different function or operator might not be supported on one data warehouse while supported on other.\n\n## Version 0.4.0\n\n_2 March 2023_\n\nWe are proud to announce the latest version of PB 0.4.0, which includes several new features and improvements.\n\n**What’s New**\n\n*   **Redshift** - We are excited to share that we now offer Redshift integration. With YAML, you can now effortlessly create ID stitched and feature table models on your Redshift warehouse, without any difficulty.\n    \n*   **Incremental ID stitching**: You can now stitch together data from multiple sources in incremental mode. When new data is added to your source tables, only that data will be fetched, without needing to reload the whole table each time! This shall result in significant performance improvements from earlier versions, especially if the delta of new data is much smaller compared to what’s already been stitched.\n    \n*   **Insert**: A new command, allowing users to add sample data to their warehouse, without having to manually add them.\n    \n*   Schema has been migrated to 25. This includes the following changes:\n    \n    *   **models/profiles.yaml** - Renamed entityvar to `entity_var` and inputvar to `input_var`\n    *   **pb\\_project.yaml** - Renamed profile to `connection`.\n    *   **siteconfig.yaml** - Renamed profiles to `connections`.\n\nBe sure to use the `migrate auto` command to upgrade your project and the connections file.\n\n**Improvements**\n\n*   The command `init profile` has been renamed to `init connection`.\n*   Lots of modifications under the hood.\n\n**Bug Fixes**\n\nResolved issue on default values in an entity var, ensuring that the values are properly set.\n\n**Known Issues**\n\n*   Warning: While the run command is being executed, canceling it by pressing Ctrl+C doesn’t work as expected. Though it will stop the program’s execution on the CLI, the query will keep running on the data warehouse. This is a documented Snowflake behavior.\n*   In a model, an input can’t use columns named “MAIN\\_ID”, “OTHER\\_ID”, “OTHER\\_ID\\_TYPE”, or “VALID\\_AT” in its ID SQL.\n*   When creating a connection via `init` command, pressing the Ctrl+C command doesn’t exit the application.\n*   `migrate auto` jumbles up the order and removes comments.\n*   On Redshift, some experimental commands such as discover do not work.\n*   The command `migrate auto` migrates siteconfig in your home directory but not any local one.\n*   While working with same type of data in Snowflake and Redshift you might encounter errors where it works on Snowflake but not on Redshift. This is due to the fact that implicit casting of different data types for different function or operator might not be supported on one data warehouse while supported on other.\n\n## Version 0.3.1\n\n_3 February 2023_\n\nThis version addresses a crucial defect, so please make sure to update your version. Note that you won’t have to update your schema for this release.\n\n## Version 0.3.0\n\n_25 January 2023_\n\nWe have got a new name! WHT is now called Profile Builder (PB), RS360 is now Profiles. Be sure to check out our newest release that comes with several new features for an enhanced user experience.\n\n**What’s New**\n\n*   **Migrate** - A new command that will enable you to migrate your project to a newer schema. It has two subcommands:\n    \n    *   **Manual** - You will get to know steps you need to follow to manually migrate the project yourself. It will include both breaking and non-breaking changes.\n    *   **Auto** - Automatically migrate from one version to another.\n*   We have made a few significant changes to YAML. The changes consist of:\n    \n    *   Bumping schema version from 9 → 18.\n    *   Entityvar (Feature Table) - We have renamed tablevar, tablefeature and feature to entityvar; as they all were adding columns to an entity with nearly identical YAML. A new `vars:` section of feature table YAML contains list of inputvars and entityvars. Whereas `features:` field same YAML is a list of entityvar names which should be part of the final output table.\n    *   ID Stitcher is now linked to an entity. As a result, all tables using that entity will use the linked ID Stitcher. Earlier, an ID stitcher was linked to a feature model.\n    *   Some of the terms in yaml spec are changed to make it closer to SQL terminologies. For entityvar and inputvar spec: value → select, filter → where , ref → from. In inputs spec: sql → select.\n    *   Project file has a new key named `include_untimed`. If set to `true`, data without timestamps are included when running models. This reduces data errors for timestamp materials. Also, we have deprecated the flag `require-time-clean` in the `run` command.\n    *   Id types can now be re-used between entities. In the project file, entities now have a list of id types names, instead of a list of definitions. In the inputs file, a required entity field is added to the ID list that specifies which entity this ID type is being extracted for.\n    *   Now an inputvar can also read from a macro, just like tablevar.\n    *   Global Macros - You can now define macros in a separate YAML file inside your models folder. They can then be used across different models in your project. Thus a macro becomes independent that can be reused across multiple models.\n    *   wht\\_project.yaml is renamed to pb\\_project.yaml and ml-features.yaml to profiles.yaml.\n*   **Cleanup Materials** - A new command that allows you to review all the created materials and then delete them (NOTE: experimental feature).\n    \n*   **Discover** - A new subcommand `discover materials` has been added. Using it, you can now discover all the materials associated with a project.\n    \n*   **Compile/Run** - GIT URL now supports tags. To use, execute the command `pb compile -w git@github.com:<orgname>/<repo>/tag/<tag_version>/<folderpath>`.\n    \n\n**Improvements**\n\n*   **Web app** - The UI is now more intuitive and user-friendly.\n*   Log tracing is now enabled by default for most commands. Log files are stored in logs/logfile.log of your current working directory. They store upto 10 MB data. Also, the logger file now stores more granular information for easier debugging in case of unexpected errors.\n*   Significant performance improvements in creating ID stitched tables, in case a lot of duplicates are present.\n*   Add extra columns (Hash, SeqNos) to differentiate between entries for commands to discover sources and entities.\n*   When you execute a profile via run command, then the generated SQL gets saved in the output folder.\n*   Added .gitignore file to init project command, to prevent unnecessary files being added to GIT Repo. Such as, .DS\\_Store, output and logs folders.\n*   Tonnes of changes under the hood.\n\n**Bug Fixes**\n\n*   Fixed the bug where window functions were creating multiple rows (duplicates) per main id.\n*   Resolved the bug in inputvars which was doing joins on main\\_id instead of row id.\n*   Executing the command init profile now inputs values in the same order as on the web app.\n*   Resolved the bug where extra gitcreds\\[\\] and warehouse lines were added on overwriting a profile that already existed.\n*   A few redundant parameters were being shown in the validate access command which have been removed.\n*   Removed a couple of redundant subcommands in the init project.\n\n**Known Issues:**\n\n*   Warning: While the run command is being executed, canceling it by pressing Ctrl+C doesn’t work as expected. Though it will stop the program’s execution on the CLI, the query will keep running on the data warehouse. This is a documented Snowflake behavior.\n*   In a model, an input can’t use columns named “MAIN\\_ID”, “OTHER\\_ID”, “OTHER\\_ID\\_TYPE”, or “VALID\\_AT” in its ID SQL.\n*   When creating a profile via `init` command, pressing the Ctrl+C command doesn’t exit the application.\n*   Web app doesn’t allow you to select a date older than 30 days.\n*   Migrate auto jumbles up the order and removes comments.\n\n## Version 0.2.2\n\n_12 November 2022_\n\nOur November release is significant as it has several fixes and improvements for an enhanced experience. Check it out and be sure to let us know your feedback.\n\n**What’s New**\n\n*   **ID Stitcher / Feature Table** - You can now define a view as source, in addition to table, in the inputs file. This is particularly of use when you need to support an sql query that’s complex or out of scope for PB. To use it, in your inputs file define the edge\\_source as `view: <view_name>` instead of `table: <table_name>`.\n*   **Inputvars** - A new identifier which adds temporary helper columns to an input table, for use in calculating a featuretable.\n*   **Window Functions** - In your model file, you can now add window function support to features, tablevars, tablefeatures and inputvars. Also, you can add filters to features.\n\n**Improvements**\n\n*   Schema version 9 makes it more streamlined to define the model. We welcome your feedback for further improvements on this.\n*   Compile command now show errors if the input SQL is buggy.\n*   Discover - subcommands `entities` and `features` now show a few more fields.\n*   Discover - Export to CSV works for subcommands and also generates files in the output folder.\n*   Init pb-project - Based on feedback, it now generates a README file and also has simpler YAML files with comments. It should now be easier for our users to create a model and get it running.\n*   Several internal refactorings on how the application works.\n*   Web app - Massive improvements under the hood related to UI elements, preserving state when entering data, showing correct data and validations, and displaying run time in user’s local time zone.\n\n**Bug Fixes**\n\n*   Fixed the issue where every time `pb run` was executed for a feature table, it was adding a new row to the output of `pb discover features`.\n*   Resolved the bug where error wasn’t shown if an unknown flag was used.\n*   There was an issue generating material tables on a new schema, which has now been resolved.\n*   Bug fix on generating empty SQL files from input models.\n*   Fixed bug where model names with \\_ in the name would sometimes fail to update the latest view pointer correctly.\n*   Web app - Artifacts list now shows different folders for different runs to isolate them.\n*   Web app - When the PB project is running, the screen now shows correct start timestamp.\n*   Web app - Date filters to find PB runs are now working.\n*   Web app - Scheduling UI is now fully responsive about when the run will take place.\n*   Web app - Resolved the issue where a project would run only once and was then showing error.\n\n**Known Issues:**\n\n*   Warning: While the run command is being executed, canceling it by pressing Ctrl+C doesn’t work as expected. Though it will stop the program’s execution on the CLI, the query will keep running on the data warehouse. This is a documented Snowflake behavior.\n*   In a model, an input can’t use columns named “MAIN\\_ID”, “OTHER\\_ID”, “OTHER\\_ID\\_TYPE”, or “VALID\\_AT” in its ID SQL.\n*   When creating a profile via `init` command, pressing the Ctrl+C command doesn’t exit the application.\n*   Logger file generation is disabled at the moment.\n*   Some no-op parameters are shown upon passing the help flag(-h) to `validate access` command.\n\n## Version 0.2.0\n\n_5 October 2022_\n\nThe September release is our largest update yet. We have added a lot of quality of life improvements and net new features to the PB product line. We plan on releasing even more features in our mid-October release to further improve the usability of the product as well as add additional features that will further help form the core of the product. A substantial amount of the features in this release were based directly off feedback from the first beta testing with external users and internal stakeholders. Please feel free to walk through our newest release. We welcome and encourage all constructive feedback on the product.\n\n**What’s New**\n\n*   **Feature Table** - After encouraging feedback from beta testing of the ID Stitcher, we are feeling more confident about sharing our C360 feature table functionality with beta customers. During testing of this release, we benchmarked ourselves against the feature set that our E-Commerce ML models expect. Many features were implemented successfully. Some needed functionality which could not be pushed through QA gates in this September release. Nevertheless, the feature table YAML is now ready for internal customers to explore.\n*   **Web App** - We are now ready to share the scheduling functionality within the web app. This will allow the user to schedule, and automatically run PB models from the Rudder backplane. Any artifacts and log files created during the execution of PB projects are also available for the user to explore. This critical functionality will enable users to debug their cloud PB runs.\n*   **Validate** - A new command, `pb validate` allows users to run various tests on the project related configurations and validate the privileges associated with the role used for running the project. For example, the subcommand `pb validate access` does an exhaustive test of required privileges before accessing the warehouse.\n*   **Version** - This is another new command that provides information on the current version of the app.\n*   **Logger** - When you execute the compile and run commands, all errors and success messages that were previously only displayed on screen, are now also logged in a file inside the project output folder.\n*   **Discover** - You can now export the output of the discover command in a CSV file. The ability to discover across all schemas in one’s warehouse is also added.\n\n**Improvements**\n\n*   We have made many changes to the way ID Stitcher config is written. We are forming a more complete opinion on the semantic model representation for customer’s data. Entities, IDs, and ID types are now defined in the PB project file. The model file syntax is also more organized and easier to write. To see examples of the new syntax check out the section on [Identity stitching](https://www.rudderstack.com/docs/archive/profiles/0.14/core-concepts/identity-stitching/) or sample files by executing command `pb init pb-project`. The sample project file also contains include and exclude filters, to illustrate their usage.\n*   In PB command invocation, whenever a file is written, its location is now shown on the console and in log files.\n*   Many enhancements on how errors are handled inside the application.\n*   Massive improvements under the hood.\n\n**Bug Fixes**\n\n*   Fixed the issue in ID stitching where it was not picking up singleton components (i.e. the ones with only 1 edge), due to which they were getting skipped in the final output table.\n*   In the `init` command, not entering any value for target wasn’t setting it to default value as “dev”.\n*   Pressing Ctrl+C wasn’t exiting the application.\n*   The command `init profile` now appends to an existing profile, instead of overwriting it.\n*   Fixed the issue in `discover` command where the material table name was being displayed instead of the model name.\n\n**Known Issues:**\n\n*   Warning: While the run command is being executed, canceling it by pressing Ctrl+C doesn’t work as expected. Though it will stop the program’s execution on the CLI, the query will keep running on the data warehouse. This is a documented Snowflake behavior.\n*   In a model, an input can’t use columns named “MAIN\\_ID”, “OTHER\\_ID”, “OTHER\\_ID\\_TYPE”, or “VALID\\_AT” in its ID SQL.\n*   The web app is not showing a description and last run on the landing page.\n*   In the web app, date filters to find PB runs aren’t working.\n*   In the web app, when the PB project is running, the screen shows an incorrect start timestamp.\n*   Artifacts list changes when a project is running versus when it completes execution. Since all runs on the same Kubernetes pod share the same project folder, we are creating artifacts of different runs under the same parent folder. So, the same folder is currently shown for different runs of the project. In the next release, we will configure different folders for different runs to isolate them.\n*   In case of feature table models, the compile command doesn’t always show error if the input SQL is buggy. Thise error may still be found when the model is run.\n*   When creating a profile via `init` command, pressing the Ctrl+C command doesn’t exit the application.\n*   Creating a PB Project doesn’t currently include a sample independent ID stitcher. Instead, it is a child model to the generated feature table model.\n*   We are working toward better readability of the logger file. We welcome any feedback here.\n*   The command `pb discover features` needs to show a few more fields.\n*   Every time `pb run` is executed for a feature table, it adds a new row to the output of pb discover features. Only one row should appear for each feature.\n*   Export to CSV for the discover command should work for subcommands and also generate files in an output folder.\n*   Some no-op parameters are shown upon passing the help flag(-h) to `validate access` command.\n*   In some cases, error isn’t shown if an unknown flag is used.\n*   Scheduling UI isn’t sometimes fully responsive about when the run will take place.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> The documentation for September release does not completely match with the current release. We are currently working on updating the documentation and will have new versions out soon. Please contact the Data Apps team if you are confused by some deviation.\n\n## Version 0.1.0\n\n_18 August 2022_\n\nWe are now in beta! Please do try out PB and share your feedback with us, so that we can make it better.\n\n**What’s New**\n\n*   **ID Stitcher** - ID Stitching solves the problem of tying different identities together, for the same user across different sessions/devices. With v0.1.0, we launch PB ID Stitching. It provides an easy to use and powerful interface to specify Id Stitching inputs.\n*   **Command Line Interface** - Our CLI tool works on Linux, Windows and Mac machines. Using it you can setup a profile having connection to your Database, make a PB project, create SQL from models, run ID stitcher models directly on the Warehouse, and discover all the created models/entities/sources on DW.\n\n**Improvements**\n\n*   We have enhanced the speed of Discover and Compile commands, from minutes to a few seconds.\n*   The description of a few commands in Help has been improved.\n\n**Bug Fixes**\n\n*   The command for discovering entities wasn’t working, which has now been resolved.\n*   Fixed the bug on init profile command where siteconfig wasn’t getting created on first-time installations.\n*   A few bugs resolved related to output of discover command.\n\n**Known Issues:**\n\n*   Warning: While the run command is being executed, please do not cancel it by pressing Ctrl+C. Though it will stop the program’s execution on CLI, the query will keep running on the data warehouse. This is a documented Snowflake behaviour.\n*   Null ID’s in ID stitcher. If first listed Id is null, the entire row may be ignored. That means, results are silently incorrect.\n*   If first listed ID is null, the entire row may be ignored. The first listed ID is assumed to be the key ID. If it is ever null the results may be incorrect.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Changelog | RudderStack Docs",
    "description": "Changelog for all the Profiles versions.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.14/faq/",
    "markdown": "# Profiles FAQ | RudderStack Docs\n\nCommonly asked questions on RudderStack Profiles.\n\n* * *\n\n*     34 minute read  \n    \n\nThis guide contains solutions for some of the commonly asked questions on Profiles. For queries or issues not listed in this guide, contact [RudderStack Support](mailto:support@rudderstack.com).\n\n## Setup and installation\n\n**I have installed Python3, yet when I install and execute `pb` it doesn’t return anything on screen.**\n\nTry restarting your Terminal/Shell/PowerShell and try again.\n\nYou can also try to find the location of your Python executable. PB would be installed where the executables embedded in other Python packages are installed.\n\n**I am an existing user who updated to the new version and now I am unable to use the PB tool. On Windows, I get the error:** `'pb' is not recognized as an internal or external command, operable program or batch file.`\n\nExecute the following commands to do a fresh install:\n\n1.  `pip3 uninstall profiles-rudderstack-bin`\n2.  `pip3 uninstall profiles-rudderstack`\n3.  `pip3 install profiles-rudderstack --no-cache-dir`\n\n**I am unable to download, getting** `ERROR: Package 'profiles-rudderstack' requires a different Python: 3.7.10 not in '>=3.8, <=3.10'`\n\nUpdate your Python 3 to a version greater than or equal to 3.8 and less than or equal to 3.10.\n\n**I am unable to download profile builder by running `pip3 install profiles-rudderstack` even though I have Python installed.**\n\nFirstly, make sure that Python3 is correctly installed. You can also try to substitute `pip3` with `pip` and execute the install command.\n\nIf that doesn’t work, it’s high likely that Python3 is accessible from a local directory.\n\n1.  Navigate to that directory and try the install command again.\n2.  After installation, PB should be accessible from anywhere.\n3.  Validate that you’re able to access the path using `which pb`.\n4.  You may also execute `echo $PATH` to view current path settings.\n5.  In case it doesn’t show the path then you can find out where |ProductName| is installed using :substitution-code:`pip3 show profiles-rudderstack`. This command will display a list of the files associated with the application, including the location in which it was installed, navigate to that directory.\n6.  Navigate to `/bin` subdirectory and execute command `ls` to confirm that `pb` is present there.\n7.  To add the path of the location where PB is installed via pip3, execute: `export PATH=$PATH:<path_to_application>`. This will add the path to your system’s PATH variable, making it accessible from any directory. It is important to note that the path should be complete and not relative to the current working directory.\n\nIf you still face issues, then you can try to install it manually. [Contact us](mailto:support@rudderstack.com) for the executable file and download it on your machine. Follow the below steps afterwards:\n\n1.  Create `rudderstack` directory: `sudo mkdir /usr/local/rudderstack`.\n2.  Move the downloaded file to that directory: `sudo mv <name_of_downloaded_file> /usr/local/rudderstack/pb`.\n3.  Grant executable permission to the file: `chmod +x /usr/local/rudderstack/pb`.\n4.  Navigate to directory `/usr/local/rudderstack` from your file explorer. Ctrl+Click on pb and select **Open** to run it from Terminal.\n5.  Symlink to a filename pb in `/usr/local/bin` so that command can locate it from env PATH. Create file if it does not exist: `sudo touch /usr/local/bin/pb`. Then execute`sudo ln -sf /usr/local/rudderstack/pb /usr/local/bin/pb`.\n6.  Verify the installation by running `pb` in Terminal. In case you get error `command not found: pb` then check if `/usr/local/bin` is defined in PATH by executing command: `echo $PATH`. If not, then add `/usr/local/bin` to PATH.\n\n1.  If the Windows firewall prompts you after downloading, proceed with `Run Anyway`.\n2.  Rename the executable as `pb`.\n3.  Move the file to a safe directory such as `C:\\\\Program Files\\\\Rudderstack`, create the directory if not present.\n4.  Set the path of `pb.exe` file in environment variables.\n5.  Verify the installation by running `pb` in command prompt.\n\n**When I try to install Profile Builder tool using pip3 I get error message saying: `Requirement already satisfied`**\n\nTry the following steps:\n\n1.  Uninstall PB using `pip3 uninstall profiles-rudderstack`.\n2.  Install again using `pip3 install profiles-rudderstack`.\n\nNote that this won’t remove your existing data such as models and siteconfig files.\n\n**I have multiple models in my project. Can I run only a single model?**\n\nYes, you can. In your spec YAML file for the model you don’t want to run, set `materialization` to `disabled`:\n\n```\nmaterialization:\n    enable_status: disabled\n```\n\nA sample `profiles.yaml` file highlighted a disabled model:\n\n```\nmodels:\n- name: test_sql\n  model_type: sql_template\n  model_spec:\n    validity_time: 24h# 1 day\n    materialization:                \n      run_type: discrete\n      enable_status: disabled  // Disables running the model.\n    single_sql: |\n        {%- with input1 = this.DeRef(\"inputs/tbl_a\") -%}\n          select id1 as new_id1, {{input1}}.*\n            from {{input1}}\n        {%- endwith -%}        \n    occurred_at_col: insert_ts\n    ids:\n      - select: \"new_id1\"\n        type: test_id\n        entity: user\n```\n\n**I am facing this error while ugrading my Profiles project: `pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. profiles-pycorelib 0.2.2 requires profiles-rudderstack!=0.10.6,<=0.10.7,>=0.10.5, but you have profiles-rudderstack 0.11.0 which is incompatible.`**\n\nThis is because you must uninstall and then reinstall the `pycorelib` library while upgrading to a recent version.\n\n* * *\n\n## Warehouse issues\n\n**I have two separate roles to read from input tables and write to output tables? How should I define the roles?**\n\nYou need to create an additional role as a union of those two roles. PB project needs to read the input tables and write the results back to the warehouse schema.\n\nFurthermore, each run is executed using a single role as specified in the `siteconfig.yaml` file. Hence, it is best in terms of security to create a new role which has read as well as write access for all the relevant inputs and the output schema.\n\n**Can I refer a table present in another database within the warehouse?**\n\n*   **Snowflake**: You can refer tables from cross-database as long as they are in same warehouse.\n*   **Bigquery**: You can refer tables from cross-projects.\n*   **Databricks**: You can refer tables from database in the same or another warehouse.\n*   **Redshift**: For the following setups:\n    *   Cluster with DC2 Nodes: You cannot use tables from cross-database in the project.\n    *   Cluster with RA3 Nodes/Serverless: You can refer tables from cross-database as long as they are in same warehouse.\n\n**How do I test if the role I am using has sufficient privileges to access the objects in the warehouse?**\n\nYou can use the `pb validate access` command to validate the access privileges on all the input/output objects.\n\n**While working with Profiles, how can I use the tables in my BigQuery warehouse that are partitioned on the time criteria?**\n\nTo refer to the partitioned tables in your Profiles project, you must include a filter based on the partitioned column. To do so, add `is_event_stream: true` and use the partitioned filter as `occurred_at_col: timestamp` while defining your `inputs.yaml` file:\n\n```\n- name: rsTracks\n  contract:\n    is_optional: false\n    is_event_stream: true\n    with_entity_ids:\n      - user\n    with_columns:\n      - name: timestamp\n      - name: user_id\n      - name: anonymous_id\n  app_defaults:\n    table: profiles_new.tracks\n    occurred_at_col: timestamp\n    ids:\n      - select: \"user_id\"\n        type: user_id\n        entity: user\n      - select: \"anonymous_id\"\n        type: anonymous_id\n        entity: user\n```\n\n* * *\n\n## Compile command\n\n**I am trying to execute the `compile` command by fetching a repo via GIT URL but getting this error: `making git new public keys: ssh: no key found`**\n\nYou need to add the OpenSSH private key to your `siteconfig.yaml` file. If you get the error `could not find expected` afterwards, try correcting the spacing in your `siteconfig.yaml` file.\n\n**While trying to segregate identity stitching and feature table in separate model files, I am getting this error: `mapping values are not allowed in this context`**\n\nThis is due to the spacing issue in `siteconfig.yaml` file. You may create a new project to compare the spacing. Also, make sure you haven’t missed any keys.\n\n**While using v0.13, I notice that two subfolders are created inside the output folder for compile and run, even if I execute only the `pb run command`. What exactly is the difference between them?**\n\nRudderStack generates two subfolders for easy debugging as compiling is a substep of running the project. If you encounter an error during the project run and are not able to get the corresponding SQL generated for this step, you can still rely on the SQL generated during the compile step to debug the error.\n\n**I want to build profiles over my Snowflake warehouse data which is pulled in using Salesforce (CRM tool). Is it necessary that the data in my Snowflake warehouse flows via RudderStack? Can I build an entity model for Salesforce users that references the Snowflake table?**\n\nRudderStack Profiles lets you use any data present in your warehouse. It does not need to come in via RudderStack. Further, you can define the entities in a `pb_project.yaml` file and use them declaratively while describing the columns of your input sources.\n\n* * *\n\n## Command progress & lifecycle\n\n**I executed a command and it is taking too long. Is there a way to kill a process on data warehouse?**\n\nIt could be due to the other queries running simultaneously on your warehouse. To clear them up, open the **Queries** tab in your warehouse and manually kill the long running processes.\n\n**Due to the huge data, I am experiencing long execution times. My screen is getting locked, thereby preventing the process from getting completed. What can I do?**\n\nYou can use the `screen` command on UNIX/MacOS to detach your screen and allow the process to run in the background. You can use your terminal for other tasks, thus avoiding screen lockouts and allowing the query to complete successfully.\n\nHere are some examples:\n\n*   To start a new screen session and execute a process in detached mode: `screen -L -dmS profiles_rn_1 pb run`. Here:\n    *   `-L` flag enables logging.\n    *   `-dmS` starts as a daemon process in detached mode.\n    *   `profiles_rn_1` is the process name.\n*   To list all the active screen sessions: `screen -ls`.\n*   To reattach to a detached screen session: `screen -r [PID or screen name]`.\n\n**The CLI was running earlier but it is unable to access the tables now. Does it delete the view and create again?**\n\nYes, every time you run the project, Profiles creates a new materials table and replaces the view.\n\nHence, you need to grant a select on future views/tables in the respective schema and not just the existing views/tables.\n\n**Does the CLI support downloading a git repo using siteconfig before executing** `pb run` **? Or do I have to manually clone the repo first?**\n\nYou can pass the Git URL as a parameter instead of project’s path, as shown:\n\n**When executing** `run` **command, I get a message:** `Please use sequence number ... to resume this project in future runs` **. Does it mean that a user can exit using Ctrl+C and later if they give this seq\\_no then it’ll continue from where it was cancelled earlier?**\n\nThe `pb run --seq_no <>` flag allows for the provision of a sequence number to run the project. This flag can either resume an existing project or use the same context to run it again.\n\nWith the introduction of time-grain models, multiple sequence numbers can be assigned and used for a single project run.\n\n**What flag should I set to force a run for the same input data (till a specified timestamp), even if a previous run exists?**\n\nYou can execute `pb run --force --model_refs models/my_id_stitcher,entity/user/user_var_1,entity/user/user_var_2,...`\n\n**Can the hash change even if schema version did not change?**\n\nYes, as the hash versions depends on project’s implementation while the schema versions are for the project’s YAML layout.\n\n**Is there a way to pick up from a point where my last pb run failed on a subsequent run? For large projects, I don’t want to have to rerun all of the features if something failed as some of these take several hours to run**\n\nYes, you can just execute the run command with the specific sequence number, for example, `pb run —seq_no 8`.\n\n**What is the intent of `pb discover models` and `pb discover materials` command?**\n\nYou can use `pb discover models` to list all the models from registry and `pb discover materials` to list all the materials from the registry.\n\n**I got this while running `pb show models`. What is “Maybe Enabled”?**\n\n[![](https://www.rudderstack.com/docs/images/profiles/show_models.webp)](https://www.rudderstack.com/docs/images/profiles/show_models.webp)\n\nIn the show models command, the enable status is computed without looking at tables in the warehouse. Imagine a model`M` that has an optional input column. So, `M` is enabled if and only if the optional input column is present. Hence, it may or may not be enabled, depending on whether the input column is present or not.\n\n**How can I handle my Profiles project in the development and production workspace in RudderStack?**\n\nProfiles support git branches in the RudderStack dashboard. Refer [Supported Git URLs](https://www.rudderstack.com/docs/archive/profiles/0.14/example/packages/#supported-git-urls) for more information.\n\nIn case you wish to run only one project in the CLI and run them differently in dev and prod, you can use **targets**:\n\n1.  Create a connection using `pb init connection` and give a connection name (say `test`). Then, give a default target name, say _prod_. Enter remaining details.\n2.  Create another connection using `pb init connection` and give the same connection name as before (`test`). Then, give a different target name, say `dev`. Enter remaining connection details for connecting to your warehouse.\n3.  When you execute a command via CLI, you need to pass `-t` flag. The first connection you’ve defined is the default one, hence, you don’t need to pass a flag explicitly. However, you can pass it for the other one. For example, `pb run -t dev`.\n\nTargets aren’t yet supported in the UI. So while you can run the same project on different instances (prod, dev) in the CLI; in the UI you have to make either a different project or a different branch/tag/subfolder.\n\n**I am getting an “operation timed out” error even though the `pb validate access` command worked fine.**\n\nRetry the command run after some time. It should resolve the issue.\n\n**I have defined a version constraint in my project and migrated it to the latest schema using `pb migrate auto` command. The project is migrated except the `python_requirements` key which has the same version constraints. How do I change that?**\n\nYou need to manually change the project version in CLI as the version constraints don’t change automatically.\n\n* * *\n\n## Identity stitching\n\n**There are many large size connected components in my warehouse. To increase the accuracy of stitched data, I want to increase the number of iterations. Is it possible?**\n\nThe default value of the largest diameter, that is, the longest path length in connected components, is 30.\n\nYou can increase it by defining a `max_iterations` key under `model_spec` of your ID stitcher model in `models/profiles.yaml`, and specifying its value as the max diameter of connected components.\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> Note that the algorithm can give incorrect results in case of large number of iterations.\n\n**Do I need to write different query each time for viewing the data of created tables?**\n\nNo, you can instead use a view name, which always points to the latest created material table. For example, if you’ve defined **user\\_stitching** in your `models/profiles.yaml` file, then execute `SELECT * FROM MY_WAREHOUSE.MY_SCHEMA.user_stitching`.\n\n**In my model, I have set the key `validity_time: 24h`. What happens when the validity of generated tables expire? Will re-running the identity stitching model generate the same hash until the validity expires?**\n\nFirstly, hash does not depend on the timestamp, it depends on the yaml in the underlying code. That’s why the material name is `material_name_HASH_SEQNO`. The sequence number (SEQNO) depends on timestamp.\n\nSecondly, a material generated for a specific timestamp (aside for the timeless timestamp) is not regenerated unless you do a `pb run --force`. The CLI checks if the material you are requesting already exists in the database, and if it does, returns that. The `validity_time` is an extension of that.\n\nFor a model with `validity_time: 24h` and inputs having the `timestamp` columns, if you request a material for latest time, but one was generated for that model 5 minutes ago, the CLI will return that one instead. Using the CLI to run a model always generates a model for a certain timestamp, it’s just if you don’t specify a timestamp then it uses the current timestamp.\n\nSo, for a model with validity\\_time (vt), and having the `timestamp` columns, if you request a material for t1, but one already exists for t0 where t1-vt <= t0 <= t1, the CLI will return that one instead.\n\nIf multiple materials exist that satisfy the requirement, then it returns the one with the timestamp closest to t1.\n\n**I want to use `customer_id` instead of `main_id` as the ID type. So I changed the name in `pb_project.yaml`, however now I am getting this error: `Error: validating project sample_attribution: listing models for child source models/: error listing models: error building model domain_profile_id_stitcher: main id type main_id not in project id types`.**\n\nIn addition to making changes in the file `pb_project.yaml` file, you also need to set `main_id_type: customer_id` in the `models/profiles.yaml` file.\n\n**I ran identity stitching model but not able to see the output tables under the list of tables in Snowflake. What might be wrong?**\n\nIn Snowflake, you can check the **Databases** > **Views** dropdown from the left sidebar. For example, if your model name is `domain_profile_id_stitcher`, you should be able to see the table with this name. In case it is still not visible, try changing the role using dropdown menu from the top right section.\n\n**I am using a view as an input source but getting an error that the view is not accessible, even though it exists in DB.**\n\nViews need to be refreshed from time-to-time. You can try recreating the view in your warehouse and also execute a `select *` on the same.\n\n**What might be the reason for following errors:**\n\n*   `processing no result iterator: pq: cannot change number of columns in view`. The output view name already exists in some other project. To fix this, try dropping the view or changing its name.\n    \n*   `creating Latest View of moldel 'model_name': processing no result iterator: pq: cannot change data type of view column \"valid_at\"` Drop the view `domain_profile` in your warehouse and execute the command again.\n    \n*   `processing no result iterator: pq: column \"rudder_id\" does not exist`. This occurs when you execute a PB project with a model name, having `main_id` in it, and then you run another project with the same model name but no `main_id`. To resolve this, try dropping the earlier materials using `cleanup materials` command.\n    \n\n**I have a source table in which `email` gets stored in the column for `user_id`, so the field has a mix of different ID types. I have to tie it to another table where email is a separate field. When doing so, I have two separate entries for email, as type `email` and `user_id`. What should I do?**\n\nYou can implement the following line in the inputs tables in question:\n\n```\n  - select: case when lower(user_id) like '%@%' THEN lower(user_id) else null end\n    type: email \n    entity: user\n    to_default_stitcher: true\n```\n\n**How do I validate the results of identity stitching model?**\n\nContact [RudderStack Support](mailto:support@rudderstack.com) if you need help in validating the clusters.\n\n**Which identifiers would you recommend that I include in the ID stitcher for an ecommerce Profiles project?**\n\nWe suggest including identifiers that are unique for every user and can be tracked across different platforms and devices. These identifiers might include but not limited to:\n\n*   Email ID\n*   Phone number\n*   Device ID\n*   Anonymous ID\n*   User names\n\nThese identifiers can be specified in the file `profiles.yaml` file in the identity stitching model.\n\nRemember, the goal of identity stitching is to create a unified user profile by correlating all of the different user identifiers into one canonical identifier, so that all the data related to a particular user or entity can be associated with that user or entity.\n\n**If I run `--force` with an ID Stitcher model and also pass a `--seq_no` for the most recent run, will it still recreate the full ID Graph? Also, is there a way to know if the model was run incrementally or not?**\n\nThis will re-run the ID stitcher and if it is incremental, it will look for the most recent run of the stitcher. After finding the existing run for that `seq_no`, it will use it as the base. This is because the base for an incremental run could be the current `seq_no`. If you do not want to do this, you can pass the `rebase_incremental` flag.\n\n**I am getting a bunch of NULL `VALID_AT` timestamps. Is it because the table where the data is being referenced from does not have a timestamp fields specified? Will this impact anything in the downstream?**\n\nYes, if there is no timestamp field in the input table (or it is NULL for the row from where the edge source was pulled), then `VALID_AT` column would have NULL value. This only affects the `VALID_AT` column in the final table and nothing in the ID stitching.\n\n**Which identifiers should I include in my `inputs.yaml` file?**\n\nInclude all the IDs that contribute to the ID stitcher model.\n\n**Should I re-run the stitching process once all `user_id`’s have been sorted out with market prefixes? I want to ensure that users are captured separately instead of being grouped under one `rudder_id`.**\n\nIt is recommended to use the `--rebase-incremental` flag and re-run the stitching process from scratch. While it may not be necessary in all cases, doing so ensures a fresh start and avoids any potential pooling of users under a single `rudder_id`. It’s important to note that if you make any changes to the YAML configuration, such as modifying the entity or model settings, the model’s hash will automatically update. However, some changes may not be captured automatically (for example, if you didn’t change YAML but simply edited column values in the input table), so manually rebasing is a good practice.\n\n**While running my ID stitcher model, I get the error “Could not find parent table for alias “”**\n\nThis is because RudderStack tries to access the cross-database objects (views/tables) for inputs, which is only supported on Redshift [RA3 node type clusters](https://docs.aws.amazon.com/redshift/latest/dg/cross-database_usage.html).\n\nTo resolve the issue, you can upgrade your cluster to RA3 node type or copy data from source objects to the database specified in the siteconfig file. **I want to use a SQL model for an exclusion filter which references tables that are not used in the ID stitching process. Do I still need to add those tables to the `inputs.yaml` file?**\n\nIt is not necessary to add the table references to the `inputs.yaml` file. However, it is advised to add it for the following reasons:\n\n*   You can rule out any access/permissions issues for the referenced tables.\n*   The `contract` field in `inputs.yaml` would help you handle errors if the required column doesn’t exist.\n\n* * *\n\n## Feature Table\n\n**How can I run a feature table without running its dependencies?**\n\nSuppose you want to re-run the user entity\\_var `days_active` and the `rsTracks` input\\_var `last_seen` for a previous run with `seq_no 18`.\n\nThen, execute the following command:\n\n```\npb run --force --model_refs entity/user/days_active,inputs/rsTracks/last_seen --seq_no 18\n```\n\n**I have imported a library project but it throws an error: `no matching model found for modelRef rsTracks in source inputs`.**\n\nYou can exclude the missing inputs of the library project by mapping them to nil in the `pb_project.yaml` file.\n\n**Can I run models which consider the input data within a specified time period?**\n\nYes, you can do so by using the `begin_time` and `end_time` parameters with the `run` command. For example, if you want to run the models for data from 2nd February, 2023, use:\n\n```\n$ pb run --begin_time 2023-01-02T12:00:00.0Z\n```\n\nIf you want to run the nmodels for data between 2 May 2022 and 30 April 2023, use:\n\n```\n$ pb run --begin_time 2022-05-01T12:00:00.0Z --end_time 2023-04-30T12:00:00.0Z\n```\n\nIf you want to run the models incrementally (run them from scratch ignoring any previous materials) irrespective of timestamp, use:\n\n```\n$ pb run --rebase_incremental\n```\n\n**Is it possible to run the feature table model independently, or does it require running alongside the ID stitcher model?**\n\nYou can provide a specific timestamp while running the project, instead of using the default latest time. PB recognizes if you have previously executed an identity stitching model for that time and reuses that table instead of generating it again.\n\nYou can execute a command similar to: `pb run --begin_time 2023-06-02T12:00:00.0Z --end_time 2023-06-03T12:00:00.0Z`. Note that:\n\n*   To reuse a specific identity stitching model, the timestamp value must match exactly to when it was run.\n*   If you have executed identity stitching model in the incremental mode and do not have an exact timestamp for reusing it, you can select any timestamp **greater** than a non-deleted run. This is because subsequent stitching takes less time.\n*   To perform another identity stitching using PB, pick a timestamp (for example, `1681542000`) and stick to it while running the feature table model. For example, the first time you execute `pb run --begin_time 2023-06-02T12:00:00.0Z --end_time 2023-06-03T12:00:00.0Z`, it will run the identity stitching model along with the feature models. However, in subsequent runs, it will reuse the identity stitching model and only run the feature table models.\n\n**While trying to add a feature table, I get an error at line 501, but I do not have these many lines in my YAML.**\n\nThe line number refers to the generated SQL file in the output folder. Check the console for the exact file name with the sequence number in the path.\n\n**While creating a feature table, I get this error:** `Material needs to be created but could not be: processing no result iterator: 001104 (42601): Uncaught exception of type 'STATEMENT ERROR': 'SYS _W. FIRSTNAME' in select clause is neither an aggregate nor in the group by clause.`\n\nThis error occurs when you use a window function `any_value` that requires a window frame clause. For example:\n\n```\n  - entity_var:\n      name: email\n      select: LAST_VALUE(email)\n      from: inputs/rsIdentifies\n      window:\n        order_by: \n        - timestamp desc\n```\n\n**Is it possible to create a feature out of an identifier? For example, I have a RS user\\_main\\_id with two of user\\_ids stitched to it. Only one of the user\\_ids has a purchase under it. Is it possible to show that user\\_id in the feature table for this particular user\\_main\\_id?**\n\nIf you know which input/warehouse table served as the source for that particular ID type, then you can create features from any input and also apply a `WHERE` clause within the entity\\_var.\n\nFor example, you can create an aggregate array of user\\_id’s from the purchase history table, where total\\_price > 0 (exclude refunds, for example). Or, if you have some LTV table with user\\_id’s, you could exclude LTV < 0.\n\n**Is it possible to reference an input var in another input var?**\n\nYes - input vars are similar to adding additional columns to the original table. You can use an input var `i1v1` in the definition of input var `i1v2` as long as both input vars are defined in the same input (or SQL model) `i1`.\n\n**I have not defined any input vars on I1. Why is the system still creating I1\\_var\\_table?**\n\nWhen you define an entity var using I1, an internal input var (for entity’s `main_id`) is created which creates `I1_var_table`. RudderStack team is evaluating whether internal input vars should create the var table or not.\n\n**I have an input model I1. Why is the system creating Material\\_I1\\_var\\_table\\_XXXXXX\\_N?**\n\nThis material table is created to keep the input vars defined on I1.\n\n**I am trying to run a single `entity_var` model. How should I reference it?**\n\nThe right way to reference an entity var is: `entity/<entity-name>/<entity-var-name>`.\n\n**I have two identical named fields in two `user` tables and I want my Profiles project to pick the most recently updated one (from either of the `user` tables). What is the best way to do it?**\n\nDefine different `entity_vars` (one for each input) and then pick the one with a non-null value and higher priority.\n\n**What does running material mean?**\n\nIt means that the output (material) table is being created in your warehouse. For example, an output table named `material_user_id_stitcher_3acd249d_21` would mean:\n\n*   `material`: Prefix for all the objects created by Profiles in your warehouse, such as ID stitcher and feature tables.\n*   `user_id_stitcher`: View created in your schema. It will always point to latest ID stitcher table. This name is the same as defined in the `models/profiles.yaml` file.\n*   `3acd249d`: Unique hash which remains the same for every model unless you make any changes to the model’s config, inputs or the config of model’s inputs.\n*   `21`: Sequence number for the run. It is a proxy for the context timestamp. Context timestamp is used to checkpoint input data. Any input row with `occured_at` timestamp value greater than the context timestamp cannot be used in the associated run.\n\n* * *\n\n## YAML\n\n**Are there any best practices I should follow when writing the PB project’s YAML files?**\n\n*   Use spaces instead of tabs.\n*   Always use proper casing. For example: id\\_stitching, and not id\\_Stitching.\n*   Make sure that the source table you are referring to, exists in data warehouse or data has been loaded into it.\n*   If you’re pasting table names from your Snowflake Console, remove the double quotes in the `inputs.yaml` file.\n*   Make sure your syntax is correct. You can compare with the sample files.\n*   Indentation is meaningful in YAML, make sure that the spaces have same level as given in sample files.\n\n**How do I debug my YAML file step-by-step?**\n\nYou can use the `--model_args` parameter of the `pb run` command to do so. It lets you run your YAML file till a specific feature/tablevar. For example:\n\n```\n$ pb run -p samples/attribution --model_args domain_profile:breakpoint:blacklistFlag\n```\n\nSee [run command](https://www.rudderstack.com/docs/archive/profiles/0.14/cli-user-guide/commands/#run) for more information.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> This is only applicable to versions prior to v0.9.0.\n\n**Can I use double quotes when referencing another entity\\_var in a macro?**\n\nYou can use an escape character. For example:\n\n```\n  - entity_var:\n      name: days_since_last_seen\n      select: \"{{macro_datediff('{{user.Var(\\\"max_timestamp_bw_tracks_pages\\\")}}')}}\"\n```\n\nAlso, if have a case statement, then you can add something like the following:\n\n`select: CASE WHEN {{user.Var(\"max_timestamp_tracks\")}}>={{user.Var(\"max_timestamp_pages\")}} THEN {{user.Var(\"max_timestamp_tracks\")}} ELSE {{user.Var(\"max_timestamp_pages\")}} END`\n\n**Is it possible to define default arguments in macros?**\n\nNo, RudderStack does not support default arguments in macros.\n\n* * *\n\n## ML/Python Models\n\n**Despite deleting WhtGitCache folder and adding keys to siteconfig, I get this error:** `Error: loading project: populating dependencies for project:base_features, model: churn_30_days_model: getting creator recipe while trying to get ProjectFolder: fetching git folder for git@github.com:rudderlabs/rudderstack-profiles-classifier.git: running git plain clone: repository not found`\n\nIf your token is valid, then replace `git@github.com:rudderlabs/rudderstack-profiles-classifier.git` with `https://github.com/rudderlabs/rudderstack-profiles-classifier.git` in the `profile-ml` file.\n\n**Why is my Profiles project taking so long to run?**\n\nThe first Profiles project run usually takes longer, especially if you are building predictive features.\n\n**I am debugging an error in ML models where I see a view with the model name, without material/hash prefix and suffix but it does not get refreshed even after all the entity vars are created and the material\\_<feature\\_table\\_model> table is also created. What might be the reason?**\n\nIt is because this view is now moved to `PostProjectRunCb`, meaning, it is created async after material Force run step.\n\n* * *\n\n## Activation API\n\n**While using Redis destination, I am facing an error: `These sample records were rejected by the destination`**?\n\nThis error is observed if you have enabled **Cluster mode** setting for Redis in the [RudderStack’s configuration settings](https://www.rudderstack.com/docs/destinations/streaming-destinations/redis/#connection-settings) but you are on the Redis free plan.\n\nTo overcome this, ensure that the Redis plan you are using allows clustering. Alternatively, you can turn off the **Cluster mode** setting.\n\n**Does the user-profiles API (old) and activation API (new) behave differently in updating a key that maps to two different primary keys? For example:**\n\n| Primary key | user\\_id | Feature\\_1 | Feature\\_2 |\n| --- | --- | --- | --- |\n| PK1 | U1  | F1  | null |\n| PK2 | U1  | null | F2  |\n\nUser profiles API\n\n```\n{\n  \"userId\": \"U1\",\n  \"profile\": {\n    \"feature_1\": \"F1\",\n    \"feature_2\": \"F2\"\n  }\n}\n```\n\nActivation API\n\n```\n{\n  \"entity\": \"entity_name\",\n  \"id\": {\n    \"type\": \"user_id\",\n    \"value\": \"U1\"\n  },\n  \"data\": {\n    \"model_name\": {\n      \"feature_1\": null,\n      \"feature_2\": F2\n    }\n  }\n}\n```\n\nIn user profiles API, RudderStack updates the value for a specific key (that is, feature\\_1 in this case). In activation API, RudderStack syncs the entire row as value for the `model_name` key.\n\n**Is it possible to use the Activation API without any Profiles project?**\n\nNo, the Activation API can only be used with a Profiles project and not on any of your non-Profiles output tables.\n\n**I have toggled on the Activation API option in the RudderStack dashboard to generate a Reverse ETL pipeline (connected to the Redis destination) and have defined a single ID in the `feature_views` key. However, two Reverse ETL pipelines are generated on running the project. Which one should I use and what is the difference between the two?**\n\nProfiles generates two `feature_views` models if you define a single ID under the `feature_views` key. One is the default feature view with `main_id` as the identifier and the other is based on the identifier you have defined.\n\nRudderStack assigns the default names to the view such as `user_feature_view` (default one with `main_id` as the identifier), or `feature_view_with_email` (email as the identifier), etc. You can also specify the final view’s name in the `name` key.\n\n## Profiles UI\n\n**I have included some features in the RudderStack dashboard while creating the Profiles project but when I click “download this project”, my project files does not include any feature. What might be the reason?**\n\nIf you have selected pre-defined features from any library project, they are referred to as `profiles-multieventstream-features` in the project by default.\n\nIf you have created any features using the custom feature functionality, they will be a part of your `models/resources.yaml` file. \n\n**While choosing pre-defined features in the RudderStack dashboard, I can preview code for only some of the features. What might be the reason?**\n\nYou can preview the code only for entity var based features. This functionality is not available for features built from ML and SQL models.\n\n**While creating a Profiles project by importing from Git, I dont see any warehouse options in the dropdown selector in the `Validate Profiles project` section. What might be the reason?**\n\nA Profiles project looks for the supported warehouse destinations configured for that workspace. Hence, make sure you have configured any of the following [warehouse destinations](https://www.rudderstack.com/docs/destinations/warehouse-destinations/) in your RudderStack dashboard:\n\n*   Snowflake\n*   Databricks\n*   Redshift\n*   BigQuery\n\n**Why am I not able to see the Concurrency option in the Settings tab of my Profiles project?**\n\nRudderStack supports the **Concurrency** option only for the Snowflake warehouse currently. You will not be able to see this option if you have configured your Profiles project using the Redshift, BigQuery, or Databricks warehouse.\n\n**I have chosen some pre-defined predictive features while creating a Profiles project in the RudderStack dashboard but my project fails on running. What might be the reason?**\n\nOne of the probable reasons could be the lack of adequate data in your input source. Try following the steps suggested in the error message. In case the issue still persists, [contact](https://rudderstack.com/join-rudderstack-slack-community) our support team.\n\n## Miscellaneous\n\n**Why am I getting _Authentication FAILED_ error on my data warehouse while executing the run/compile commands?**\n\nSome possible reasons for this error might be:\n\n*   Incorrect warehouse credentials.\n*   Insufficient user permissions to read and write data. You can ask your administrator to change your role or grant these privileges.\n\n**Why am I getting _Object does not exist or not authorized_ error on running this SQL query: `SELECT * FROM \"MY_WAREHOUSE\".\"MY_SCHEMA\".\"Material_domain_profile_c0635987_6\"`?**\n\nYou must remove double quotes from your warehouse and schema names before running the query, that is `SELECT * FROM MY_WAREHOUSE.MY_SCHEMA.Material_domain_profile_c0635987_6`.\n\n**Is there a way to obtain the timestamp of any material table?**\n\nYes, you can use the `GetTimeFilteringColSQL()` method to get the timestamp column of any material. It filters out rows based on the timestamp. It returns the `occurred_at_col` in case of an event\\_stream table or `valid_at` in case the material has that column. In absense of both, it returns an empty string. For example:\n\n```\n  SELECT * FROM {<from_material>}\n    WHERE\n      <from_material>.GetTimeFilteringColSQL() > <some_timestamp>;\n```\n\n**What is the difference between setting up Profiles in the RudderStack dashboard and Profile Builder CLI tool?**\n\nYou can run Profiles in the RudderStack dashboard or via [Profile Builder CLI](https://www.rudderstack.com/docs/archive/profiles/0.14/get-started/profile-builder/).\n\nThe main difference is that the RudderStack dashboard only generates outputs based on the pre-defined templates. However, you can augment those outputs by downloading the config file and updating it manually.\n\nOn the other hand, the CLI tool lets you achieve the end-to-end flow via creating a Profile Builder project.\n\n**Does the Profiles tool have logging enabled by default for security and compliance purposes?**\n\nLogging is enabled by default for nearly all the commands executed by CLI (`init`, `validate access`, `compile`, `run`, `cleanup`, etc.). Logs for all the output shown on screen are stored in the file `logfile.log` in the **logs** directory of your project folder. This includes logs for both successful and failed runs. RudderStack appends new entries at the end of the file once a command is executed.\n\nSome exceptions where the logs are not stored are:\n\n*   `query`: The logger file stores the printing output and does not store the actual database output. However, you can access the SQL queries logs in your warehouse.\n*   `help`: For any command.\n\n**In the warehouse, I see lots of material\\_user\\_id\\_stitcher\\_ tables generated in the rs\\_profiles schema. How do I identify the latest ID stitched table?**\n\nThe view `user_id_stitcher` will always point to the latest generated ID stitcher. You may check its definition to see the exact table name it is referring to.\n\n**How can I remove the material tables that are no longer needed?**\n\nTo clean up all the materials older than a specific duration, for example 10 days, execute the following command:\n\n```\npb cleanup materials -r 10\n```\n\nThe minimum value you can set here is `1`. So if you have run the ID stitcher today, then you can remove all the older materials using `pb cleanup materials -r 1`.\n\n**Which tables and views are important in Profiles schema that should not be deleted?**\n\n*   `material_registry`\n*   `material_registry_<number>`\n*   `pb_ct_version`\n*   `ptr_to_latest_seqno_cache`\n*   `wht_seq_no`\n*   `wht_seq_no_<number>`\n*   Views whose names match your models in the YAML files.\n*   Material tables from the latest run (you may use the `pb cleanup materials` command to delete materials older than a specific duration).\n\n**I executed the auto migrate command and now I see a bunch of nested** `original_project_folder`. **Are we migrating through each different version of the tool?**\n\nThis is a symlink to the original project. Click on it in the Finder (Mac) to open the original project folder.\n\n**I am getting a **`ssh: handshake failed`** error when referring to a public project hosted on GitHub. It throws error for https:// path and works fine for ssh: path. I have set up token in GitHub and added to siteconfig.yaml file but I still get this error.**\n\nYou need to follow a different format for `gitcreds:` in siteconfig. See [SiteConfiguration](https://www.rudderstack.com/docs/archive/profiles/0.14/cli-user-guide/structure/#site-configuration-file-configuration.md) for the format.\n\nAfter changing `siteconfig`, if you still get an error, then clear the `WhtGitCache` folder inside the directory having the `siteconfig` file.\n\n**If I add filters to** `id_types` **in the project file, then do all rows that include any of those values get filtered out of the analysis, or is it just the specific value of that id type that gets filered?**\n\nThe PB tool does not extract rows. Instead, it extracts pairs from rows.\n\nSo if you had a row with email, user\\_id, and anonymous\\_id and the anonymous\\_id is excluded, then the PB tool still extracts the email, user\\_id edge from the row.\n\n**In the material registry table, what does** `status: 2` **mean?**\n\n*   `status: 2` means that the material has successfully completed its run.\n*   `status: 1` means that the material did not complete its run.\n\n**I am using Windows and get the following error:** `Error: while trying to migrate project: applying migrations: symlink <path>: A required privilege is not held by the client`.\n\nYour user requires privileges to create a symlink. You may either grant extra privileges to the user or try with a user containing Admin privileges on PowerShell. In case that doesn’t help, try to install and use it via WSL (Widows subsystem for Linux).\n\n**Can I specify any git account like CommitCode while configuring a project in the web app?**\n\nProfiles UI supports repos hosted on GitHub, BitBucket and GitLab.\n\n**If I want to run multiple select models, how can I run something like: `pb run --model_refs \"models/ewc_user_id_graph_all, models/ewc_user_id_graph, models/ewc_user_id_graph_v2`**\n\nYou can do so by passing `--model_refs` multiple times per model:\n\n`pb run -p samples/test_feature_table --model_refs 'models/test_id__, user/all' --migrate_on_load` OR `pb run -p samples/test_feature_table --model_refs models/test_id__ --model_refs user/all --migrate_on_load`\n\n**How can I keep my Profiles projects up to date along with updating the Python package and migrating the schema version?**\n\nYou can check for the latest Profiles updates in the [changelog](https://www.rudderstack.com/docs/archive/profiles/0.14/changelog/).\n\nTo update the Python package and migrate the schema version, you can standardise on a single pip release across the org and use the schema version that is native to that binary. When you move to a different binary, migrate your projects to the schema version native to it.\n\nContact Profiles support team in our [Community Slack](https://rudderstack.com/join-rudderstack-slack-community) for specific questions.\n\n**I am facing this error on adding a custom ID `visitor_id` under the `id_types` field in the `pb_project.yaml` file:**\n\n`could not create project: failed to read project yaml Error: validating project sample_attribution: getting models for folder: user: error listing models: error building model user_default_id_stitcher: id type visitor_id not in project id types`\n\nWhile adding a custom ID type, you must extend the package to include its specification in the `pb_project.yaml` file as well. In this case, add the key `extends:` followed by name of the same/different id\\_type that you wish to extend, and corresponding filters with include/exclude values like below:\n\n```\nid_types:\n - name: visitor_id\n   extends: visitor_id\n  filters:\n   - type: exclude\n     value: \"someexcludedvalue\"\n```\n\n**Can I keep multiple projects in a Git Repo?**\n\nYes, you can create multiple folders in your project repo and keep different projects in each folder. While running the project, you can use any suitable URL to run a specific project:\n\n`https://github.com/<org-name>/<repo-name>/tree/<branch-name>/path/to/project` `https://github.com/<org-name>/<repo-name>/tag/<tag-name>/path/to/project` `https://github.com/<org-name>/<repo-name>/commit/<commit-hash>/path/to/project`\n\nSee [Supported Git URLs](https://www.rudderstack.com/docs/archive/profiles/0.14/example/packages/#supported-git-urls) for more information. **Can a models folder contain subfolders?**\n\nYes, you can manually add subfolders to the models folder and reference their path in the `pb_project.yaml` file:\n\n```\nmodel_folders:\n  - models/inputs\n  - models/inputs/web.yml\n```\n\nA sample folder structure is shown:\n\n```\n.\n├── models/\n│   ├── inputs/\n|   │   ├── web.yml\n|   │   ├── mobile.yml\n|   │   └── server.yml\n│   └── ...\n```\n\n**How is Activations different from Audiences?**\n\nActivations qualify as Audiences with a minor exception of having a Profiles project as a source instead of a Reverse ETL source (with schema, database, table etc).\n\n**I am running a Profiles project with the `timegrains` parameter and noticed that multiple subfolders having different `seq_no` are generated. Which `seq_no` should I use to resume an earlier run?**\n\nFor the CLI project, you can resume the project run using CLI commands(like `run`, `compile`, etc.) and passing the `--seq_no` displayed at the top of the terminal output. For the UI project, you cannot choose to stop/resume the project run.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Profiles FAQ | RudderStack Docs",
    "description": "Commonly asked questions on RudderStack Profiles.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.13/overview/",
    "markdown": "# Profiles Overview | RudderStack Docs\n\nCreate unified customer records in your warehouse using RudderStack Profiles.\n\nAvailable Plans\n\n*   enterprise\n\n* * *\n\n*     3 minute read  \n    \n\nModern data teams rely on their warehouse as a single source of truth for customer data. RudderStack’s **Profiles** unifies every user touchpoint and trait into comprehensive customer profiles, establishing the data warehouse as the core of the customer data platform.\n\nWith Profiles, data teams can efficiently resolve identities and create user features to produce a comprehensive customer 360 table.\n\nThe following self-guided tour shows how to use Profiles:\n\n## Highlights\n\nSee the following guides to learn more about Profiles features and their usage:\n\n| Guide | Description |\n| --- | --- |\n| [Quickstart](https://www.rudderstack.com/docs/archive/profiles/0.13/get-started/) | Create your first Profiles project using the [RudderStack dashboard](https://www.rudderstack.com/docs/archive/profiles/0.13/get-started/quickstart-ui/) or [Profile Builder (PB) CLI tool](https://www.rudderstack.com/docs/archive/profiles/0.13/get-started/profile-builder/). |\n| [Identity stitching](https://www.rudderstack.com/docs/archive/profiles/0.13/core-concepts/identity-stitching/) | Stitch different identifiers across multiple channels to create a comprehensive user profile. |\n| [Feature development](https://www.rudderstack.com/docs/archive/profiles/0.13/core-concepts/feature-development/) | Enhance the unified profiles with additional data points and features. |\n| [Warehouse permissions](https://www.rudderstack.com/docs/archive/profiles/0.13/permissions/) | Grant RudderStack the required permissions on your data warehouse. |\n| [Project structure](https://www.rudderstack.com/docs/archive/profiles/0.13/cli-user-guide/structure/) | Know the detailed project structure of a Profiles project. |\n| [Commands](https://www.rudderstack.com/docs/archive/profiles/0.13/cli-user-guide/commands/) | List of commands you can use for the Profiles CLI project. |\n| [Cohorts](https://www.rudderstack.com/docs/archive/profiles/0.13/cohorts/) | Build core customer segments and use them for targeted campaigns. |\n| [Activations](https://www.rudderstack.com/docs/archive/profiles/0.13/activations/) | Activate your cohorts data in the downstream destinations. |\n| [Predictions](https://www.rudderstack.com/docs/archive/profiles/0.13/predictions/) | Create predictive features using the data present in your warehouse. |\n| [Examples](https://www.rudderstack.com/docs/archive/profiles/0.13/example/) | Create sample Profiles projects using different model types. |\n| [Glossary](https://www.rudderstack.com/docs/archive/profiles/0.13/resources/glossary/) | Commonly used Profiles terminology. |\n\n## Why use Profiles?\n\nData teams often face challenges when building a comprehensive customer view. Maintaining large, complex SQL models or working around the limitations of rigid SaaS platforms is time-consuming and expensive at scale.\n\nProfiles simplifies this process of unifying customer data by automating the manual data engineering and modeling required to build an identity graph, layer new data sources into customer profiles, and compute user features that leverage data from diverse sources.\n\nUsing Profiles, data teams can quickly build and easily maintain a comprehensive customer 360 table and make it available to downstream teams and tools.\n\n#### Move faster with an end-to-end platform\n\nProfiles integrates directly with RudderStack’s other pipelines:\n\n*   [Event Stream](https://www.rudderstack.com/docs/sources/event-streams/) and [Cloud Extract](https://www.rudderstack.com/docs/sources/extract/) (ETL) pipelines have known schemas and unique identifiers through the ingested customer data. Profiles can produce a baseline identity graph, and user features out of the box. Data teams can then augment the graph and features using any other data in their warehouse.\n*   [Reverse ETL](https://www.rudderstack.com/docs/sources/reverse-etl/) pipeline makes it easy to send data from the customer 360 table directly to the downstream tools used by marketing, customer success, product, and other teams.\n\n#### Enrich user profiles with features\n\nYou can [enhance user profiles](https://www.rudderstack.com/docs/profiles/core-concepts/feature-development/) with additional data points and features. When new data sources are added, discovered, or calculated, data teams can add them to their Profiles configuration without having to clean data and update complex models and dependencies.\n\nThe features/traits can include demographic information, preferences, purchase history, browsing behavior, or other static or computed data points.\n\n#### Unlock deeper insights\n\nProfiles extends its capabilities to support features derived from complex concepts such as funnels, organizational metrics, and machine learning models. You can understand your customer’s journey through your sales funnel or locate each user across the histogram of customer metric values by simply defining a trait.\n\n#### Deliver personalization and recommendations\n\nUsing Profiles, you can ship projects like personalization significantly faster by focusing entirely on activating key user features instead of cleaning and modeling data to build them.\n\n#### Predict user conversions and churn\n\nYou can leverage [Predictions](https://www.rudderstack.com/docs/archive/profiles/0.13/predictions/) to build predictive features that help you predict in advance whether a lead is likely to convert, or a customer is likely to churn or make a purchase.\n\n## Who can leverage Profiles?\n\nProfiles is built for data engineers, data scientists, and technical marketers. You can define identity stitching, and user features as configuration files without requiring deep SQL, Python, or technical knowledge.\n\nYou can define the associated properties or attributes that provide detailed information for each new feature. For example, if the feature is `purchase_history`, its properties can include the date of purchase, product category, or order value.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Profiles Overview | RudderStack Docs",
    "description": "Create unified customer records in your warehouse using RudderStack Profiles.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.13/how-profiles-works/",
    "markdown": "# How Profiles Works | RudderStack Docs\n\nKnow how Profiles collect, unify, and activate your data to enhance the overall customer experience.\n\n* * *\n\n*     3 minute read  \n    \n\nRudderStack helps you build a complete CDP on top of your data warehouse in three stages - **Collect**, **Unify**, and **Activate**.\n\nThe following sections highlight RudderStack’s comprehensive solution at every stage to create a complete customer profile.\n\n[![Profiles Overview](https://www.rudderstack.com/docs/images/profiles/profiles-overview.png)](https://www.rudderstack.com/docs/images/profiles/profiles-overview.png)\n\n### Collect\n\nFirst, RudderStack collects and stores all the source data in your warehouse. This includes:\n\n*   Event data, for example, user interaction data from web and mobile apps.\n*   Data from cloud sources, for example, CRM platforms like Salesforce, support tools like Zendesk, etc.\n\n#### Known data\n\nRudderStack collects all the information from:\n\n1.  First-party data: Data collected from the enterprise’s own mobile application, websites, POS systems, etc.\n2.  Third-party apps like SalesForce CRM, Zendesk Support, ecommerce payments via Stripe, etc.\n\nThere is a known ID for all of these by which RudderStack collects the data like email, user ID, etc.\n\n#### Unknown Data\n\nThis includes unknown user attributes like `anonymousId` captured from the RudderStack SDKs on the web/mobile apps. It is helpful in tracking user activities in cases where they are not logged in.\n\nThe difference between known and unknown data is that in the former, we have information about the user. First-party data can be known data if a user is logged in. For example, the data from cloud sources will always be known. However, data from the event stream sources can be known or unknown depending on whether a user had logged in.\n\nAs the data is collected, you can apply relevant transformations to it for compliance/security purposes like data governance, privacy, etc.\n\nThe below image highlights a snapshot of the `identifies` and `tracks` [tables](https://www.rudderstack.com/docs/destinations/warehouse-destinations/warehouse-schema/#schema), that RudderStack leverages for unifying the data.\n\n[![Identifies and Tracks tables for stitching](https://www.rudderstack.com/docs/images/profiles/identifies-tracks-stitch.png)](https://www.rudderstack.com/docs/images/profiles/identifies-tracks-stitch.png)\n\n### Unify\n\nAt this stage, Profiles takes over and does the following:\n\n#### ID Stitching\n\nProfiles stitches together all the known and unknown IDs into a single table. The IDs are linked using an autogenerated ID known as `rudderId`, which is akin to a golden record. Imagine a 1-to-many relationship, in which one `rudderId` has multiple values for other IDs like user ID, anonymous ID, email, etc.\n\nWith a `rudderId`, you can easily identify that a customer - who shopped on your website 6 months ago, anonymously browsed from mobile 4 months ago, raised a complaint with the support team 2 months ago - is actually the same customer. RudderStack represents them as different nodes/edges of the ID stitching graph.\n\n[![Identity stitching](https://www.rudderstack.com/docs/images/profiles/identity-stitching.png)](https://www.rudderstack.com/docs/images/profiles/identity-stitching.png)\n\n#### Feature Views\n\nIf the entity features/traits are spread across multiple entity vars and ML models, you can use Feature views to get them together into a single view. These models are usually defined in the `pb_project.yaml` file by creating entries under `feature_views` key with corresponding entity.\n\n[![Features generation](https://www.rudderstack.com/docs/images/profiles/features-generation.png)](https://www.rudderstack.com/docs/images/profiles/features-generation.png)\n\n#### Feature Table\n\nThe entity vars specified in the project are unified into a view. A **Feature Table** is a unified customer profile containing useful information for each customer. Once all the known and unknown identities are stitched together, you can trace back activities for all such identifiers and aggregate them under the common `rudderId`. This is helpful in calculating features across all such interactions.\n\nSome common use cases are computing the customer’s total LTV (lifetime value), purchase history, number of days a customer was active, etc.\n\n### Activate\n\nActivation is a two-step process. In the first step, the user creates the target audience using RudderStack’s [Audiences](https://www.rudderstack.com/docs/data-pipelines/reverse-etl/features/audiences/) feature. They would then use Reverse ETL to route this audience information (also persisted in the same cloud warehouse) to downstream marketing tools like Braze, Mailchimp, etc.\n\n[![Audience Builder / Cohorts](https://www.rudderstack.com/docs/images/profiles/audience-builder-cohorts.png)](https://www.rudderstack.com/docs/images/profiles/audience-builder-cohorts.png)\n\nAdditionally - you can use the feature tables as inputs to ML models for use cases like churn prediction, lifetime value (LTV) prediction, etc. Again, you can route the output of such models to marketing platforms via Reverse ETL.\n\nAs you keep collecting data, Profiles continues to unify and send it for activation.\n\n[![Profiles lifecycle](https://www.rudderstack.com/docs/images/profiles/profiles-life-cycle.png)](https://www.rudderstack.com/docs/images/profiles/profiles-life-cycle.png)\n\nIn this way, you can make informed decisions, run personalized marketing campaigns, and enhance the overall customer experience across multiple platforms.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "How Profiles Works | RudderStack Docs",
    "description": "Know how Profiles collect, unify, and activate your data to enhance the overall customer experience.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.13/",
    "markdown": "# Profiles 0.13 | RudderStack Docs\n\nPrivacy Overview\n\nThis site uses cookies to improve your experience while you navigate through the website. Out of these cookies, the cookies that are categorized as necessary are stored on your browser as they are as essential for the working of basic functionalities of the website. We also use third-party cookies that help us analyze and understand how you use this website. These cookies will be stored in your browser only with your consent. You also have the option to opt-out of these cookies. But opting out of some of these cookies may have an effect on your browsing experience.\n\nNecessary cookies are absolutely essential for the website to function properly. This category only includes cookies that ensures basic functionalities and security features of the website. These cookies do not store any personal information.",
    "title": "Profiles 0.13 | RudderStack Docs",
    "description": "Documentation for Profiles v0.13",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.13/get-started/",
    "markdown": "# Quickstart | RudderStack Docs\n\nPrivacy Overview\n\nThis site uses cookies to improve your experience while you navigate through the website. Out of these cookies, the cookies that are categorized as necessary are stored on your browser as they are as essential for the working of basic functionalities of the website. We also use third-party cookies that help us analyze and understand how you use this website. These cookies will be stored in your browser only with your consent. You also have the option to opt-out of these cookies. But opting out of some of these cookies may have an effect on your browsing experience.\n\nNecessary cookies are absolutely essential for the website to function properly. This category only includes cookies that ensures basic functionalities and security features of the website. These cookies do not store any personal information.",
    "title": "Quickstart | RudderStack Docs",
    "description": "Know the different ways to create a Profiles project.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.14/resources/glossary/",
    "markdown": "# Glossary | RudderStack Docs\n\nFamiliarize yourself with the commonly used terms across RudderStack Profiles.\n\n* * *\n\n*     7 minute read  \n    \n\n## Cohorts\n\nA Cohort refers to a subset of instances of an entity meeting a specified set of characteristics, behaviors, or attributes. Using cohorts, you can define core customer segments and drive targeted marketing campaigns and deliver personalized experiences.\n\nFor example, you can define a cohort for users who purchased items in the last 90 days, users based in a specific geographical location, etc.\n\n## Custom Models (Python)\n\nOne can build custom Python models for ML by downloading pre-defined Python templates. The results are usually saved as attributes of related entities (for example, `churnProbability`).\n\nPython models and pymodels work differently. In case of Python, developers (both RudderStack and external) develop new model types in python using [profiles-rudderstack](https://pypi.org/project/profiles-rudderstack/) package. An example python package implementing a number of Python models is [profiles-pycorelib](https://pypi.org/project/profiles-pycorelib/).\n\n## Edge sources\n\nThe `edge_sources` field provides the input sources for an identity stitching model. You can specify it in the `models/profiles.yaml` file to list the input sources defined in the `inputs.yaml` file.\n\n## Entity\n\nEntity refers to a digital representation of a class of real world distinct objects for which you can create a profile. An entity can be a user, account, customer, product, or any other object that requires profiling.\n\n## `Entity var`/Entity features\n\nThese are various attributes related to an entity whose profile you are trying to create. For example, they can be `name`, `city`, `LastVisitTimestamp`, etc. for the `user` entity. Each attribute is called an `entity_var`, and it is derived by performing calculation or aggregation on a set of values. Together, all the attributes create a complete picture of the entity. By default, every `entity_var` gets stored as a feature, such as `days_active`, `last_seen`, etc.\n\n## Feature Views\n\nIf the features/traits of an entity are spread across multiple entity vars and ML models, you can use Feature Views to get them together into a single view. These models are usually defined in `pb_project.yaml` file by creating entries under `feature_views` key with corresponding entity.\n\n## Features\n\nFeatures are inputs for the machine learning model. In a general sense, features are pieces of user information we already know. For example, number of days they opened the app in the past week, items they left in the cart, etc.\n\n## Feature tables (legacy)\n\nFeature tables are the outputs based on events, user attributes, and other defined criteria across any data set in your data warehouse. You can define models that can create feature tables for users with ID stitching, ML notebooks and external sources, etc.\n\n## ID Stitcher\n\nData usually comes from different sources and these sources may assign different IDs. To track a user’s journey (or any other entity) uniquely across all these data sources, we need to stitch together all these IDs. ID stitching helps map different IDs of the same user (or any other entity) to a single canonical ID. It does this by doing connected component analysis over the Id-Id edge graph specified in its configuration.\n\n## ID Collator\n\nID Collator is similar to ID Stitcher. It is used when entity has only a single ID type associated (for example, session IDs). In these cases, connected component analysis is not required and we use a simpler model type called ID Collator. It consolidates all entity IDs from multiple input tables into a single collated list.\n\n## Inputs\n\nInputs refers to the input data sources used to create the material (output) tables in the warehouse. The inputs file (`models/inputs.yaml`) lists the tables/views you use as input sources, including the column name and SQL expression for retrieving the values.\n\nYou can use data from various input sources such as event stream (loaded from event data), ETL extract (loaded from Cloud Extract), and any existing tables in the warehouse (generated by external tools).\n\n## Input var\n\nInstead of a single value per entity ID, it represents a single value per row of an input model. Think of it as representing addition of an additional column to an input model. It can be used to define entity features. However, it is not itself an entity feature because it does not represent a single value per entity ID.\n\n## Label\n\nLabel is the output of the machine learning model and is the metric we want to predict. In our case, it is the unknown user trait we want to know in advance.\n\n## Machine learning model\n\nA machine learning model can be thought of as a function that takes in some input parameters and returns an output.\n\nUnlike regular programming, this function is not explicitly defined. Instead, a high level architecture is defined and several pieces are filled by looking at the data. This whole process of filling the gaps is driven by different optimisation algorithms as they try to learn complex patterns in the input data that explain the output.\n\n## Materialization\n\nMaterialization refers to the process of creating output tables/views in a warehouse by running models. You can define the following fields for materialization:\n\n*   `output_type`: Determines the type of output you want to create in your warehouse. Allowed values are:\n    \n    *   `table`: Output is built and stored in a table in the warehouse.\n    *   `view`: Output is built and stored as a view in the warehouse.\n    *   `ephemeral`: Output is created in the form of temporary data which serves as an intermediary stage for being consumed by another model.\n*   `run_type`: Determines the run type of models. Allowed values are:\n    \n    *   `discrete` (default): In this mode, the model runs in a full refresh mode, calculating its results from the input sources. A SQL model supports only the `discrete` run type.\n    *   `incremental`: In this mode, the model calculates its results from the previous run and only reads row inserts and updates from the input sources. It only updates or inserts data and does not delete anything making it efficient. However, only the identity stitching model supports this mode.\n\n## Material tables\n\nWhen you run the PB models, they produce materials - that is, tables/views in the database that contain the results of that model run. These output tables are known as material tables.\n\n## Predictions\n\nThe model’s output is called a prediction. A good model makes predictions that are close to the actual label. You generally need predictions where the labels are not available. In our case, most often the labels come a few days later.\n\n## Prediction horizon days\n\nThis refer to the number of days in advance when we make the prediction.\n\nFor example, statements like “A user is likely to sign-up in the next 30 days, 90 days, etc.” are often time-bound, that is, the predictions are meaningless without the time period.\n\n## Profile Builder (PB)\n\nProfile Builder (PB) is a command-line interface (CLI) tool that simplifies data transformation within your warehouse. It generates customer profiles by stitching data together from multiple sources. It takes existing tables or the output of other transformations as input to generate output tables or views based on your specifications.\n\n## PB project\n\nA PB project is a collection of interdependent warehouse transformations. These transformations are run over the warehouse to query the data for sample outputs, discover features in the warehouse, and more.\n\n## PB model\n\nAny transformation that can be applied to the warehouse data is called a PB model. RudderStack supports various types of models like ID stitching, feature tables, Python models, etc.\n\n## Schema versions\n\nEvery PB release supports a specific set of project schemas. A project schema determines the correct layout of a PB project, including the exact keys and their values in all project files.\n\n## SQL template models\n\nSometimes the standard model types provided by Profiles are insufficient to capture complex use cases. In such cases, RudderStack supports the use of SQL template models to explicitly templatize SQL.\n\nSQL template models can be used as an input to an entity-var/ input-var or as an edge-source in id-stitcher.\n\n## Timegrain\n\nUsing `time_grain` parameter for a model, you can restrict the context timestamp of that model to the specified time boundary. In other words, a feature v1 with `time_grain` value of a `day` will look at all the data up to UTC 00:00 hrs of any particular day only.\n\nIf you compute that feature at 3:00PM or 5:00PM, the result would be the same because its inputs change only at the very beginning of the day. Similarly, if `time_grain` for a model is set to a `week`, it needs to be run only once a week. Running it twice within the week won’t change its results.\n\n## Training\n\nTraining refers to the process of a machine learning model looking at the available data and trying to learn a function that explains the [labels](#label).\n\nOnce you train a model on historic users, you can use it to make predictions for new users. You need to keep retraining the model as you get new data so that the model continues to learn any emerging patterns in the new users.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Glossary | RudderStack Docs",
    "description": "Familiarize yourself with the commonly used terms across RudderStack Profiles.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.13/get-started/quickstart-ui/",
    "markdown": "# Profiles UI | RudderStack Docs\n\nCreate your Profiles project from the RudderStack dashboard.\n\n* * *\n\n*     5 minute read  \n    \n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> While creating a Profiles project, you can choose either of the below:\n> \n> *   **Profile Builder (PB) CLI** which gives you the flexibility to create, develop, and debug your Profiles project using various commands in fine detail. You can explore and implement the exhaustive list of features and functionalities offered by Profiles.\n> *   **Profiles UI** which provides a step-by-step intuitive workflow in the RudderStack dashboard. You can configure your project, schedule its run, explore the outputs and the user profiles.\n\nThis guide lists the detailed steps to create a Profiles project in the RudderStack dashboard.\n\n## Create Profiles project\n\n1.  Log in to the [RudderStack dashboard](https://app.rudderstack.com/) and go to **Unify** > **Profiles** option in the left sidebar.\n2.  Click **Create project**.\n\n[![Choose predefined template](https://www.rudderstack.com/docs/images/profiles/create-project.webp)](https://www.rudderstack.com/docs/images/profiles/create-project.webp)\n\n3.  Select **Basic Entity Setup** on the next screen.\n\n[![Choose predefined template](https://www.rudderstack.com/docs/images/profiles/basic-setup-profiles.webp)](https://www.rudderstack.com/docs/images/profiles/basic-setup-profiles.webp)\n\n4.  Enter a unique name and description for your Profiles project.\n5.  Select a data warehouse from the dropdown and the source(s) connected to the warehouse.\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> RudderStack currently supports the [Snowflake](https://www.rudderstack.com/docs/destinations/warehouse-destinations/snowflake/), [Redshift](https://www.rudderstack.com/docs/destinations/warehouse-destinations/redshift/), [Databricks](https://www.rudderstack.com/docs/destinations/warehouse-destinations/delta-lake/), and [BigQuery](https://www.rudderstack.com/docs/destinations/warehouse-destinations/bigquery/) warehouses for creating a Profiles project.\n> \n> You must connect an [event stream source](https://www.rudderstack.com/docs/sources/event-streams/) to one of the above warehouses and sync data at least once so that they populate in the dropdown.\n> \n> For example, in the below image, the Node event stream source (_es src_) is connected to the Snowflake warehouse destination (_cohort sample_ ) containing the schema _SAMPLE\\_SHOPIFY\\_DATA_.\n\n[![Choose predefined template](https://www.rudderstack.com/docs/images/profiles/source.webp)](https://www.rudderstack.com/docs/images/profiles/source.webp)\n\n6.  Click **Add mapping** and provide values to map the existing ID types from the above-selected event stream source:\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> You can see some predefined mappings for the identifiers which comes from the [Profiles library project](https://github.com/rudderlabs/profiles-multieventstream-features/tree/main).\n> \n> Here, **Event** column represents the tables present in the warehouse, **Property** column represents the column name in that table, and **ID type** represents the type of identifier.\n\n[![Choose predefined template](https://www.rudderstack.com/docs/images/profiles/map-id.webp)](https://www.rudderstack.com/docs/images/profiles/map-id.webp)\n\n7.  Define features either by adding a custom feature or selecting a pre-defined feature as shown:\n\n[![Choose predefined template](https://www.rudderstack.com/docs/images/profiles/define-features.webp)](https://www.rudderstack.com/docs/images/profiles/define-features.webp)\n\n*   To add a custom feature, click **Add a custom feature** and enter the relevant feature details:\n\n[![Choose predefined template](https://www.rudderstack.com/docs/images/profiles/custom-feature.webp)](https://www.rudderstack.com/docs/images/profiles/custom-feature.webp)\n\n*   To use a pre-defined feature, select the required feature from the **Template features library**.\n\n8.  Select the [schedule type](https://www.rudderstack.com/docs/sources/reverse-etl/sync-schedule-settings/).\n9.  Enter the warehouse details where you want to store this Profiles project.\n10.  Finally, review all the provided details and click **Create Profiles project**.\n\n### Convert to a Git project\n\nOnce you set up a Profiles project in the dashboard, you can convert it to a configuration-based Git repository by [uploading it either to GitHub, GitLab, or Bitbucket](https://www.rudderstack.com/docs/archive/profiles/0.13/example/packages/#supported-git-urls):\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> You cannot revert to your UI-based project after Git conversion. Once converted, all the project edits and updates can only be done via Git repository.\n\n1.  Navigate to the **Unify** > **Profiles** option in the left sidebar to view all your Profiles projects.\n2.  Click **Convert to Git** button next to the project you want to convert.\n3.  [Create a new Git repository](https://git-scm.com/book/en/v2/Git-Basics-Getting-a-Git-Repository).\n4.  Enter the [SSH URL of your Git repository](https://git-scm.com/book/en/v2/Git-on-the-Server-The-Protocols) to be used for your Profiles project and click **Continue**.\n5.  Copy the SSH public key and click **Add deploy key** which will take you to your git repository’s **Deploy keys** section. See deploy keys section in the [GitHub](https://docs.github.com/en/authentication/connecting-to-github-with-ssh/managing-deploy-keys#set-up-deploy-keys), [GitLab](https://docs.gitlab.com/ee/user/project/deploy_keys/), or [Bitbucket](https://bitbucket.org/blog/deployment-keys) documentation for more information.\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> Make sure your Git repository has at least one commit for successful validation.\n\n7.  Click **Add deploy key**, add a **Title**, paste your SSH public key, and click **Add key**.\n8.  Click **Continue** to let RudderStack verify read access.\n9.  Click **Download project** to download the Profiles project you set up in the dashboard.\n10.  Commit the downloaded project to your Git repository.\n11.  Once the project is committed successfully, return to the dashboard and click **Convert Profiles project**.\n\n## Run project\n\nOnce created, you can run your Profiles project using either of the following ways:\n\n*   Clicking **Run** in the **History** tab of the project.\n*   Programmatically using the [Profiles API](https://www.rudderstack.com/docs/api/profiles-api/).\n\n## Download project\n\nTo download the Profiles project, click the arrow icon corresponding to your Profiles project and click **Download this project**:\n\n[![Activation API](https://www.rudderstack.com/docs/images/profiles/cohorts-view-ui.webp)](https://www.rudderstack.com/docs/images/profiles/cohorts-view-ui.webp)\n\nOnce downloaded, you can view the [project folder structure](https://www.rudderstack.com/docs/archive/profiles/0.13/cli-user-guide/structure/), modify the files, or run various [commands](https://www.rudderstack.com/docs/archive/profiles/0.13/cli-user-guide/commands/) to execute the desired use-cases.\n\n## Project details\n\nTo view the Profile project details, click the arrow icon corresponding to your Profiles project:\n\n| Option | Description |\n| --- | --- |\n| **Entities** | Lists the entities, cohorts, features, activations, etc. for your Profiles project. |\n| **History** | Displays the history of Profile runs. |\n| **Settings** | Displays your profile settings and lets you delete your Profiles project. You can edit the project by clicking the edit icon next to each section. |\n\n### Profile details\n\nYou can also view the details of a specific profile in your Profiles project:\n\n1.  In your Profiles project’s **Entities** tab, click **View** button across the entity for which you want to see the profile:\n    \n    [![Activation API](https://www.rudderstack.com/docs/images/profiles/entity_view.webp)](https://www.rudderstack.com/docs/images/profiles/entity_view.webp)\n    \n2.  Click **Profile Lookup** tab to search a profile record.\n    \n3.  Type an available unique identifier like `email`, `phone number`, `user id`, `anonymous id` etc. and click **Search user profile**.\n    \n    [![Activation API](https://www.rudderstack.com/docs/images/profiles/profiles-lookup.webp)](https://www.rudderstack.com/docs/images/profiles/profiles-lookup.webp)\n    \n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> Note that:\n> \n> *   It can take a few minutes for the data preview to show up in your profile’s **History** tab.\n> *   If you keep getting a blank screen, it may be because you do not have sufficient access. Make sure you have a [Connections Admin](https://www.rudderstack.com/docs/dashboard-guides/user-management/#resource-roles) resource role with [access to PII](https://www.rudderstack.com/docs/dashboard-guides/data-management/#limiting-access-to-pii-related-features). In case the problem persists, contact [RudderStack support](mailto:support@rudderstack.com).\n\n## FAQ\n\n**When trying to fetch data for a lib project, then data/columns are shown as blank. What should I do?**\n\nYou’ll need to sync data from a source to a destination. If data is synced from the source you are using and not from some pre-existing tables in the destination, the missing column/data issues should not occur.\n\n**I am not able to see Unify tab on the web app though I have admin privileges. What should I do?**\n\nDisable any adblockers on your web browser.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Profiles UI | RudderStack Docs",
    "description": "Create your Profiles project from the RudderStack dashboard.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.13/get-started/profile-builder/",
    "markdown": "# Profile Builder CLI | RudderStack Docs\n\nCreate a Profiles project using the Profile Builder (PB) tool.\n\n* * *\n\n*     10 minute read  \n    \n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> While creating a Profiles project, you can choose either of the below:\n> \n> *   **Profile Builder (PB) CLI** which gives you the flexibility to create, develop, and debug your Profiles project using various commands in fine detail. You can explore and implement the exhaustive list of features and functionalities offered by Profiles.\n> *   **Profiles UI** which provides a step-by-step intuitive workflow in the RudderStack dashboard. You can configure your project, schedule its run, explore the outputs and the user profiles.\n\n**Profile Builder (PB)** is a command-line interface (CLI) tool that simplifies data transformation within your warehouse. It generates customer profiles by stitching data together from multiple sources.\n\nThis guide lists the detailed steps to install and use the Profile Builder (PB) tool to create, configure, and run a new project.\n\n## Prerequisites\n\nYou must have:\n\n*   [Python 3](https://www.python.org/downloads/) installed on your machine.\n*   Admin privileges on your machine.\n\n## Steps\n\nTo set up a project using the PB tool, follow these steps:\n\n### 1: Install PB\n\nInstall the Profile Builder tool by running the following command:\n\n```\npip3 install profiles-rudderstack\n```\n\nIf you have already installed PB, use the following command to update its version:\n\n```\npip3 install profiles-rudderstack -U\n```\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> RudderStack recommends using a Python virtual environment to maintain an isolated and clean environment.\n> \n> ```\n> pipx install profiles-rudderstack\n> ```\n\nValidate Profile Builder’s version after install using:\n\nSee also: [Setup and installation FAQ](https://www.rudderstack.com/docs/archive/profiles/0.13/faq/#setup-and-installation)\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> If you are an existing user, migrate your project to the new schema. See [Migrate your existing project](#migrate-your-existing-project) for more information.\n\n### 2: Create warehouse connection\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> RudderStack supports **Snowflake**, **Redshift**, **BigQuery**, and **Databricks** warehouses for Profiles. You must grant certain [warehouse permissions](https://www.rudderstack.com/docs/archive/profiles/0.13/permissions/) to let RudderStack read from schema having the source tables (for example, `tracks` and `identifies` tables generated via Event Stream sources), and write data in a new schema created for Profiles.\n\nCreate a warehouse connection to allow PB to access your data:\n\nThen, follow the prompts to enter details about your warehouse connection.\n\nA sample connection for a Snowflake account is as follows:\n\n```\nEnter Connection Name: test\nEnter target:  (default:dev):  # Press enter, leaving it to default\nEnter account: ina13147.us-east-1\nEnter warehouse: rudder_warehouse\nEnter dbname: your_rudderstack_db \nEnter schema: rs_profiles # A segregated schema for storing tables/views created by Profiles\nEnter user: profiles_test_user\nEnter password: <password>\nEnter role: profiles_role\nAppend to /Users/<user_name>/.pb/siteconfig.yaml? [y/N]\ny\n```\n\n*   **Connection Name**: Name of the connection in the project file.\n*   **Target**: Environment name, such as `dev`, `prod`, `test`, etc. You can specify any target name and create a separate connection for the same.\n*   **Account**: See [Snowflake documentation](https://docs.snowflake.com/en/user-guide/admin-account-identifier.html) for more information.\n*   **Warehouse**: Name of the warehouse.\n*   **Database name**: Name of the database inside warehouse where model outputs will be written.\n*   **Schema**: Name of the schema inside database where you’ll store identity stitcher and entity features.\n*   **User**: Name of the user in data warehouse.\n*   **Password**: Password for the above user.\n*   **Role**: Name of the user role.\n\nRudderStack supports various user authentication mechanisms for Redshift:\n\n```\nEnter Connection Name: test\nEnter target:  (default:dev):  # Press enter, leaving it to default\nEnter dbname: your_rudderstack_db \nEnter schema: rs_profiles # A segregated schema for storing tables/views created by Profiles\nEnter user: profiles_test_user\nEnter sslmode: options - [disable require]: disable # Enter \"require\" in case your Redshift connection mandates sslmode. \nHow would you like to authenticate with the warehouse service? Please select your method by entering the corresponding number:\ny\n[1] Warehouse Credentials: Log in using your username and password.\n        Format: [Username, Password]\n[2] AWS Programmatic Credentials: Authenticate using one of the following AWS credentials methods:\n        a) Direct input of AWS Access Key ID, Secret Access Key, and an optional Session Token.\n                Format: [AWS Access Key ID, Secret Access Key, Session Token (Optional)]\n        b) AWS configuration profile stored on your system.\n                Format: [AWS Configuration Profile Name]\n        c) Use an AWS Secrets Manager ARN to securely retrieve credentials.\n                Format: [Secret ARN]\n```\n\n*   **Connection Name**: Name of the connection in the project file.\n*   **Target**: Environment name, such as `dev`, `prod`, `test`, etc. You can specify any target name and create a separate connection for the same.\n*   **Host**: Log in to AWS Console and go to **Clusters** to know about host.\n*   **Port**: Port number to connect to the warehouse.\n*   **Database name**: Name of the database inside warehouse where model outputs will be written.\n*   **Schema**: Name of the schema inside database where you’ll store identity stitcher and entity features.\n*   **User**: Name of the user in data warehouse.\n*   **Password**: Password for the above user.\n\n### Warehouse credentials\n\nIf you choose to use the warehouse credentials (option 1), enter the following details:\n\n```\nEnter host: warehouseabc.us-west-1.redshift.amazonaws.com\nEnter port: 5439\nEnter password: <password>\nEnter tunnel_info: Do you want to use SSH Tunnel to connect with the warehouse? (y/n): y\nEnter ssh user: user\nEnter ssh host: 1234\nEnter ssh port: 12345\nEnter ssh private key file path: /Users/alex/.ssh/id_ed25519.pub\nAppend to /Users/alex/.pb/siteconfig.yaml? [y/N] y\n```\n\n### AWS Programmatic Credentials\n\nIf you choose to use AWS Access Key ID, Secret Access Key, and an optional Session Token from AWS Programmatic Credentials (option 2a), enter the following details:\n\n**For Redshift Cluster**\n\n```\nWhich Redshift Service you want to connect with?\n        [c]Redshift Cluster\n        [s]Redshift Serverless\nc\nEnter Redshift Cluster Identifier: cluster-id\nEnter access_key_id: aid\nEnter secret_access_key: ***\nEnter session_token: If you are using temporary security credentials, please specify the session token, otherwise leave it empty.\nstoken\nAppend to /Users/alex/.pb/siteconfig.yaml? [y/N] y\n```\n\n**For Redshift Serverless**\n\n```\nWhich Redshift Service you want to connect with?\n        [c]Redshift Cluster\n        [s]Redshift Serverless\ns\nEnter Redshift Serverless Workgroup Name: wg-name\nEnter access_key_id: aid\nEnter secret_access_key: ***\nEnter session_token: If you are using temporary security credentials, please specify the session token, otherwise leave it empty.\nAppend to /Users/alex/.pb/siteconfig.yaml? [y/N] y\n```\n\nIf you choose to use AWS configuration profile stored on your system from AWS Programmatic Credentials (option 2b), enter the following details:\n\n**For Redshift Cluster**\n\n```\nWhich Redshift Service you want to connect with?\n        [c]Redshift Cluster\n        [s]Redshift Serverless\nc\nEnter Redshift Cluster Identifier: ci   \nEnter shared_profile: default\nEnter region: us-east-1\nAppend to /Users/alex/.pb/siteconfig.yaml? [y/N] y\n```\n\n**For Redshift Serverless**\n\n```\nWhich Redshift Service you want to connect with?\n        [c]Redshift Cluster\n        [s]Redshift Serverless\ns\nEnter Redshift Serverless Workgroup Name: serverless-wg\nEnter shared_profile: default\nAppend to /Users/alex/.pb/siteconfig.yaml? [y/N] y\n```\n\nIf you choose to use an AWS Secrets Manager ARN to securely retrieve credentials from AWS Programmatic Credentials (option 2c), enter the following details:\n\n**For Redshift Cluster**\n\n```\nWhich Redshift Service you want to connect with?\n        [c]Redshift Cluster\n        [s]Redshift Serverless\nc\nEnter Redshift Cluster Identifier: cluster-identifier\nEnter secrets_arn: ************************\nAppend to /Users/alex/.pb/siteconfig.yaml? [y/N] y\n```\n\nA sample connection for a Databricks account is as follows:\n\n```\nEnter Connection Name: test\nEnter target:  (default:dev):  # Press enter, leaving it to default\nEnter host: a1.8.azuredatabricks.net # The hostname or URL of your Databricks cluster\nEnter port: 443 # The port number used for establishing the connection. Usually it is 443 for https connections.\nEnter http_endpoint: /sql/1.0/warehouses/919uasdn92h # The path or specific endpoint you wish to connect to.\nEnter access_token: <password> # The access token created for authenticating the instance.\nEnter user: profiles_test_user # Username of your Databricks account.\nEnter schema: rs_profiles # A segregated schema for storing tables/views created by Profiles\nEnter catalog: your_rudderstack_db # The database or catalog having data that you’ll be accessing.\nAppend to /Users/<user_name>/.pb/siteconfig.yaml? [y/N]\ny\n```\n\n*   **Connection Name**: Name of the connection in the project file.\n*   **Target**: Environment name, such as `dev`, `prod`, `test`, etc. You can specify any target name and create a separate connection for the same.\n*   **Host**: Host name or URL of your Databricks cluster.\n*   **Port**: Port number for establishing the connection, usually `443` for `https` connections.\n*   **http\\_endpoint**: Path or specific endpoint you wish to connect to.\n*   **access\\_token**: Access token for authenticating the instance.\n*   **User**: Username of your Databricks account.\n*   **Schema**: Name of the schema to store your output tables/views.\n*   **Catalog**: Name of the database or catalog from where you want to access the data.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> RudderStack currently supports Databricks on Azure. To get the Databricks connection details:\n> \n> 1.  Log in to your Azure’s Databricks Web UI.\n> 2.  Click on **SQL Warehouses** on the left.\n> 3.  Select the warehouse to connect to.\n> 4.  Select the **Connection Details** tab.\n\nA sample connection for a BigQuery account is as follows:\n\n```\nEnter Connection Name: test\nEnter target:  (default:dev):  # Press enter, leaving it to default\nEnter credentials: json file path: # File path of your BQ JSON file, for example, /Users/alexm/Downloads/big.json. Entering an incorrect path will exit the program.\nEnter project_id: profiles121\nEnter schema: rs_profiles\nAppend to /Users/<user_name>/.pb/siteconfig.yaml? [y/N]\ny\n```\n\nThis creates a [site configuration file](https://www.rudderstack.com/docs/archive/profiles/0.13/cli-user-guide/structure/) inside your home directory: `~/.pb/siteconfig.yaml`. If you don’t see the file, enable the **View hidden files** option.\n\n### 3: Create project\n\nRun the following command to create a sample project:\n\n```\npb init pb-project -o MyProfilesProject\n```\n\nThe above command creates a new project in the **MyProfilesProject** folder with the following structure:\n\n[![Project structure](https://www.rudderstack.com/docs/images/profiles/project-structure.webp)](https://www.rudderstack.com/docs/images/profiles/project-structure.webp)\n\nSee [Project structure](https://www.rudderstack.com/docs/archive/profiles/0.13/cli-user-guide/structure/) for more information on the PB project files.\n\nNavigate to the `pb_project.yaml` file and set the value of `connection:` to the connection name as defined in the previous step.\n\n### 4: Change input sources\n\n*   Navigate to your project and open the `models/inputs.yaml` file. Here, you will see a list of tables/views along with their respective ID types.\n*   Replace the placeholder table names with the actual table names in the `table` field.\n\nSee [Project structure](https://www.rudderstack.com/docs/archive/profiles/0.13/cli-user-guide/structure/#inputs) for more information on setting these values.\n\n### 5: Validate project\n\nNavigate to your project and validate your warehouse connection and [inputInputs refers to the input data sources used to create the material (output) tables in the warehouse.](https://www.rudderstack.com/docs/resources/glossary/#input) sources:\n\nIf there are no errors, proceed to the next step. In case of errors, check if your warehouse schemas and tables have the [required permissions](https://www.rudderstack.com/docs/archive/profiles/0.13/permissions/).\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> Currently, this command is not supported for BigQuery warehouse.\n\n### 6: Generate SQL files\n\nCompile the project:\n\nThis generates SQL files in the `output/` folder that you can run directly on the warehouse. In case of any compilation errors, you will see them on your screen and also in the `logs/logfile.log` file.\n\n### 7: Generate output tables\n\nRun the project and generate [material tables](https://www.rudderstack.com/docs/archive/profiles/0.13/resources/glossary/#material-tables):\n\nThis command generates and runs the SQL files in the warehouse, creating the material tables.\n\n### 8: View generated tables\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> The view `user_default_id_stitcher` will always point to the latest generated ID stitcher and `user_profile` to the latest feature table.\n\nYou can run the `pb show models` command to get the exact name and path of the generated ID stitcher/feature table. See [show](https://www.rudderstack.com/docs/archive/profiles/0.13/cli-user-guide/commands/#show) command for more information.\n\nThen, execute the below query to view the generated tables in the warehouse:\n\n```\nselect * from <table_name> limit 10;\n```\n\nHere’s what the columns imply:\n\n![ID Stitcher Table](https://www.rudderstack.com/docs/images/profiles/idstitcher-table.webp)\n\n*   **user\\_main\\_id**: Rudder ID generated by Profile Builder. Think of a 1-to-many relationship, with one Rudder ID connected to different IDs belonging to same user such as User ID, Anonymous ID, Email, Phone number, etc.\n*   **other\\_id**: ID in input source tables that is stitched to a Rudder ID.\n*   **other\\_id\\_type**: Type of the other ID to be stitched (User ID, Anonymous ID, Email, etc).\n*   **valid\\_at**: Date at which the corresponding ID value occurred in the source tables. For example, the date at which a customer was first browsing anonymously, or when they logged into the CRM with their email ID, etc.\n\n![Feature Table](https://www.rudderstack.com/docs/images/profiles/feature-table.webp)\n\n*   **user\\_main\\_id**: Rudder ID generated by Profile Builder.\n*   **valid\\_at**: Date when the feature table entry was created for this record.\n*   **first\\_seen, last\\_seen, country, first\\_name, etc.** - All features for which values are computed.\n\n## Migrate your existing project\n\nTo migrate an existing PB project to the [schema version](https://www.rudderstack.com/docs/archive/profiles/0.13/resources/glossary/#schema-versions) supported by your PB binary, navigate to your project’s folder. Then, run the following command to replace the contents of the existing folder with the new one:\n\n```\npb migrate auto --inplace\n```\n\nA confirmation message appears on screen indicating that the migration is complete. A sample message for a user migrating their project from version 25 to 44:\n\n```\n2023-10-17T17:48:33.104+0530\tINFO\tmigrate/migrate.go:161\t\nProject migrated from version 25 to version 44\n```\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Profile Builder CLI | RudderStack Docs",
    "description": "Create a Profiles project using the Profile Builder (PB) tool.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.13/core-concepts/data-modeling/",
    "markdown": "# Data Modeling | RudderStack Docs\n\nModel your unorganized and scattered warehouse data using RudderStack’s Profiles.\n\n* * *\n\n*     3 minute read  \n    \n\nProfiles models your organizational data by analyzing all the data in your warehouse to create unified customer profiles and enrich them with features to help you scale your business efficiently and swiftly.\n\nWhen you run the Profiles project, it creates an identity graph and feature views as outputs. You can augment the graph and create new user features by writing simple definitions in a configuration file or via SQL models.\n\n[![Profiles data modeling](https://www.rudderstack.com/docs/images/profiles/data-modeling.webp)](https://www.rudderstack.com/docs/images/profiles/data-modeling.webp)\n\n## Highlights\n\n*   Flexibility to use event stream, ETL, or any external tools as input sources.\n*   Support to define various entities like user, product, organization, etc.\n*   Intelligent merging of entities with different identifiers, like stitching Salesforce IDs.\n*   Ease of creating features/traits for any entity and using them to deliver personalization.\n*   Support to create core customer segments and activate them in downstream systems.\n*   Deal with advanced use-case scenarios using entity\\_vars/ML models.\n\n## Use varied input sources\n\nRudderStack Profiles gives you the flexibility of using a variety of input sources. These sources are essentially the tables or views which you can create using:\n\n*   [Event Stream](https://www.rudderstack.com/docs/sources/event-streams/) (loaded from event data)\n*   ETL extract (loaded from [Cloud Extract](https://www.rudderstack.com/docs/sources/extract/) (loaded from event data))\n*   Existing tables in the warehouse (generated by external tools like DBT).\n\n## Define entities\n\nEntities refer to an object for which you can create a profile. RudderStack allows you to use the desired object as an entity. For example, user, customer, product, or any other object that requires profiling.\n\nYou can define the entities in `pb_project.yaml` file and use them declaratively while describing the columns of your input sources.\n\n## Unify entities\n\nOnce you define the entities, you can resolve different identities for an entity using the process of [identity stitching](https://www.rudderstack.com/docs/archive/profiles/0.13/core-concepts/identity-stitching/). It matches the different identifiers across multiple devices, digital touchpoints, and other data (like offline point-of-sale interactions) to build a comprehensive identity graph. The identity graph includes nodes (identifiers) and their relationships (edges), and it is generated as a transparent table in the warehouse.\n\nFor example, you can stitch Salesforce IDs or other ID types.\n\n## Enrich with features\n\nOnce you map all the available identifiers to an individual user or entity, it is easier to collect their traits and compute the user features you want in your customer 360 table.\n\nUsing the identity graph as a map, the Profiles **entity var** models let you define or perform calculations over the customer data in your warehouse. Each `var` materialises as a column in the `entity_var` table and represents a distinct feature. In addition, ML models can also use the identify graph as well as other entity vars, to create new features. Finally, **feature view** model lets you unify entity vars as well as ML faetures into a single view.\n\nYou don’t need any other tool or deep technical/SQL expertise to create these features. Trait definition is in a single unified framework and there is no need to move data across silos.\n\nTo implement advanced use cases, you can use [custom SQL queries](https://www.rudderstack.com/docs/archive/profiles/0.13/example/sql-model/) to define user features.\n\n## Define cohorts\n\nUsing cohorts, you can define core customer segments in the warehouse via a simple YAML config and the entire business can use them as a single source of truth. It is a subset of instances of an entity meeting a specified set of characteristics, behaviours, or attributes. For example, if you have user as an entity, you can define cohorts such as known users, new users, or North American users, etc.\n\nBy leveraging cohorts, you can target specific customer segments by enabling targeted campaigns and analysis. See [Cohorts](https://www.rudderstack.com/docs/sources/event-streams/cloud-apps/auth0/) for more information.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Data Modeling | RudderStack Docs",
    "description": "Model your unorganized and scattered warehouse data using RudderStack's Profiles.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.13/get-started/import-from-git/",
    "markdown": "# Import Profiles Project from Git\n\nPrivacy Overview\n\nThis site uses cookies to improve your experience while you navigate through the website. Out of these cookies, the cookies that are categorized as necessary are stored on your browser as they are as essential for the working of basic functionalities of the website. We also use third-party cookies that help us analyze and understand how you use this website. These cookies will be stored in your browser only with your consent. You also have the option to opt-out of these cookies. But opting out of some of these cookies may have an effect on your browsing experience.\n\nNecessary cookies are absolutely essential for the website to function properly. This category only includes cookies that ensures basic functionalities and security features of the website. These cookies do not store any personal information.",
    "title": "Import Profiles Project from Git | RudderStack Docs",
    "description": "Import an existing Profiles project from the Git repository in RudderStack dashboard.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.13/get-started/sample-data/",
    "markdown": "# Snowflake Sample Data | RudderStack Docs\n\nSample data for Snowflake\n\n* * *\n\n*     2 minute read  \n    \n\nRudderStack provides a sample data set for the Snowflake warehouse, available in the [Snowflake marketplace](https://app.snowflake.com/marketplace/listing/GZT0Z856CMJ/rudderstack-inc-rudderstack-event-data-for-quickstart). You can use this data to run the Profiles project and [Predictive features](https://www.rudderstack.com/docs/archive/profiles/0.13/predictions/) through the UI or the CLI.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> The number of columns in this data set are intentionally limited to make the data set easily understandable. Also, all email addresses are generated randomly and no PII is used in the generation of this data set.\n\nThe following tables, properties, and user information is included in the data set:\n\n## Tables\n\nThis data set includes below-mentioned RudderStack event data tables:\n\n*   `PAGES` - Page view events from anonymous and known users.\n*   `TRACKS` - Summarized tracked user actions (like `login`, `signup`, `order_completed`, etc.).\n*   `IDENTIFIES` - Identify calls run when a user provides a unique identifier (i.e., upon `signup`).\n*   `ORDER_COMPLETED` - Detailed payloads from tracked `order_completed` events.\n\nAs of January 2023, here are the approximate number of rows in each table:\n\n*   `PAGES`: ~43k\n*   `TRACKS`: ~14k\n*   `IDENTIFIES`: ~4.8k\n*   `ORDER_COMPLETED`: ~2.2k\n\nThese volumes follow the pattern of a normal eCommerce conversion funnel (pageview, signup, order). Specifically, here’s a rough breakdown of the user journey by volume:\n\n*   30% - Never sign in\n*   10% - Sign in but never add an item to cart\n*   40% - Add to cart and abandon\n*   20% - Make purchases\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> Note that this data includes _future_ data until Apr 2024, and starts in June 2023. This is to ensure that future users can still run the project with ‘current’ data. RudderStack team will refresh the data periodically throughout the year.\n\n## Properties\n\nThis data set includes a _subset_ of the standard properties found in the [Warehouse schema spec](https://www.rudderstack.com/docs/destinations/warehouse-destinations/warehouse-schema/) for each table. The required columns for running Profiles and Predictions projects are also present.\n\n### User information\n\nThe user data includes a subset of our standard properties for `identify` calls.\n\nThis data set contains a total of ~10k unique users by `anonymousId`. About half of these unique users (~4.8k) are known users (with an associated `identify` call).\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Snowflake Sample Data | RudderStack Docs",
    "description": "Sample data for Snowflake",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.13/core-concepts/identity-stitching/",
    "markdown": "# Identity Stitching | RudderStack Docs\n\nStitch multiple identifiers together to create a unified and comprehensive profile.\n\n* * *\n\n*     4 minute read  \n    \n\nIdentity stitching combines unique identifiers across your digital touchpoints to identify users and create a unified, omnichannel view of your customers. Using this feature, you can:\n\n*   **Understand user behavior**: Consolidate and connect customer data from various sources to better understand customers’ preferences, behaviors, and interactions across multiple touchpoints.\n*   **Provide personalized support**: Deliver personalized marketing messages and experiences to your customers. Ensures that the right message reaches the right person at the right time, increasing the effectiveness of marketing campaigns.\n*   **Enrich user profile with features**: Enhance user profiles with additional data points and features. These features can include demographic information, preferences, purchase history, browsing behavior, or any other static or computed data points.\n\n## Problem of multiple identities\n\nCompanies gather user data across digital touchpoints like websites, mobile apps, enterprise systems like CRMs, marketing platforms, etc. During this process, a single user is identified with multiple identifiers across their product journey, like their email ID, phone number, device ID, anonymous ID, and more. Also, the user information is spread across dozens of devices, accounts, or products as they often change their devices and use work and personal emails together.\n\nThe user data stored in the warehouse contains unstructured objects that represent one or more user (or entity) identities. Competitive businesses need to clarify this mess of data points into an accurate model of customer behavior and build personalized relationships.\n\nTo create a unified user profile, it is essential to correlate all of the different user identifiers into one canonical identifier so that all the data related to a particular user or entity can be associated with that user or entity.\n\nThis unification step, called **Identity Stitching**, ties all the user data from these tables into a unified view, giving you a 360-degree view of the user.\n\n## Perform identity stitching\n\nYou can use the RudderStack’s [identity stitching model](https://www.rudderstack.com/docs/archive/profiles/0.13/example/id-stitcher/) to define the identifiers you want to combine together. You can also define the input sources, like [Event Stream](https://www.rudderstack.com/docs/sources/event-streams/), [Cloud Extract](https://www.rudderstack.com/docs/sources/extract/) sources, which automatically produce an identity graph because all the schemas and unique identifiers are known.\n\n## Unify identities across all platforms and devices\n\nIdentity stitching is the process of matching different identifiers across multiple devices, digital touchpoints, and other data (like offline point-of-sale interactions) to build a comprehensive identity graph. This identity graph includes nodes (identifiers) and their relationships (edges), and it is generated as a transparent table in the warehouse.\n\nRudderStack performs identity stitching by mapping all the unique identifiers into a single canonical ID (for example, `rudder_id`), then uses that ID to make user feature development easier (for example, summing the payment events against a single `rudder_id`).\n\n[![single identity created from different identities](https://www.rudderstack.com/docs/images/profiles/id-stitching.webp)](https://www.rudderstack.com/docs/images/profiles/id-stitching.webp)\n\n## Identity graph\n\nIdentity stitching starts with the creation of an identity graph. The identity graph is a database housing the entity identifiers where you can identify and connect details related to your customer journeys. Further, it stitches them together in one customer profile representing their whole identity.\n\nThe most fundamental data in an identity graph is the ID tag associated with a device, account, network, session, transaction, or any other anonymous identifier that can engage with your company. Once you’ve collected this data and associated it with a single customer identity (wherever possible), your customer data becomes more reliable, and you can move on to achieve higher goals.\n\nAn identity graph incorporates models that help it in ingesting new information. As you add a new data point with whatever connections are immediately known, the graph database determines if it fits into any existing customer identifier. If there is a clear link - such as a matching device ID or conclusive biographical data like a credit card number - the graph incorporates the data into a relevant user node.\n\n[![identity graph](https://www.rudderstack.com/docs/images/profiles/identity-graph.webp)](https://www.rudderstack.com/docs/images/profiles/identity-graph.webp)\n\n## Notable features\n\n*   Use different input sources like RudderStack’s [Event Stream](https://www.rudderstack.com/docs/sources/event-streams/), [Cloud Extract](https://www.rudderstack.com/docs/sources/extract/) sources, or any existing tables in the warehouse.\n*   Merge identities for entities like a user, customer, product, business account, etc.\n*   Stitch identities from all the required channels like web, mobile, marketing platforms, etc.\n*   Use identity stitching results to develop user features and deliver personalized campaigns.\n\n#### See also\n\n*   [Sample identity stitching project](https://www.rudderstack.com/docs/archive/profiles/0.13/example/id-stitcher/)\n*   [Problem of Identity resolution](https://www.rudderstack.com/blog/the-tale-of-identity-graph-and-identity-resolution/)\n*   [How to achieve ID mapping in a data warehouse](https://www.rudderstack.com/blog/identity-graph-and-identity-resolution-in-sql/)\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Identity Stitching | RudderStack Docs",
    "description": "Stitch multiple identifiers together to create a unified and comprehensive profile.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.13/core-concepts/feature-development/",
    "markdown": "# Feature Development | RudderStack Docs\n\nEnrich unified profiles with the required features/traits to drive targeted campaigns.\n\n* * *\n\n*     3 minute read  \n    \n\nOnce you have [performed identity stitching](https://www.rudderstack.com/docs/archive/profiles/0.13/core-concepts/identity-stitching/#how-to-perform-identity-stitching) to map the individual entities to known identifiers, you can use its output to enhance the unified profiles with additional data points and features.\n\nYou can define the features/traits in your warehouse tables and further perform calculations over this data to devise meaningful outcomes, which can help marketing teams to run effective campaigns.\n\n## Define features\n\nYou can use `var_group` to define features. Each `var_group` can have multiple entity vars which can be considered as features for that entity. The Profiles project generates and runs SQL in the background and automatically adds the resulting features to a [feature view](https://www.rudderstack.com/docs/archive/profiles/0.13/example/feature-views/).\n\nYou can produce a customer [feature view](https://www.rudderstack.com/docs/archive/profiles/0.13/example/feature-views/) or a feature view for specific projects like personalization, recommendations, or analytics.\n\nYou can combine the features to create even more features. You can also use [custom SQL queries](https://www.rudderstack.com/docs/archive/profiles/0.13/example/sql-model/) to enrich unified user profiles for advanced use cases.\n\nA sample `pb_project.yaml` file with a definition of a feature\\_view::\n\n```\n...\nentities:\n  - name: user\n    id_types:\n      - main_id\n      - user_id\n    feature_views:\n      using_ids:\n        - id: user_id\n```\n\nA sample configuration file to create `var_groups`:\n\n```\nvar_groups:\n  - name: user_vars\n    entity_key: user\n    vars:\n      - entity_var:\n          name: first_seen\n          select: min(timestamp::date)\n          from: inputs/rsTracks\n          is_feature: false\n      - entity_var:\n          name: last_seen\n          select: max(timestamp::date)\n          from: inputs/rsTracks\n          is_feature: false\n      - entity_var:\n          name: user_lifespan\n          select: '{{user.Var(\"last_seen\")}} - {{user.Var(\"first_seen\")}}'\n          description: Life Time Value of a customer\n      - entity_var:\n          name: days_active\n          select: count(distinct timestamp::date)\n          from: inputs/rsTracks\n          description: No. of days a customer was active\n# ID stitcher\nmodels:\n  - name: domain_profile_id_stitcher\n    model_type: id_stitcher\n    model_spec:\n      validity_time: 24h # 1 day\n      entity_key: user\n      materialization:\n        run_type: incremental\n      edge_sources:\n        - from: inputs/rsIdentifies\n        - from: inputs/rsTracks\n```\n\n## Benefits\n\n*   You can use the output of the identity graph to define or compute features based on given ID types. Feature views creates a view which will have all or a specified set of features on that entity from across the project based on the identifier column provided.\n*   As the number of features/traits increases, Profiles makes the maintenance process much easier by using a configuration file (as opposed to large and complex SQL queries).\n*   Profiles generates highly performant SQL to build feature views, which helps mitigate computing costs and engineering resources when the data sets become large, dependencies become complex, and features require data from multiple sources.\n\n### Use-cases\n\n*   Create analytics queries like demographic views, user activity views, etc.\n*   Send data using a [Reverse ETL](https://www.rudderstack.com/docs/sources/reverse-etl/) pipeline to various cloud destinations.\n*   Use RudderStack Audiences to send customer profiles to marketing tools (available for beta customers).\n*   Define traits/features using `entity_vars` and ML models and unify them using the Feature Views model.\n\n#### See also\n\n*   [Sample feature views project](https://www.rudderstack.com/docs/archive/profiles/0.13/example/feature-views/)\n*   [Sample SQL model project](https://www.rudderstack.com/docs/archive/profiles/0.13/example/sql-model/)\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Feature Development | RudderStack Docs",
    "description": "Enrich unified profiles with the required features/traits to drive targeted campaigns.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.13/cli-user-guide/structure/",
    "markdown": "# Project Structure | RudderStack Docs\n\nKnow the specifications of a site configuration file, PB project structure, configuration files, and their parameters.\n\n* * *\n\n*     10 minute read  \n    \n\nOnce you complete the [Profile Builder CLI](https://www.rudderstack.com/docs/archive/profiles/0.13/get-started/profile-builder/) steps, you will be able to see the Profiles project on your machine.\n\n## Site configuration file\n\nRudderStack creates a site configuration file (`~/.pb/siteconfig.yaml`) while [creating a warehouse connection](https://www.rudderstack.com/docs/archive/profiles/0.13/get-started/profile-builder/#2-create-warehouse-connection). It contains the following details including secrets (if any):\n\n*   Warehouse connection details and its credentials.\n*   Git repository connection credentials (if any). See [Associate SSH Key to Git Project](https://www.rudderstack.com/docs/archive/profiles/0.13/example/packages/#associate-ssh-key-to-git-project) for more information.\n\n> ![success](https://www.rudderstack.com/docs/images/tick.svg)\n> \n> If you have multiple Profiles projects and they use different warehouse connections, you can store the details for multiple connections in the same site configuration file.\n\nA sample site configuration file containing multiple warehouse connection details is shown below:\n\n```\nconnections:\n  prod-db-profile:\n      target: dev\n      outputs:\n          dev:\n              account: inb828.us-west-3\n              dbname: MAT_STORE\n              password: password\n              role: PROFILES_ROLE\n              schema: AB_SCHEMA\n              type: snowflake\n              user: rik\n              warehouse: PROD_WAREHOUSE\n  test-db-profile:\n      target: test\n      outputs:\n          db:\n              access_token: dabasihasdho\n              catalog: rs_dev\n              host: adb-98.18.azuredatabricks.net\n              http_endpoint: /sql/1.0/warehouses/919uasdn92h\n              port: 443\n              schema: rs_profiles\n              type: databricks\n              user: johndoe@abc.onmicrosoft.com\n          dev:\n              account: uk12.us-west-1\n              dbname: RUDDERSTACK_DB\n              password: password\n              role: RS_ROLE\n              schema: RS_PROFILES\n              type: snowflake\n              user: johndoe\n              warehouse: RS_WAREHOUSE\n          redshift_v1:\n              dbname: warehouse_rs\n              host: warehouse.abc.us-east-3.redshift.amazonaws.com\n              password: password\n              port: 5419\n              schema: rs_profiles\n              type: redshift\n              user: redshift_user\n          redshift_v2:\n              workgroup_name: warehouse_workgroup\n              region: us-east-1\n              driver: v2\n              sslmode: require\n              dbname: warehouse_rs\n              schema: rs_profiles\n              type: redshift\n              access_key_id: ******************\n              secret_access_key: ******************************\n           big:\n              credentials:\n                auth_provider_x509_cert_url: https://www.googleapis.com/oauth2/v1/certs\n                auth_uri: https://accounts.google.com/o/oauth2/auth\n                client_email: johndoe@big-query-integration-poc.iam.gserviceaccount.com\n                client_id: \"123345678909872\"\n                client_x509_cert_url: https://www.googleapis.com/robot/v1/metadata/x509/johndoe%40big-query-integration-poc.iam.gserviceaccount.com\n                private_key: |\n                    -----BEGIN PRIVATE KEY-----                    \n                   ## private key\n                    -----END PRIVATE KEY-----\n                private_key_id: 5271368bhjbd72y278222e233w23e231e\n              project_id: big-query-integration-poc\n                token_uri: https://oauth2.googleapis.com/token\n                type: service_account\n                project_id: rs_profiles\n              schema: rs_profiles\n              type: bigquery\n              user: johndoe@big-query-integration-poc.iam.gserviceaccount.com\ngitcreds:\n - reporegex: \"git@github.com:REPO_OWNER/*\" # in case of ssh url\n   key: |\n       -----BEGIN OPENSSH PRIVATE KEY-----\n       **********************************************************************\n       **********************************************************************\n       **********************************************************************\n       **********************************************************************\n       ****************************************************************\n       -----END OPENSSH PRIVATE KEY-----       \n - reporegex: \"https://github.com/rudderlabs/*\" # https url\n   basic_auth:\n     username: oauth2\n     password: ... # your personal access token with read permission\npy_models:\n    enabled: true # in case you are using Python models in your project, else set it to false\n    python_path: /opt/anaconda3/bin/python # the path where Python is installed (run `which python` to get the full path). If `py_models` is not enabled, set it to `\"\"`. For Windows, you may pass the path value as: python.exe\n    credentials_presets: null\n    allowed_git_urls_regex: \"\"\ncache_dir: /Users/YOURNAME/.pb/WhtGitCache/ # For Windows, the directory path will have forward slash (\\)\nfilepath: /Users/YOURNAME/.pb/siteconfig.yaml # For Windows, the file path will have forward slash (\\)\n```\n\n## Profiles project structure\n\nThe following image shows the folder structure of the project:\n\n[![Project structure](https://www.rudderstack.com/docs/images/profiles/project-structure.webp)](https://www.rudderstack.com/docs/images/profiles/project-structure.webp)\n\n### `pb_project.yaml`\n\nThe `pb_project.yaml` file contains the project details like the name, schema version, warehouse connection, [entityEntity refers to a digital representation of a class of real world distinct objects for which you can create a profile.](https://www.rudderstack.com/docs/resources/glossary/#entity) names along with ID types, etc.\n\nA sample `pb_project.yaml` file with entity type as `user`:\n\n```\n# Project name\nname: sample_attribution\n\n# Project's yaml schema version\nschema_version: 63\n\n# WH Connection to use\nconnection: test\n\n# Model folders to use\nmodel_folders:\n  - models\n\n# Entities in the project and their ids\nentities:\n  - name: user\n    # Change the following to set a custom ID stitcher(optional).\n    # id_stitcher: models/user_id_stitcher\n    id_types:\n      - main_id\n      - user_id\n      - anonymous_id\n      - email\n    # Feature views - to get all features/traits of an entity into a single view (optional)\n    feature_views: \n      using_ids: \n        - id: user_id \n          name: with_user_id\n          \n# lib packages can be imported to signify that this project's properties are inherited\npackages:\n  - name: corelib\n    url: \"https://github.com/rudderlabs/rudderstack-profiles-corelib/tag/schema_{{best_schema_version}}\"\n\n# Profiles can also use certain model types defined in Python.\n# Examples include ML models. Those dependencies are specified here.\npython_requirements:\n  - profiles-pycorelib==0.1.0\n```\n\nThe following table explains the fields used in the above file:\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `name` | String | Name of the project. |\n| `schema_version` | Integer | Project’s YAML version. Each new schema version comes with improvements and added functionalities. |\n| `connection` | String | Connection name from [`siteconfig.yaml`](https://www.rudderstack.com/docs/archive/profiles/0.13/cli-user-guide/structure/) used for connecting to the warehouse. |\n| `model_folders` | String | Names of folders where model files are stored. |\n| [`entities`](#entities) | List | Lists all the entities used in the project for which you can define models. Each entry for an entity here is a JSON object specifying entity’s name and attributes. |\n| [`packages`](#packages) | List | List of packages with their name and URL. Optionally, you can also extend ID types filters for including or excluding certain values from this list. |\n\n##### `entities`\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `name` | String | Name of the entity used in the project. |\n| `id_stitcher` | String | (Optional) Reference path of the [custom ID stitcher model](https://www.rudderstack.com/docs/archive/profiles/0.13/example/id-stitcher/#sample-project-for-custom-id-stitcher) (for example, `models/name_of_id_stitcher`). |\n| [`id_types`](https://www.rudderstack.com/docs/archive/profiles/0.13/example/packages/#modify-id-types) | List | List of all identifier types associated with the current entity. |\n| `feature_views` | List | (Optional) Lists all the view names along with their ID’s being served for [feature views model](https://www.rudderstack.com/docs/archive/profiles/0.13/example/feature-views/). |\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> The identifiers listed in `id_types` may have a many-to-one relationship with an entity but each ID must belong to a single entity.\n> \n> For example, a `user` entity might have `id_types` as the `salesforce_id`, `anonymous_id`, `email`, and `session_id` (a user may have many session IDs over time). However, it should not include something like `ip_address`, as a single IP can be used by different users at different times and it is not considered as a user identifier.\n\n##### `packages`\n\nYou can import library packages in a project signifying where the project inherits its properties from.\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `name` | String | Specify a name. |\n| `url` | String | HTTPS URL of the lib package, with a tag for the best schema version. |\n\n### `inputs.yaml`\n\nThe `inputs.yaml` file lists all the input sources which should be used to run [models](#models) and eventually create outputs. You can also define specific constraints on the input sources using the [`contract`](https://www.rudderstack.com/docs/archive/profiles/0.13/example/packages/#model-contracts) key.\n\nRudderStack supports the following input sources:\n\n*   **Table**: Specify the table’s name in the `table` key.\n*   **View**: Specify the view’s name in the `view` key.\n*   **S3 bucket**: Specify the path of the CSV file in your bucket in the `s3` key. See [Use Amazon S3 bucket as input](https://www.rudderstack.com/docs/archive/profiles/0.13/example/packages/#use-amazon-s3-bucket-as-input) for more information.\n*   **Local CSV file**: Specify the file path in the `csv` key. See [Use CSV file as input](https://www.rudderstack.com/docs/archive/profiles/0.13/example/packages/#use-csv-file-as-input) for more information.\n\nYou can also specify the table/view along with the column name and SQL expression for retrieving values. The input specification may also include metadata and the constraints on those columns.\n\nA sample `inputs.yaml` file:\n\n```\ninputs:\n  - name: salesforceTasks\n    contract:\n      is_optional: false\n      is_event_stream: true\n      with_entity_ids:\n        - user\n      with_columns:\n        - name: activitydate\n        - name: whoid\n    app_defaults:\n      table: salesforce.task\n      # For BigQuery, it is recommended to use view (view: _views_<view_name>) instead of table for event streaming data sets.\n      occurred_at_col: activitydate\n      ids:\n        # column name or sql expression\n        - select: \"whoid\" \n          type: salesforce_id\n          entity: user\n          to_default_stitcher: true\n  - name: salesforceContact\n    contract:\n      is_optional: false\n      is_event_stream: true\n      with_entity_ids:\n        - user\n      with_columns:\n        - name: createddate\n        - name: id\n        - name: email\n    app_defaults:\n      table: salesforce.contact\n      # For BigQuery, it is recommended to use view (view: _views_<view_name>) instead of table for event streaming data sets.\n      occurred_at_col: createddate\n      ids:\n        - select: \"id\"\n          type: salesforce_id\n          entity: user\n          to_default_stitcher: true\n        - select: \"case when lower(email) like any ('%gmail%', '%yahoo%') then lower(email)  else split_part(lower(email),'@',2) end\"\n          type: email\n          entity: user\n          to_default_stitcher: true\n  - name: websitePageVisits\n    contract:\n      is_optional: false\n      is_event_stream: true\n      with_entity_ids:\n        - user\n      with_columns:\n        - name: timestamp\n        - name: anonymous_id\n        - name: context_traits_email\n        - name: user_id\n    app_defaults:\n      table: autotrack.pages\n      # For BigQuery, it is recommended to use view (view: _views_<view_name>) instead of table for event streaming data sets.\n      occurred_at_col: timestamp\n      ids:\n        - select: \"anonymous_id\"\n          type: rudder_anon_id\n          entity: user\n          to_default_stitcher: true\n        # below sql expression check the email type, if it is gmail and yahoo return email otherwise spilt email return domain of email.  \n        - select: \"case when lower(coalesce(context_traits_email, user_id)) like any ('%gmail%', '%yahoo%') then lower(coalesce(context_traits_email, user_id))  \\\n              else split_part(lower(coalesce(context_traits_email, user_id)),'@',2) end\"\n          type: email\n          entity: user\n          to_default_stitcher: true\n```\n\nThe following table explains the fields used in the above file:\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `name` | String | Name of the input model. |\n| `contract` | Dictionary | A model contract provides essential information about the model like the necessary columns and entity IDs that it should contain. This is crucial for other models that depend on it, as it helps find errors early and closer to the point of their origin. |\n| `app_defaults` | Dictionary | Values that input defaults to when you run the project directly. For library projects, you can remap the inputs and override the app defaults while importing the library projects. |\n\n##### `contract`\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `is_optional` | Boolean | Whether the model’s existence in the warehouse is mandatory. |\n| `is_event_stream` | Boolean | Whether the table/view is a series/stream of events. A model that has a `timestamp` column is an event stream model. |\n| `with_entity_ids` | List | List of all entities with which the model is related. A model M1 is considered related to model M2 if there is an ID of model M2 in M1’s output columns. |\n| `with_columns` | List | List of all ID columns that this contract is applicable for. |\n\n##### `app_defaults`\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `table`/`view` | String | Name of the warehouse table/view containing the data. You can prefix the table/view with an external schema or database in the same warehouse, if applicable. Note that you can specify either a table or view but not both. |\n| `s3` | String | Name of the CSV file in your Amazon S3 bucket containing the data. |\n| `csv` | String | Name of the CSV file in your local storage containing the data. The file path should be relative to the project folder. |\n| `occurred_at_col` | String | Name of the column in table/view containing the timestamp. |\n| [`ids`](#ids) | List | Specifies the list of all IDs present in the source table along with their column names (or column SQL expressions).<br><br>**Note**: Some input columns may contain IDs of associated entities. By their presence, such ID columns associate the row with the entity of the ID. The ID Stitcher may use these declarations to automatically discover ID-to-ID edges. |\n\n##### `ids`\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `select` | String | Specifies the column name to be used as the identifier. You can also specify a SQL expression if some transformation is required.<br><br>**Note**: You can also refer table from another Database/Schema in the same data warehouse. For example, `table: <database_name>.<schema_name>.<table_name>`. |\n| `type` | String | Type of identifier. All the ID types of a project are declared in [`pb_project.yaml`](#project-details). You can specify additional filters on the column expression.<br><br>**Note**: Each ID type is linked only with a single entity. |\n| `entity` | String | Entity name defined in the [`pb_project.yaml`](#project-details) file to which the ID belongs. |\n| `to_default_stitcher` | Boolean | Set this **optional** field to `false` for the ID to be excluded from the default ID stitcher. |\n\n### `profiles.yaml`\n\nThe `profiles.yaml` file lists `entity_vars`/`input_vars` used to create the output tables under `var_groups`.\n\nThe following `profiles.yaml` file defines a group of vars named `vars_list`. It also defines two models namely `user_profile`, and `user_python_model`:\n\n```\nvar_groups:\n  name: vars_list\n  entity_key: user # This is the name defined in project file. If we change that, we need to change the name here too.\n  vars:\n    - entity_var:\n        name: is_mql\n        select: max(case when salesForceLeadsTable.mql__c == 'True' then 1 else 0 end)\n        from: inputs/salesForceLeadsTable\n        description: Whether a domain is mql or not\n    - entity_var:\n        name: blacklistFlag\n        select: max(case when exclude_reason is not null then 1 else 0 end)\n        from: inputs/blacklistDomains\n        where: (context_sources_job_run_id = (select top 1 context_sources_job_run_id from blacklistDomains order by timestamp desc))\n        is_feature: false\n    - entity_var:\n        name: ignore_domain\n        select: case when {{user.Var(\"blacklistFlag\")}} = 1 or {{user.Var(\"domainSummary_account_type\")}} like '%free%' then 1 else 0 end\n        description: Whether a domain should be ignored for the analysis\n    - entity_var:\n        name: salesEvents\n        select: json_agg(activitydate, case when (type='Email' or tasksubtype = 'Email') then case when lower(subject) like '%[in]%' then 'sf_inbound_email' \\\n              else 'sf_outbound_email' end when macro(call_conversion) then 'sf_call' else null end as event)\n        from: inputs/salesforceTasks\n        description: Salesforce touches are converted to one of following events - sf_inbound_email, sf_outbound_email, sf_call, null\n        is_feature: false\n    - entity_var:\n        name: webhookFormSubmit\n        select:  min(timestamp)\n        from: inputs/webhookSource\n        where: variable_1 is null and timestamp < sales_conversion_timestamp and timestamp > var('start_date')\nmodels:\n  - name: user_profile\n    model_type: feature_table_model\n    model_spec:\n      validity_time: 24h # 1 day\n      entity_key: user\n```\n\n##### `var_groups`\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `name` | String | A unique name for the var\\_group. |\n| `entity_key` | String | The entity to which the var\\_group belongs to. |\n| `vars` | Object | This section is used to specify variables, with the help of `entity_var` and `input_var`. Aggregation on stitched ID type is done by default and is implicit. |\n\nOptionally, you can create models using the above vars. The following fields are common for all the model types:\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `name` | String | Name of the model. Note that a table with the same name is created in the data warehouse. For example, if you define the name as `user_table`, the output table will be named something like `Material_user_table_<rest-of-generated-hash>_<timestamp-number>`. |\n| `model_type` | String | Defines the type of model. Possible values are: `id_stitcher`, `feature_table_model`, `sql_template`, `entity_cohort`, `id_collator`, `python_model`, `feature_views`, etc. |\n| `model_spec` | Object | Creates a detailed configuration specification for the target model. Different schema is applicable for different model types as explained in each section below. |\n\nRudderStack supports the following model types:\n\n*   [Feature Views](https://www.rudderstack.com/docs/archive/profiles/0.13/example/feature-views/)\n*   [Feature Table (legacy)](https://www.rudderstack.com/docs/archive/profiles/0.13/example/feature-table/)\n*   [SQL Template](https://www.rudderstack.com/docs/archive/profiles/0.13/example/sql-model/)\n*   [ID Stitcher](https://www.rudderstack.com/docs/archive/profiles/0.13/example/id-stitcher/)\n*   [ID Collator](https://www.rudderstack.com/docs/archive/profiles/0.13/example/id-collator/)\n*   [Python model](https://www.rudderstack.com/docs/archive/profiles/0.13/predictions/#python-model)\n*   [Packages](https://www.rudderstack.com/docs/archive/profiles/0.13/example/packages/)\n\n### `README.md`\n\nThe `README.md` file provides a quick overview on how to use PB along with SQL queries for data analysis.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Project Structure | RudderStack Docs",
    "description": "Know the specifications of a site configuration file, PB project structure, configuration files, and their parameters.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.14/resources/yaml-refresher/",
    "markdown": "# YAML Best Practices | RudderStack Docs\n\nQuick overview of YAML and its basics for use in Profiles.\n\n* * *\n\n*     4 minute read  \n    \n\nYAML is the preferred choice for writing Profile Builder files due to its simplicity and ease of use.\n\nThis guide explains the base concepts, syntax, and best practices for writing code in YAML.\n\n## What is YAML?\n\n[YAML](https://yaml.org/), short for **YAML Ain’t Markup Language** or **Yet Another Markup Language**, is a data serialization format often used in config files and exchange of data. A YAML file uses indentation, specific characters, and line breaks for representing various data structures.\n\n## Sample YAML file\n\nBelow is how a sample YAML document looks like. It contains key-value pairs where the keys are on the left, followed by a colon (`:`), and the associated values are on the right. The hierarchy and data structure is defined using indentation. The next section explains this in more detail.\n\n```\n# This is a comment\nperson:\n  name: Ruddy Buddy # Note the spacing used for indentation\n  age: 42\n  is_employed: true\n  address: # An object called address\n    street: Jefferson Davis Highway\n    city: Ruther Glen\n    state: Vermont\n    phone: 555-90-210\n  favorite_sports: # A list\n    - soccer\n    - baseball\n```\n\nThe above code has details of an object called `person` with properties like `name`, `age`, `gender`, `is_student`, `address` and `favorite sports`.\n\nHere’s how the same YAML file looks in the JSON format:\n\n```\n{\n  \"person\": {\n    \"name\": \"Ruddy Buddy\",\n    \"age\": 42,\n    \"is_employed\": true,\n    \"address\": {\n      \"street\": \"Jefferson Davis Highway\",\n      \"city\": \"Ruther Glen\",\n      \"state\": \"Vermont\",\n      \"phone\": \"555-90-210\"\n    },\n    \"favorite_sports\": [\n      \"soccer\",\n      \"baseball\"\n    ]\n  }\n}\n```\n\n## Indentation\n\nIn YAML, the indentation is done using spaces - to define the structure of data. Throughout the YAML file, the number of spacing should be consistent. Typically, we use two spaces for indentation. YAML is whitespace-sensitive, so do not mix spaces and tabs.\n\n```\n# Example of correct indentation\nperson:\n  name: Ruddy Buddy # We used 2 spaces\n  age: 42\n\n# Example of incorrect indentation\nperson:\n  name: Ruddy Buddy \n    age: 42 # We mixed spacing and tabs\n```\n\nAs shown above, YAML has single-line comments that start with hash (`#`) symbol, for providing additional explanation or context in the code. Comments are used to improve readability and they do not affect the code’s functionality.\n\n```\n# YAML comment\nperson:\n  name: Ruddy Buddy # Name of the person\n  age: 42 # Age of the person\n```\n\n## Data types in YAML\n\nYAML supports several data types:\n\n*   **Scalars**: Represent strings, numbers, and boolean values.\n*   **Sequences**: Represent lists and are denoted using a hyphen (`-`).\n*   **Mappings**: Key-value pairs used to define objects or dictionaries using colon (`:`).\n\n```\n# Example of data types in YAML\nperson:\n  name: Ruddy Buddy # Scalar (string)\n  age: 42 # Scalar (number)\n  is_employed: true # Scalar (boolean)\n  address: # Mapping (object)\n    street: Jefferson Davis Highway\n    city: Ruther Glen\n    state: Vermont\n    phone: 555-90-210\n  favorite_sports: # Sequence (list)\n    - soccer\n    - baseball\n```\n\n## Chomp modifiers\n\nYAML provides two chomp modifiers for handling line breaks in scalar values.\n\n*   `>`: Removes all newlines and replaces them with spaces.\n\n```\ndescription: >\n  Here is an example of long description\n  which has multiple lines. Later, it\n  will be converted into a single line.  \n```\n\n*   `|`: Preserves line breaks and spaces.\n\n```\ndescription: |\n  Here is another long description, however\n  it will preserve newlines and so the original\n  format shall be as-it-is.  \n```\n\n## Special characters\n\nYou can use escape symbols for special characters in YAML. For example, writing an apostrophe in description can cause the YAML parser to fail. In this case, you can use the escape character.\n\n## Best practices for writing YAML\n\nFollow these best practices for writing clean YAML code in your Profiles projects:\n\n*   Always keep consistent indentation (preferably spaces over tabs).\n*   Give meaningful names to your keys.\n*   Avoid excessive nesting.\n*   YAML is case sensitive, so be mindful of that.\n*   Add comments wherever required.\n*   Use blank lines to separate sections like ID stitcher, feature table, etc.\n*   If your strings contain special characters, then use escape symbols.\n*   Make sure you end the quotes in strings to avoid errors.\n*   Use chomp modifiers for multi-line SQL.\n\n## Conclusion\n\nThe above guidelines constitute some best practices to write effective [Builder](https://www.rudderstack.com/docs/profiles/get-started/profile-builder/) code in Profiles. You can also see the following references:\n\n*   [YAML for VS Code](https://marketplace.visualstudio.com/items?itemName=redhat.vscode-yaml): Extension for comprehensive YAML support in Visual Studio Code.\n*   [YAML Lint](https://www.yamllint.com/) for linting.\n\nFor more information or in any case of any issues, [contact](mailto:support@rudderstack.com) the RudderStack team.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "YAML Best Practices | RudderStack Docs",
    "description": "Quick overview of YAML and its basics for use in Profiles.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.13/permissions/",
    "markdown": "# Warehouse Permissions | RudderStack Docs\n\nGrant RudderStack the required permissions on your data warehouse.\n\n* * *\n\n*     7 minute read  \n    \n\nRudderStack supports **Snowflake**, **Redshift**, **Databricks**, and **BigQuery** for creating unified user profiles.\n\nTo read and write data to the warehouse, RudderStack requires specific warehouse permissions as explained in the following sections.\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> Keeping separate schemas for projects running via CLI and web is recommended. This way projects run from the CLI will never risk overwriting your production data.\n\n## Snowflake\n\nSnowflake uses a combination of DAC and RBAC models for [access control](https://docs.snowflake.com/en/user-guide/security-access-control-overview.html). However, RudderStack chooses an RBAC-based access control mechanism as multiple users can launch the [Profile Builder CLI](https://www.rudderstack.com/docs/archive/profiles/0.13/get-started/profile-builder/).\n\nAlso, it is not ideal to tie the result of an individual user run with that user. Hence, it is recommended to create a generic role (for example, `PROFILES_ROLE`) with the following privileges:\n\n*   Read access to all the inputs to the model (can be shared in case of multiple schemas/tables).\n*   Write access to the schemas and common tables as the PB project creates material (output) tables.\n\nIf you want to access any material created from the project run, the role (`PROFILES_ROLE`) must also have read access to all of those schemas.\n\nBelow are some sample commands which grant the required privileges to the role (`PROFILES_ROLE`) in a Snowflake warehouse:\n\n```\n-- Create role\nCREATE ROLE PROFILES_ROLE;\nSHOW ROLES; -- To validate\n```\n\n```\n-- Create user\nCREATE USER PROFILES_TEST_USER PASSWORD='<StrongPassword>' DEFAULT_ROLE='PROFILES_ROLE';\nSHOW USERS; -- To validate\n```\n\n```\n-- Grant role to user and database\nGRANT ROLE PROFILES_ROLE TO USER PROFILES_TEST_USER;\nGRANT USAGE ON DATABASE YOUR_RUDDERSTACK_DB TO ROLE PROFILES_ROLE;\n```\n\n```\n-- Create separate schema for Profiles and grant privileges to role\nCREATE SCHEMA YOUR_RUDDERSTACK_DB.RS_PROFILES;\nGRANT ALL PRIVILEGES ON SCHEMA YOUR_RUDDERSTACK_DB.RS_PROFILES TO ROLE PROFILES_ROLE;\nGRANT USAGE ON WAREHOUSE RUDDER_WAREHOUSE TO ROLE PROFILES_ROLE;\nGRANT USAGE ON SCHEMA YOUR_RUDDERSTACK_DB.EVENTSSCHEMA TO ROLE PROFILES_ROLE;\n```\n\nFor accessing input sources, you can individually grant select on tables/views, or give blanket grant to all in a schema.\n\n```\n-- Assuming we want read access to tables/views in schema EVENTSSCHEMA\nGRANT SELECT ON ALL TABLES IN SCHEMA YOUR_RUDDERSTACK_DB.RS_PROFILES TO PROFILES_ROLE;\nGRANT SELECT ON FUTURE TABLES IN SCHEMA YOUR_RUDDERSTACK_DB.RS_PROFILES TO PROFILES_ROLE;\nGRANT SELECT ON ALL VIEWS IN SCHEMA YOUR_RUDDERSTACK_DB.RS_PROFILES TO PROFILES_ROLE;\nGRANT SELECT ON FUTURE VIEWS IN SCHEMA YOUR_RUDDERSTACK_DB.RS_PROFILES TO PROFILES_ROLE;\n```\n\n```\n-- Assuming we want read access to tracks and identifies tables in schema EVENTSSCHEMA\nGRANT SELECT ON TABLE YOUR_RUDDERSTACK_DB.RS_PROFILES.TRACKS TO PROFILES_ROLE;\nGRANT SELECT ON TABLE YOUR_RUDDERSTACK_DB.RS_PROFILES.IDENTIFIES TO PROFILES_ROLE;\n```\n\n## Redshift\n\nSuppose the inputs/edge sources are in a single schema `website_eventstream` and the name of the newly created Profiles user is `rudderstack_admin`. In this case, the requirements are as follows:\n\n*   A separate schema `rs_profiles` (to store all the common and output tables).\n*   The `rudderstack_admin` user should have all the privileges on the above schema and the associated tables.\n*   The `rudderstack_admin` user should have `USAGE` privilege on schemas that have the edge sources and input tables (`website_eventstream`) and read (`SELECT`) privileges on specific tables as well. This privilege can extend to the migration schema and other schemas from where data from warehouses comes in.\n*   The `rudderstack_admin` user should have privileges to use `plpythonu` to create some UDFs.\n\nThe sample commands are as follows:\n\n```\nCREATE USER rudderstack_admin WITH PASSWORD '<strong_unique_password>';\nCREATE SCHEMA rs_profiles;\nGRANT ALL ON SCHEMA \"rs_profiles\" TO rudderstack_admin;\nGRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA \"rs_profiles\" TO rudderstack_admin;\nGRANT USAGE ON SCHEMA \"website_eventstream\" TO rudderstack_admin;\nGRANT USAGE ON LANGUAGE plpythonu TO rudderstack_admin;\n```\n\n```\nGRANT SELECT ON ALL TABLES IN SCHEMA \"website_eventstream\" TO rudderstack_admin;\n```\n\nTo give access to only specific input tables/views referred in your Profiles project, use the below command:\n\n```\nGRANT SELECT ON TABLE \"<YOUR_SCHEMA>\".\"<YOUR_TABLE>\" TO rudderstack_admin;\n```\n\n### Supported inputs\n\nRudderStack supports the following input types for Redshift warehouse/serverless:\n\n*   Redshift cluster with DC2 type nodes with following types as inputs:\n    *   Redshift internal tables\n    *   External schema and tables only for inputs (not supported as output)\n    *   CSV files stored on S3 as inputs\n*   Redshift cluster with RA3 type nodes with following types as inputs:\n    *   Redshift internal tables\n    *   External schema and tables only for inputs (not supported as output)\n    *   CSV files stored on S3 as inputs\n    *   Cross DB input tables\n*   Redshift serverless with following types as inputs:\n    *   Redshift internal tables\n    *   External schema and tables (not supported as output)\n    *   CSV files stored on S3 as inputs\n    *   Cross DB input tables RudderStack also supports various authentication mechanisms to authenticate the user running the Profiles project. Refer [Redshift warehouse connection](https://www.rudderstack.com/docs/archive/profiles/0.13/get-started/profile-builder/#2-create-warehouse-connection) for more information.\n\n## Databricks\n\n1.  Open the Databricks UI.\n2.  Create a new user.\n3.  Reuse an existing catalog or create a new one by clicking **Create Catalog**.\n4.  Grant `USE SCHEMA` privilege on the catalog.\n5.  Create a separate schema to write objects created by RudderStack Profiles.\n6.  Grant all privileges on this schema.\n7.  Grant privileges to access relevant schemas for the input tables. For example, if an input schema is in a schema named `website_eventstream`, then you can run the following commands to assign a blanket grant to all schemas or only specific tables/views referred in your Profiles project:\n\n```\nCREATE USER rudderstack_admin WITH PASSWORD <strong_unique_password>;\nGRANT USE SCHEMA ON CATALOG <catalog name> TO rudderstack_admin;\nCREATE SCHEMA RS_PROFILES;\nGRANT ALL PRIVILEGES ON SCHEMA RS_PROFILES TO rudderstack_admin;\nGRANT SELECT ON SCHEMA website_eventstream TO rudderstack_admin;\n```\n\n```\nGRANT SELECT ON ALL TABLES IN SCHEMA \"website_eventstream\" TO rudderstack_admin;\n```\n\nTo give access to only specific input tables/views referred in your Profiles project, use the below command:\n\n```\nGRANT SELECT ON TABLE public.input_table TO rudderstack_admin; \n```\n\n## BigQuery\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> For BigQuery, RudderStack recommends you to use a view instead of table for streaming data sets.\n\nYou must first assign the `BigQuery Job User` role to your service account. To do so:\n\n1.  Open the [BigQuery UI](https://console.cloud.google.com/) (Google Cloud Console).\n2.  Select **IAM & Admin** > **IAM** from the left sidebar.\n3.  Click **GRANT ACCESS** to grant permissions to your service account.\n4.  Enter your service account email in the **New principals** field.\n5.  In the **Assign roles** section, select `BigQuery Job User` role from the role list.\n6.  Click **Save**.\n\nAlternatively, you can add the following **IAM Policy Binding**:\n\n```\n{\n  \"bindings\": [\n    {\n      \"role\": \"roles/bigquery.jobUser\",\n      \"members\": [\n        \"serviceAccount:your-service-account@your-project.iam.gserviceaccount.com\"\n      ]\n    }\n  ]\n}\n```\n\nFurther, assign the dataset or project level roles to your service account:\n\n1.  Open the [BigQuery UI](https://console.cloud.google.com/) (Google Cloud Console).\n2.  Select **BigQuery** from the sidebar.\n3.  Select the dataset from your schema to which you want to grant access.\n4.  In the **Dataset info** window, click **SHARING** > **Permissions**.\n5.  Click **Add Principal**.\n6.  Enter the service account email in the **New principals** field.\n7.  In the **Assign roles** section, select the following roles from the role list:\n    *   **BigQuery Data Viewer**: Allows read access to the dataset.\n    *   **BigQuery Data Editor**: Allows read and write access to the dataset.\n\nAlternatively, you can add the following **IAM Policy Binding**:\n\n```\n{\n  \"bindings\": [\n    {\n      \"role\": \"roles/bigquery.dataViewer\",\n      \"members\": [\n        \"serviceAccount:your-service-account@your-project.iam.gserviceaccount.com\"\n      ],\n      \"condition\": {\n        \"title\": \"Access to dataset1\",\n        \"description\": \"Allow data viewing access to dataset1\",\n        \"expression\": \"resource.name.startsWith('projects/your-project-id/datasets/dataset1')\"\n      }\n    },\n    {\n      \"role\": \"roles/bigquery.dataEditor\",\n      \"members\": [\n        \"serviceAccount:your-service-account@your-project.iam.gserviceaccount.com\"\n      ],\n      \"condition\": {\n        \"title\": \"Access to dataset2\",\n        \"description\": \"Allow data editing access to dataset2\",\n        \"expression\": \"resource.name.startsWith('projects/your-project-id/datasets/dataset2')\"\n      }\n    }\n  ]\n}\n```\n\nIf you have input relations in many datasets from a single project, you can assign the required privileges for the complete Google Cloud project. This allows your service account to access the relation from all the datasets of the project.\n\n1.  Open the [BigQuery UI](https://console.cloud.google.com/) (Google Cloud Console).\n2.  Select **IAM & Admin** > **IAM** from the left sidebar.\n3.  Click **GRANT ACCESS** to grant permissions to your service account.\n4.  Enter the service account email in the **New principals** field.\n5.  In the **Assign roles** section, select the following roles from the role list:\n    *   **BigQuery Data Viewer**: Allows read access to the dataset.\n    *   **BigQuery Data Editor**: Allows read and write access to the dataset.\n\nAlternatively, you can add the following **IAM Policy Binding**:\n\n```\n{\n  \"bindings\": [\n    {\n      \"role\": \"roles/bigquery.dataViewer\",\n      \"members\": [\n        \"serviceAccount:your-service-account@your-project.iam.gserviceaccount.com\"\n      ]\n    },\n    {\n      \"role\": \"roles/bigquery.dataEditor\",\n      \"members\": [\n        \"serviceAccount:your-service-account@your-project.iam.gserviceaccount.com\"\n      ]\n    }\n  ]\n}\n```\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Warehouse Permissions | RudderStack Docs",
    "description": "Grant RudderStack the required permissions on your data warehouse.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.13/cli-user-guide/commands/",
    "markdown": "# Commands | RudderStack Docs\n\nLearn about the Profiles commands and how to use them.\n\n* * *\n\n*     12 minute read  \n    \n\nThe Profile Builder tool supports specific commands, making executing the usual operations easier. The basic syntax of executing a command is:\n\n```\n$ pb <command> <subcommand> [parameters]\n```\n\n## Supported commands\n\nYou can use the following Profile Builder commands:\n\n### cleanup\n\nDisplays and removes materials, older than the retention time period specified by the user (default value is 180 days).\n\n```\npb cleanup materials -r <number of days>\n```\n\n**Optional Parameter**\n\n| Parameter | Description |\n| --- | --- |\n| `-r` | Retention time in number of days.<br><br>**Example**: If you pass 1, then all the materials created prior to one day (24 hours) are listed. This is followed by prompts asking you for confirmation, after which you can view the material names and delete them. |\n\n### compile\n\nGenerates SQL queries from models.\n\nIt creates SQL queries from the `models/profiles.yaml` file, storing the generated results in the **Output** subfolder in the project’s folder. With each run, a new folder is created inside it. You can manually execute these SQL files on the warehouse.\n\n**Optional parameters**\n\n| Parameter | Description |\n| --- | --- |\n| `clean_output` | Empties the output folder(s) before executing the command. |\n| `-c` | Uses a site configuration file other than the one in `.pb` directory.<br><br>**Example**: `$ pb compile -c MyOtherConnection/siteconfig.yaml` |\n| `-t` | Defines target name (mentioned in `siteconfig.yaml`) or timestamp in building the model.<br><br>**Example**: If your `siteconfig.yaml` has two targets, `dev` and `test`, and you want to use the `test` instance: `$ pb compile -t test` |\n| `--begin_time` | Timestamp to be used as a start time in building model. |\n| `--end_time` | Timestamp to be used as an end time in building model. |\n| `--migrate_on_load` | Whether to automatically migrate the project and packages to the latest version. Defaults to false. |\n| `--migrated_folder_path` | Folder location of the migrated project. Defaults to sub-directory of the project folder. |\n| `-p` | *   Uses a project file (`pb_project.yaml`) other than the one in current directory.  <br>    **Example**: `$ pb compile -p MyOtherProject`.<br>  <br>*   Fetches project from a URL such as GitHub.  <br>    **Example**:`$ pb compile -p git@github.com:<orgname>/<repo>`. You can also fetch a specific tag, like `$ pb compile -p git@github.com:<orgname>/<repo>/tag/<tag_version>/<folderpath>` |\n| `--rebase_incremental` | Rebases any incremental models (build afresh from their inputs) instead of starting from a previous run. You can do this every once in a while to address the stale data or migration/cleanup of an input table. |\n\n### discover\n\nDiscovers elements in the warehouse, such as models, entities, features and sources.\n\nIt allows you to discover all the registered elements in the warehouse.\n\n**Subcommands**\n\nDiscover all the `models`, `entities`, `features`, `sources`, and `materials` in the warehouse.\n\n```\n$ pb discover models\n$ pb discover entities\n$ pb discover features\n$ pb discover sources\n$ pb discover materials\n```\n\n**Optional parameters**\n\n| Parameter | Description |\n| --- | --- |\n| `-e` | Discovers specific entities with their name.<br><br>**Example**: `$ pb discover -e 'Name'` |\n| `-m` | Discovers a specific model.<br><br>**Example**: `$ pb discover -m 'MY_DATABASE.PROD_SCHEMA.CREATED_MODEL'` |\n| `-c` | Uses a site config other than the default one.<br><br>**Example**: `$ pb discover -c siteconfig.yaml` |\n| `-s` | Discovers entities in a specified schema. |\n| `-s \"*\"` | Discovers entities across all schemas (case-sensitive). |\n| `-u` | Discovers entities having the specified source URL’s.<br><br>**Example**: To discover all the entities coming from GitHub: `$ pb discover -u %github%` |\n| `-t` | Selects target (mentioned in `siteconfig.yaml`). |\n| `-p` | Uses project folder other than the one in current directory.<br><br>**Example**: `$ pb discover -p ThisFolder/ThatSubFolder/SomeOtherProject/` |\n| `-f` | Specifies a file path to dump the discovery output into a csv file.<br><br>**Example**: `$ pb discover -f path/to/csv_file.csv` |\n| `-k` | Restricts discovery of the specified model keys.<br><br>**Example**: `$ pb discover -k entity_key:mode_type:model_name` |\n| `--csv_file` | Specify this flag with a file path to dump the discovery output into a csv file. |\n\n**Examples**\n\n```\n# Discover all the models\n$ pb discover models\n\n# Discover a model with specific name\n$ pb discover -m 'RUDDER_WEB_EVENTS.PROD_SCHEMA.feature_profile'\n\n# Discover all features having 'max' in their name\n$ pb discover features -u %max%\n\n# Discover all the entities for a specific profile\n$ pb discover entities -c siteconfig.yaml\n\n# Discover all materials for target dev\n$ pb discover materials -t dev\n\n# Export output of discover command to a CSV file in output folder\n$ pb discover -f my-custom-name.csv\n\n# Export all sources to a CSV file in output folder\n$ pb discover sources -f my-custom-name.csv\n```\n\n### help\n\nProvides list information for any command.\n\n**Subcommand**\n\nGet usage information for a specific command, with subcommands, and optional parameters.\n\n### init\n\nCreates connection and initializes projects.\n\n**Subcommands**\n\nInputs values for a warehouse connection and then stores it in the `siteconfig.yaml` file.\n\nGenerates files in a folder named **HelloPbProject** with sample data. You can change it as per project information, models, etc.\n\n**Optional parameters**\n\n| Parameter | Description |\n| --- | --- |\n| `pb-project -o` | Creates a Profile Builder project with a different name by specifying it as an additional parameter.<br><br>**Example**: To create a Profile Builder project with the name **SomeOtherProject**: `$ pb init pb-project -o SomeOtherProject` |\n| `connection -c` | Creates `siteconfig.yaml` at a location other than `.pb` inside home directory.<br><br>**Example**: To create `myconfig.yaml` in the current folder: `$ pb init connection -c myconfig.yaml`. |\n\n### insert\n\nAllows you to store the test dataset in your (Snowflake) warehouse . It creates the tables `sample_rs_demo_identifies` and `sample_rs_demo_tracks` in your warehouse schema specified in the `test` connection.\n\n```\n# Select the first connection named test having target and output as dev, of type Snowflake.\n$ pb insert\n# By default it'll pick up connection named test. To use connection named red:\n$ pb insert -n red\n# To pick up connection named red, with target test .\n$ pb insert -n red -t test\n```\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> This command is supported only for Snowflake currently.\n\n### migrate\n\nMigrate your project to the latest schema.\n\n**Subcommands**\n\nBased on the current schema version of your project, it enlists all the steps needed to migrate it to the latest one.\n\nAutomatically migrate from one version to another.\n\nTo migrate your models:\n\n**Schema 44 onwards**\n\nNavigate to the folder where your project files are stored. Then execute one of the following:\n\n*   `pb migrate auto --inplace`: Replaces contents of existing folder with the migrated folder.\n*   `pb migrate auto -d <MigratedFolder>`: Keeps the original project intact and stores the migrated project in another folder.\n\n**Schema 43 -> 44:**\n\nUse `{{entity-name.Var(var-name)}}` to refer to an `entity-var` or an `input-var`.\n\nFor example, for entity\\_var `user_lifespan` in your HelloPbProject, change `select: last_seen - first_seen` to `select: '{{user.Var(\"last_seen\")}} - {{user.Var(\"first_seen\")}}'`.\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> Note that:\n> \n> *   You must use two curly brackets.\n> *   Anything contained within double curly brackets must be written in double quotes (`\" \"`). If you use single quotes within double quotes, then use the escape character (`\\`) that comes when using macros.\n\nFurther, navigate to the folder where your project files are stored. Then execute one of the following:\n\n*   `pb migrate auto --inplace`: Replaces contents of existing folder with the migrated folder.\n*   `pb migrate auto -d <MigratedFolder>`: Keeps the original project intact and stores the migrated project in another folder.\n\n**Linear dependency**\n\nSpecify this parameter when entity as vars migration is not done (till version 43). After the migration is done, it’s not necessary to mention this parameter and can be removed.\n\n```\n  compatibility_mode:\n    linear_dependency_of_vars: true\n```\n\n**Optional parameters**\n\n| Parameter | Description |\n| --- | --- |\n| `-p` | Uses a project file other than the one in current directory. |\n| `-c` | Uses a `siteconfig.yaml` file other than the one in your home directory. |\n| `-t` | Target name (defaults to the one specified in `siteconfig.yaml` file). |\n| `-v` | Version to which the project needs to be migrated (defaults to the latest version). |\n| `-d` | Destination folder to store the migrated project files.<br><br>**Example**: `pb migrate auto -d FolderName` |\n| `--force` | Ignores warnings (if any) and migrates the project. |\n| `--inplace` | Overwrites the source folder and stores migrated project files in place of original.<br><br>**Example**: `pb migrate auto --inplace` |\n| `-p` | Uses a project folder other than the one in current directory.<br><br>**Example**: `$ pb discover -p ThisFolder/ThatSubFolder/SomeOtherProject/` |\n| `-f` | Specifies a file path to dump the discovery output into a csv file.<br><br>**Example**: `$ pb discover -f path/to/csv_file.csv` |\n| `-k` | Restricts discovery of the specified model keys.<br><br>**Example**: `$ pb discover -k entity_key:mode_type:model_name` |\n\n### run\n\nCreates identity stitcher or feature table model in the Warehouse.\n\nIt generates the SQL files from models and executes them in the warehouse. Once executed, you can see the output table names, which are accessible from the warehouse.\n\n**Optional parameters**\n\nThe `run` command shares the same parameters as the [`compile`](#compile) command, in addition to the following ones:\n\n| Parameter | Description |\n| --- | --- |\n| `--force` | Does a force run even if the material already exists. |\n| `-- write_output_csv` | Writes all the generated tables to CSV files in the specified directory.<br><br>**Example**: `$ pb run -- WriteOutputHere.csv` |\n| `--model_args` | Customizes behavior of an individual model by passing configuration params to it.<br><br>The only argument type supported currently is `breakpoint` for feature table models.<br><br>The `breakpoint` parameter lets you generate and run SQL only till a specific feature/tablevar. You can specify it in the format `modelName:argType:argName` where argName is the name of feature/tablevar.<br><br>**Example**: `$ pb run --model_args domain_profile:breakpoint:salesforceEvents` |\n| `--model_refs` | Restricts the operation to a specified model. You can specify model references like `pb run --model_refs models/<model-name> --seq_no latest` |\n| `--seq_no` | Sequence number for the run, for example, 0, 1, 2,…, latest/new. The default value is `new`. You can check run logs or use discover commands to know about existing sequence numbers. |\n| `--ignore_model_errors` | Allows the project to continue to run in case of an erroneous model. The execution will not stop due to one bad model. |\n| `--grep_var_dependencies` | Uses regex pattern matching over fields from vars to find references to other vars and set dependencies. By default, it is set to `true`. |\n| `--concurrency` | (_Experimental_) Lets you run the models concurrently in a warehouse (wherever possible) based on the dependency graph. In CLI, you can specify the concurrency level for running models in a project via `pb run --concurrency <int>` (default int value is 1). Currently, this is supported only for Snowflake warehouse. It is recommended to use this option judiciously as applying a large value may not be supported by your warehouse. The concurrency limit for Snowflake is 20. To increase the limit, see [Snowflake docs](https://community.snowflake.com/s/question/0D50Z00008VjQDkSAN/how-to-handle-thenumberofwaitersexceedsthe20statementslimit-error). |\n| `--begin_time` | Timestamp to be used as a start time in building model. |\n| `--end_time` | Timestamp to be used as an end time in building model. |\n| `--migrate_on_load` | Whether to automatically migrate the project and packages to the latest version. Defaults to false. |\n| `--migrated_folder_path` | Folder location of the migrated project. Defaults to sub-directory of the project folder. |\n| `--include_untimed` | Whether to include data without timestamps when running models. Defaults to true. |\n\n### show\n\nObtains a comprehensive overview of models, id\\_clusters, packages, and more in a project. Its capacity to provide detailed information makes it particularly useful when searching for specific details, like all the models in your project.\n\n**Subcommands**\n\n1.  `pb show models`\n\nThis command lets you view information about the models in your project. The output includes the following information about each model:\n\n*   **Warehouse name**: Name of the table/view to be created in the warehouse.\n*   **Model type**: Whether its an identity stitching, feature table, SQL model etc.\n*   **Output type**: Whether the output type is `ephemeral`, `table`, or `view`.\n*   **Run type**: Whether the model’s run type is `discrete` or `incremental`.\n*   **SQL type**: Whether the SQL type of the model is `single_sql` or `multi_sql`.\n\n2.  `pb show models --json`\n\nThis subcommand saves all model details in a JSON file.\n\n3.  `pb show dependencies`\n\nThis subcommand generates a graph file (`dependencies.png`) highlighting the dependencies of all models in your project.\n\n4.  `pb show dataflow`\n\nThis subcommand generates a graph file (`dataflow.png`) highlighting the data flow of all models in your project.\n\n5.  `pb show idstitcher-report --id_stitcher_model models/<ModelName> --migrate_on_load`\n\nThis subcommand creates a detailed report about the identity stitching model runs. To know the exact modelRef to be used, you can execute `pb show models`. By default, it picks up the last run, which can be changed using flag `-l`. The output consists of:\n\n*   **ModelRef**: The model reference name.\n*   **Seq No**: Sequence number of the run for which you are creating the report.\n*   **Material Name**: Output name as created in warehouse.\n*   **Creation Time**: Time when the material object was created.\n*   **Model Converged**: Indicates a successful run if `true`.\n*   **Pre Stitched IDs before run**: Count of all the IDs before stitching.\n*   **Post Stitched IDs after run**: Count of unique IDs after stitching.\n\nProfile Builder also generates a HTML report with relevant results and graphics including largest cluster, ID graph, etc. It is saved in `output` folder and the exact path is shown on screen when you execute the command.\n\n6.  `pb show entity-lookup -v '<trait value>'`\n\nThis subcommand lists all the features associated with an entity using any of the traits (flag `-v`) as ID types (email, user id, etc. that you are trying to discover).\n\n**Optional parameters**\n\n| Parameter | Description |\n| --- | --- |\n| `--entity string` | (Optional) Passes the entity value. (default `user`). |\n| `-h` | Displays help information for the command. |\n| `-p` | Specifies the project path to list the models. If not specified, it uses the project in the current directory. |\n| `-c` | File location of the `siteconfig.yaml` (defaults to the one in your home directory). |\n| `-t` | Target name (defaults to the target specified in `siteconfig.yaml` file). |\n| `--include_disabled` | Lets the disabled models be a part of the generated graph image (applicable to [`dataflow` and `dependencies`](#show)). |\n| `--seq_no` | Specifies a particular run for an ID stitcher model (applicable for [`idstitcher-report`](#show)). |\n\n### query\n\nExecutes SQL query on the warehouse and prints the output on screen (10 rows by default).\n\nFor example, if you want to print the output of a specific table/view named `user_id_stitcher`, run the following query:\n\n```\npb query \"select * from user_id_stitcher\"\n```\n\nTo reference a model with the name `user_default_id_stitcher` for a previous run with seq\\_no 26, you can execute:\n\n```\npb query 'select * from {{this.DeRef(\"path/to/user_default_id_stitcher\")}} limit 10' --seq_no=26\n```\n\n**Optional parameters**:\n\n| Parameter | Description |\n| --- | --- |\n| `-f` | Exports output to a CSV file. |\n| `-max_rows` | Maximum number of rows to be printed (default is 10). |\n| `-seq_no` | Sequence number for the run. |\n\n### validate\n\nValidates aspects of the project and configuration.\n\nIt allows you to run various tests on the project-related configurations and validate those. This includes but is not limited to validating the project configuration, privileges associated with the role specified in the site configuration of the project’s connection, etc.\n\n**Subcommands**\n\nRuns tests on the role specified in the site configuration file and validates if the role has privileges to access all the related objects in the warehouse. It throws an error if the role does not have required privileges to access the input tables or does not have the permissions to write the material output in the output schema.\n\n### version\n\nShows the Profile Builder’s current version along with its GitHash and native schema version.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Commands | RudderStack Docs",
    "description": "Learn about the Profiles commands and how to use them.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.13/predictions/",
    "markdown": "# Predictions (Early Access) | RudderStack Docs\n\nUse Profiles’ predictive features to train machine learning models.\n\n* * *\n\n*     7 minute read  \n    \n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> Predictions is part of our [Early Access Program](https://www.rudderstack.com/docs/resources/early-access-program/), where we work with early users and customers to test new features and get feedback before making them generally available. These features are functional but can change as we improve them. We recommend connecting with our team before running them in production.\n> \n> [Contact us](https://www.rudderstack.com/contact/) to get access to this feature.\n\nPredictions extends Profiles’ standard [feature development](https://www.rudderstack.com/docs/archive/profiles/0.13/core-concepts/feature-development/) functionality. It lets you easily create predictive features in your warehouse and answer questions like:\n\n*   Is a customer likely to churn in the next 30 days?\n*   Will a user make a purchase in the next 7 days?\n*   Is a lead going to convert?\n*   How much is a user likely to spend in the next 90 days?\n\nFurther, you can add the predicted feature to user profiles in your warehouse automatically and deliver ML-based segments and audiences to your marketing, product, and customer success teams.\n\nThe following self-guided tour shows you how to build the predictive traits. You can also follow the [Predictions sample project](https://www.rudderstack.com/docs/archive/profiles/0.13/example/predictive-features-snowflake/) guide and build the project yourself, including sample data.\n\n## Use cases\n\n*   **Churn prediction**: Predicting churn is one of the crucial initiatives across businesses. Without a predicted churn score, your actions are reactive, whereas you can act proactively with a user trait like `is_likely_to_churn`. Once you have such features, you can activate them with the appropriate outreach programs to prevent user churn.\n    \n*   **Customer LTV prediction**: Predictions helps you understand your customers’ purchasing behavior over time. You can predict how much amount a particular customer is likely to spend within the predicted time range.\n    \n\n## Python model\n\nYou can generate predictive features using a `python_model` which involves two key steps - `train` and `predict`.\n\nThe following `profiles.yaml` file shows how to use a `python_model`:\n\n```\nmodels:\n  - name: shopify_churn\n    model_type: python_model\n    model_spec:\n      occurred_at_col: insert_ts\n      entity_key: user\n      validity_time: 24h # 1 day\n      py_repo_url: https://github.com/rudderlabs/rudderstack-profiles-classifier.git # Do not modify \n      # this value as the actual logic resides in this repo.\n      train:\n        file_extension: .json\n        file_validity: 60m\n        inputs: &inputs\n          - packages/feature_table/models/shopify_user_features\n        config:\n          data:\n            label_column: is_churned_7_days \n            label_value: 1\n            prediction_horizon_days: 7\n            output_profiles_ml_model: *model_name\n            eligible_users: lower(country) = 'us' and amount_spent_overall > 0\n            inputs: *inputs\n            entity_column: user_main_id\n            recall_to_precision_importance: 1.0\n          preprocessing: \n            ignore_features: [name, gender, device_type]\n      predict:\n        inputs:\n          - packages/feature_table/models/shopify_user_features\n        config:\n          outputs:\n            column_names:\n              percentile: &percentile_name percentile_churn_score_7_days\n              score: churn_score_7_days\n            feature_meta_data: &feature_meta_data\n              features:\n                - name: *percentile_name\n                  description: 'Percentile of churn score. Higher the percentile, higher the probability of churn'\n```\n\n#### Model parameters\n\nThe detailed list of parameters used in the `python_model` along with their description are listed below:\n\n| Parameter | Description |\n| --- | --- |\n| `py_repo_url`  <br>Required | The actual logic for Predictions resides in this remote repository. DO NOT modify this value. |\n| `file_extension`  <br>Required | Indicates the file type. This is a static value and does not need to be modified. |\n| `file_validity`  <br>Required | If the last trained model is older than this duration, then the model is trained again. |\n| `inputs`  <br>Required | Path to the base feature table project. You must add `&inputs` to it. |\n| `label_column`  <br>Required | Name of the feature (`entity_var`) you want to predict. It is defined in the feature table model. |\n| `label_value` | Expected label value for users who performed the event |\n| `prediction_horizon_days`  <br>Required | Number of days in future for which you want to make the prediction.<br><br>See [Prediction horizon days](https://www.rudderstack.com/docs/profiles/glossary/#prediction-horizon-days) for more information. |\n| `output_profiles_ml_model`  <br>Required | Name of the output model. |\n| `eligible_users` | Eligibilty criteria for the users for which you want to define predictive features. You can set this criteria by defining a SQL statement referring to the different `entity_vars`. To build a model for all the available users, you can leave this parameter as blank.<br><br>For example, if you want to train the model and make predictions only for the paying users from US, then define `country='US' and is_payer=true`. |\n| `config.data.inputs` | Path to the referenced project. |\n| `entity_column` | If you change the value of`id_column_name` in the ID stitcher model, you should specify it here. This field is optional otherwise. |\n| `recall_to_precision_importance` | Also referred to as **beta** in f-beta score, this field is used in classification models to fine-tune the model threshold and give more weight to recall against precision.<br><br>**Note**: This is an optional parameter. If not specified, it defaults to `1.0`, giving equal weight to precision and recall. |\n| `ignore_features` | List of columns from the feature table which the model should ignore while training. |\n| `percentile`  <br>Required | Name of column in output table having percentile score. |\n| `score`  <br>Required | Name of column in output table having probabilistic score. |\n| `description`  <br>Required | Custom description for the predictive feature. |\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> If you want to run your python model locally using a [CLI setup](https://www.rudderstack.com/docs/archive/profiles/0.13/get-started/profile-builder/), you must set up a python environment with the required packages and add the python path to your [siteconfig.yaml](https://www.rudderstack.com/docs/archive/profiles/0.13/cli-user-guide/structure/#site-configuration-file-configuration.md) file.\n\n## Project setup\n\nThis section highlights the project setup steps for a sample churn prediction and LTV model.\n\n### Prerequisites\n\n*   You must be using a [Snowflake](https://www.rudderstack.com/docs/destinations/warehouse-destinations/snowflake/), [BigQuery](https://www.rudderstack.com/docs/destinations/warehouse-destinations/bigquery/), or [Redshift](https://www.rudderstack.com/docs/destinations/warehouse-destinations/redshift/) warehouse.\n*   You must set up a standard Profiles project with a [feature table model](https://www.rudderstack.com/docs/archive/profiles/0.13/example/feature-table/).\n*   **Optional**: If you are using Snowflake, you might need to create a [Snowpark](https://www.snowflake.com/en/data-cloud/snowpark/)\\-optimized warehouse if your dataset is significantly large.\n\n### Churn prediction/LTV model\n\n#### 1\\. Create a Profiles project with Feature Table model\n\nFollow the [Feature table](https://www.rudderstack.com/docs/archive/profiles/0.13/example/feature-table/) guide to create a Profiles project. Your project must include the definition of the feature you want to predict.\n\nFor example, to predict 30-day inactive churn, you should define it as a feature (`entity_var`) in the feature table so that the model knows how to compute this for historic users.\n\n```\nentity_var:\n  name: churn_30_days\n  select: case when days_since_last_seen >= 30 then 1 else 0 end\n```\n\n#### 2\\. Create a python model and train it\n\nCreate a [`python_model`](#python-model) and pass the Feature table model as an input.\n\nAdd the following set of parameters in the `train` block:\n\n```\ntrain:\n    file_extension: .json\n    file_validity: 168h\n    inputs: &inputs\n      - packages/feature_table/models/shopify_user_features\n    config:\n      data: &model_data_input_configs\n        label_column: churn_30_days\n        label_value: 1\n        prediction_horizon_days: 30\n        output_profiles_ml_model: *model_name\n        eligible_users: ''\n        inputs: *inputs\n        entity_column: user_main_id\n        recall_to_precision_importance: 1.0\n      preprocessing: \n        ignore_features: [name, gender, device_type]\n```\n\n```\ntrain:\n    file_extension: .json\n    file_validity: 168h\n    inputs: &inputs\n      - packages/feature_table/models/shopify_user_features\n    config:\n      data: &model_data_input_configs\n        label_column: amount_spent_past_7_days\n        task: regression\n        prediction_horizon_days: 7\n        output_profiles_ml_model: *model_name\n        eligible_users: ''\n        inputs: *inputs\n        entity_column: user_main_id\n      preprocessing: \n        ignore_features: [name, gender, device_type]\n```\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> Note that:\n> \n> *   Unlike churn prediction, you should not specify the `label_value` and `recall_to_precision_importance` fields.\n> *   The LTV model introduces a new parameter called `task` which you must set to `regression`. Profiles assumes a classification model by default, unless explicitly specified otherwise.\n\n#### 3\\. Define predictive features\n\nAdd the following set of parameters in the `predict` block:\n\n```\npredict:\n    inputs:\n      - packages/feature_table/models/shopify_user_features\n    config:\n      data: *model_data_input_configs\n      outputs:\n        column_names:\n          percentile: &percentile_name percentile_churn_score_7_days\n          score: churn_score_7_days\n        feature_meta_data: &feature_meta_data\n          features:\n            - name: *percentile_name\n              description: 'Percentile of churn score. Higher the percentile, higher the probability of churn'\n```\n\n```\npredict:\n    inputs:\n      - packages/feature_table/models/shopify_user_features\n    config:\n      data: *model_data_configs\n      preprocessing: *model_prep_configs\n      outputs:\n        column_names:\n          percentile: &percentile_name percentile_predicted_amount_spent\n          score: predicted_amount_spent\n        feature_meta_data: &feature_meta_data\n          features:\n            - name: *percentile_name\n              description: 'Percentile of predicted future LTV. Higher the percentile, higher the expected LTV.'\n```\n\n#### 4\\. Run your project\n\nOnce you have created the project, you can choose to run it using either of the following ways:\n\n**Using Profile CLI**\n\nIf you have created your Predictions Profiles project locally, run it using the `pb run` [CLI](https://www.rudderstack.com/docs/archive/profiles/0.13/get-started/profile-builder/) command to generate output tables.\n\n**Using Profiles UI**\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> [Contact us](mailto:support@rudderstack.com) to enable this feature for your account.\n\nRun your Predictions Profiles project by first uploading it to a Git repository and then [importing it in the RudderStack dashboard](https://www.rudderstack.com/docs/archive/profiles/0.13/get-started/import-from-git/#steps).\n\n## Output\n\nOnce your project run is completed, you can:\n\n*   View the output materials in your warehouse for the predictive features.\n*   Check the predicted value for any given user in the RudderStack dashboard’s [Profile Lookup](https://www.rudderstack.com/docs/archive/profiles/0.13/get-started/quickstart-ui/#profile-details) section.\n*   View all predictive features in the **Entities** tab of your Profiles project:\n\n[![New personal access token in RudderStack dashboard](https://www.rudderstack.com/docs/images/profiles/predictive-features-2.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-2.webp)\n\nClick **Predictive features** to see the following view:\n\n[![New personal access token in RudderStack dashboard](https://www.rudderstack.com/docs/images/profiles/predictive-features.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features.webp)\n\nThe value of a predictive feature is a probability. You can consider it as `true` or `false` based on your threshold.\n\n## See Also\n\n*   [Predictive features](https://github.com/rudderlabs/rudderstack-profiles-classifier): Builds predictive features such as churn prediction, conversion prediction, etc.\n*   [Shopify churn model](https://github.com/rudderlabs/rudderstack-profiles-shopify-churn/): Builds a churn prediction score on top of the [Shopify library project](https://github.com/rudderlabs/profiles-shopify-features).\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Predictions (Early Access) | RudderStack Docs",
    "description": "Use Profiles' predictive features to train machine learning models.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.13/activations/",
    "markdown": "# Activations | RudderStack Docs\n\nPrivacy Overview\n\nThis site uses cookies to improve your experience while you navigate through the website. Out of these cookies, the cookies that are categorized as necessary are stored on your browser as they are as essential for the working of basic functionalities of the website. We also use third-party cookies that help us analyze and understand how you use this website. These cookies will be stored in your browser only with your consent. You also have the option to opt-out of these cookies. But opting out of some of these cookies may have an effect on your browsing experience.\n\nNecessary cookies are absolutely essential for the website to function properly. This category only includes cookies that ensures basic functionalities and security features of the website. These cookies do not store any personal information.",
    "title": "Activations | RudderStack Docs",
    "description": "Activate your cohorts data in downstream systems to run targeted campaigns.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.13/cohorts/",
    "markdown": "# Cohorts | RudderStack Docs\n\nCreate core customer segments in your warehouse and use them for targeted campaigns.\n\n* * *\n\n*     5 minute read  \n    \n\n**Cohort** is a subset of [entityEntity refers to a digital representation of a class of real world distinct objects for which you can create a profile.](https://www.rudderstack.com/docs/resources/glossary/#entity) instances meeting a specified set of characteristics, behaviors, or attributes. For example, if you have user as an entity, you can define cohorts like known users, new users, or North American users.\n\nUsing RudderStack Profiles, you can create the desired cohorts for entities and target specific user segments by enabling targeted campaigns and analysis.\n\n## Define cohorts\n\nProfiles lets you define cohorts as a model under `model_type` field in your `profiles.yaml` file:\n\n*   **Default cohort**: When you define an entity, a default cohort `<entity>/all` (`user/all` for the `user` entity) is created automatically. It contains the set of all instances of that entity. Any other cohort you define for that entity is derived from it.\n*   **Derived cohort**: When you define a cohort based on a pre-existing cohort (base cohort), it becomes a derived cohort. A derived cohort inherits the features of the base cohort. You can filter out the member instances of the base cohort based on a set of characteristics, behaviors, or attributes for the derived cohort. You must specify the base cohort in the derived cohort’s definition using the `extends` field.\n\nFor example, `known_users` is a cohort derived from the base cohort `user/all` (set of all users), whereas `known_mobile_users` is derived from its base cohort `known_users`.\n\nWhen you run a Profiles project including cohorts, the output of the cohort is stored in a table/view with the same name.\n\n### Sample cohort\n\nYou can apply filters using the `include`/`exclude` clauses to specify a boolean expression over any of the entity vars defined on the base cohort or its ancestors. Certain features might hold relevance only for the specific cohorts. For example, `SSN` feature may only be applicable for American users.\n\n**Example 1**: Let’s consider the following `profiles.yaml` file which defines a cohort `knownUsUsers` to include users from US with a linked email address.\n\n```\nmodels:\n  - name: knownUsUsers\n    model_type: entity_cohort\n    model_spec:\n      extends: user/all\n      materialization:\n        output_type: table\n      filter_pipeline:\n        - type: exclude\n            # exclude users which don't have any linked email.\n          value: \"{{ user.Var('id_type_email_count') }} = 0\"\n        - type: include\n            # include users with country US.\n          value: \"{{ user.Var('country') }} = 'US'\"   \n```\n\nHere, the `extends` keyword specifies the base cohort `users/all`. You can also specify the path of a custom defined base cohort, if applicable.\n\n**Example 2**: Let’s derive the `us_credit_card_users` cohort from the `knownUsUsers` as a base cohort. It filters the known US users who possess a credit card. The `extends` field specifies the path of the base cohort which is `models/knownUsUsers`.\n\n```\n-  name: us_credit_card_users\n     model_type: entity_cohort\n     model_spec:\n       extends: models/knownUsUsers\n       materialization:\n         output_type: view\n       filter_pipeline:\n         - type: include\n           value: \"{{ user.Var('has_credit_card') }} = 1\"\n```\n\n### Associate features with cohort\n\nYou can also use `var_groups` to target a cohort instead of an entire entity which will provide a comprehensive 360-degree view combining relevant features.\n\nTo do so, associate features with a cohort by specifying the `entity_cohort` key and passing the cohort’s path to it within a `var_group`, as shown:\n\n```\nvar_groups:\n  - name: known_us_users_vars\n    entity_cohort: models/knownUsUsers\n    vars:\n  \t- entity_var:\n  \t    name: has_credit_card\n  \t    from: inputs/rsIdentifies\n  \t    select: first_value(has_credit_card)\n            where: has_credit_card is not null \n            default: false\n  - name: user_vars\n    entity_key: user\n    vars:\n      - entity_var:\n          name: max_timestamp\n          select: max(timestamp)\n          from: inputs/rsIdentifies\n```\n\nTo apply the features to the entire user entity, you can use an `entity_key` in `user_vars`.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> In a `var_group`, you can use either `entity_key` or `entity_cohort` but not both. Setting `entity_key` as `user` is equivalent to setting `entity_cohort` as `user/all`.\n\n### Feature view of cohort\n\nYou can establish a holistic **360 feature view** of a cohort within its definition. This view consolidates all the features associated with the specified identifiers, providing a complete overview of the cohort.\n\nThe following example shows how to define a feature view for the `knownUsUsers` cohort:\n\n```\nmodels:\n  - name: knownUsUsers\n    model_type: entity_cohort\n    model_spec:\n      extends: users/all\n      materialization:\n        output_type: table\n      filter_pipeline:\n        - type: exclude\n            # exclude users which don't have any linked email.\n          value: \"{{ user.Var('id_type_email_count') }} = 0\"\n        - type: include\n            # include users with country US.\n          value: \"{{ user.Var('country') }} = 'US'\"\n      # to define a 360 feature view of knownUsUsers cohort [optional]\n      feature_views:\n        # view with entity's `main_id` as identifier\n        name: known_us_users_feature_view\n        using_ids:\n          - id: email\n            # view with `email` as identifier\n            name: us_users_with_email\n```\n\nHere, the `known_us_users_feature_view` view contains all the features of the `knownUsUsers` cohort and uses `main_id` as the identifier. There is another `us_users_with_email` view which also contains all the features of the `knownUsUsers` cohort but uses `email` as the identifier (specified in `using_ids` field).\n\n## Use cohorts\n\nOnce you have defined cohorts in your `profiles.yaml` file, you can choose to run your project in either of the following ways:\n\n### Profile CLI\n\nRun your [Profiles CLI project](https://www.rudderstack.com/docs/archive/profiles/0.13/get-started/profile-builder/) using the `pb run` command to generate output tables.\n\n### Profiles UI\n\nTo view cohorts in the RudderStack dashboard, you can make your Profiles CLI project available in a Git repository and import it in the RudderStack dashboard. See [Import Profiles Project from Git](https://www.rudderstack.com/docs/archive/profiles/0.13/get-started/import-from-git/) for more information.\n\nOnce imported, you can run your project by navigating to the **History** tab and clicking **Run**. After a successful run of the project, you can view the output for cohorts in the **Entities** tab of the project:\n\n[![Activation API](https://www.rudderstack.com/docs/images/profiles/cohorts-view-ui.webp)](https://www.rudderstack.com/docs/images/profiles/cohorts-view-ui.webp)\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> Contact Profiles support team in RudderStack’s [Community Slack](https://rudderstack.com/join-rudderstack-slack-community) if you are unable to see the **Entities** tab.\n\nYou can further activate your cohorts data by syncing it to the downstream destinations. See [Activations](https://www.rudderstack.com/docs/archive/profiles/0.13/activations/) for more information.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Cohorts | RudderStack Docs",
    "description": "Create core customer segments in your warehouse and use them for targeted campaigns.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.12/overview/",
    "markdown": "# Profiles Overview | RudderStack Docs\n\nCreate unified customer records in your warehouse using RudderStack Profiles.\n\nAvailable Plans\n\n*   enterprise\n\n* * *\n\n*     3 minute read  \n    \n\nModern data teams rely on their warehouse as a single source of truth for customer data. RudderStack’s **Profiles** unifies every user touchpoint and trait into comprehensive customer profiles, establishing the data warehouse as the core of the customer data platform.\n\nWith Profiles, data teams can efficiently resolve identities and create user features to produce a comprehensive customer 360 table.\n\nThe following self-guided tour shows how to use Profiles:\n\n## Highlights\n\nSee the following guides to learn more about Profiles features and their usage:\n\n| Guide | Description |\n| --- | --- |\n| [Quickstart](https://www.rudderstack.com/docs/archive/profiles/0.12/get-started/) | Create your first Profiles project using the [RudderStack dashboard](https://www.rudderstack.com/docs/archive/profiles/0.12/get-started/quickstart-ui/) or [Profile Builder (PB) CLI tool](https://www.rudderstack.com/docs/archive/profiles/0.12/get-started/profile-builder/). |\n| [Identity stitching](https://www.rudderstack.com/docs/archive/profiles/0.12/core-concepts/identity-stitching/) | Stitch different identifiers across multiple channels to create a comprehensive user profile. |\n| [Feature development](https://www.rudderstack.com/docs/archive/profiles/0.12/core-concepts/feature-development/) | Enhance the unified profiles with additional data points and features. |\n| [Warehouse permissions](https://www.rudderstack.com/docs/archive/profiles/0.12/permissions/) | Grant RudderStack the required permissions on your data warehouse. |\n| [Project structure](https://www.rudderstack.com/docs/archive/profiles/0.12/cli-user-guide/structure/) | Know the detailed project structure of a Profiles project. |\n| [Commands](https://www.rudderstack.com/docs/archive/profiles/0.12/cli-user-guide/commands/) | List of commands you can use for the Profiles CLI project. |\n| [Predictions](https://www.rudderstack.com/docs/archive/profiles/0.12/predictions/) | Create predictive features using the data present in your warehouse. |\n| [Examples](https://www.rudderstack.com/docs/archive/profiles/0.12/example/) | Create sample Profiles projects using different model types. |\n| [Glossary](https://www.rudderstack.com/docs/archive/profiles/0.12/resources/glossary/) | Commonly used Profiles terminology. |\n\n## Why use Profiles?\n\nData teams often face challenges when building a comprehensive customer view. Maintaining large, complex SQL models or working around the limitations of rigid SaaS platforms is time-consuming and expensive at scale.\n\nProfiles simplifies this process of unifying customer data by automating the manual data engineering and modeling required to build an identity graph, layer new data sources into customer profiles, and compute user features that leverage data from diverse sources.\n\nUsing Profiles, data teams can quickly build and easily maintain a comprehensive customer 360 table and make it available to downstream teams and tools.\n\n#### Move faster with an end-to-end platform\n\nProfiles integrates directly with RudderStack’s other pipelines:\n\n*   [Event Stream](https://www.rudderstack.com/docs/sources/event-streams/) and [Cloud Extract](https://www.rudderstack.com/docs/sources/extract/) (ETL) pipelines have known schemas and unique identifiers through the ingested customer data. Profiles can produce a baseline identity graph, and user features out of the box. Data teams can then augment the graph and features using any other data in their warehouse.\n*   [Reverse ETL](https://www.rudderstack.com/docs/sources/reverse-etl/) pipeline makes it easy to send data from the customer 360 table directly to the downstream tools used by marketing, customer success, product, and other teams.\n\n#### Enrich user profiles with features\n\nYou can [enhance user profiles](https://www.rudderstack.com/docs/archive/profiles/0.12/core-concepts/feature-development/) with additional data points and features. When new data sources are added, discovered, or calculated, data teams can add them to their Profiles configuration without having to clean data and update complex models and dependencies.\n\nThe features/traits can include demographic information, preferences, purchase history, browsing behavior, or other static or computed data points.\n\n#### Unlock deeper insights\n\nProfiles extends its capabilities to support features derived from complex concepts such as funnels, organizational metrics, and machine learning models. You can understand your customer’s journey through your sales funnel or locate each user across the histogram of customer metric values by simply defining a trait.\n\n#### Deliver personalization and recommendations\n\nUsing Profiles, you can ship projects like personalization significantly faster by focusing entirely on activating key user features instead of cleaning and modeling data to build them.\n\n#### Predict user conversions and churn\n\nYou can leverage [Predictions](https://www.rudderstack.com/docs/archive/profiles/0.12/predictions/) to build predictive features that help you predict in advance whether a lead is likely to convert, or a customer is likely to churn or make a purchase.\n\n## Who can leverage Profiles?\n\nProfiles is built for data engineers, data scientists, and technical marketers. You can define identity stitching, and user features as configuration files without requiring deep SQL, Python, or technical knowledge.\n\nYou can define the associated properties or attributes that provide detailed information for each new feature. For example, if the feature is `purchase_history`, its properties can include the date of purchase, product category, or order value.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Profiles Overview | RudderStack Docs",
    "description": "Create unified customer records in your warehouse using RudderStack Profiles.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.12/",
    "markdown": "# Profiles 0.12 | RudderStack Docs\n\nPrivacy Overview\n\nThis site uses cookies to improve your experience while you navigate through the website. Out of these cookies, the cookies that are categorized as necessary are stored on your browser as they are as essential for the working of basic functionalities of the website. We also use third-party cookies that help us analyze and understand how you use this website. These cookies will be stored in your browser only with your consent. You also have the option to opt-out of these cookies. But opting out of some of these cookies may have an effect on your browsing experience.\n\nNecessary cookies are absolutely essential for the website to function properly. This category only includes cookies that ensures basic functionalities and security features of the website. These cookies do not store any personal information.",
    "title": "Profiles 0.12 | RudderStack Docs",
    "description": "Documentation for Profiles v0.12",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.13/activation-api/",
    "markdown": "# Activation API (Early Access) | RudderStack Docs\n\nExpose user profiles stored in your Redis instance over an API.\n\n* * *\n\n*     7 minute read  \n    \n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> The Activation API is part of our [Early Access Program](https://www.rudderstack.com/docs/resources/early-access-program/), where we work with users and customers to test new features and get feedback before making them generally available. These features are fully functional but can change as we improve them. We recommend connecting with our team before running them in production.\n> \n> [Contact us](https://www.rudderstack.com/contact/) if you would like access to this feature.\n\nWith RudderStack’s Activation API, you can fetch enriched user traits stored in your Redis instance and use them for near real-time personalization for your target audience.\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> You must have a working Redis instance in place before setting up the connection.\n\n[![Activation API](https://www.rudderstack.com/docs/images/profiles/activation-api.webp)](https://www.rudderstack.com/docs/images/profiles/activation-api.webp)\n\n## Overview\n\nA brief summary of how the Activation API works:\n\n1.  Sync all your customer 360 data from your Profiles project to your Redis store.\n2.  The Activation API sits on top of this Redis instance and provides endpoints for retrieving and using the enriched user data for personalization.\n\n## How to use the Activation API\n\n1.  In your Profiles project settings, scroll down to **Activation API** and turn on the **Enable sync to Redis** toggle.\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> Before you enable the Activation API toggle, make sure that:\n> \n> *   You have at least one successful Profiles run.\n> *   Your `pb_project.yaml` > `entities` defines a `feature_views` property.\n\n[![Enable Redis sync for using Activation API](https://www.rudderstack.com/docs/images/profiles/enable-redis.webp)](https://www.rudderstack.com/docs/images/profiles/enable-redis.webp)\n\n2.  Enter the [account credentials for your Redis instance](#redis-configuration) and click **Create**. This will also create a Redis destination in your dashboard.\n\n[![Enable Redis sync for using Activation API](https://www.rudderstack.com/docs/images/profiles/enable-redis-sync.webp)](https://www.rudderstack.com/docs/images/profiles/enable-redis-sync.webp)\n\n3.  [Generate a personal access token](#faq) with **Admin** role in your RudderStack dashboard. You will need this token for authenticating the Activation API.\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> Personal access token with an **Admin** role is only available to the **Org Admin** users. Refer [User management](https://www.rudderstack.com/docs/dashboard-guides/user-management/#org-admin) for more information.\n\nNote your Redis destination ID from the [**Settings**](https://www.rudderstack.com/docs/dashboard-guides/destinations/#destination-details) tab. Further, [use the Activation API endpoint](#get-user-profiles) to access your Redis instance and get user data.\n\nThis API uses [Bearer Authentication](https://swagger.io/docs/specification/authentication/bearer-authentication/) for authenticating all requests. Set the [personal access token](#faq) as the bearer token for authentication.\n\n## Base URL\n\n```\nhttps://profiles.rudderstack.com/v1/\n```\n\n## Get user profiles\n\n#### Request body\n\nString\n\nRedis destination ID.\n\nObject\n\nID containing `type` and `value`\n\n```\n{\n  \"entity\": <entity_type>,  // User, project, account, etc.\n  \"destinationId\": <redis_destination_id> , // Redis destination ID\n  \"id\": {\n    \"type\": <id_type>,\n    \"value\": <id_value>\n  }\n}\n```\n\n#### Example request\n\n```\nPOST /v1/activation HTTP/1.1\nHost: profiles.rudderstack.com\nContent-Type: application/json\nAuthorization: Bearer <personal_access_token>\nContent-Length: 90\n\n{\n \"entity\": <entity_type>,\n \"destinationId\": <redis_destination_id>, // Redis destination ID\n \"id\": {\n   \"type\": <id_type>,\n   \"value\": <id_value>\n }\n}\n```\n\n```\ncurl --location 'https://profiles.rudderstack.com/v1/activation' \\\n--header 'Content-Type: application/json' \\\n--header 'Authorization: Bearer <personal_access_token>' \\\n--data '{\n \"entity\": <entity_type>,\n \"destinationId\": <redis_destination_id>, // Redis destination ID\n \"id\": {\n   \"type\": <id_type>,\n   \"value\": <id_value>\n }\n}'\n```\n\n```\nconst axios = require('axios');\nlet data = JSON.stringify({\n  \"destinationId\": <redis_destination_id>,\n  \"entity\": <entity_type>,\n  \"id\": {\n    \"type\": <id_type>,\n    \"value\": <id_value>\n  }\n});\n\nlet config = {\n  method: 'post',\n  maxBodyLength: Infinity,\n  url: 'https://profiles.rudderstack.com/v1/activation',\n  headers: {\n    'Content-Type': 'application/json',\n    'authorization': 'Bearer <personal_access_token>'\n  },\n  data: data\n};\n\naxios.request(config)\n  .then((response) => {\n    console.log(JSON.stringify(response.data));\n  })\n  .catch((error) => {\n    console.log(error);\n  });\n```\n\n#### Responses\n\n*   If the personal access token is absent or trying to access a destination to which it does not have access:\n\n```\nstatusCode: 401\nResponse: {\n  \"error\": \"Unauthorized request. Please check your access token\"\n}\n```\n\n*   If the destination is not Redis or the destination ID is absent/blank:\n\n```\nstatusCode: 404\nResponse: {\n  \"error\": \"Invalid Destination. Please verify you are passing the right destination ID\"\n}\n```\n\n*   If ID is present:\n\n```\nstatusCode: 200\nResponse:\n{\n  \"entity\": <entity_type>,\n  \"id\": {\n    \"type\": <id_type>,\n    \"value\": <id_value>\n  },\n  \"data\": {\n    <traits_from_Redis>\n  }\n}\n```\n\n*   If ID is not present in Redis:\n\n```\nstatusCode: 200\nResponse:\n{\n  \"entity\": <entity_type>,\n  \"id\": {\n    \"type\": <id_type>,\n    \"value\": <id_value>\n  },\n  \"data\": {}\n}\n```\n\n## Use case\n\nYou can use the Activation API for real-time personalization. Once you fetch the user traits from your Redis instance via the API, you can pull them into your client application to alter the application behavior in real-time based on user interactions.\n\nYou can respond immediately with triggered, user-focused messaging based on actions like page views or app clicks and provide a better customer experience.\n\n[![Real time personalization use case](https://www.rudderstack.com/docs/images/profiles/activation-api-use-case.webp)](https://www.rudderstack.com/docs/images/profiles/activation-api-use-case.webp)\n\n## Redis configuration\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> You must have a working Redis instance in place before setting up the connection.\n\n*   **Address**: Enter the public endpoint of your Redis database. If you are using [Redis Cloud](https://app.redislabs.com/#/), you can find this endpoint by going to your Redis database and navigating to **Configuration** tab > **General**.\n\n[![Redis database public endpoint](https://www.rudderstack.com/docs/images/profiles/redis-public-endpoint.webp)](https://www.rudderstack.com/docs/images/profiles/redis-public-endpoint.webp)\n\n*   **Password**: Enter the database password. You can find it in the **Security** section of the **Configuration** tab:\n\n[![Redis database password](https://www.rudderstack.com/docs/images/profiles/redis-database-password.webp)](https://www.rudderstack.com/docs/images/profiles/redis-database-password.webp)\n\n*   **Cluster Mode**: Turn on this setting if you’re connecting to a Redis cluster.\n*   **Secure**: Enable this setting to secure the TLS communication between RudderStack Redis client and your Redis server.\n\n## Data mapping\n\nRudderStack creates multiple Reverse ETL sources automatically based on your Profiles project. You will see separate sources for different `id_served` connected to the same Redis destination.\n\nThe following `pb_project.yaml` snippet shows the sources to be created:\n\n```\nentities:\n  - name: user\n    id_types:\n      - main_id\n      - user_id\n    feature_views:\n      using_ids:\n        - id: email\n          name: features_by_email # Optional. Takes default view name, if not specified.\n        - id: salesforce_id\n          name: salesforce_id_stitched_features\n      features: # Optional\n        - from: models/cart_feature_table\n          include:\n            - \"*\"\n```\n\n## FAQ\n\n#### How do I generate a personal access token to use the Activation API?\n\n1.  Log in to your [RudderStack dashboard](https://app.rudderstack.com/).\n2.  Go to **Settings** > **Your Profile** > **Account** tab and scroll down to **Personal access tokens**. Then, click **Generate new token**:\n\n[![New personal access token in RudderStack dashboard](https://www.rudderstack.com/docs/images/rudderstack-api/personal-access-token-1.webp)](https://www.rudderstack.com/docs/images/rudderstack-api/personal-access-token-1.webp)\n\n3.  Enter the **Token name**. Set **Role** to **Admin** and click **Generate**.\n\n[![Personal access token name and role](https://www.rudderstack.com/docs/images/rudderstack-api/personal-access-token-2.webp)](https://www.rudderstack.com/docs/images/rudderstack-api/personal-access-token-2.webp)\n\n4.  Use the personal access token to authenticate to the Activation API.\n\n[![Personal access token details](https://www.rudderstack.com/docs/images/rudderstack-api/personal-access-token-3.webp)](https://www.rudderstack.com/docs/images/rudderstack-api/personal-access-token-3.webp)\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> Save the generated token securely as it will not be visible again once you close this window.\n\n#### How can I make Profiles work with the Activation API?\n\nTo use the Activation API with your Profiles project, you need a successful run of your Profiles project that is not past the retention period.\n\nTo enable the Activation API for your Profiles project, turn on the **Enable sync to Redis** setting. A Profile run will then sync automatically.\n\n[![Toggle API in Settings](https://www.rudderstack.com/docs/images/rudderstack-api/activation-api-toggle-settings.png)](https://www.rudderstack.com/docs/images/rudderstack-api/activation-api-toggle-settings.png)\n\n#### Why am I getting an error trying to enable API in my instance for a custom project hosted on GitHub?\n\nFor GitHub projects, you need to explicitly add the IDs of the custom project that need to be served.\n\nIn your `pb_project.yaml` file, you can specify them as shown:\n\n```\nentities:\n  - name: user\n    id_types:\n      - main_id\n      - user_id\n    feature_views:\n      name: user_feature_view\n      using_ids:\n        - id: email\n          name: features_by_email\n        - id: salesforce_id\n          name: salesforce_id_stitched_features\n      features:\n        - from: models/cart_feature_table\n          include:\n            - \"*\"\n```\n\n#### If I force a full resync, stop it, and then start a new sync, does RudderStack always perform a full sync the next time?\n\nIt depends on the state of the task when it was canceled.\n\n*   If the sync is cancelled while RudderStack is preparing a snapshot, then next run depends on the state of the previous successful run and any mapping changes.\n*   If it is cancelled after the sync data is prepared, the next run is incremental.\n\nGenerally if a sync is cancelled manually, it is recommended to trigger a full sync if the previous cancelled task was a full sync. If the previously cancelled sync was incremental, triggering an incremental sync is recommended.\n\n#### Does RudderStack perform a full sync if I add a new column?\n\nRudderStack does not change the sync mode if you make any column additions. It triggers a full sync only if you change/update the data mappings, for example, if the newly added column is sent to the destination via the [Visual Data Mapper](https://www.rudderstack.com/docs/sources/reverse-etl/visual-data-mapper/).\n\nFor Profiles activation syncs, RudderStack updates the mappings and automatically sends all columns from the customer 360 view by triggering a full sync.\n\n#### Suppose I’m running a full sync and the Profiles job is running in parallel and finishes eventually. What happens to the scheduled sync? Does it get queued?\n\nRudderStack first creates a temporary snapshot copy of any sync when it starts. So its syncing the created copy. Even if a Profiles job is running in parallel, the sync - if started - is not impacted by it.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Activation API (Early Access) | RudderStack Docs",
    "description": "Expose user profiles stored in your Redis instance over an API.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.12/how-profiles-works/",
    "markdown": "# How Profiles Works | RudderStack Docs\n\nKnow how Profiles collect, unify, and activate your data to enhance the overall customer experience.\n\n* * *\n\n*     3 minute read  \n    \n\nRudderStack helps you build a complete CDP on top of your data warehouse in three stages - **Collect**, **Unify**, and **Activate**.\n\nThe following sections highlight RudderStack’s comprehensive solution at every stage to create a complete customer profile.\n\n[![Profiles Overview](https://www.rudderstack.com/docs/images/profiles/profiles-overview.png)](https://www.rudderstack.com/docs/images/profiles/profiles-overview.png)\n\n### Collect\n\nFirst, RudderStack collects and stores all the source data in your warehouse. This includes:\n\n*   Event data, for example, user interaction data from web and mobile apps.\n*   Data from cloud sources, for example, CRM platforms like Salesforce, support tools like Zendesk, etc.\n\n#### Known data\n\nRudderStack collects all the information from:\n\n1.  First-party data: Data collected from the enterprise’s own mobile application, websites, POS systems, etc.\n2.  Third-party apps like SalesForce CRM, Zendesk Support, ecommerce payments via Stripe, etc.\n\nThere is a known ID for all of these by which RudderStack collects the data like email, user ID, etc.\n\n#### Unknown Data\n\nThis includes unknown user attributes like `anonymousId` captured from the RudderStack SDKs on the web/mobile apps. It is helpful in tracking user activities in cases where they are not logged in.\n\nThe difference between known and unknown data is that in the former, we have information about the user. First-party data can be known data if a user is logged in. For example, the data from cloud sources will always be known. However, data from the event stream sources can be known or unknown depending on whether a user had logged in.\n\nAs the data is collected, you can apply relevant transformations to it for compliance/security purposes like data governance, privacy, etc.\n\nThe below image highlights a snapshot of the `identifies` and `tracks` [tables](https://www.rudderstack.com/docs/destinations/warehouse-destinations/warehouse-schema/#schema), that RudderStack leverages for unifying the data.\n\n[![Identifies and Tracks tables for stitching](https://www.rudderstack.com/docs/images/profiles/identifies-tracks-stitch.png)](https://www.rudderstack.com/docs/images/profiles/identifies-tracks-stitch.png)\n\n### Unify\n\nAt this stage, Profiles takes over and does the following:\n\n#### ID Stitching\n\nProfiles stitches together all the known and unknown IDs into a single table. The IDs are linked using an autogenerated ID known as `rudderId`, which is akin to a golden record. Imagine a 1-to-many relationship, in which one `rudderId` has multiple values for other IDs like user ID, anonymous ID, email, etc.\n\nWith a `rudderId`, you can easily identify that a customer - who shopped on your website 6 months ago, anonymously browsed from mobile 4 months ago, raised a complaint with the support team 2 months ago - is actually the same customer. RudderStack represents them as different nodes/edges of the ID stitching graph.\n\n[![Identity stitching](https://www.rudderstack.com/docs/images/profiles/identity-stitching.png)](https://www.rudderstack.com/docs/images/profiles/identity-stitching.png)\n\n#### Entity Traits 360\n\nIf the features/traits of an entity are spread across multiple entity vars and ML models, you can use Entity Traits 360 to get them together into a single view. These models are usually defined in the `pb_project.yaml` file by creating entries under `serve_traits` key with corresponding entity.\n\n[![Features generation](https://www.rudderstack.com/docs/images/profiles/features-generation.png)](https://www.rudderstack.com/docs/images/profiles/features-generation.png)\n\n#### Feature Table\n\nThe entity vars specified in the project are unified into a view. A **Feature Table** is a unified customer profile containing useful information for each customer. Once all the known and unknown identities are stitched together, you can trace back activities for all such identifiers and aggregate them under the common `rudderId`. This is helpful in calculating features across all such interactions.\n\nSome common use cases are computing the customer’s total LTV (lifetime value), purchase history, number of days a customer was active, etc.\n\n### Activate\n\nActivation is a two-step process. In the first step, the user creates the target audience using RudderStack’s [Audiences](https://www.rudderstack.com/docs/data-pipelines/reverse-etl/features/audiences/) feature. They would then use Reverse ETL to route this audience information (also persisted in the same cloud warehouse) to downstream marketing tools like Braze, Mailchimp, etc.\n\n[![Audience Builder / Cohorts](https://www.rudderstack.com/docs/images/profiles/audience-builder-cohorts.png)](https://www.rudderstack.com/docs/images/profiles/audience-builder-cohorts.png)\n\nAdditionally - you can use the feature tables as inputs to ML models for use cases like churn prediction, lifetime value (LTV) prediction, etc. Again, you can route the output of such models to marketing platforms via Reverse ETL.\n\nAs you keep collecting data, Profiles continues to unify and send it for activation.\n\n[![Profiles lifecycle](https://www.rudderstack.com/docs/images/profiles/profiles-life-cycle.png)](https://www.rudderstack.com/docs/images/profiles/profiles-life-cycle.png)\n\nIn this way, you can make informed decisions, run personalized marketing campaigns, and enhance the overall customer experience across multiple platforms.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "How Profiles Works | RudderStack Docs",
    "description": "Know how Profiles collect, unify, and activate your data to enhance the overall customer experience.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.13/example/id-stitcher/",
    "markdown": "# Identity Stitching | RudderStack Docs\n\nStep-by-step tutorial on how to stitch together different user identities.\n\n* * *\n\n*     7 minute read  \n    \n\nThis guide provides a detailed walkthrough on how to use a PB project and create output tables in a warehouse for a custom identity stitching model.\n\n## Prerequisites\n\n*   Familiarize yourself with:\n    \n    *   A basic Profile Builder project by following the [Profile Builder CLI](https://www.rudderstack.com/docs/archive/profiles/0.13/get-started/profile-builder/) steps.\n    *   [Structure](https://www.rudderstack.com/docs/archive/profiles/0.13/cli-user-guide/structure/) of a Profile Builder project and the parameters used in different files.\n\n## Output\n\nAfter [running the project](https://www.rudderstack.com/docs/archive/profiles/0.13/get-started/profile-builder/#7-generate-output-tables), you can view the generated material tables:\n\n1.  Log in to your Snowflake console.\n2.  Click **Worksheets** from the top navigation bar.\n3.  In the left sidebar, click **Database** and the corresponding **Schema** to view the list of all tables. You can hover over a table to see the full table name along with its creation date and time.\n4.  Write a SQL query like `select * from <table_name> limit 10;` and execute it to see the results:\n\n1.  Open [Postico2](https://eggerapps.at/postico2/). If required, create a new connection by entering the relevant details. Click **Test Connection** followed by **Connect**.\n2.  Click the **+** icon next to **Queries** in the left sidebar.\n3.  You can click **Database** and the corresponding schema to view the list of all tables/views.\n4.  Double click on the appropriate view name to paste the name on an empty worksheet.\n5.  You can prefix `SELECT *` from the view name pasted previously and suffix `LIMIT 10;` at the end.\n6.  Press Cmd+Enter keys, or click the **Run** button to execute the query.\n\n1.  Enter your Databricks workspace URL in the web browser and log in with your username and password.\n2.  Click the **Catalog** icon in left sidebar.\n3.  Choose the appropriate catalog from the list and click on it to view the contents.\n4.  You will see list of tables/views. Click on the appropriate table/view name to paste the name on the worksheet.\n5.  Then, you can prefix `SELECT * FROM` before the pasted view name and suffix `LIMIT 10;` at the end.\n6.  Select the query text. Press Cmd+Enter or click the **Run** button to execute the query.\n\n1.  Log in to your [Google Cloud Console](https://console.cloud.google.com/)\n2.  Search for Bigquery in the search bar.\n3.  Select Bigquery from **Product and Pages** to open the Bigquery **Explorer**.\n4.  Select the correct project from top left drop down menu.\n5.  In the left sidebar, click the project ID, then the corresponding dataset view list of all the tables and views.\n6.  Write a SQL query like `select * from <table_name> limit 10;` and execute it to see the results.\n\nA sample output containing the results in Snowflake:\n\n![Generated tables (Snowflake)](https://www.rudderstack.com/docs/images/profiles/snowflake-console.webp)\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> Profiles project includes an ID stitcher model (`default_id_stitcher`) by default even if you do not define any specs for creating one. It takes all the input sources and ID types defined in the file `inputs.yaml` file. Also, it creates a custom ID stitcher when you define an ID stitcher model explicitly along with the specs.\n\n## Sample project for Custom ID Stitcher\n\nThis sample project considers multiple user identifiers in different warehouse tables to ties them together to create a unified user profile. The following sections describe how to define your PB project files:\n\n### Project detail\n\nThe [`pb_project.yaml`](https://www.rudderstack.com/docs/archive/profiles/0.13/cli-user-guide/structure/#project-details) file defines the project details such as name, schema version, connection name and the entities which represent different identifiers.\n\nThere can be different ID types for an entity. You can include all such identifiers in the `id_types` field under `entities`. `main_id` specified under `id_types` is not an ID type but a placeholder for the column which serves as the primary identifier for that entity.\n\nIn case of `id_stitcher` model, the `main_id` for the entity is `rudder_id` (predefined ID type) by default. For other models, any other ID type can be the `main_id`, for example `session_id`. Hence, if you want to specify the ID type of a column as a primary identifier, you can specify `main_id`.\n\n```\n# Project name\nname: sample_id_stitching\n# Project's yaml schema version\nschema_version: 63\n# Warehouse connection\nconnection: test\n# Folder containing models\nmodel_folders:\n  - models\n# Entities in this project and their ids.\nentities:\n  - name: user\n    id_stitcher: models/user_id_stitcher # modelRef of custom ID stitcher model\n    id_types:\n      - main_id # You need to add ``main_id`` to the list only if you have defined ``main_id_type: main_id`` in the id stitcher buildspec.\n      - user_id # one of the identifier from your data source.\n      - email\n# lib packages can be imported in project signifying that this project inherits its properties from there\npackages:\n  - name: corelib\n    url: \"https://github.com/rudderlabs/profiles-corelib/tag/schema_{{best_schema_version}}\"\n    # if required then you can extend the package definition such as for ID types.\n```\n\n### Input\n\nThe [input file](https://www.rudderstack.com/docs/archive/profiles/0.13/cli-user-guide/structure/#inputs) (`models/inputs.yaml`) file includes the input table references and corresponding SQL for the above-mentioned entities:\n\n```\ninputs:\n- name: rsIdentifies\n  contract: # constraints that a model adheres to\n    is_optional: false\n    is_event_stream: true\n    with_entity_ids:\n      - user\n    with_columns:\n      - name: timestamp\n      - name: user_id\n      - name: anonymous_id\n      - name: email\n  app_defaults:\n    table: rudder_events_production.web.identifies # one of the WH table RudderStack generates when processing identify or track events.\n    occurred_at_col: timestamp\n    ids:\n      - select: \"user_id\" # kind of identity sql to pick this column from above table.\n        type: user_id\n        entity: user # as defined in project file\n        to_default_stitcher: true # default value\n      - select: \"anonymous_id\"\n        type: anonymous_id\n        entity: user\n        to_default_stitcher: true\n      - select: \"lower(email)\" # can use sql.\n        type: email\n        entity: user\n        to_default_stitcher: true\n- name: rsTracks\n  contract:\n    is_optional: false\n    is_event_stream: true\n    with_entity_ids:\n      - user\n    with_columns:\n      - name: timestamp\n      - name: user_id\n      - name: anonymous_id\n  app_defaults:\n    table: rudder_events_production.web.tracks # another table in WH maintained by RudderStack processing track events.\n    occurred_at_col: timestamp\n    ids:\n      - select: \"user_id\"\n        type: user_id\n        entity: user\n        to_default_stitcher: true\n      - select: \"anonymous_id\"\n        type: anonymous_id\n        entity: user\n        to_default_stitcher: true\n```\n\nColumns specified under `ids` field are automatically sent for identity stitching unless you specify `to_default_stitcher` as `false`.\n\n### Model\n\nProfiles **Identity stitching** model maps and unifies all the specified identifiers (in `pb_project.yaml` file) across different platforms. It tracks the user journey uniquely across all the data sources and stitches them together to a `rudder_id`.\n\nA sample `profiles.yaml` file specifying an identity stitching model (`user_id_stitcher`) with relevant inputs:\n\n```\nmodels:\n  - name: user_id_stitcher\n    model_type: id_stitcher\n    model_spec:\n      validity_time: 24h\n      entity_key: user\n      materialization:\n        run_type: incremental # default value is `discrete` for a custom ID stitcher and `incremental` for the default ID stitcher.\n      incremental_timedelta: 12h\n      main_id_type: main_id\n      edge_sources:\n        - from: inputs/rsIdentifies\n        - from: inputs/rsTracks\n```\n\n##### Model specification fields\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `validity_time` | Time | Specifies the validity of the model with respect to its timestamp. For example, a model run as part of a scheduled nightly job for 2009-10-23 00:00:00 UTC with `validity_time`: `24h` would still be considered potentially valid and usable for any run requests, which do not require precise timestamps between 2009-10-23 00:00:00 UTC and 2009-10-24 00:00:00 UTC. This specifies the validity of generated feature table. Once the validity is expired, scheduling takes care of generating new tables. For example: 24h for 24 hours, 30m for 30 minutes, 3d for 3 days |\n| `entity_key` | String | Specifies the relevant entity from your `input.yaml` file. For example, here it should be set to `user`. |\n| `materialization` | List | Adds the key `run_type`: `incremental` to run the project in incremental mode. This mode considers row inserts and updates from the `edge_sources` input. These are inferred by checking the timestamp column for the next run. One can provide buffer time to consider any lag in data in the warehouse for the next incremental run like if new rows are added during the time of its run. If you do not specify this key then it’ll default to `run_type`: `discrete`. |\n| `incremental_timedelta` | List | (Optional )If materialization key is set to `run_type`: `incremental`, then this field sets how far back data should be fetched prior to the previous material for a model (to handle data lag, for example). The default value is 4 days. |\n| `main_id_type` | ProjectRef | (Optional) ID type reserved for the output of the identity stitching model, often set to `main_id`. It must not be used in any of the inputs and must be listed as an id type for the entity being stitched. If you do not set it, it defaults to `rudder_id`. Do not add this key unless it’s explicitly required, like if you want your identity stitcher table’s `main_id` column to be called `main_id`. For more information, see below. |\n| `edge_sources` | List | Specifies inputs for the identity stitching model as mentioned in the `inputs.yaml` file. |\n\n## Use cases\n\nThis section describes some common identity stitching use cases:\n\n*   **Identifiers from multiple data sources**: You can consider multiple identifiers and tables by:\n    \n    *   Adding entities in `pb_project.yaml` representing identifiers.\n    *   Adding references to table and corresponding sql in `models/inputs.yaml`\n    *   Adding table reference names defined in `models/inputs.yaml` as `edge_sources` in your model definition.\n*   **Leverage Sql Support**: You can use SQL in your `models/inputs.yaml` to achieve different scenarios. For example, you want to tag all the internal users in your organization as one entity. You can use the email domain as the identifier by adding a SQL query to extract the email domain as the identifier value: `lower(split_part({{email_col}}, '@', 2))`\n    \n*   **Custom ID Stitcher**: You can define a custom ID stitcher by defining the required id stitching model in `models/profiles.yaml`.\n    \n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Identity Stitching | RudderStack Docs",
    "description": "Step-by-step tutorial on how to stitch together different user identities.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.13/example/id-collator/",
    "markdown": "# ID Collator | RudderStack Docs\n\nStep-by-step tutorial stitching different user identities together.\n\n* * *\n\n*     3 minute read  \n    \n\nID Stitching is one of the most important features of Profiles. Being able to perform ID stitching to determine the accounts belonging to the same customer/user is very important to get a 360-degree view of that user.\n\nHowever many a times, we may not require ID stitching for a particular entity, especially if there are no edges in the ID graph of an entity. To build a feature table on such an entity, you will still need to perform ID stitching. Although this approach is not wrong, it is computationally redundant.\n\nProfiles provides the ID Collator is to get all IDs of that particular entity from various input tables and create one collated list of IDs.\n\n## Sample project\n\nLet’s take a case where we have defined two entities in our project - one is `user` and the other is `session`.\n\nIf `user` entity has multiple IDs defined, there are basically edges which make the use of an ID stitcher logical. On the other hand, `session` may have only one ID, `ssn_id`, there won’t be any possibility of edges. In such a case, all we need is a complete list of `ssn_id`.\n\nHere is the corresponding inputs and entities definition.\n\n```\nentities:\n  - name: user\n    id_column_name: user_rud_id\n    id_types:\n      - user_id\n      - anonymous_id\n  - name: session\n    id_column_name: session_id\n    id_types:\n      - ssn_id\n```\n\nProject file:\n\n```\ninputs:\n  - name: user_accounts\n    table: tbl_user_accounts\n    occurred_at_col: insert_ts\n    ids:\n      - select: \"user_id\"\n        type: user_id\n        entity: user\n  - name: sign_in\n    table: tbl_sign_in\n    occurred_at_col: insert_ts\n    ids:\n      - select: \"user_id\"\n        type: user_id\n        entity: user\n      - select: \"ssn_id\"\n        type: ssn_id\n        entity: session\n      - select: \"anonymous_id\"\n        type: anonymous_id\n        entity: user\n  - name: sign_up\n    table: tbl_sign_up\n    occurred_at_col: insert_ts\n    ids:\n      - select: \"user_id\"\n        type: user_id\n        entity: user\n      - select: \"ssn_id\"\n        type: ssn_id\n        entity: session\n      - select: \"anonymous_id\"\n        type: anonymous_id\n        entity: user\n```\n\nHere, the `entity: session` has only one ID type. Creating an ID stitcher for such an entity is possible but unnecessary.\n\nUsing all the models having `ssn_id`, we can just make a union of all `ssn_id` and get all distinct values of it and obtain the final list of sessions.\n\nThe underlying SQL will look as follows:\n\n```\nSELECT ssn_id AS session_id FROM sign_in\nUNION\nSELECT ssn_id AS session_id FROM sign_up;\n```\n\n## YAML Changes\n\nThe YAML writer cannot define a custom ID collator the way they define a custom ID stitcher. If an entity has no edges, the PB project will automatically figure out if an ID collator is needed. To exclude certain inputs (having the required ID) from being used in the collation, we can just set `to_id_stitcher: false` in the input.\n\n```\nentities:\n  - name: session\n    id_column_name: session_id\n    id_types:\n      - ssn_id\n```\n\nThe `id_column_name` is a new field added in the entity definition which will be the name of the ID column and it applies to both ID stitcher and ID collator.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> In the ID collator, you won’t generate a UUID like in ID stitcher.\n\n## Comparing ID Collator and ID Stitcher\n\n| ID Stitcher | ID Collator |\n| --- | --- |\n| Uses edges to converge the ID graph. | Collates all distinct IDs as there is only one ID Type and no edges are present. |\n| Higher cost of computation. | Lower cost of computation. |\n| A UUID is generated and used as the unique identifier for the entity. | Collates the existing IDs only. |\n| The generated ID is always of the type: `rudder_id` | The ID column of the generated ID collator table/view will be of the ID type of the corresponding ID. |\n| User may override the default ID stitcher with custom one. | You cannot override the default ID collator, though you can define a custom ID stitcher to override default ID collator. |\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "ID Collator | RudderStack Docs",
    "description": "Step-by-step tutorial stitching different user identities together.",
    "languageCode": "en"
  }
]