 image from its contents:",
  "title": "Examples using the Docker Engine SDKs and Docker API | Docker Docs\n",
  "description": "Examples on how to perform a given Docker operation using the Go and Python SDKs and the HTTP API using curl.",
  "languageCode": "en"
},
{
  "url": "https://docs.docker.com/build/guide/intro/",
  "markdown": "# Introduction | Docker Docs\n\nThe starting resources for this guide include a simple Go project and a Dockerfile. From this starting point, the guide illustrates various ways that you can improve how you build the application with Docker.\n\nTo follow this guide:\n\n1.  Install [Docker Desktop or Docker Engine](https://docs.docker.com/get-docker/)\n2.  Clone or create a new repository from the [application example on GitHub](https://github.com/dockersamples/buildme)\n\nThe example project for this guide is a client-server application for translating messages to a fictional language.\n\nHere’s an overview of the files included in the project:\n\nThe `cmd/` directory contains the code for the two application components: client and server. The client is a user interface for writing, sending, and receiving messages. The server receives messages from clients, translates them, and sends them back to the client.\n\nA Dockerfile is a text document in which you define the build steps for your application. You write the Dockerfile in a domain-specific language, called the Dockerfile syntax.\n\nHere's the Dockerfile used as the starting point for this guide:\n\nHere’s what this Dockerfile does:\n\n1.  `# syntax=docker/dockerfile:1`\n    \n    This comment is a [Dockerfile parser directive](https://docs.docker.com/reference/dockerfile/#parser-directives). It specifies which version of the Dockerfile syntax to use. This file uses the `dockerfile:1` syntax which is best practice: it ensures that you have access to the latest Docker build features.\n    \n2.  `FROM golang:1.21-alpine`\n    \n    The `FROM` instruction uses version `1.21-alpine` of the `golang` official image.\n    \n3.  `WORKDIR /src`\n    \n    Creates the `/src` working directory inside the container.\n    \n4.  `COPY . .`\n    \n    Copies the files in the build context to the working directory in the container.\n    \n5.  `RUN go mod download`\n    \n    Downloads the necessary Go modules to the container. Go modules is the dependency management tool for the Go programming language, similar to `npm install` for JavaScript, or `pip install` for Python.\n    \n6.  `RUN go build -o /bin/client ./cmd/client`\n    \n    Builds the `client` binary, which is used to send messages to be translated, into the `/bin` directory.\n    \n7.  `RUN go build -o /bin/server ./cmd/server`\n    \n    Builds the `server` binary, which listens for client translation requests, into the `/bin` directory.\n    \n8.  `ENTRYPOINT [ \"/bin/server\" ]`\n    \n    Specifies a command to run when the container starts. Starts the server process.\n    \n\nTo build an image using a Dockerfile, you use the `docker` command-line tool. The command for building an image is `docker build`.\n\nRun the following command to build the image.\n\nThis creates an image with the tag `buildme`. An image tag is the name of the image.\n\nThe image you just built contains two binaries, one for the server and one for the client. To see the translation service in action, run a container that hosts the server component, and then run another container that invokes the client.\n\nTo run a container, you use the `docker run` command.\n\n1.  Run a container from the image in detached mode.\n    \n    This starts a container named `buildme`.\n    \n2.  Run a new command in the `buildme` container that invokes the client binary.\n    \n\nThe `docker exec` command opens a terminal user interface where you can submit messages for the backend (server) process to translate.\n\nWhen you're done testing, you can stop the container:\n\nThis section gave you an overview of the example application used in this guide, an introduction to Dockerfiles and building. You've successfully built a container image and created a container from it.\n\nRelated information:\n\n*   [Dockerfile reference](https://docs.docker.com/reference/dockerfile/)\n*   [`docker build` CLI reference](https://docs.docker.com/reference/cli/docker/image/build/)\n*   [`docker run` CLI reference](https://docs.docker.com/reference/cli/docker/container/run/)\n\nThe next section explores how you can use layer cache to improve build speed.",
  "title": "Introduction | Docker Docs\n",
  "description": "An introduction to the Docker Build guide",
  "languageCode": "en"
},
{
  "url": "https://docs.docker.com/build/guide/layers/",
  "markdown": "# Layers | Docker Docs\n\nThe order of Dockerfile instructions matters. A Docker build consists of a series of ordered build instructions. Each instruction in a Dockerfile roughly translates to an image layer. The following diagram illustrates how a Dockerfile translates into a stack of layers in a container image.\n\n![From Dockerfile to layers](https://docs.docker.com/build/guide/images/layers.png)\n\nWhen you run a build, the builder attempts to reuse layers from earlier builds. If a layer of an image is unchanged, then the builder picks it up from the build cache. If a layer has changed since the last build, that layer, and all layers that follow, must be rebuilt.\n\nThe Dockerfile from the previous section copies all project files to the container (`COPY . .`) and then downloads application dependencies in the following step (`RUN go mod download`). If you were to change any of the project files, then that would invalidate the cache for the `COPY` layer. It also invalidates the cache for all of the layers that follow.\n\n![Layer cache is bust](https://docs.docker.com/build/guide/images/cache-bust.png)\n\nBecause of the current order of the Dockerfile instructions, the builder must download the Go modules again, despite none of the packages having changed since the last time.\n\nYou can avoid this redundancy by reordering the instructions in the Dockerfile. Change the order of the instructions so that downloading and installing dependencies occur before the source code is copied over to the container. In that way, the builder can reuse the \"dependencies\" layer from the cache, even when you make changes to your source code.\n\nGo uses two files, called `go.mod` and `go.sum`, to track dependencies for a project. These files are to Go, what `package.json` and `package-lock.json` are to JavaScript. For Go to know which dependencies to download, you need to copy the `go.mod` and `go.sum` files to the container. Add another `COPY` instruction before `RUN go mod download`, this time copying only the `go.mod` and `go.sum` files.\n\nNow if you edit your source code, building the image won't cause the builder to download the dependencies each time. The `COPY . .` instruction appears after the package management instructions, so the builder can reuse the `RUN go mod download` layer.\n\n![Reordered](https://docs.docker.com/build/guide/images/reordered-layers.png)\n\nOrdering your Dockerfile instructions appropriately helps you avoid unnecessary work at build time.\n\nRelated information:\n\n*   [Docker build cache](https://docs.docker.com/build/cache/)\n*   [Dockerfile best practices](https://docs.docker.com/build/building/best-practices/)\n\nThe next section shows how you can make the build run faster, and make the resulting output smaller, using multi-stage builds.",
  "title": "Layers | Docker Docs\n",
  "description": "Improving the initial Dockerfile using layers",
  "languageCode": "en"
},
{
  "url": "https://docs.docker.com/compose/compose-file/15-profiles/",
  "markdown": "# Profiles | Docker Docs\n\nWith profiles you can define a set of active profiles so your Compose application model is adjusted for various usages and environments.\n\nThe [services](https://docs.docker.com/compose/compose-file/05-services/) top-level element supports a `profiles` attribute to define a list of named profiles. Services without a `profiles` attribute are always enabled.\n\nA service is ignored by Compose when none of the listed `profiles` match the active ones, unless the service is explicitly targeted by a command. In that case its profile is added to the set of active profiles.\n\n> **Note**\n> \n> All other top-level elements are not affected by `profiles` and are always active.\n\nReferences to other services (by `links`, `extends` or shared resource syntax `service:xxx`) do not automatically enable a component that would otherwise have been ignored by active profiles. Instead Compose returns an error.\n\nIn the above example:\n\n*   If the Compose application model is parsed with no profile enabled, it only contains the `web` service.\n*   If the profile `test` is enabled, the model contains the services `test_lib` and `coverage_lib`, and service `web`, which is always enabled.\n*   If the profile `debug` is enabled, the model contains both `web` and `debug_lib` services, but not `test_lib` and `coverage_lib`, and as such the model is invalid regarding the `depends_on` constraint of `debug_lib`.\n*   If the profiles `debug` and `test` are enabled, the model contains all services; `web`, `test_lib`, `coverage_lib` and `debug_lib`.\n*   If Compose is executed with `test_lib` as the explicit service to run, `test_lib` and the `test` profile are active even if `test` profile is not enabled.\n*   If Compose is executed with `coverage_lib` as the explicit service to run, the service `coverage_lib` and the profile `test` are active and `test_lib` is pulled in by the `depends_on` constraint.\n*   If Compose is executed with `debug_lib` as the explicit service to run, again the model is invalid regarding the `depends_on` constraint of `debug_lib`, since `debug_lib` and `test_lib` have no common `profiles` listed.\n*   If Compose is executed with `debug_lib` as the explicit service to run and profile `test` is enabled, profile `debug` is automatically enabled and service `test_lib` is pulled in as a dependency starting both services `debug_lib` and `test_lib`.\n\nSee how you can use `profiles` in [Docker Compose](https://docs.docker.com/compose/profiles/).",
  "title": "Profiles | Docker Docs\n",
  "description": "Learn about profiles",
  "languageCode": "en"
},
{
  "url": "https://docs.docker.com/build/guide/mounts/",
  "markdown": "# Mounts | Docker Docs\n\nThis section describes how to use cache mounts and bind mounts with Docker builds.\n\nCache mounts let you specify a persistent package cache to be used during builds. The persistent cache helps speed up build steps, especially steps that involve installing packages using a package manager. Having a persistent cache for packages means that even if you rebuild a layer, you only download new or changed packages.\n\nCache mounts are created using the `--mount` flag together with the `RUN` instruction in the Dockerfile. To use a cache mount, the format for the flag is `--mount=type=cache,target=<path>`, where `<path>` is the location of the cache directory that you wish to mount into the container.\n\nThe target path to use for the cache mount depends on the package manager you’re using. The application example in this guide uses Go modules. That means that the target directory for the cache mount is the directory where the Go module cache gets written to. According to the [Go modules reference](https://go.dev/ref/mod#module-cache), the default location for the module cache is `$GOPATH/pkg/mod`, and the default value for `$GOPATH` is `/go`.\n\nUpdate the build steps for downloading packages and compiling the program to mount the `/go/pkg/mod` directory as a cache mount:\n\nThe `-x` flag added to the `go mod download` command prints the download executions that take place. Adding this flag lets you see how the cache mount is being used in the next step.\n\nBefore you rebuild the image, clear your build cache. This ensures that you're starting from a clean slate, making it easier to see exactly what the build is doing.\n\nNow it’s time to rebuild the image. Invoke the build command, this time together with the `--progress=plain` flag, while also redirecting the output to a log file.\n\nWhen the build has finished, inspect the `log1.txt` file. The logs show how the Go modules were downloaded as part of the build.\n\nNow, in order to see that the cache mount is being used, change the version of one of the Go modules that your program imports. By changing the module version, you're forcing Go to download the new version of the dependency the next time you build. If you weren’t using cache mounts, your system would re-download all modules. But because you've added a cache mount, Go can reuse most of the modules and only download the package versions that doesn't already exist in the `/go/pkg/mod` directory.\n\nUpdate the version of the `chi` package that the server component of the application uses:\n\nNow, run another build, and again redirect the build logs to a log file:\n\nNow if you inspect the `log2.txt` file, you’ll find that only the `chi` package that was changed has been downloaded:\n\nThere are a few more small optimizations that you can implement to improve the Dockerfile. Currently, it's using the `COPY` instruction to pull in the `go.mod` and `go.sum` files before downloading modules. Instead of copying those files over to the container’s filesystem, you can use a bind mount. A bind mount makes the files available to the container directly from the host. This change removes the need for the additional `COPY` instruction (and layer) entirely.\n\nSimilarly, you can use the same technique to remove the need for the second `COPY` instruction as well. Specify bind mounts in the `build-client` and `build-server` stages for mounting the current working directory.\n\nThis section has shown how you can improve your build speed using cache and bind mounts.\n\nRelated information:\n\n*   [Dockerfile reference](https://docs.docker.com/reference/dockerfile/#run---mount)\n*   [Bind mounts](https://docs.docker.com/storage/bind-mounts/)\n\nThe next section of this guide is an introduction to making your builds configurable, using build arguments.",
  "title": "Mounts | Docker Docs\n",
  "description": "Introduction to cache mounts and bind mounts in builds",
  "languageCode": "en"
},
{
  "url": "https://docs.docker.com/build/guide/build-args/",
  "markdown": "# Build arguments | Docker Docs\n\nBuild arguments is a great way to add flexibility to your builds. You can pass build arguments at build-time, and you can set a default value that the builder uses as a fallback.\n\nA practical use case for build arguments is to specify runtime versions for build stages. Your image uses the `golang:1.21-alpine` image as a base image. But what if someone wanted to use a different version of Go for building the application? They could update the version number inside the Dockerfile, but that’s inconvenient, it makes switching between versions more tedious than it has to be. Build arguments make life easier:\n\nThe `ARG` keyword is interpolated in the image name in the `FROM` instruction. The default value of the `GO_VERSION` build argument is set to `1.21`. If the build doesn't receive a `GO_VERSION` build argument, the `FROM` instruction resolves to `golang:1.21-alpine`.\n\nTry setting a different version of Go to use for building, using the `--build-arg` flag for the build command:\n\nRunning this command results in a build using the `golang:1.19-alpine` image.\n\nYou can also make use of build arguments to modify values in the source code of your program, at build time. This is useful for dynamically injecting information, avoiding hard-coded values. With Go, consuming external values at build time is done using linker flags, or `-ldflags`.\n\nThe server part of the application contains a conditional statement to print the app version, if a version is specified:\n\nYou could declare the version string value directly in the code. But, updating the version to line up with the release version of the application would require updating the code ahead of every release. That would be both tedious and error-prone. A better solution is to pass the version string as a build argument, and inject the build argument into the code.\n\nThe following example adds an `APP_VERSION` build argument to the `build-server` stage. The Go compiler uses the value of the build argument to set the value of a variable in the code.\n\nNow the version of the server is injected when building the binary, without having to update the source code. To verify this, you can build the `server` target and start a container with `docker run`. The server outputs `v0.0.1` as the version on startup.\n\nThis section showed how you can use build arguments to make builds more configurable, and inject values at build-time.\n\nRelated information:\n\n*   [`ARG` Dockerfile reference](https://docs.docker.com/reference/dockerfile/#arg)\n\nThe next section of this guide shows how you can use Docker builds to create not only container images, but executable binaries as well.",
  "title": "Build arguments | Docker Docs\n",
  "description": "Introduction to configurable builds, using build args",
  "languageCode": "en"
},
{
  "url": "https://docs.docker.com/engine/api/v1.24/",
  "markdown": "# Engine API v1.24 | Docker Docs\n\n*   The daemon listens on `unix:///var/run/docker.sock` but you can [Bind Docker to another host/port or a Unix socket](https://docs.docker.com/engine/reference/commandline/dockerd/#bind-docker-to-another-host-port-or-a-unix-socket).\n*   The API tends to be REST. However, for some complex commands, like `attach` or `pull`, the HTTP connection is hijacked to transport `stdout`, `stdin` and `stderr`.\n*   A `Content-Length` header should be present in `POST` requests to endpoints that expect a body.\n*   To lock to a specific version of the API, you prefix the URL with the version of the API to use. For example, `/v1.18/info`. If no version is included in the URL, the maximum supported API version is used.\n*   If the API version specified in the URL is not supported by the daemon, a HTTP `400 Bad Request` error message is returned.\n\nThe Engine API uses standard HTTP status codes to indicate the success or failure of the API call. The body of the response will be JSON in the following format:\n\n```\n{\n    \"message\": \"page not found\"\n}\n```\n\nThe status codes that are returned for each endpoint are specified in the endpoint documentation below.\n\n### [3.1 Containers](#31-containers)\n\n#### [List containers](#list-containers)\n\n`GET /containers/json`\n\nList containers\n\n**Example request**:\n\n```\nGET /v1.24/containers/json?all=1&before=8dfafdbc3a40&size=1 HTTP/1.1\n```\n\n**Example response**:\n\n```\nHTTP/1.1 200 OK\nContent-Type: application/json\n\n[\n     {\n             \"Id\": \"8dfafdbc3a40\",\n             \"Names\":[\"/boring_feynman\"],\n             \"Image\": \"ubuntu:latest\",\n             \"ImageID\": \"d74508fb6632491cea586a1fd7d748dfc5274cd6fdfedee309ecdcbc2bf5cb82\",\n             \"Command\": \"echo 1\",\n             \"Created\": 1367854155,\n             \"State\": \"exited\",\n             \"Status\": \"Exit 0\",\n             \"Ports\": [{\"PrivatePort\": 2222, \"PublicPort\": 3333, \"Type\": \"tcp\"}],\n             \"Labels\": {\n                     \"com.example.vendor\": \"Acme\",\n                     \"com.example.license\": \"GPL\",\n                     \"com.example.version\": \"1.0\"\n             },\n             \"SizeRw\": 12288,\n             \"SizeRootFs\": 0,\n             \"HostConfig\": {\n                     \"NetworkMode\": \"default\"\n             },\n             \"NetworkSettings\": {\n                     \"Networks\": {\n                             \"bridge\": {\n                                      \"IPAMConfig\": null,\n                                      \"Links\": null,\n                                      \"Aliases\": null,\n                                      \"NetworkID\": \"7ea29fc1412292a2d7bba362f9253545fecdfa8ce9a6e37dd10ba8bee7129812\",\n                                      \"EndpointID\": \"2cdc4edb1ded3631c81f57966563e5c8525b81121bb3706a9a9a3ae102711f3f\",\n                                      \"Gateway\": \"172.17.0.1\",\n                                      \"IPAddress\": \"172.17.0.2\",\n                                      \"IPPrefixLen\": 16,\n                                      \"IPv6Gateway\": \"\",\n                                      \"GlobalIPv6Address\": \"\",\n                                      \"GlobalIPv6PrefixLen\": 0,\n                                      \"MacAddress\": \"02:42:ac:11:00:02\"\n                              }\n                     }\n             },\n             \"Mounts\": [\n                     {\n                              \"Name\": \"fac362...80535\",\n                              \"Source\": \"/data\",\n                              \"Destination\": \"/data\",\n                              \"Driver\": \"local\",\n                              \"Mode\": \"ro,Z\",\n                              \"RW\": false,\n                              \"Propagation\": \"\"\n                     }\n             ]\n     },\n     {\n             \"Id\": \"9cd87474be90\",\n             \"Names\":[\"/coolName\"],\n             \"Image\": \"ubuntu:latest\",\n             \"ImageID\": \"d74508fb6632491cea586a1fd7d748dfc5274cd6fdfedee309ecdcbc2bf5cb82\",\n             \"Command\": \"echo 222222\",\n             \"Created\": 1367854155,\n             \"State\": \"exited\",\n             \"Status\": \"Exit 0\",\n             \"Ports\": [],\n             \"Labels\": {},\n             \"SizeRw\": 12288,\n             \"SizeRootFs\": 0,\n             \"HostConfig\": {\n                     \"NetworkMode\": \"default\"\n             },\n             \"NetworkSettings\": {\n                     \"Networks\": {\n                             \"bridge\": {\n                                      \"IPAMConfig\": null,\n                                      \"Links\": null,\n                                      \"Aliases\": null,\n                                      \"NetworkID\": \"7ea29fc1412292a2d7bba362f9253545fecdfa8ce9a6e37dd10ba8bee7129812\",\n                                      \"EndpointID\": \"88eaed7b37b38c2a3f0c4bc796494fdf51b270c2d22656412a2ca5d559a64d7a\",\n                                      \"Gateway\": \"172.17.0.1\",\n                                      \"IPAddress\": \"172.17.0.8\",\n                                      \"IPPrefixLen\": 16,\n                                      \"IPv6Gateway\": \"\",\n                                      \"GlobalIPv6Address\": \"\",\n                                      \"GlobalIPv6PrefixLen\": 0,\n                                      \"MacAddress\": \"02:42:ac:11:00:08\"\n                              }\n                     }\n             },\n             \"Mounts\": []\n     },\n     {\n             \"Id\": \"3176a2479c92\",\n             \"Names\":[\"/sleepy_dog\"],\n             \"Image\": \"ubuntu:latest\",\n             \"ImageID\": \"d74508fb6632491cea586a1fd7d748dfc5274cd6fdfedee309ecdcbc2bf5cb82\",\n             \"Command\": \"echo 3333333333333333\",\n             \"Created\": 1367854154,\n             \"State\": \"exited\",\n             \"Status\": \"Exit 0\",\n             \"Ports\":[],\n             \"Labels\": {},\n             \"SizeRw\":12288,\n             \"SizeRootFs\":0,\n             \"HostConfig\": {\n                     \"NetworkMode\": \"default\"\n             },\n             \"NetworkSettings\": {\n                     \"Networks\": {\n                             \"bridge\": {\n                                      \"IPAMConfig\": null,\n                                      \"Links\": null,\n                                      \"Aliases\": null,\n                                      \"NetworkID\": \"7ea29fc1412292a2d7bba362f9253545fecdfa8ce9a6e37dd10ba8bee7129812\",\n                                      \"EndpointID\": \"8b27c041c30326d59cd6e6f510d4f8d1d570a228466f956edf7815508f78e30d\",\n                                      \"Gateway\": \"172.17.0.1\",\n                                      \"IPAddress\": \"172.17.0.6\",\n                                      \"IPPrefixLen\": 16,\n                                      \"IPv6Gateway\": \"\",\n                                      \"GlobalIPv6Address\": \"\",\n                                      \"GlobalIPv6PrefixLen\": 0,\n                                      \"MacAddress\": \"02:42:ac:11:00:06\"\n                              }\n                     }\n             },\n             \"Mounts\": []\n     },\n     {\n             \"Id\": \"4cb07b47f9fb\",\n             \"Names\":[\"/running_cat\"],\n             \"Image\": \"ubuntu:latest\",\n             \"ImageID\": \"d74508fb6632491cea586a1fd7d748dfc5274cd6fdfedee309ecdcbc2bf5cb82\",\n             \"Command\": \"echo 444444444444444444444444444444444\",\n             \"Created\": 1367854152,\n             \"State\": \"exited\",\n             \"Status\": \"Exit 0\",\n             \"Ports\": [],\n             \"Labels\": {},\n             \"SizeRw\": 12288,\n             \"SizeRootFs\": 0,\n             \"HostConfig\": {\n                     \"NetworkMode\": \"default\"\n             },\n             \"NetworkSettings\": {\n                     \"Networks\": {\n                             \"bridge\": {\n                                      \"IPAMConfig\": null,\n                                      \"Links\": null,\n                                      \"Aliases\": null,\n                                      \"NetworkID\": \"7ea29fc1412292a2d7bba362f9253545fecdfa8ce9a6e37dd10ba8bee7129812\",\n                                      \"EndpointID\": \"d91c7b2f0644403d7ef3095985ea0e2370325cd2332ff3a3225c4247328e66e9\",\n                                      \"Gateway\": \"172.17.0.1\",\n                                      \"IPAddress\": \"172.17.0.5\",\n                                      \"IPPrefixLen\": 16,\n                                      \"IPv6Gateway\": \"\",\n                                      \"GlobalIPv6Address\": \"\",\n                                      \"GlobalIPv6PrefixLen\": 0,\n                                      \"MacAddress\": \"02:42:ac:11:00:05\"\n                              }\n                     }\n             },\n             \"Mounts\": []\n     }\n]\n```\n\n**Query parameters**:\n\n*   **all** – 1/True/true or 0/False/false, Show all containers. Only running containers are shown by default (i.e., this defaults to false)\n*   **limit** – Show `limit` last created containers, include non-running ones.\n*   **since** – Show only containers created since Id, include non-running ones.\n*   **before** – Show only containers created before Id, include non-running ones.\n*   **size** – 1/True/true or 0/False/false, Show the containers sizes\n*   **filters** - a JSON encoded value of the filters (a `map[string][]string`) to process on the containers list. Available filters:\n*   `exited=<int>`; -- containers with exit code of `<int>` ;\n*   `status=`(`created`|`restarting`|`running`|`paused`|`exited`|`dead`)\n*   `label=key` or `label=\"key=value\"` of a container label\n*   `isolation=`(`default`|`process`|`hyperv`) (Windows daemon only)\n*   `ancestor`\\=(`<image-name>[:<tag>]`, `<image id>` or `<image@digest>`)\n*   `before`\\=(`<container id>` or `<container name>`)\n*   `since`\\=(`<container id>` or `<container name>`)\n*   `volume`\\=(`<volume name>` or `<mount point destination>`)\n*   `network`\\=(`<network id>` or `<network name>`)\n\n**Status codes**:\n\n*   **200** – no error\n*   **400** – bad parameter\n*   **500** – server error\n\n#### [Create a container](#create-a-container)\n\n`POST /containers/create`\n\nCreate a container\n\n**Example request**:\n\n```\nPOST /v1.24/containers/create HTTP/1.1\nContent-Type: application/json\nContent-Length: 12345\n\n{\n       \"Hostname\": \"\",\n       \"Domainname\": \"\",\n       \"User\": \"\",\n       \"AttachStdin\": false,\n       \"AttachStdout\": true,\n       \"AttachStderr\": true,\n       \"Tty\": false,\n       \"OpenStdin\": false,\n       \"StdinOnce\": false,\n       \"Env\": [\n               \"FOO=bar\",\n               \"BAZ=quux\"\n       ],\n       \"Cmd\": [\n               \"date\"\n       ],\n       \"Entrypoint\": \"\",\n       \"Image\": \"ubuntu\",\n       \"Labels\": {\n               \"com.example.vendor\": \"Acme\",\n               \"com.example.license\": \"GPL\",\n               \"com.example.version\": \"1.0\"\n       },\n       \"Volumes\": {\n         \"/volumes/data\": {}\n       },\n       \"Healthcheck\":{\n          \"Test\": [\"CMD-SHELL\", \"curl localhost:3000\"],\n          \"Interval\": 1000000000,\n          \"Timeout\": 10000000000,\n          \"Retries\": 10,\n          \"StartPeriod\": 60000000000\n       },\n       \"WorkingDir\": \"\",\n       \"NetworkDisabled\": false,\n       \"MacAddress\": \"12:34:56:78:9a:bc\",\n       \"ExposedPorts\": {\n               \"22/tcp\": {}\n       },\n       \"StopSignal\": \"SIGTERM\",\n       \"HostConfig\": {\n         \"Binds\": [\"/tmp:/tmp\"],\n         \"Tmpfs\": { \"/run\": \"rw,noexec,nosuid,size=65536k\" },\n         \"Links\": [\"redis3:redis\"],\n         \"Memory\": 0,\n         \"MemorySwap\": 0,\n         \"MemoryReservation\": 0,\n         \"KernelMemory\": 0,\n         \"CpuPercent\": 80,\n         \"CpuShares\": 512,\n         \"CpuPeriod\": 100000,\n         \"CpuQuota\": 50000,\n         \"CpusetCpus\": \"0,1\",\n         \"CpusetMems\": \"0,1\",\n         \"IOMaximumBandwidth\": 0,\n         \"IOMaximumIOps\": 0,\n         \"BlkioWeight\": 300,\n         \"BlkioWeightDevice\": [{}],\n         \"BlkioDeviceReadBps\": [{}],\n         \"BlkioDeviceReadIOps\": [{}],\n         \"BlkioDeviceWriteBps\": [{}],\n         \"BlkioDeviceWriteIOps\": [{}],\n         \"MemorySwappiness\": 60,\n         \"OomKillDisable\": false,\n         \"OomScoreAdj\": 500,\n         \"PidMode\": \"\",\n         \"PidsLimit\": -1,\n         \"PortBindings\": { \"22/tcp\": [{ \"HostPort\": \"11022\" }] },\n         \"PublishAllPorts\": false,\n         \"Privileged\": false,\n         \"ReadonlyRootfs\": false,\n         \"Dns\": [\"8.8.8.8\"],\n         \"DnsOptions\": [\"\"],\n         \"DnsSearch\": [\"\"],\n         \"ExtraHosts\": null,\n         \"VolumesFrom\": [\"parent\", \"other:ro\"],\n         \"CapAdd\": [\"NET_ADMIN\"],\n         \"CapDrop\": [\"MKNOD\"],\n         \"GroupAdd\": [\"newgroup\"],\n         \"RestartPolicy\": { \"Name\": \"\", \"MaximumRetryCount\": 0 },\n         \"NetworkMode\": \"bridge\",\n         \"Devices\": [],\n         \"Sysctls\": { \"net.ipv4.ip_forward\": \"1\" },\n         \"Ulimits\": [{}],\n         \"LogConfig\": { \"Type\": \"json-file\", \"Config\": {} },\n         \"SecurityOpt\": [],\n         \"StorageOpt\": {},\n         \"CgroupParent\": \"\",\n         \"VolumeDriver\": \"\",\n         \"ShmSize\": 67108864\n      },\n      \"NetworkingConfig\": {\n          \"EndpointsConfig\": {\n              \"isolated_nw\" : {\n                  \"IPAMConfig\": {\n                      \"IPv4Address\":\"172.20.30.33\",\n                      \"IPv6Address\":\"2001:db8:abcd::3033\",\n                      \"LinkLocalIPs\":[\"169.254.34.68\", \"fe80::3468\"]\n                  },\n                  \"Links\":[\"container_1\", \"container_2\"],\n                  \"Aliases\":[\"server_x\", \"server_y\"]\n              }\n          }\n      }\n  }\n```\n\n**Example response**:\n\n```\n  HTTP/1.1 201 Created\n  Content-Type: application/json\n\n  {\n       \"Id\":\"e90e34656806\",\n       \"Warnings\":[]\n  }\n```\n\n**JSON parameters**:\n\n*   **Hostname** - A string value containing the hostname to use for the container. This must be a valid RFC 1123 hostname.\n*   **Domainname** - A string value containing the domain name to use for the container.\n*   **User** - A string value specifying the user inside the container.\n*   **AttachStdin** - Boolean value, attaches to `stdin`.\n*   **AttachStdout** - Boolean value, attaches to `stdout`.\n*   **AttachStderr** - Boolean value, attaches to `stderr`.\n*   **Tty** - Boolean value, Attach standard streams to a `tty`, including `stdin` if it is not closed.\n*   **OpenStdin** - Boolean value, opens `stdin`,\n*   **StdinOnce** - Boolean value, close `stdin` after the 1 attached client disconnects.\n*   **Env** - A list of environment variables in the form of `[\"VAR=value\", ...]`\n*   **Labels** - Adds a map of labels to a container. To specify a map: `{\"key\":\"value\", ... }`\n*   **Cmd** - Command to run specified as a string or an array of strings.\n*   **Entrypoint** - Set the entry point for the container as a string or an array of strings.\n*   **Image** - A string specifying the image name to use for the container.\n*   **Volumes** - An object mapping mount point paths (strings) inside the container to empty objects.\n*   **Healthcheck** - A test to perform to check that the container is healthy.\n    *   ```\n        **Test** - The test to perform. Possible values are:\n            + `{}` inherit healthcheck from image or parent image\n            + `{\"NONE\"}` disable healthcheck\n            + `{\"CMD\", args...}` exec arguments directly\n            + `{\"CMD-SHELL\", command}` run command with system's default shell\n        ```\n        \n    *   ```\n        **Interval** - The time to wait between checks in nanoseconds. It should be 0 or at least 1000000 (1 ms). 0 means inherit.\n        ```\n        \n    *   ```\n        **Timeout** - The time to wait before considering the check to have hung. It should be 0 or at least 1000000 (1 ms). 0 means inherit.\n        ```\n        \n    *   ```\n        **Retries** - The number of consecutive failures needed to consider a container as unhealthy. 0 means inherit.\n        ```\n        \n    *   ```\n        **StartPeriod** - The time to wait for container initialization before starting health-retries countdown in nanoseconds. It should be 0 or at least 1000000 (1 ms). 0 means inherit.\n        ```\n        \n*   **WorkingDir** - A string specifying the working directory for commands to run in.\n*   **NetworkDisabled** - Boolean value, when true disables networking for the container\n*   **ExposedPorts** - An object mapping ports to an empty object in the form of: `\"ExposedPorts\": { \"<port>/<tcp|udp>: {}\" }`\n*   **StopSignal** - Signal to stop a container as a string or unsigned integer. `SIGTERM` by default.\n*   **HostConfig**\n    *   **Binds** – A list of volume bindings for this container. Each volume binding is a string in one of these forms:\n        \n        *   `host-src:container-dest` to bind-mount a host path into the container. Both `host-src`, and `container-dest` must be an _absolute_ path.\n        *   `host-src:container-dest:ro` to make the bind mount read-only inside the container. Both `host-src`, and `container-dest` must be an _absolute_ path.\n        *   `volume-name:container-dest` to bind-mount a volume managed by a volume driver into the container. `container-dest` must be an _absolute_ path.\n        *   `volume-name:container-dest:ro` to mount the volume read-only inside the container. `container-dest` must be an _absolute_ path.\n    *   **Tmpfs** – A map of container directories which should be replaced by tmpfs mounts, and their corresponding mount options. A JSON object in the form `{ \"/run\": \"rw,noexec,nosuid,size=65536k\" }`.\n        \n    *   **Links** - A list of links for the container. Each link entry should be in the form of `container_name:alias`.\n        \n    *   **Memory** - Memory limit in bytes.\n        \n    *   **MemorySwap** - Total memory limit (memory + swap); set `-1` to enable unlimited swap. You must use this with `memory` and make the swap value larger than `memory`.\n        \n    *   **MemoryReservation** - Memory soft limit in bytes.\n        \n    *   **KernelMemory** - Kernel memory limit in bytes.\n        \n    *   **CpuPercent** - An integer value containing the usable percentage of the available CPUs. (Windows daemon only)\n        \n    *   **CpuShares** - An integer value containing the container's CPU Shares (ie. the relative weight vs other containers).\n        \n    *   **CpuPeriod** - The length of a CPU period in microseconds.\n        \n    *   **CpuQuota** - Microseconds of CPU time that the container can get in a CPU period.\n        \n    *   **CpusetCpus** - String value containing the `cgroups CpusetCpus` to use.\n        \n    *   **CpusetMems** - Memory nodes (MEMs) in which to allow execution (0-3, 0,1). Only effective on NUMA systems.\n        \n    *   **IOMaximumBandwidth** - Maximum IO absolute rate in terms of IOps.\n        \n    *   **IOMaximumIOps** - Maximum IO absolute rate in terms of bytes per second.\n        \n    *   **BlkioWeight** - Block IO weight (relative weight) accepts a weight value between 10 and 1000.\n        \n    *   **BlkioWeightDevice** - Block IO weight (relative device weight) in the form of: `\"BlkioWeightDevice\": [{\"Path\": \"device_path\", \"Weight\": weight}]`\n        \n    *   **BlkioDeviceReadBps** - Limit read rate (bytes per second) from a device in the form of: `\"BlkioDeviceReadBps\": [{\"Path\": \"device_path\", \"Rate\": rate}]`, for example: `\"BlkioDeviceReadBps\": [{\"Path\": \"/dev/sda\", \"Rate\": \"1024\"}]\"`\n        \n    *   **BlkioDeviceWriteBps** - Limit write rate (bytes per second) to a device in the form of: `\"BlkioDeviceWriteBps\": [{\"Path\": \"device_path\", \"Rate\": rate}]`, for example: `\"BlkioDeviceWriteBps\": [{\"Path\": \"/dev/sda\", \"Rate\": \"1024\"}]\"`\n        \n    *   **BlkioDeviceReadIOps** - Limit read rate (IO per second) from a device in the form of: `\"BlkioDeviceReadIOps\": [{\"Path\": \"device_path\", \"Rate\": rate}]`, for example: `\"BlkioDeviceReadIOps\": [{\"Path\": \"/dev/sda\", \"Rate\": \"1000\"}]`\n        \n    *   **BlkioDeviceWriteIOps** - Limit write rate (IO per second) to a device in the form of: `\"BlkioDeviceWriteIOps\": [{\"Path\": \"device_path\", \"Rate\": rate}]`, for example: `\"BlkioDeviceWriteIOps\": [{\"Path\": \"/dev/sda\", \"Rate\": \"1000\"}]`\n        \n    *   **MemorySwappiness** - Tune a container's memory swappiness behavior. Accepts an integer between 0 and 100.\n        \n    *   **OomKillDisable** - Boolean value, whether to disable OOM Killer for the container or not.\n        \n    *   **OomScoreAdj** - An integer value containing the score given to the container in order to tune OOM killer preferences.\n        \n    *   **PidMode** - Set the PID (Process) Namespace mode for the container; `\"container:<name|id>\"`: joins another container's PID namespace `\"host\"`: use the host's PID namespace inside the container\n        \n    *   **PidsLimit** - Tune a container's pids limit. Set -1 for unlimited.\n        \n    *   **PortBindings** - A map of exposed container ports and the host port they should map to. A JSON object in the form `{ <port>/<protocol>: [{ \"HostPort\": \"<port>\" }] }` Take note that `port` is specified as a string and not an integer value.\n        \n    *   **PublishAllPorts** - Allocates an ephemeral host port for all of a container's exposed ports. Specified as a boolean value.\n        \n        Ports are de-allocated when the container stops and allocated when the container starts. The allocated port might be changed when restarting the container.\n        \n        The port is selected from the ephemeral port range that depends on the kernel. For example, on Linux the range is defined by `/proc/sys/net/ipv4/ip_local_port_range`.\n        \n    *   **Privileged** - Gives the container full access to the host. Specified as a boolean value.\n        \n    *   **ReadonlyRootfs** - Mount the container's root filesystem as read only. Specified as a boolean value.\n        \n    *   **Dns** - A list of DNS servers for the container to use.\n        \n    *   **DnsOptions** - A list of DNS options\n        \n    *   **DnsSearch** - A list of DNS search domains\n        \n    *   **ExtraHosts** - A list of hostnames/IP mappings to add to the container's `/etc/hosts` file. Specified in the form `[\"hostname:IP\"]`.\n        \n    *   **VolumesFrom** - A list of volumes to inherit from another container. Specified in the form `<container name>[:<ro|rw>]`\n        \n    *   **CapAdd** - A list of kernel capabilities to add to the container.\n        \n    *   **Capdrop** - A list of kernel capabilities to drop from the container.\n        \n    *   **GroupAdd** - A list of additional groups that the container process will run as\n        \n    *   **RestartPolicy** – The behavior to apply when the container exits. The value is an object with a `Name` property of either `\"always\"` to always restart, `\"unless-stopped\"` to restart always except when user has manually stopped the container or `\"on-failure\"` to restart only when the container exit code is non-zero. If `on-failure` is used, `MaximumRetryCount` controls the number of times to retry before giving up. The default is not to restart. (optional) An ever increasing delay (double the previous delay, starting at 100mS) is added before each restart to prevent flooding the server.\n        \n    *   **UsernsMode** - Sets the usernamespace mode for the container when usernamespace remapping option is enabled. supported values are: `host`.\n        \n    *   **NetworkMode** - Sets the networking mode for the container. Supported standard values are: `bridge`, `host`, `none`, and `container:<name|id>`. Any other value is taken as a custom network's name to which this container should connect to.\n        \n    *   **Devices** - A list of devices to add to the container specified as a JSON object in the form `{ \"PathOnHost\": \"/dev/deviceName\", \"PathInContainer\": \"/dev/deviceName\", \"CgroupPermissions\": \"mrw\"}`\n        \n    *   **Ulimits** - A list of ulimits to set in the container, specified as `{ \"Name\": <name>, \"Soft\": <soft limit>, \"Hard\": <hard limit> }`, for example: `Ulimits: { \"Name\": \"nofile\", \"Soft\": 1024, \"Hard\": 2048 }`\n        \n    *   **Sysctls** - A list of kernel parameters (sysctls) to set in the container, specified as `{ <name>: <Value> }`, for example: `{ \"net.ipv4.ip_forward\": \"1\" }`\n        \n    *   **SecurityOpt**: A list of string values to customize labels for MLS systems, such as SELinux.\n        \n    *   **StorageOpt**: Storage driver options per container. Options can be passed in the form `{\"size\":\"120G\"}`\n        \n    *   **LogConfig** - Log configuration for the container, specified as a JSON object in the form `{ \"Type\": \"<driver_name>\", \"Config\": {\"key1\": \"val1\"}}`. Available types: `json-file`, `syslog`, `journald`, `gelf`, `fluentd`, `awslogs`, `splunk`, `etwlogs`, `none`. `json-file` logging driver.\n        \n    *   **CgroupParent** - Path to `cgroups` under which the container's `cgroup` is created. If the path is not absolute, the path is considered to be relative to the `cgroups` path of the init process. Cgroups are created if they do not already exist.\n        \n    *   **VolumeDriver** - Driver that this container users to mount volumes.\n        \n    *   **ShmSize** - Size of `/dev/shm` in bytes. The size must be greater than 0. If omitted the system uses 64MB.\n        \n\n**Query parameters**:\n\n*   **name** – Assign the specified name to the container. Must match `/?[a-zA-Z0-9_-]+`.\n\n**Status codes**:\n\n*   **201** – no error\n*   **400** – bad parameter\n*   **404** – no such image\n*   **406** – impossible to attach (container not running)\n*   **409** – conflict\n*   **500** – server error\n\n#### [Inspect a container](#inspect-a-container)\n\n`GET /containers/(id or name)/json`\n\nReturn low-level information on the container `id`\n\n**Example request**:\n\n```\n  GET /v1.24/containers/4fa6e0f0c678/json HTTP/1.1\n```\n\n**Example response**:\n\n```\nHTTP/1.1 200 OK\nContent-Type: application/json\n\n{\n\t\"AppArmorProfile\": \"\",\n\t\"Args\": [\n\t\t\"-c\",\n\t\t\"exit 9\"\n\t],\n\t\"Config\": {\n\t\t\"AttachStderr\": true,\n\t\t\"AttachStdin\": false,\n\t\t\"AttachStdout\": true,\n\t\t\"Cmd\": [\n\t\t\t\"/bin/sh\",\n\t\t\t\"-c\",\n\t\t\t\"exit 9\"\n\t\t],\n\t\t\"Domainname\": \"\",\n\t\t\"Entrypoint\": null,\n\t\t\"Env\": [\n\t\t\t\"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"\n\t\t],\n\t\t\"ExposedPorts\": null,\n\t\t\"Hostname\": \"ba033ac44011\",\n\t\t\"Image\": \"ubuntu\",\n\t\t\"Labels\": {\n\t\t\t\"com.example.vendor\": \"Acme\",\n\t\t\t\"com.example.license\": \"GPL\",\n\t\t\t\"com.example.version\": \"1.0\"\n\t\t},\n\t\t\"MacAddress\": \"\",\n\t\t\"NetworkDisabled\": false,\n\t\t\"OnBuild\": null,\n\t\t\"OpenStdin\": false,\n\t\t\"StdinOnce\": false,\n\t\t\"Tty\": false,\n\t\t\"User\": \"\",\n\t\t\"Volumes\": {\n\t\t\t\"/volumes/data\": {}\n\t\t},\n\t\t\"WorkingDir\": \"\",\n\t\t\"StopSignal\": \"SIGTERM\"\n\t},\n\t\"Created\": \"2015-01-06T15:47:31.485331387Z\",\n\t\"Driver\": \"overlay2\",\n\t\"ExecIDs\": null,\n\t\"HostConfig\": {\n\t\t\"Binds\": null,\n\t\t\"IOMaximumBandwidth\": 0,\n\t\t\"IOMaximumIOps\": 0,\n\t\t\"BlkioWeight\": 0,\n\t\t\"BlkioWeightDevice\": [{}],\n\t\t\"BlkioDeviceReadBps\": [{}],\n\t\t\"BlkioDeviceWriteBps\": [{}],\n\t\t\"BlkioDeviceReadIOps\": [{}],\n\t\t\"BlkioDeviceWriteIOps\": [{}],\n\t\t\"CapAdd\": null,\n\t\t\"CapDrop\": null,\n\t\t\"ContainerIDFile\": \"\",\n\t\t\"CpusetCpus\": \"\",\n\t\t\"CpusetMems\": \"\",\n\t\t\"CpuPercent\": 80,\n\t\t\"CpuShares\": 0,\n\t\t\"CpuPeriod\": 100000,\n\t\t\"Devices\": [],\n\t\t\"Dns\": null,\n\t\t\"DnsOptions\": null,\n\t\t\"DnsSearch\": null,\n\t\t\"ExtraHosts\": null,\n\t\t\"IpcMode\": \"\",\n\t\t\"Links\": null,\n\t\t\"Memory\": 0,\n\t\t\"MemorySwap\": 0,\n\t\t\"MemoryReservation\": 0,\n\t\t\"KernelMemory\": 0,\n\t\t\"OomKillDisable\": false,\n\t\t\"OomScoreAdj\": 500,\n\t\t\"NetworkMode\": \"bridge\",\n\t\t\"PidMode\": \"\",\n\t\t\"PortBindings\": {},\n\t\t\"Privileged\": false,\n\t\t\"ReadonlyRootfs\": false,\n\t\t\"PublishAllPorts\": false,\n\t\t\"RestartPolicy\": {\n\t\t\t\"MaximumRetryCount\": 2,\n\t\t\t\"Name\": \"on-failure\"\n\t\t},\n\t\t\"LogConfig\": {\n\t\t\t\"Config\": null,\n\t\t\t\"Type\": \"json-file\"\n\t\t},\n\t\t\"SecurityOpt\": null,\n\t\t\"Sysctls\": {\n\t\t        \"net.ipv4.ip_forward\": \"1\"\n\t\t},\n\t\t\"StorageOpt\": null,\n\t\t\"VolumesFrom\": null,\n\t\t\"Ulimits\": [{}],\n\t\t\"VolumeDriver\": \"\",\n\t\t\"ShmSize\": 67108864\n\t},\n\t\"HostnamePath\": \"/var/lib/docker/containers/ba033ac4401106a3b513bc9d639eee123ad78ca3616b921167cd74b20e25ed39/hostname\",\n\t\"HostsPath\": \"/var/lib/docker/containers/ba033ac4401106a3b513bc9d639eee123ad78ca3616b921167cd74b20e25ed39/hosts\",\n\t\"LogPath\": \"/var/lib/docker/containers/1eb5fabf5a03807136561b3c00adcd2992b535d624d5e18b6cdc6a6844d9767b/1eb5fabf5a03807136561b3c00adcd2992b535d624d5e18b6cdc6a6844d9767b-json.log\",\n\t\"Id\": \"ba033ac4401106a3b513bc9d639eee123ad78ca3616b921167cd74b20e25ed39\",\n\t\"Image\": \"04c5d3b7b0656168630d3ba35d8889bd0e9caafcaeb3004d2bfbc47e7c5d35d2\",\n\t\"MountLabel\": \"\",\n\t\"Name\": \"/boring_euclid\",\n\t\"NetworkSettings\": {\n\t\t\"Bridge\": \"\",\n\t\t\"SandboxID\": \"\",\n\t\t\"HairpinMode\": false,\n\t\t\"LinkLocalIPv6Address\": \"\",\n\t\t\"LinkLocalIPv6PrefixLen\": 0,\n\t\t\"Ports\": null,\n\t\t\"SandboxKey\": \"\",\n\t\t\"SecondaryIPAddresses\": null,\n\t\t\"SecondaryIPv6Addresses\": null,\n\t\t\"EndpointID\": \"\",\n\t\t\"Gateway\": \"\",\n\t\t\"GlobalIPv6Address\": \"\",\n\t\t\"GlobalIPv6PrefixLen\": 0,\n\t\t\"IPAddress\": \"\",\n\t\t\"IPPrefixLen\": 0,\n\t\t\"IPv6Gateway\": \"\",\n\t\t\"MacAddress\": \"\",\n\t\t\"Networks\": {\n\t\t\t\"bridge\": {\n\t\t\t\t\"NetworkID\": \"7ea29fc1412292a2d7bba362f9253545fecdfa8ce9a6e37dd10ba8bee7129812\",\n\t\t\t\t\"EndpointID\": \"7587b82f0dada3656fda26588aee72630c6fab1536d36e394b2bfbcf898c971d\",\n\t\t\t\t\"Gateway\": \"172.17.0.1\",\n\t\t\t\t\"IPAddress\": \"172.17.0.2\",\n\t\t\t\t\"IPPrefixLen\": 16,\n\t\t\t\t\"IPv6Gateway\": \"\",\n\t\t\t\t\"GlobalIPv6Address\": \"\",\n\t\t\t\t\"GlobalIPv6PrefixLen\": 0,\n\t\t\t\t\"MacAddress\": \"02:42:ac:12:00:02\"\n\t\t\t}\n\t\t}\n\t},\n\t\"Path\": \"/bin/sh\",\n\t\"ProcessLabel\": \"\",\n\t\"ResolvConfPath\": \"/var/lib/docker/containers/ba033ac4401106a3b513bc9d639eee123ad78ca3616b921167cd74b20e25ed39/resolv.conf\",\n\t\"RestartCount\": 1,\n\t\"State\": {\n\t\t\"Error\": \"\",\n\t\t\"ExitCode\": 9,\n\t\t\"FinishedAt\": \"2015-01-06T15:47:32.080254511Z\",\n\t\t\"OOMKilled\": false,\n\t\t\"Dead\": false,\n\t\t\"Paused\": false,\n\t\t\"Pid\": 0,\n\t\t\"Restarting\": false,\n\t\t\"Running\": true,\n\t\t\"StartedAt\": \"2015-01-06T15:47:32.072697474Z\",\n\t\t\"Status\": \"running\"\n\t},\n\t\"Mounts\": [\n\t\t{\n\t\t\t\"Name\": \"fac362...80535\",\n\t\t\t\"Source\": \"/data\",\n\t\t\t\"Destination\": \"/data\",\n\t\t\t\"Driver\": \"local\",\n\t\t\t\"Mode\": \"ro,Z\",\n\t\t\t\"RW\": false,\n\t\t\t\"Propagation\": \"\"\n\t\t}\n\t]\n}\n```\n\n**Example request, with size information**:\n\n```\nGET /v1.24/containers/4fa6e0f0c678/json?size=1 HTTP/1.1\n```\n\n**Example response, with size information**:\n\n```\nHTTP/1.1 200 OK\nContent-Type: application/json\n\n{\n....\n\"SizeRw\": 0,\n\"SizeRootFs\": 972,\n....\n}\n```\n\n**Query parameters**:\n\n*   **size** – 1/True/true or 0/False/false, return container size information. Default is `false`.\n\n**Status codes**:\n\n*   **200** – no error\n*   **404** – no such container\n*   **500** – server error\n\n#### [List processes running inside a container](#list-processes-running-inside-a-container)\n\n`GET /containers/(id or name)/top`\n\nList processes running inside the container `id`. On Unix systems this is done by running the `ps` command. This endpoint is not supported on Windows.\n\n**Example request**:\n\n```\nGET /v1.24/containers/4fa6e0f0c678/top HTTP/1.1\n```\n\n**Example response**:\n\n```\nHTTP/1.1 200 OK\nContent-Type: application/json\n\n{\n   \"Titles\" : [\n     \"UID\", \"PID\", \"PPID\", \"C\", \"STIME\", \"TTY\", \"TIME\", \"CMD\"\n   ],\n   \"Processes\" : [\n     [\n       \"root\", \"13642\", \"882\", \"0\", \"17:03\", \"pts/0\", \"00:00:00\", \"/bin/bash\"\n     ],\n     [\n       \"root\", \"13735\", \"13642\", \"0\", \"17:06\", \"pts/0\", \"00:00:00\", \"sleep 10\"\n     ]\n   ]\n}\n```\n\n**Example request**:\n\n```\nGET /v1.24/containers/4fa6e0f0c678/top?ps_args=aux HTTP/1.1\n```\n\n**Example response**:\n\n```\nHTTP/1.1 200 OK\nContent-Type: application/json\n\n{\n  \"Titles\" : [\n    \"USER\",\"PID\",\"%CPU\",\"%MEM\",\"VSZ\",\"RSS\",\"TTY\",\"STAT\",\"START\",\"TIME\",\"COMMAND\"\n  ]\n  \"Processes\" : [\n    [\n      \"root\",\"13642\",\"0.0\",\"0.1\",\"18172\",\"3184\",\"pts/0\",\"Ss\",\"17:03\",\"0:00\",\"/bin/bash\"\n    ],\n    [\n      \"root\",\"13895\",\"0.0\",\"0.0\",\"4348\",\"692\",\"pts/0\",\"S+\",\"17:15\",\"0:00\",\"sleep 10\"\n    ]\n  ],\n}\n```\n\n**Query parameters**:\n\n*   **ps\\_args** – `ps` arguments to use (e.g., `aux`), defaults to `-ef`\n\n**Status codes**:\n\n*   **200** – no error\n*   **404** – no such container\n*   **500** – server error\n\n#### [Get container logs](#get-container-logs)\n\n`GET /containers/(id or name)/logs`\n\nGet `stdout` and `stderr` logs from the container `id`\n\n> **Note**: This endpoint works only for containers with the `json-file` or `journald` logging drivers.\n\n**Example request**:\n\n```\n GET /v1.24/containers/4fa6e0f0c678/logs?stderr=1&stdout=1&timestamps=1&follow=1&tail=10&since=1428990821 HTTP/1.1\n```\n\n**Example response**:\n\n```\n HTTP/1.1 101 UPGRADED\n Content-Type: application/vnd.docker.raw-stream\n Connection: Upgrade\n Upgrade: tcp\n\n {% raw %}\n {{ STREAM }}\n {% endraw %}\n```\n\n**Query parameters**:\n\n*   **details** - 1/True/true or 0/False/false, Show extra details provided to logs. Default `false`.\n*   **follow** – 1/True/true or 0/False/false, return stream. Default `false`.\n*   **stdout** – 1/True/true or 0/False/false, show `stdout` log. Default `false`.\n*   **stderr** – 1/True/true or 0/False/false, show `stderr` log. Default `false`.\n*   **since** – UNIX timestamp (integer) to filter logs. Specifying a timestamp will only output log-entries since that timestamp. Default: 0 (unfiltered)\n*   **timestamps** – 1/True/true or 0/False/false, print timestamps for every log line. Default `false`.\n*   **tail** – Output specified number of lines at the end of logs: `all` or `<number>`. Default all.\n\n**Status codes**:\n\n*   **101** – no error, hints proxy about hijacking\n*   **200** – no error, no upgrade header found\n*   **404** – no such container\n*   **500** – server error\n\n#### [Inspect changes on a container's filesystem](#inspect-changes-on-a-containers-filesystem)\n\n`GET /containers/(id or name)/changes`\n\nInspect changes on container `id`'s filesystem\n\n**Example request**:\n\n```\nGET /v1.24/containers/4fa6e0f0c678/changes HTTP/1.1\n```\n\n**Example response**:\n\n```\nHTTP/1.1 200 OK\nContent-Type: application/json\n\n[\n     {\n             \"Path\": \"/dev\",\n             \"Kind\": 0\n     },\n     {\n             \"Path\": \"/dev/kmsg\",\n             \"Kind\": 1\n     },\n     {\n             \"Path\": \"/test\",\n             \"Kind\": 1\n     }\n]\n```\n\nValues for `Kind`:\n\n*   `0`: Modify\n*   `1`: Add\n*   `2`: Delete\n\n**Status codes**:\n\n*   **200** – no error\n*   **404** – no such container\n*   **500** – server error\n\n#### [Export a container](#export-a-container)\n\n`GET /containers/(id or name)/export`\n\nExport the contents of container `id`\n\n**Example request**:\n\n```\nGET /v1.24/containers/4fa6e0f0c678/export HTTP/1.1\n```\n\n**Example response**:\n\n```\nHTTP/1.1 200 OK\nContent-Type: application/octet-stream\n\n{% raw %}\n{{ TAR STREAM }}\n{% endraw %}\n```\n\n**Status codes**:\n\n*   **200** – no error\n*   **404** – no such container\n*   **500** – server error\n\n#### [Get container stats based on resource usage](#get-container-stats-based-on-resource-usage)\n\n`GET /containers/(id or name)/stats`\n\nThis endpoint returns a live stream of a container's resource usage statistics.\n\n**Example request**:\n\n```\nGET /v1.24/containers/redis1/stats HTTP/1.1\n```\n\n**Example response**:\n\n```\n  HTTP/1.1 200 OK\n  Content-Type: application/json\n\n  {\n     \"read\" : \"2015-01-08T22:57:31.547920715Z\",\n     \"pids_stats\": {\n        \"current\": 3\n     },\n     \"networks\": {\n             \"eth0\": {\n                 \"rx_bytes\": 5338,\n                 \"rx_dropped\": 0,\n                 \"rx_errors\": 0,\n                 \"rx_packets\": 36,\n                 \"tx_bytes\": 648,\n                 \"tx_dropped\": 0,\n                 \"tx_errors\": 0,\n                 \"tx_packets\": 8\n             },\n             \"eth5\": {\n                 \"rx_bytes\": 4641,\n                 \"rx_dropped\": 0,\n                 \"rx_errors\": 0,\n                 \"rx_packets\": 26,\n                 \"tx_bytes\": 690,\n                 \"tx_dropped\": 0,\n                 \"tx_errors\": 0,\n                 \"tx_packets\": 9\n             }\n     },\n     \"memory_stats\" : {\n        \"stats\" : {\n           \"total_pgmajfault\" : 0,\n           \"cache\" : 0,\n           \"mapped_file\" : 0,\n           \"total_inactive_file\" : 0,\n           \"pgpgout\" : 414,\n           \"rss\" : 6537216,\n           \"total_mapped_file\" : 0,\n           \"writeback\" : 0,\n           \"unevictable\" : 0,\n           \"pgpgin\" : 477,\n           \"total_unevictable\" : 0,\n           \"pgmajfault\" : 0,\n           \"total_rss\" : 6537216,\n           \"total_rss_huge\" : 6291456,\n           \"total_writeback\" : 0,\n           \"total_inactive_anon\" : 0,\n           \"rss_huge\" : 6291456,\n           \"hierarchical_memory_limit\" : 67108864,\n           \"total_pgfault\" : 964,\n           \"total_active_file\" : 0,\n           \"active_anon\" : 6537216,\n           \"total_active_anon\" : 6537216,\n           \"total_pgpgout\" : 414,\n           \"total_cache\" : 0,\n           \"inactive_anon\" : 0,\n           \"active_file\" : 0,\n           \"pgfault\" : 964,\n           \"inactive_file\" : 0,\n           \"total_pgpgin\" : 477\n        },\n        \"max_usage\" : 6651904,\n        \"usage\" : 6537216,\n        \"failcnt\" : 0,\n        \"limit\" : 67108864\n     },\n     \"blkio_stats\" : {},\n     \"cpu_stats\" : {\n        \"cpu_usage\" : {\n           \"percpu_usage\" : [\n              8646879,\n              24472255,\n              36438778,\n              30657443\n           ],\n           \"usage_in_usermode\" : 50000000,\n           \"total_usage\" : 100215355,\n           \"usage_in_kernelmode\" : 30000000\n        },\n        \"system_cpu_usage\" : 739306590000000,\n        \"throttling_data\" : {\"periods\":0,\"throttled_periods\":0,\"throttled_time\":0}\n     },\n     \"precpu_stats\" : {\n        \"cpu_usage\" : {\n           \"percpu_usage\" : [\n              8646879,\n              24350896,\n              36438778,\n              30657443\n           ],\n           \"usage_in_usermode\" : 50000000,\n           \"total_usage\" : 100093996,\n           \"usage_in_kernelmode\" : 30000000\n        },\n        \"system_cpu_usage\" : 9492140000000,\n        \"throttling_data\" : {\"periods\":0,\"throttled_periods\":0,\"throttled_time\":0}\n     }\n  }\n```\n\nThe `precpu_stats` is the cpu statistic of _previous_ read, which is used for calculating the cpu usage percent. It is not the exact copy of the `cpu_stats` field.\n\n**Query parameters**:\n\n*   **stream** – 1/True/true or 0/False/false, pull stats once then disconnect. Default `true`.\n\n**Status codes**:\n\n*   **200** – no error\n*   **404** – no such container\n*   **500** – server error\n\n#### [Resize a container TTY](#resize-a-container-tty)\n\n`POST /containers/(id or name)/resize`\n\nResize the TTY for container with `id`. The unit is number of characters. You must restart the container for the resize to take effect.\n\n**Example request**:\n\n```\n  POST /v1.24/containers/4fa6e0f0c678/resize?h=40&w=80 HTTP/1.1\n```\n\n**Example response**:\n\n```\n  HTTP/1.1 200 OK\n  Content-Length: 0\n  Content-Type: text/plain; charset=utf-8\n```\n\n**Query parameters**:\n\n*   **h** – height of `tty` session\n*   **w** – width\n\n**Status codes**:\n\n*   **200** – no error\n*   **404** – No such container\n*   **500** – Cannot resize container\n\n#### [Start a container](#start-a-container)\n\n`POST /containers/(id or name)/start`\n\nStart the container `id`\n\n**Example request**:\n\n```\nPOST /v1.24/containers/e90e34656806/start HTTP/1.1\n```\n\n**Example response**:\n\n```\nHTTP/1.1 204 No Content\n```\n\n**Query parameters**:\n\n*   **detachKeys** – Override the key sequence for detaching a container. Format is a single character `[a-Z]` or `ctrl-<value>` where `<value>` is one of: `a-z`, `@`, `^`, `[`, `,` or `_`.\n\n**Status codes**:\n\n*   **204** – no error\n*   **304** – container already started\n*   **404** – no such container\n*   **500** – server error\n\n#### [Stop a container](#stop-a-container)\n\n`POST /containers/(id or name)/stop`\n\nStop the container `id`\n\n**Example request**:\n\n```\nPOST /v1.24/containers/e90e34656806/stop?t=5 HTTP/1.1\n```\n\n**Example response**:\n\n```\nHTTP/1.1 204 No Content\n```\n\n**Query parameters**:\n\n*   **t** – number of seconds to wait before killing the container\n\n**Status codes**:\n\n*   **204** – no error\n*   **304** – container already stopped\n*   **404** – no such container\n*   **500** – server error\n\n#### [Restart a container](#restart-a-container)\n\n`POST /containers/(id or name)/restart`\n\nRestart the container `id`\n\n**Example request**:\n\n```\nPOST /v1.24/containers/e90e34656806/restart?t=5 HTTP/1.1\n```\n\n**Example response**:\n\n```\nHTTP/1.1 204 No Content\n```\n\n**Query parameters**:\n\n*   **t** – number of seconds to wait before killing the container\n\n**Status codes**:\n\n*   **204** – no error\n*   **404** – no such container\n*   **500** – server error\n\n#### [Kill a container](#kill-a-container)\n\n`POST /containers/(id or name)/kill`\n\nKill the container `id`\n\n**Example request**:\n\n```\nPOST /v1.24/containers/e90e34656806/kill HTTP/1.1\n```\n\n**Example response**:\n\n```\nHTTP/1.1 204 No Content\n```\n\n**Query parameters**:\n\n*   **signal** - Signal to send to the container: integer or string like `SIGINT`. When not set, `SIGKILL` is assumed and the call waits for the container to exit.\n\n**Status codes**:\n\n*   **204** – no error\n*   **404** – no such container\n*   **500** – server error\n\n#### [Update a container](#update-a-container)\n\n`POST /containers/(id or name)/update`\n\nUpdate configuration of one or more containers.\n\n**Example request**:\n\n```\n   POST /v1.24/containers/e90e34656806/update HTTP/1.1\n   Content-Type: application/json\n   Content-Length: 12345\n\n   {\n     \"BlkioWeight\": 300,\n     \"CpuShares\": 512,\n     \"CpuPeriod\": 100000,\n     \"CpuQuota\": 50000,\n     \"CpusetCpus\": \"0,1\",\n     \"CpusetMems\": \"0\",\n     \"Memory\": 314572800,\n     \"MemorySwap\": 514288000,\n     \"MemoryReservation\": 209715200,\n     \"KernelMemory\": 52428800,\n     \"RestartPolicy\": {\n       \"MaximumRetryCount\": 4,\n       \"Name\": \"on-failure\"\n     }\n   }\n```\n\n**Example response**:\n\n```\n   HTTP/1.1 200 OK\n   Content-Type: application/json\n\n   {\n       \"Warnings\": []\n   }\n```\n\n**Status codes**:\n\n*   **200** – no error\n*   **400** – bad parameter\n*   **404** – no such container\n*   **500** – server error\n\n#### [Rename a container](#rename-a-container)\n\n`POST /containers/(id or name)/rename`\n\nRename the container `id` to a `new_name`\n\n**Example request**:\n\n```\nPOST /v1.24/containers/e90e34656806/rename?name=new_name HTTP/1.1\n```\n\n**Example response**:\n\n```\nHTTP/1.1 204 No Content\n```\n\n**Query parameters**:\n\n*   **name** – new name for the container\n\n**Status codes**:\n\n*   **204** – no error\n*   **404** – no such container\n*   **409** - conflict name already assigned\n*   **500** – server error\n\n#### [Pause a container](#pause-a-container)\n\n`POST /containers/(id or name)/pause`\n\nPause the container `id`\n\n**Example request**:\n\n```\nPOST /v1.24/containers/e90e34656806/pause HTTP/1.1\n```\n\n**Example response**:\n\n```\nHTTP/1.1 204 No Content\n```\n\n**Status codes**:\n\n*   **204** – no error\n*   **404** – no such container\n*   **500** – server error\n\n#### [Unpause a container](#unpause-a-container)\n\n`POST /containers/(id or name)/unpause`\n\nUnpause the container `id`\n\n**Example request**:\n\n```\nPOST /v1.24/containers/e90e34656806/unpause HTTP/1.1\n```\n\n**Example response**:\n\n```\nHTTP/1.1 204 No Content\n```\n\n**Status codes**:\n\n*   **204** – no error\n*   **404** – no such container\n*   **500** – server error\n\n#### [Attach to a container](#attach-to-a-container)\n\n`POST /containers/(id or name)/attach`\n\nAttach to the container `id`\n\n**Example request**:\n\n```\nPOST /v1.24/containers/16253994b7c4/attach?logs=1&stream=0&stdout=1 HTTP/1.1\n```\n\n**Example response**:\n\n```\nHTTP/1.1 101 UPGRADED\nContent-Type: application/vnd.docker.raw-stream\nConnection: Upgrade\nUpgrade: tcp\n\n{% raw %}\n{{ STREAM }}\n{% endraw %}\n```\n\n**Query parameters**:\n\n*   **detachKeys** – Override the key sequence for detaching a container. Format is a single character `[a-Z]` or `ctrl-<value>` where `<value>` is one of: `a-z`, `@`, `^`, `[`, `,` or `_`.\n*   **logs** – 1/True/true or 0/False/false, return logs. Default `false`.\n*   **stream** – 1/True/true or 0/False/false, return stream. Default `false`.\n*   **stdin** – 1/True/true or 0/False/false, if `stream=true`, attach to `stdin`. Default `false`.\n*   **stdout** – 1/True/true or 0/False/false, if `logs=true`, return `stdout` log, if `stream=true`, attach to `stdout`. Default `false`.\n*   **stderr** – 1/True/true or 0/False/false, if `logs=true`, return `stderr` log, if `stream=true`, attach to `stderr`. Default `false`.\n\n**Status codes**:\n\n*   **101** – no error, hints proxy about hijacking\n*   **200** – no error, no upgrade header found\n*   **400** – bad parameter\n*   **404** – no such container\n*   **409** - container is paused\n*   **500** – server error\n\n**Stream details**:\n\nWhen using the TTY setting is enabled in [`POST /containers/create`](#create-a-container) , the stream is the raw data from the process PTY and client's `stdin`. When the TTY is disabled, then the stream is multiplexed to separate `stdout` and `stderr`.\n\nThe format is a **Header** and a **Payload** (frame).\n\n**HEADER**\n\nThe header contains the information which the stream writes (`stdout` or `stderr`). It also contains the size of the associated frame encoded in the last four bytes (`uint32`).\n\nIt is encoded on the first eight bytes like this:\n\n```\nheader := [8]byte{STREAM_TYPE, 0, 0, 0, SIZE1, SIZE2, SIZE3, SIZE4}\n```\n\n`STREAM_TYPE` can be:\n\n*   0: `stdin` (is written on `stdout`)\n*   1: `stdout`\n*   2: `stderr`\n\n`SIZE1, SIZE2, SIZE3, SIZE4` are the four bytes of the `uint32` size encoded as big endian.\n\n**PAYLOAD**\n\nThe payload is the raw stream.\n\n**IMPLEMENTATION**\n\nThe simplest way to implement the Attach protocol is the following:\n\n```\n1.  Read eight bytes.\n2.  Choose `stdout` or `stderr` depending on the first byte.\n3.  Extract the frame size from the last four bytes.\n4.  Read the extracted size and output it on the correct output.\n5.  Goto 1.\n```\n\n#### [Attach to a container (websocket)](#attach-to-a-container-websocket)\n\n`GET /containers/(id or name)/attach/ws`\n\nAttach to the container `id` via websocket\n\nImplements websocket protocol handshake according to [RFC 6455](http://tools.ietf.org/html/rfc6455)\n\n**Example request**\n\n```\nGET /v1.24/containers/e90e34656806/attach/ws?logs=0&stream=1&stdin=1&stdout=1&stderr=1 HTTP/1.1\n```\n\n**Example response**\n\n```\n{% raw %}\n{{ STREAM }}\n{% endraw %}\n```\n\n**Query parameters**:\n\n*   **detachKeys** – Override the key sequence for detaching a container. Format is a single character `[a-Z]` or `ctrl-<value>` where `<value>` is one of: `a-z`, `@`, `^`, `[`, `,` or `_`.\n*   **logs** – 1/True/true or 0/False/false, return logs. Default `false`.\n*   **stream** – 1/True/true or 0/False/false, return stream. Default `false`.\n\n**Status codes**:\n\n*   **200** – no error\n*   **400** – bad parameter\n*   **404** – no such container\n*   **500** – server error\n\n#### [Wait a container](#wait-a-container)\n\n`POST /containers/(id or name)/wait`\n\nBlock until container `id` stops, then returns the exit code\n\n**Example request**:\n\n```\nPOST /v1.24/containers/16253994b7c4/wait HTTP/1.1\n```\n\n**Example response**:\n\n```\nHTTP/1.1 200 OK\nContent-Type: application/json\n\n{\"StatusCode\": 0}\n```\n\n**Status codes**:\n\n*   **200** – no error\n*   **404** – no such container\n*   **500** – server error\n\n#### [Remove a container](#remove-a-container)\n\n`DELETE /containers/(id or name)`\n\nRemove the container `id` from the filesystem\n\n**Example request**:\n\n```\nDELETE /v1.24/containers/16253994b7c4?v=1 HTTP/1.1\n```\n\n**Example response**:\n\n```\nHTTP/1.1 204 No Content\n```\n\n**Query parameters**:\n\n*   **v** – 1/True/true or 0/False/false, Remove the volumes associated to the container. Default `false`.\n*   **force** - 1/True/true or 0/False/false, Kill then remove the container. Default `false`.\n*   **link** - 1/True/true or 0/False/false, Remove the specified link associated to the container. Default `false`.\n\n**Status codes**:\n\n*   **204** – no error\n*   **400** – bad parameter\n*   **404** – no such container\n*   **409** – conflict\n*   **500** – server error\n\n#### [Retrieving information about files and folders in a container](#retrieving-information-about-files-and-folders-in-a-container)\n\n`HEAD /containers/(id or name)/archive`\n\nSee the description of the `X-Docker-Container-Path-Stat` header in the following section.\n\n#### [Get an archive of a filesystem resource in a container](#get-an-archive-of-a-filesystem-resource-in-a-container)\n\n`GET /containers/(id or name)/archive`\n\nGet a tar archive of a resource in the filesystem of container `id`.\n\n**Query parameters**:\n\n*   **path** - resource in the container's filesystem to archive. Required.\n    \n    If not an absolute path, it is relative to the container's root directory. The resource specified by **path** must exist. To assert that the resource is expected to be a directory, **path** should end in `/` or `/.` (assuming a path separator of `/`). If **path** ends in `/.` then this indicates that only the contents of the **path** directory should be copied. A symlink is always resolved to its target.\n    \n    > **Note**: It is not possible to copy certain system files such as resources under `/proc`, `/sys`, `/dev`, and mounts created by the user in the container.\n    \n\n**Example request**:\n\n```\nGET /v1.24/containers/8cce319429b2/archive?path=/root HTTP/1.1\n```\n\n**Example response**:\n\n```\nHTTP/1.1 200 OK\nContent-Type: application/x-tar\nX-Docker-Container-Path-Stat: eyJuYW1lIjoicm9vdCIsInNpemUiOjQwOTYsIm1vZGUiOjIxNDc0ODQwOTYsIm10aW1lIjoiMjAxNC0wMi0yN1QyMDo1MToyM1oiLCJsaW5rVGFyZ2V0IjoiIn0=\n\n{% raw %}\n{{ TAR STREAM }}\n{% endraw %}\n```\n\nOn success, a response header `X-Docker-Container-Path-Stat` will be set to a base64-encoded JSON object containing some filesystem header information about the archived resource. The above example value would decode to the following JSON object (whitespace added for readability):\n\nA `HEAD` request can also be made to this endpoint if only this information is desired.\n\n**Status codes**:\n\n*   **200** - success, returns archive of copied resource\n*   **400** - client error, bad parameter, details in JSON response body, one of:\n    *   must specify path parameter (**path** cannot be empty)\n    *   not a directory (**path** was asserted to be a directory but exists as a file)\n*   **404** - client error, resource not found, one of: – no such container (container `id` does not exist)\n    *   no such file or directory (**path** does not exist)\n*   **500** - server error\n\n`PUT /containers/(id or name)/archive`\n\nUpload a tar archive to be extracted to a path in the filesystem of container `id`.\n\n**Query parameters**:\n\n*   **path** - path to a directory in the container to extract the archive's contents into. Required.\n    \n    If not an absolute path, it is relative to the container's root directory. The **path** resource must exist.\n    \n*   **noOverwriteDirNonDir** - If \"1\", \"true\", or \"True\" then it will be an error if unpacking the given content would cause an existing directory to be replaced with a non-directory and vice versa.\n    \n\n**Example request**:\n\n```\nPUT /v1.24/containers/8cce319429b2/archive?path=/vol1 HTTP/1.1\nContent-Type: application/x-tar\n\n{% raw %}\n{{ TAR STREAM }}\n{% endraw %}\n```\n\n**Example response**:\n\n```\nHTTP/1.1 200 OK\n```\n\n**Status codes**:\n\n*   **200** – the content was extracted successfully\n*   **400** - client error, bad parameter, details in JSON response body, one of:\n    *   must specify path parameter (**path** cannot be empty)\n    *   not a directory (**path** should be a directory but exists as a file)\n    *   unable to overwrite existing directory with non-directory (if **noOverwriteDirNonDir**)\n    *   unable to overwrite existing non-directory with directory (if **noOverwriteDirNonDir**)\n*   **403** - client error, permission denied, the volume or container rootfs is marked as read-only.\n*   **404** - client error, resource not found, one of: – no such container (container `id` does not exist)\n    *   no such file or directory (**path** resource does not exist)\n*   **500** – server error\n\n### [3.2 Images](#32-images)\n\n#### [List Images](#list-images)\n\n`GET /images/json`\n\n**Example request**:\n\n```\nGET /v1.24/images/json?all=0 HTTP/1.1\n```\n\n**Example response**:\n\n```\nHTTP/1.1 200 OK\nContent-Type: application/json\n\n[\n  {\n     \"RepoTags\": [\n       \"ubuntu:12.04\",\n       \"ubuntu:precise\",\n       \"ubuntu:latest\"\n     ],\n     \"Id\": \"8dbd9e392a964056420e5d58ca5cc376ef18e2de93b5cc90e868a1bbc8318c1c\",\n     \"Created\": 1365714795,\n     \"Size\": 131506275,\n     \"VirtualSize\": 131506275,\n     \"Labels\": {}\n  },\n  {\n     \"RepoTags\": [\n       \"ubuntu:12.10\",\n       \"ubuntu:quantal\"\n     ],\n     \"ParentId\": \"27cf784147099545\",\n     \"Id\": \"b750fe79269d2ec9a3c593ef05b4332b1d1a02a62b4accb2c21d589ff2f5f2dc\",\n     \"Created\": 1364102658,\n     \"Size\": 24653,\n     \"VirtualSize\": 180116135,\n     \"Labels\": {\n        \"com.example.version\": \"v1\"\n     }\n  }\n]\n```\n\n**Example request, with digest information**:\n\n```\nGET /v1.24/images/json?digests=1 HTTP/1.1\n```\n\n**Example response, with digest information**:\n\n```\nHTTP/1.1 200 OK\nContent-Type: application/json\n\n[\n  {\n    \"Created\": 1420064636,\n    \"Id\": \"4986bf8c15363d1c5d15512d5266f8777bfba4974ac56e3270e7760f6f0a8125\",\n    \"ParentId\": \"ea13149945cb6b1e746bf28032f02e9b5a793523481a0a18645fc77ad53c4ea2\",\n    \"RepoDigests\": [\n      \"localhost:5000/test/busybox@sha256:cbbf2f9a99b47fc460d422812b6a5adff7dfee951d8fa2e4a98caa0382cfbdbf\"\n    ],\n    \"RepoTags\": [\n      \"localhost:5000/test/busybox:latest\",\n      \"playdate:latest\"\n    ],\n    \"Size\": 0,\n    \"VirtualSize\": 2429728,\n    \"Labels\": {}\n  }\n]\n```\n\nThe response shows a single image `Id` associated with two repositories (`RepoTags`): `localhost:5000/test/busybox`: and `playdate`. A caller can use either of the `RepoTags` values `localhost:5000/test/busybox:latest` or `playdate:latest` to reference the image.\n\nYou can also use `RepoDigests` values to reference an image. In this response, the array has only one reference and that is to the `localhost:5000/test/busybox` repository; the `playdate` repository has no digest. You can reference this digest using the value: `localhost:5000/test/busybox@sha256:cbbf2f9a99b47fc460d...`\n\nSee the `docker run` and `docker build` commands for examples of digest and tag references on the command line.\n\n**Query parameters**:\n\n*   **all** – 1/True/true or 0/False/false, default false\n*   **filters** – a JSON encoded value of the filters (a map\\[string\\]\\[\\]string) to process on the images list. Available filters:\n*   `dangling=true`\n*   `label=key` or `label=\"key=value\"` of an image label\n*   `before`\\=(`<image-name>[:<tag>]`, `<image id>` or `<image@digest>`)\n*   `since`\\=(`<image-name>[:<tag>]`, `<image id>` or `<image@digest>`)\n*   **filter** - only return images with the specified name\n\n#### [Build image from a Dockerfile](#build-image-from-a-dockerfile)\n\n`POST /build`\n\nBuild an image from a Dockerfile\n\n**Example request**:\n\n```\nPOST /v1.24/build HTTP/1.1\nContent-Type: application/x-tar\n\n{% raw %}\n{{ TAR STREAM }}\n{% endraw %}\n```\n\n**Example response**:\n\n```\nHTTP/1.1 200 OK\nContent-Type: application/json\n\n{\"stream\": \"Step 1/5...\"}\n{\"stream\": \"...\"}\n{\"error\": \"Error...\", \"errorDetail\": {\"code\": 123, \"message\": \"Error...\"}}\n```\n\nThe input stream must be a `tar` archive compressed with one of the following algorithms: `identity` (no compression), `gzip`, `bzip2`, `xz`.\n\nThe archive must include a build instructions file, typically called `Dockerfile` at the archive's root. The `dockerfile` parameter may be used to specify a different build instructions file. To do this, its value must be the path to the alternate build instructions file to use.\n\nThe archive may include any number of other files, which are accessible in the build context (See the [_ADD build command_](https://docs.docker.com/engine/reference/builder/#add)).\n\nThe Docker daemon performs a preliminary validation of the `Dockerfile` before starting the build, and returns an error if the syntax is incorrect. After that, each instruction is run one-by-one until the ID of the new image is output.\n\nThe build is canceled if the client drops the connection by quitting or being killed.\n\n**Query parameters**:\n\n*   **dockerfile** - Path within the build context to the `Dockerfile`. This is ignored if `remote` is specified and points to an external `Dockerfile`.\n*   **t** – A name and optional tag to apply to the image in the `name:tag` format. If you omit the `tag` the default `latest` value is assumed. You can provide one or more `t` parameters.\n*   **remote** – A Git repository URI or HTTP/HTTPS context URI. If the URI points to a single text file, the file's contents are placed into a file called `Dockerfile` and the image is built from that file. If the URI points to a tarball, the file is downloaded by the daemon and the contents therein used as the context for the build. If the URI points to a tarball and the `dockerfile` parameter is also specified, there must be a file with the corresponding path inside the tarball.\n*   **q** – Suppress verbose build output.\n*   **nocache** – Do not use the cache when building the image.\n*   **pull** - Attempt to pull the image even if an older image exists locally.\n*   **rm** - Remove intermediate containers after a successful build (default behavior).\n*   **forcerm** - Always remove intermediate containers (includes `rm`).\n*   **memory** - Set memory limit for build.\n*   **memswap** - Total memory (memory + swap), `-1` to enable unlimited swap.\n*   **cpushares** - CPU shares (relative weight).\n*   **cpusetcpus** - CPUs in which to allow execution (e.g., `0-3`, `0,1`).\n*   **cpuperiod** - The length of a CPU period in microseconds.\n*   **cpuquota** - Microseconds of CPU time that the container can get in a CPU period.\n*   **buildargs** – JSON map of string pairs for build-time variables. Users pass these values at build-time. Docker uses the `buildargs` as the environment context for command(s) run via the Dockerfile's `RUN` instruction or for variable expansion in other Dockerfile instructions. This is not meant for passing secret values. [Read more about the buildargs instruction](https://docs.docker.com/engine/reference/builder/#arg)\n*   **shmsize** - Size of `/dev/shm` in bytes. The size must be greater than 0. If omitted the system uses 64MB.\n*   **labels** – JSON map of string pairs for labels to set on the image.\n\n**Request Headers**:\n\n*   **Content-type** – Set to `\"application/x-tar\"`.\n    \n*   **X-Registry-Config** – A base64-url-safe-encoded Registry Auth Config JSON object with the following structure:\n    \n    ```\n        {\n            \"docker.example.com\": {\n                \"username\": \"janedoe\",\n                \"password\": \"hunter2\"\n            },\n            \"https://index.docker.io/v1/\": {\n                \"username\": \"mobydock\",\n                \"password\": \"conta1n3rize14\"\n            }\n        }\n    ```\n    \n    This object maps the hostname of a registry to an object containing the \"username\" and \"password\" for that registry. Multiple registries may be specified as the build may be based on an image requiring authentication to pull from any arbitrary registry. Only the registry domain name (and port if not the default \"443\") are required. However (for legacy reasons) the \"official\" Docker, Inc. hosted registry must be specified with both a \"https://\" prefix and a \"/v1/\" suffix even though Docker will prefer to use the v2 registry API.\n    \n\n**Status codes**:\n\n*   **200** – no error\n*   **500** – server error\n\n#### [Create an image](#create-an-image)\n\n`POST /images/create`\n\nCreate an image either by pulling it from the registry or by importing it\n\n**Example request**:\n\n```\nPOST /v1.24/images/create?fromImage=busybox&tag=latest HTTP/1.1\n```\n\n**Example response**:\n\n```\nHTTP/1.1 200 OK\nContent-Type: application/json\n\n{\"status\": \"Pulling...\"}\n{\"status\": \"Pulling\", \"progress\": \"1 B/ 100 B\", \"progressDetail\": {\"current\": 1, \"total\": 100}}\n{\"error\": \"Invalid...\"}\n...\n```\n\nWhen using this endpoint to pull an image from the registry, the `X-Registry-Auth` header can be used to include a base64-encoded AuthConfig object.\n\n**Query parameters**:\n\n*   **fromImage** – Name of the image to pull. The name may include a tag or digest. This parameter may only be used when pulling an image. The pull is cancelled if the HTTP connection is closed.\n*   **fromSrc** – Source to import. The value may be a URL from which the image can be retrieved or `-` to read the image from the request body. This parameter may only be used when importing an image.\n*   **repo** – Repository name given to an image when it is imported. The repo may include a tag. This parameter may only be used when importing an image.\n*   **tag** – Tag or digest. If empty when pulling an image, this causes all tags for the given image to be pulled.\n\n**Request Headers**:\n\n*   **X-Registry-Auth** – base64-encoded AuthConfig object, containing either login information, or a token\n    \n    *   Credential based login:\n        \n    \n    { \"username\": \"jdoe\", \"password\": \"secret\", \"email\": \"jdoe@acme.com\" } \\`\\`\\`\n    \n    *   Token based login:\n        \n    \n    { \"identitytoken\": \"9cbaf023786cd7...\" } \\`\\`\\`\n    \n\n**Status codes**:\n\n*   **200** – no error\n*   **404** - repository does not exist or no read access\n*   **500** – server error\n\n#### [Inspect an image](#inspect-an-image)\n\n`GET /images/(name)/json`\n\nReturn low-level information on the image `name`\n\n**Example request**:\n\n```\nGET /v1.24/images/example/json HTTP/1.1\n```\n\n**Example response**:\n\n```\nHTTP/1.1 200 OK\nContent-Type: application/json\n\n{\n   \"Id\" : \"sha256:85f05633ddc1c50679be2b16a0479ab6f7637f8884e0cfe0f4d20e1ebb3d6e7c\",\n   \"Container\" : \"cb91e48a60d01f1e27028b4fc6819f4f290b3cf12496c8176ec714d0d390984a\",\n   \"Comment\" : \"\",\n   \"Os\" : \"linux\",\n   \"Architecture\" : \"amd64\",\n   \"Parent\" : \"sha256:91e54dfb11794fad694460162bf0cb0a4fa710cfa3f60979c177d920813e267c\",\n   \"ContainerConfig\" : {\n      \"Tty\" : false,\n      \"Hostname\" : \"e611e15f9c9d\",\n      \"Volumes\" : null,\n      \"Domainname\" : \"\",\n      \"AttachStdout\" : false,\n      \"PublishService\" : \"\",\n      \"AttachStdin\" : false,\n      \"OpenStdin\" : false,\n      \"StdinOnce\" : false,\n      \"NetworkDisabled\" : false,\n      \"OnBuild\" : [],\n      \"Image\" : \"91e54dfb11794fad694460162bf0cb0a4fa710cfa3f60979c177d920813e267c\",\n      \"User\" : \"\",\n      \"WorkingDir\" : \"\",\n      \"Entrypoint\" : null,\n      \"MacAddress\" : \"\",\n      \"AttachStderr\" : false,\n      \"Labels\" : {\n         \"com.example.license\" : \"GPL\",\n         \"com.example.version\" : \"1.0\",\n         \"com.example.vendor\" : \"Acme\"\n      },\n      \"Env\" : [\n         \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"\n      ],\n      \"ExposedPorts\" : null,\n      \"Cmd\" : [\n         \"/bin/sh\",\n         \"-c\",\n         \"#(nop) LABEL com.example.vendor=Acme com.example.license=GPL com.example.version=1.0\"\n      ]\n   },\n   \"DockerVersion\" : \"1.9.0-dev\",\n   \"VirtualSize\" : 188359297,\n   \"Size\" : 0,\n   \"Author\" : \"\",\n   \"Created\" : \"2015-09-10T08:30:53.26995814Z\",\n   \"GraphDriver\" : {\n      \"Name\" : \"aufs\",\n      \"Data\" : null\n   },\n   \"RepoDigests\" : [\n      \"localhost:5000/test/busybox/example@sha256:cbbf2f9a99b47fc460d422812b6a5adff7dfee951d8fa2e4a98caa0382cfbdbf\"\n   ],\n   \"RepoTags\" : [\n      \"example:1.0\",\n      \"example:latest\",\n      \"example:stable\"\n   ],\n   \"Config\" : {\n      \"Image\" : \"91e54dfb11794fad694460162bf0cb0a4fa710cfa3f60979c177d920813e267c\",\n      \"NetworkDisabled\" : false,\n      \"OnBuild\" : [],\n      \"StdinOnce\" : false,\n      \"PublishService\" : \"\",\n      \"AttachStdin\" : false,\n      \"OpenStdin\" : false,\n      \"Domainname\" : \"\",\n      \"AttachStdout\" : false,\n      \"Tty\" : false,\n      \"Hostname\" : \"e611e15f9c9d\",\n      \"Volumes\" : null,\n      \"Cmd\" : [\n         \"/bin/bash\"\n      ],\n      \"ExposedPorts\" : null,\n      \"Env\" : [\n         \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"\n      ],\n      \"Labels\" : {\n         \"com.example.vendor\" : \"Acme\",\n         \"com.example.version\" : \"1.0\",\n         \"com.example.license\" : \"GPL\"\n      },\n      \"Entrypoint\" : null,\n      \"MacAddress\" : \"\",\n      \"AttachStderr\" : false,\n      \"WorkingDir\" : \"\",\n      \"User\" : \"\"\n   },\n   \"RootFS\": {\n       \"Type\": \"layers\",\n       \"Layers\": [\n           \"sha256:1834950e52ce4d5a88a1bbd131c537f4d0e56d10ff0dd69e66be3b7dfa9df7e6\",\n           \"sha256:5f70bf18a086007016e948b04aed3b82103a36bea41755b6cddfaf10ace3c6ef\"\n       ]\n   }\n}\n```\n\n**Status codes**:\n\n*   **200** – no error\n*   **404** – no such image\n*   **500** – server error\n\n#### [Get the history of an image](#get-the-history-of-an-image)\n\n`GET /images/(name)/history`\n\nReturn the history of the image `name`\n\n**Example request**:\n\n```\nGET /v1.24/images/ubuntu/history HTTP/1.1\n```\n\n**Example response**:\n\n```\nHTTP/1.1 200 OK\nContent-Type: application/json\n\n[\n    {\n        \"Id\": \"3db9c44f45209632d6050b35958829c3a2aa256d81b9a7be45b362ff85c54710\",\n        \"Created\": 1398108230,\n        \"CreatedBy\": \"/bin/sh -c #(nop) ADD file:eb15dbd63394e063b805a3c32ca7bf0266ef64676d5a6fab4801f2e81e2a5148 in /\",\n        \"Tags\": [\n            \"ubuntu:lucid\",\n            \"ubuntu:10.04\"\n        ],\n        \"Size\": 182964289,\n        \"Comment\": \"\"\n    },\n    {\n        \"Id\": \"6cfa4d1f33fb861d4d114f43b25abd0ac737509268065cdfd69d544a59c85ab8\",\n        \"Created\": 1398108222,\n        \"CreatedBy\": \"/bin/sh -c #(nop) MAINTAINER Tianon Gravi <admwiggin@gmail.com> - mkimage-debootstrap.sh -i iproute,iputils-ping,ubuntu-minimal -t lucid.tar.xz lucid http://archive.ubuntu.com/ubuntu/\",\n        \"Tags\": null,\n        \"Size\": 0,\n        \"Comment\": \"\"\n    },\n    {\n        \"Id\": \"511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158\",\n        \"Created\": 1371157430,\n        \"CreatedBy\": \"\",\n        \"Tags\": [\n            \"scratch12:latest\",\n            \"scratch:latest\"\n        ],\n        \"Size\": 0,\n        \"Comment\": \"Imported from -\"\n    }\n]\n```\n\n**Status codes**:\n\n*   **200** – no error\n*   **404** – no such image\n*   **500** – server error\n\n#### [Push an image on the registry](#push-an-image-on-the-registry)\n\n`POST /images/(name)/push`\n\nPush the image `name` on the registry\n\n**Example request**:\n\n```\nPOST /v1.24/images/test/push HTTP/1.1\n```\n\n**Example response**:\n\n```\nHTTP/1.1 200 OK\nContent-Type: application/json\n\n{\"status\": \"Pushing...\"}\n{\"status\": \"Pushing\", \"progress\": \"1/? (n/a)\", \"progressDetail\": {\"current\": 1}}}\n{\"error\": \"Invalid...\"}\n...\n```\n\nIf you wish to push an image on to a private registry, that image must already have a tag into a repository which references that registry `hostname` and `port`. This repository name should then be used in the URL. This duplicates the command line's flow.\n\nThe push is cancelled if the HTTP connection is closed.\n\n**Example request**:\n\n```\nPOST /v1.24/images/registry.acme.com:5000/test/push HTTP/1.1\n```\n\n**Query parameters**:\n\n*   **tag** – The tag to associate with the image on the registry. This is optional.\n\n**Request Headers**:\n\n*   **X-Registry-Auth** – base64-encoded AuthConfig object, containing either login information, or a token\n    \n    *   Credential based login:\n        \n    \n    { \"username\": \"jdoe\", \"password\": \"secret\", \"email\": \"jdoe@acme.com\", } \\`\\`\\`\n    \n    *   Identity token based login:\n        \n    \n    { \"identitytoken\": \"9cbaf023786cd7...\" } \\`\\`\\`\n    \n\n**Status codes**:\n\n*   **200** – no error\n*   **404** – no such image\n*   **500** – server error\n\n#### [Tag an image into a repository](#tag-an-image-into-a-repository)\n\n`POST /images/(name)/tag`\n\nTag the image `name` into a repository\n\n**Example request**:\n\n```\nPOST /v1.24/images/test/tag?repo=myrepo&tag=v42 HTTP/1.1\n```\n\n**Example response**:\n\n```\nHTTP/1.1 201 Created\n```\n\n**Query parameters**:\n\n*   **repo** – The repository to tag in\n*   **tag** - The new tag name\n\n**Status codes**:\n\n*   **201** – no error\n*   **400** – bad parameter\n*   **404** – no such image\n*   **409** – conflict\n*   **500** – server error\n\n#### [Remove an image](#remove-an-image)\n\n`DELETE /images/(name)`\n\nRemove the image `name` from the filesystem\n\n**Example request**:\n\n```\nDELETE /v1.24/images/test HTTP/1.1\n```\n\n**Example response**:\n\n```\nHTTP/1.1 200 OK\nContent-type: application/json\n\n[\n {\"Untagged\": \"3e2f21a89f\"},\n {\"Deleted\": \"3e2f21a89f\"},\n {\"Deleted\": \"53b4f83ac9\"}\n]\n```\n\n**Query parameters**:\n\n*   **force** – 1/True/true or 0/False/false, default false\n*   **noprune** – 1/True/true or 0/False/false, default false\n\n**Status codes**:\n\n*   **200** – no error\n*   **404** – no such image\n*   **409** – conflict\n*   **500** – server error\n\n#### [Search images](#search-images)\n\n`GET /images/search`\n\nSearch for an image on [Docker Hub](https://hub.docker.com/).\n\n> **Note**: The response keys have changed from API v1.6 to reflect the JSON sent by the registry server to the docker daemon's request.\n\n**Example request**:\n\n```\nGET /v1.24/images/search?term=sshd HTTP/1.1\n```\n\n**Example response**:\n\n```\nHTTP/1.1 200 OK\nContent-Type: application/json\n\n[\n        {\n            \"description\": \"\",\n            \"is_official\": false,\n            \"is_automated\": false,\n            \"name\": \"wma55/u1210sshd\",\n            \"star_count\": 0\n        },\n        {\n            \"description\": \"\",\n            \"is_official\": false,\n            \"is_automated\": false,\n            \"name\": \"jdswinbank/sshd\",\n            \"star_count\": 0\n        },\n        {\n            \"description\": \"\",\n            \"is_official\": false,\n            \"is_automated\": false,\n            \"name\": \"vgauthier/sshd\",\n            \"star_count\": 0\n        }\n...\n]\n```\n\n**Query parameters**:\n\n*   **term** – term to search\n*   **limit** – maximum returned search results\n*   **filters** – a JSON encoded value of the filters (a map\\[string\\]\\[\\]string) to process on the images list. Available filters:\n*   `stars=<number>`\n*   `is-automated=(true|false)`\n*   `is-official=(true|false)`\n\n**Status codes**:\n\n*   **200** – no error\n*   **500** – server error\n\n### [3.3 Misc](#33-misc)\n\n#### [Check auth configuration](#check-auth-configuration)\n\n`POST /auth`\n\nValidate credentials for a registry and get identity token, if available, for accessing the registry without password.\n\n**Example request**:\n\n```\nPOST /v1.24/auth HTTP/1.1\nContent-Type: application/json\nContent-Length: 12345\n\n{\n     \"username\": \"hannibal\",\n     \"password\": \"xxxx\",\n     \"serveraddress\": \"https://index.docker.io/v1/\"\n}\n```\n\n**Example response**:\n\n```\nHTTP/1.1 200 OK\n\n{\n     \"Status\": \"Login Succeeded\",\n     \"IdentityToken\": \"9cbaf023786cd7...\"\n}\n```\n\n**Status codes**:\n\n*   **200** – no error\n*   **204** – no error\n*   **500** – server error\n\n#### [Display system-wide information](#display-system-wide-information)\n\n`GET /info`\n\nDisplay system-wide information\n\n**Example request**:\n\n```\nGET /v1.24/info HTTP/1.1\n```\n\n**Example response**:\n\n```\nHTTP/1.1 200 OK\nContent-Type: application/json\n\n{\n    \"Architecture\": \"x86_64\",\n    \"ClusterStore\": \"etcd://localhost:2379\",\n    \"CgroupDriver\": \"cgroupfs\",\n    \"Containers\": 11,\n    \"ContainersRunning\": 7,\n    \"ContainersStopped\": 3,\n    \"ContainersPaused\": 1,\n    \"CpuCfsPeriod\": true,\n    \"CpuCfsQuota\": true,\n    \"Debug\": false,\n    \"DockerRootDir\": \"/var/lib/docker\",\n    \"Driver\": \"btrfs\",\n    \"DriverStatus\": [[\"\"]],\n    \"ExperimentalBuild\": false,\n    \"HttpProxy\": \"http://test:test@localhost:8080\",\n    \"HttpsProxy\": \"https://test:test@localhost:8080\",\n    \"ID\": \"7TRN:IPZB:QYBB:VPBQ:UMPP:KARE:6ZNR:XE6T:7EWV:PKF4:ZOJD:TPYS\",\n    \"IPv4Forwarding\": true,\n    \"Images\": 16,\n    \"IndexServerAddress\": \"https://index.docker.io/v1/\",\n    \"InitPath\": \"/usr/bin/docker\",\n    \"InitSha1\": \"\",\n    \"KernelMemory\": true,\n    \"KernelVersion\": \"3.12.0-1-amd64\",\n    \"Labels\": [\n        \"storage=ssd\"\n    ],\n    \"MemTotal\": 2099236864,\n    \"MemoryLimit\": true,\n    \"NCPU\": 1,\n    \"NEventsListener\": 0,\n    \"NFd\": 11,\n    \"NGoroutines\": 21,\n    \"Name\": \"prod-server-42\",\n    \"NoProxy\": \"9.81.1.160\",\n    \"OomKillDisable\": true,\n    \"OSType\": \"linux\",\n    \"OperatingSystem\": \"Boot2Docker\",\n    \"Plugins\": {\n        \"Volume\": [\n            \"local\"\n        ],\n        \"Network\": [\n            \"null\",\n            \"host\",\n            \"bridge\"\n        ]\n    },\n    \"RegistryConfig\": {\n        \"IndexConfigs\": {\n            \"docker.io\": {\n                \"Mirrors\": null,\n                \"Name\": \"docker.io\",\n                \"Official\": true,\n                \"Secure\": true\n            }\n        },\n        \"InsecureRegistryCIDRs\": [\n            \"127.0.0.0/8\"\n        ]\n    },\n    \"SecurityOptions\": [\n        \"apparmor\",\n        \"seccomp\",\n        \"selinux\"\n    ],\n    \"ServerVersion\": \"1.9.0\",\n    \"SwapLimit\": false,\n    \"SystemStatus\": [[\"State\", \"Healthy\"]],\n    \"SystemTime\": \"2015-03-10T11:11:23.730591467-07:00\"\n}\n```\n\n**Status codes**:\n\n*   **200** – no error\n*   **500** – server error\n\n#### [Show the docker version information](#show-the-docker-version-information)\n\n`GET /version`\n\nShow the docker version information\n\n**Example request**:\n\n```\nGET /v1.24/version HTTP/1.1\n```\n\n**Example response**:\n\n```\nHTTP/1.1 200 OK\nContent-Type: application/json\n\n{\n     \"Version\": \"1.12.0\",\n     \"Os\": \"linux\",\n     \"KernelVersion\": \"3.19.0-23-generic\",\n     \"GoVersion\": \"go1.6.3\",\n     \"GitCommit\": \"deadbee\",\n     \"Arch\": \"amd64\",\n     \"ApiVersion\": \"1.24\",\n     \"BuildTime\": \"2016-06-14T07:09:13.444803460+00:00\",\n     \"Experimental\": true\n}\n```\n\n**Status codes**:\n\n*   **200** – no error\n*   **500** – server error\n\n#### [Ping the docker server](#ping-the-docker-server)\n\n`GET /_ping`\n\nPing the docker server\n\n**Example request**:\n\n```\nGET /v1.24/_ping HTTP/1.1\n```\n\n**Example response**:\n\n```\nHTTP/1.1 200 OK\nContent-Type: text/plain\n\nOK\n```\n\n**Status codes**:\n\n*   **200** - no error\n*   **500** - server error\n\n#### [Create a new image from a container's changes](#create-a-new-image-from-a-containers-changes)\n\n`POST /commit`\n\nCreate a new image from a container's changes\n\n**Example request**:\n\n```\nPOST /v1.24/commit?container=44c004db4b17&comment=message&repo=myrepo HTTP/1.1\nContent-Type: application/json\nContent-Length: 12345\n\n{\n     \"Hostname\": \"\",\n     \"Domainname\": \"\",\n     \"User\": \"\",\n     \"AttachStdin\": false,\n     \"AttachStdout\": true,\n     \"AttachStderr\": true,\n     \"Tty\": false,\n     \"OpenStdin\": false,\n     \"StdinOnce\": false,\n     \"Env\": null,\n     \"Cmd\": [\n             \"date\"\n     ],\n     \"Mounts\": [\n       {\n         \"Source\": \"/data\",\n         \"Destination\": \"/data\",\n         \"Mode\": \"ro,Z\",\n         \"RW\": false\n       }\n     ],\n     \"Labels\": {\n             \"key1\": \"value1\",\n             \"key2\": \"value2\"\n      },\n     \"WorkingDir\": \"\",\n     \"NetworkDisabled\": false,\n     \"ExposedPorts\": {\n             \"22/tcp\": {}\n     }\n}\n```\n\n**Example response**:\n\n```\nHTTP/1.1 201 Created\nContent-Type: application/json\n\n{\"Id\": \"596069db4bf5\"}\n```\n\n**JSON parameters**:\n\n*   **config** - the container's configuration\n\n**Query parameters**:\n\n*   **container** – source container\n*   **repo** – repository\n*   **tag** – tag\n*   **comment** – commit message\n*   **author** – author (e.g., \"John Hannibal Smith < hannibal@a-team.com\\>\")\n*   **pause** – 1/True/true or 0/False/false, whether to pause the container before committing\n*   **changes** – Dockerfile instructions to apply while committing\n\n**Status codes**:\n\n*   **201** – no error\n*   **404** – no such container\n*   **500** – server error\n\n#### [Monitor Docker's events](#monitor-dockers-events)\n\n`GET /events`\n\nGet container events from docker, in real time via streaming.\n\nDocker containers report the following events:\n\n```\nattach, commit, copy, create, destroy, detach, die, exec_create, exec_detach, exec_start, export, health_status, kill, oom, pause, rename, resize, restart, start, stop, top, unpause, update\n```\n\nDocker images report the following events:\n\n```\ndelete, import, load, pull, push, save, tag, untag\n```\n\nDocker volumes report the following events:\n\n```\ncreate, mount, unmount, destroy\n```\n\nDocker networks report the following events:\n\n```\ncreate, connect, disconnect, destroy\n```\n\nDocker daemon report the following event:\n\n```\nreload\n```\n\n**Example request**:\n\n```\nGET /v1.24/events?since=1374067924\n```\n\n**Example response**:\n\n```\nHTTP/1.1 200 OK\nContent-Type: application/json\nServer: Docker/1.12.0 (linux)\nDate: Fri, 29 Apr 2016 15:18:06 GMT\nTransfer-Encoding: chunked\n\n{\n  \"status\": \"pull\",\n  \"id\": \"alpine:latest\",\n  \"Type\": \"image\",\n  \"Action\": \"pull\",\n  \"Actor\": {\n    \"ID\": \"alpine:latest\",\n    \"Attributes\": {\n      \"name\": \"alpine\"\n    }\n  },\n  \"time\": 1461943101,\n  \"timeNano\": 1461943101301854122\n}\n{\n  \"status\": \"create\",\n  \"id\": \"ede54ee1afda366ab42f824e8a5ffd195155d853ceaec74a927f249ea270c743\",\n  \"from\": \"alpine\",\n  \"Type\": \"container\",\n  \"Action\": \"create\",\n  \"Actor\": {\n    \"ID\": \"ede54ee1afda366ab42f824e8a5ffd195155d853ceaec74a927f249ea270c743\",\n    \"Attributes\": {\n      \"com.example.some-label\": \"some-label-value\",\n      \"image\": \"alpine\",\n      \"name\": \"my-container\"\n    }\n  },\n  \"time\": 1461943101,\n  \"timeNano\": 1461943101381709551\n}\n{\n  \"status\": \"attach\",\n  \"id\": \"ede54ee1afda366ab42f824e8a5ffd195155d853ceaec74a927f249ea270c743\",\n  \"from\": \"alpine\",\n  \"Type\": \"container\",\n  \"Action\": \"attach\",\n  \"Actor\": {\n    \"ID\": \"ede54ee1afda366ab42f824e8a5ffd195155d853ceaec74a927f249ea270c743\",\n    \"Attributes\": {\n      \"com.example.some-label\": \"some-label-value\",\n      \"image\": \"alpine\",\n      \"name\": \"my-container\"\n    }\n  },\n  \"time\": 1461943101,\n  \"timeNano\": 1461943101383858412\n}\n{\n  \"Type\": \"network\",\n  \"Action\": \"connect\",\n  \"Actor\": {\n    \"ID\": \"7dc8ac97d5d29ef6c31b6052f3938c1e8f2749abbd17d1bd1febf2608db1b474\",\n    \"Attributes\": {\n      \"container\": \"ede54ee1afda366ab42f824e8a5ffd195155d853ceaec74a927f249ea270c743\",\n      \"name\": \"bridge\",\n      \"type\": \"bridge\"\n    }\n  },\n  \"time\": 1461943101,\n  \"timeNano\": 1461943101394865557\n}\n{\n  \"status\": \"start\",\n  \"id\": \"ede54ee1afda366ab42f824e8a5ffd195155d853ceaec74a927f249ea270c743\",\n  \"from\": \"alpine\",\n  \"Type\": \"container\",\n  \"Action\": \"start\",\n  \"Actor\": {\n    \"ID\": \"ede54ee1afda366ab42f824e8a5ffd195155d853ceaec74a927f249ea270c743\",\n    \"Attributes\": {\n      \"com.example.some-label\": \"some-label-value\",\n      \"image\": \"alpine\",\n      \"name\": \"my-container\"\n    }\n  },\n  \"time\": 1461943101,\n  \"timeNano\": 1461943101607533796\n}\n{\n  \"status\": \"resize\",\n  \"id\": \"ede54ee1afda366ab42f824e8a5ffd195155d853ceaec74a927f249ea270c743\",\n  \"from\": \"alpine\",\n  \"Type\": \"container\",\n  \"Action\": \"resize\",\n  \"Actor\": {\n    \"ID\": \"ede54ee1afda366ab42f824e8a5ffd195155d853ceaec74a927f249ea270c743\",\n    \"Attributes\": {\n      \"com.example.some-label\": \"some-label-value\",\n      \"height\": \"46\",\n      \"image\": \"alpine\",\n      \"name\": \"my-container\",\n      \"width\": \"204\"\n    }\n  },\n  \"time\": 1461943101,\n  \"timeNano\": 1461943101610269268\n}\n{\n  \"status\": \"die\",\n  \"id\": \"ede54ee1afda366ab42f824e8a5ffd195155d853ceaec74a927f249ea270c743\",\n  \"from\": \"alpine\",\n  \"Type\": \"container\",\n  \"Action\": \"die\",\n  \"Actor\": {\n    \"ID\": \"ede54ee1afda366ab42f824e8a5ffd195155d853ceaec74a927f249ea270c743\",\n    \"Attributes\": {\n      \"com.example.some-label\": \"some-label-value\",\n      \"exitCode\": \"0\",\n      \"image\": \"alpine\",\n      \"name\": \"my-container\"\n    }\n  },\n  \"time\": 1461943105,\n  \"timeNano\": 1461943105079144137\n}\n{\n  \"Type\": \"network\",\n  \"Action\": \"disconnect\",\n  \"Actor\": {\n    \"ID\": \"7dc8ac97d5d29ef6c31b6052f3938c1e8f2749abbd17d1bd1febf2608db1b474\",\n    \"Attributes\": {\n      \"container\": \"ede54ee1afda366ab42f824e8a5ffd195155d853ceaec74a927f249ea270c743\",\n      \"name\": \"bridge\",\n      \"type\": \"bridge\"\n    }\n  },\n  \"time\": 1461943105,\n  \"timeNano\": 1461943105230860245\n}\n{\n  \"status\": \"destroy\",\n  \"id\": \"ede54ee1afda366ab42f824e8a5ffd195155d853ceaec74a927f249ea270c743\",\n  \"from\": \"alpine\",\n  \"Type\": \"container\",\n  \"Action\": \"destroy\",\n  \"Actor\": {\n    \"ID\": \"ede54ee1afda366ab42f824e8a5ffd195155d853ceaec74a927f249ea270c743\",\n    \"Attributes\": {\n      \"com.example.some-label\": \"some-label-value\",\n      \"image\": \"alpine\",\n      \"name\": \"my-container\"\n    }\n  },\n  \"time\": 1461943105,\n  \"timeNano\": 1461943105338056026\n}\n```\n\n**Query parameters**:\n\n*   **since** – Timestamp. Show all events created since timestamp and then stream\n*   **until** – Timestamp. Show events created until given timestamp and stop streaming\n*   **filters** – A json encoded value of the filters (a map\\[string\\]\\[\\]string) to process on the event list. Available filters:\n*   `container=<string>`; -- container to filter\n*   `event=<string>`; -- event to filter\n*   `image=<string>`; -- image to filter\n*   `label=<string>`; -- image and container label to filter\n*   `type=<string>`; -- either `container` or `image` or `volume` or `network` or `daemon`\n*   `volume=<string>`; -- volume to filter\n*   `network=<string>`; -- network to filter\n*   `daemon=<string>`; -- daemon name or id to filter\n\n**Status codes**:\n\n*   **200** – no error\n*   **400** - bad parameter\n*   **500** – server error\n\n#### [Get a tarball containing all images in a repository](#get-a-tarball-containing-all-images-in-a-repository)\n\n`GET /images/(name)/get`\n\nGet a tarball containing all images and metadata for the repository specified by `name`.\n\nIf `name` is a specific name and tag (e.g. ubuntu:latest), then only that image (and its parents) are returned. If `name` is an image ID, similarly only that image (and its parents) are returned, but with the exclusion of the 'repositories' file in the tarball, as there were no image names referenced.\n\nSee the [image tarball format](#image-tarball-format) for more details.\n\n**Example request**\n\n```\nGET /v1.24/images/ubuntu/get\n```\n\n**Example response**:\n\n```\nHTTP/1.1 200 OK\nContent-Type: application/x-tar\n\nBinary data stream\n```\n\n**Status codes**:\n\n*   **200** – no error\n*   **500** – server error\n\n#### [Get a tarball containing all images](#get-a-tarball-containing-all-images)\n\n`GET /images/get`\n\nGet a tarball containing all images and metadata for one or more repositories.\n\nFor each value of the `names` parameter: if it is a specific name and tag (e.g. `ubuntu:latest`), then only that image (and its parents) are returned; if it is an image ID, similarly only that image (and its parents) are returned and there would be no names referenced in the 'repositories' file for this image ID.\n\nSee the [image tarball format](#image-tarball-format) for more details.\n\n**Example request**\n\n```\nGET /v1.24/images/get?names=myname%2Fmyapp%3Alatest&names=busybox\n```\n\n**Example response**:\n\n```\nHTTP/1.1 200 OK\nContent-Type: application/x-tar\n\nBinary data stream\n```\n\n**Status codes**:\n\n*   **200** – no error\n*   **500** – server error\n\n#### [Load a tarball with a set of images and tags into docker](#load-a-tarball-with-a-set-of-images-and-tags-into-docker)\n\n`POST /images/load`\n\nLoad a set of images and tags into a Docker repository. See the [image tarball format](#image-tarball-format) for more details.\n\n**Example request**\n\n```\nPOST /v1.24/images/load\nContent-Type: application/x-tar\nContent-Length: 12345\n\nTarball in body\n```\n\n**Example response**:\n\n```\nHTTP/1.1 200 OK\nContent-Type: application/json\nTransfer-Encoding: chunked\n\n{\"status\":\"Loading layer\",\"progressDetail\":{\"current\":32768,\"total\":1292800},\"progress\":\"[=                                                 ] 32.77 kB/1.293 MB\",\"id\":\"8ac8bfaff55a\"}\n{\"status\":\"Loading layer\",\"progressDetail\":{\"current\":65536,\"total\":1292800},\"progress\":\"[==                                                ] 65.54 kB/1.293 MB\",\"id\":\"8ac8bfaff55a\"}\n{\"status\":\"Loading layer\",\"progressDetail\":{\"current\":98304,\"total\":1292800},\"progress\":\"[===                                               ]  98.3 kB/1.293 MB\",\"id\":\"8ac8bfaff55a\"}\n{\"status\":\"Loading layer\",\"progressDetail\":{\"current\":131072,\"total\":1292800},\"progress\":\"[=====                                             ] 131.1 kB/1.293 MB\",\"id\":\"8ac8bfaff55a\"}\n...\n{\"stream\":\"Loaded image: busybox:latest\\n\"}\n```\n\n**Example response**:\n\nIf the \"quiet\" query parameter is set to `true` / `1` (`?quiet=1`), progress details are suppressed, and only a confirmation message is returned once the action completes.\n\n```\nHTTP/1.1 200 OK\nContent-Type: application/json\nTransfer-Encoding: chunked\n\n{\"stream\":\"Loaded image: busybox:latest\\n\"}\n```\n\n**Query parameters**:\n\n*   **quiet** – Boolean value, suppress progress details during load. Defaults to `0` / `false` if omitted.\n\n**Status codes**:\n\n*   **200** – no error\n*   **500** – server error\n\n#### [Image tarball format](#image-tarball-format)\n\nAn image tarball contains one directory per image layer (named using its long ID), each containing these files:\n\n*   `VERSION`: currently `1.0` - the file format version\n*   `json`: detailed layer information, similar to `docker inspect layer_id`\n*   `layer.tar`: A tarfile containing the filesystem changes in this layer\n\nThe `layer.tar` file contains `aufs` style `.wh..wh.aufs` files and directories for storing attribute changes and deletions.\n\nIf the tarball defines a repository, the tarball should also include a `repositories` file at the root that contains a list of repository and tag names mapped to layer IDs.\n\n#### [Exec Create](#exec-create)\n\n`POST /containers/(id or name)/exec`\n\nSets up an exec instance in a running container `id`\n\n**Example request**:\n\n```\nPOST /v1.24/containers/e90e34656806/exec HTTP/1.1\nContent-Type: application/json\nContent-Length: 12345\n\n{\n  \"AttachStdin\": true,\n  \"AttachStdout\": true,\n  \"AttachStderr\": true,\n  \"Cmd\": [\"sh\"],\n  \"DetachKeys\": \"ctrl-p,ctrl-q\",\n  \"Privileged\": true,\n  \"Tty\": true,\n  \"User\": \"123:456\"\n}\n```\n\n**Example response**:\n\n```\nHTTP/1.1 201 Created\nContent-Type: application/json\n\n{\n     \"Id\": \"f90e34656806\",\n     \"Warnings\":[]\n}\n```\n\n**JSON parameters**:\n\n*   **AttachStdin** - Boolean value, attaches to `stdin` of the `exec` command.\n*   **AttachStdout** - Boolean value, attaches to `stdout` of the `exec` command.\n*   **AttachStderr** - Boolean value, attaches to `stderr` of the `exec` command.\n*   **DetachKeys** – Override the key sequence for detaching a container. Format is a single character `[a-Z]` or `ctrl-<value>` where `<value>` is one of: `a-z`, `@`, `^`, `[`, `,` or `_`.\n*   **Tty** - Boolean value to allocate a pseudo-TTY.\n*   **Cmd** - Command to run specified as a string or an array of strings.\n*   **Privileged** - Boolean value, runs the exec process with extended privileges.\n*   **User** - A string value specifying the user, and optionally, group to run the exec process inside the container. Format is one of: `\"user\"`, `\"user:group\"`, `\"uid\"`, or `\"uid:gid\"`.\n\n**Status codes**:\n\n*   **201** – no error\n*   **404** – no such container\n*   **409** - container is paused\n*   **500** - server error\n\n#### [Exec Start](#exec-start)\n\n`POST /exec/(id)/start`\n\nStarts a previously set up `exec` instance `id`. If `detach` is true, this API returns after starting the `exec` command. Otherwise, this API sets up an interactive session with the `exec` command.\n\n**Example request**:\n\n```\nPOST /v1.24/exec/e90e34656806/start HTTP/1.1\nContent-Type: application/json\nContent-Length: 12345\n\n{\n \"Detach\": false,\n \"Tty\": false\n}\n```\n\n**Example response**:\n\n```\nHTTP/1.1 200 OK\nContent-Type: application/vnd.docker.raw-stream\n\n{% raw %}\n{{ STREAM }}\n{% endraw %}\n```\n\n**JSON parameters**:\n\n*   **Detach** - Detach from the `exec` command.\n*   **Tty** - Boolean value to allocate a pseudo-TTY.\n\n**Status codes**:\n\n*   **200** – no error\n*   **404** – no such exec instance\n*   **409** - container is paused\n\n**Stream details**:\n\nSimilar to the stream behavior of `POST /containers/(id or name)/attach` API\n\n#### [Exec Resize](#exec-resize)\n\n`POST /exec/(id)/resize`\n\nResizes the `tty` session used by the `exec` command `id`. The unit is number of characters. This API is valid only if `tty` was specified as part of creating and starting the `exec` command.\n\n**Example request**:\n\n```\nPOST /v1.24/exec/e90e34656806/resize?h=40&w=80 HTTP/1.1\nContent-Type: text/plain\n```\n\n**Example response**:\n\n```\nHTTP/1.1 201 Created\nContent-Type: text/plain\n```\n\n**Query parameters**:\n\n*   **h** – height of `tty` session\n*   **w** – width\n\n**Status codes**:\n\n*   **201** – no error\n*   **404** – no such exec instance\n\n#### [Exec Inspect](#exec-inspect)\n\n`GET /exec/(id)/json`\n\nReturn low-level information about the `exec` command `id`.\n\n**Example request**:\n\n```\nGET /v1.24/exec/11fb006128e8ceb3942e7c58d77750f24210e35f879dd204ac975c184b820b39/json HTTP/1.1\n```\n\n**Example response**:\n\n```\nHTTP/1.1 200 OK\nContent-Type: application/json\n\n{\n  \"CanRemove\": false,\n  \"ContainerID\": \"b53ee82b53a40c7dca428523e34f741f3abc51d9f297a14ff874bf761b995126\",\n  \"DetachKeys\": \"\",\n  \"ExitCode\": 2,\n  \"ID\": \"f33bbfb39f5b142420f4759b2348913bd4a8d1a6d7fd56499cb41a1bb91d7b3b\",\n  \"OpenStderr\": true,\n  \"OpenStdin\": true,\n  \"OpenStdout\": true,\n  \"ProcessConfig\": {\n    \"arguments\": [\n      \"-c\",\n      \"exit 2\"\n    ],\n    \"entrypoint\": \"sh\",\n    \"privileged\": false,\n    \"tty\": true,\n    \"user\": \"1000\"\n  },\n  \"Running\": false\n}\n```\n\n**Status codes**:\n\n*   **200** – no error\n*   **404** – no such exec instance\n*   **500** - server error\n\n### [3.4 Volumes](#34-volumes)\n\n#### [List volumes](#list-volumes)\n\n`GET /volumes`\n\n**Example request**:\n\n```\nGET /v1.24/volumes HTTP/1.1\n```\n\n**Example response**:\n\n```\nHTTP/1.1 200 OK\nContent-Type: application/json\n\n{\n  \"Volumes\": [\n    {\n      \"Name\": \"tardis\",\n      \"Driver\": \"local\",\n      \"Mountpoint\": \"/var/lib/docker/volumes/tardis\",\n      \"Labels\": null,\n      \"Scope\": \"local\"\n    }\n  ],\n  \"Warnings\": []\n}\n```\n\n**Query parameters**:\n\n*   **filters** - JSON encoded value of the filters (a `map[string][]string`) to process on the volumes list. Available filters:\n    *   `name=<volume-name>` Matches all or part of a volume name.\n    *   `dangling=<boolean>` When set to `true` (or `1`), returns all volumes that are \"dangling\" (not in use by a container). When set to `false` (or `0`), only volumes that are in use by one or more containers are returned.\n    *   `driver=<volume-driver-name>` Matches all or part of a volume driver name.\n\n**Status codes**:\n\n*   **200** - no error\n*   **500** - server error\n\n#### [Create a volume](#create-a-volume)\n\n`POST /volumes/create`\n\nCreate a volume\n\n**Example request**:\n\n```\nPOST /v1.24/volumes/create HTTP/1.1\nContent-Type: application/json\nContent-Length: 12345\n\n{\n  \"Name\": \"tardis\",\n  \"Labels\": {\n    \"com.example.some-label\": \"some-value\",\n    \"com.example.some-other-label\": \"some-other-value\"\n  },\n  \"Driver\": \"custom\"\n}\n```\n\n**Example response**:\n\n```\nHTTP/1.1 201 Created\nContent-Type: application/json\n\n{\n  \"Name\": \"tardis\",\n  \"Driver\": \"custom\",\n  \"Mountpoint\": \"/var/lib/docker/volumes/tardis\",\n  \"Status\": {\n    \"hello\": \"world\"\n  },\n  \"Labels\": {\n    \"com.example.some-label\": \"some-value\",\n    \"com.example.some-other-label\": \"some-other-value\"\n  },\n  \"Scope\": \"local\"\n}\n```\n\n**Status codes**:\n\n*   **201** - no error\n*   **500** - server error\n\n**JSON parameters**:\n\n*   **Name** - The new volume's name. If not specified, Docker generates a name.\n*   **Driver** - Name of the volume driver to use. Defaults to `local` for the name.\n*   **DriverOpts** - A mapping of driver options and values. These options are passed directly to the driver and are driver specific.\n*   **Labels** - Labels to set on the volume, specified as a map: `{\"key\":\"value\",\"key2\":\"value2\"}`\n\n**JSON fields in response**:\n\nRefer to the [inspect a volume](#inspect-a-volume) section or details about the JSON fields returned in the response.\n\n#### [Inspect a volume](#inspect-a-volume)\n\n`GET /volumes/(name)`\n\nReturn low-level information on the volume `name`\n\n**Example request**:\n\n```\nGET /v1.24/volumes/tardis\n```\n\n**Example response**:\n\n```\nHTTP/1.1 200 OK\nContent-Type: application/json\n\n{\n  \"Name\": \"tardis\",\n  \"Driver\": \"custom\",\n  \"Mountpoint\": \"/var/lib/docker/volumes/tardis/_data\",\n  \"Status\": {\n    \"hello\": \"world\"\n  },\n  \"Labels\": {\n      \"com.example.some-label\": \"some-value\",\n      \"com.example.some-other-label\": \"some-other-value\"\n  },\n  \"Scope\": \"local\"\n}\n```\n\n**Status codes**:\n\n*   **200** - no error\n*   **404** - no such volume\n*   **500** - server error\n\n**JSON fields in response**:\n\nThe following fields can be returned in the API response. Empty fields, or fields that are not supported by the volume's driver may be omitted in the response.\n\n*   **Name** - Name of the volume.\n*   **Driver** - Name of the volume driver used by the volume.\n*   **Mountpoint** - Mount path of the volume on the host.\n*   **Status** - Low-level details about the volume, provided by the volume driver. Details are returned as a map with key/value pairs: `{\"key\":\"value\",\"key2\":\"value2\"}`. The `Status` field is optional, and is omitted if the volume driver does not support this feature.\n*   **Labels** - Labels set on the volume, specified as a map: `{\"key\":\"value\",\"key2\":\"value2\"}`.\n*   **Scope** - Scope describes the level at which the volume exists, can be one of `global` for cluster-wide or `local` for machine level. The default is `local`.\n\n#### [Remove a volume](#remove-a-volume)\n\n`DELETE /volumes/(name)`\n\nInstruct the driver to remove the volume (`name`).\n\n**Example request**:\n\n```\nDELETE /v1.24/volumes/tardis HTTP/1.1\n```\n\n**Example response**:\n\n```\nHTTP/1.1 204 No Content\n```\n\n**Status codes**:\n\n*   **204** - no error\n*   **404** - no such volume or volume driver\n*   **409** - volume is in use and cannot be removed\n*   **500** - server error\n\n### [3.5 Networks](#35-networks)\n\n#### [List networks](#list-networks)\n\n`GET /networks`\n\n**Example request**:\n\n```\nGET /v1.24/networks?filters={\"type\":{\"custom\":true}} HTTP/1.1\n```\n\n**Example response**:\n\n**Query parameters**:\n\n*   **filters** - JSON encoded network list filter. The filter value is one of:\n    *   `driver=<driver-name>` Matches a network's driver.\n    *   `id=<network-id>` Matches all or part of a network id.\n    *   `label=<key>` or `label=<key>=<value>` of a network label.\n    *   `name=<network-name>` Matches all or part of a network name.\n    *   `type=[\"custom\"|\"builtin\"]` Filters networks by type. The `custom` keyword returns all user-defined networks.\n\n**Status codes**:\n\n*   **200** - no error\n*   **500** - server error\n\n#### [Inspect network](#inspect-network)\n\n`GET /networks/(id or name)`\n\nReturn low-level information on the network `id`\n\n**Example request**:\n\n```\nGET /v1.24/networks/7d86d31b1478e7cca9ebed7e73aa0fdeec46c5ca29497431d3007d2d9e15ed99 HTTP/1.1\n```\n\n**Example response**:\n\n**Status codes**:\n\n*   **200** - no error\n*   **404** - network not found\n*   **500** - server error\n\n#### [Create a network](#create-a-network)\n\n`POST /networks/create`\n\nCreate a network\n\n**Example request**:\n\n**Example response**:\n\n**Status codes**:\n\n*   **201** - no error\n*   **403** - operation not supported for pre-defined networks\n*   **404** - plugin not found\n*   **500** - server error\n\n**JSON parameters**:\n\n*   **Name** - The new network's name. this is a mandatory field\n*   **CheckDuplicate** - Requests daemon to check for networks with same name. Defaults to `false`. Since Network is primarily keyed based on a random ID and not on the name, and network name is strictly a user-friendly alias to the network which is uniquely identified using ID, there is no guaranteed way to check for duplicates. This parameter CheckDuplicate is there to provide a best effort checking of any networks which has the same name but it is not guaranteed to catch all name collisions.\n*   **Driver** - Name of the network driver plugin to use. Defaults to `bridge` driver\n*   **Internal** - Restrict external access to the network\n*   **IPAM** - Optional custom IP scheme for the network\n    *   **Driver** - Name of the IPAM driver to use. Defaults to `default` driver\n    *   **Config** - List of IPAM configuration options, specified as a map: `{\"Subnet\": <CIDR>, \"IPRange\": <CIDR>, \"Gateway\": <IP address>, \"AuxAddress\": <device_name:IP address>}`\n    *   **Options** - Driver-specific options, specified as a map: `{\"option\":\"value\" [,\"option2\":\"value2\"]}`\n*   **EnableIPv6** - Enable IPv6 on the network\n*   **Options** - Network specific options to be used by the drivers\n*   **Labels** - Labels to set on the network, specified as a map: `{\"key\":\"value\" [,\"key2\":\"value2\"]}`\n\n#### [Connect a container to a network](#connect-a-container-to-a-network)\n\n`POST /networks/(id or name)/connect`\n\nConnect a container to a network\n\n**Example request**:\n\n**Example response**:\n\n```\nHTTP/1.1 200 OK\n```\n\n**Status codes**:\n\n*   **200** - no error\n*   **403** - operation not supported for swarm scoped networks\n*   **404** - network or container is not found\n*   **500** - Internal Server Error\n\n**JSON parameters**:\n\n*   **container** - container-id/name to be connected to the network\n\n#### [Disconnect a container from a network](#disconnect-a-container-from-a-network)\n\n`POST /networks/(id or name)/disconnect`\n\nDisconnect a container from a network\n\n**Example request**:\n\n**Example response**:\n\n```\nHTTP/1.1 200 OK\n```\n\n**Status codes**:\n\n*   **200** - no error\n*   **403** - operation not supported for swarm scoped networks\n*   **404** - network or container not found\n*   **500** - Internal Server Error\n\n**JSON parameters**:\n\n*   **Container** - container-id/name to be disconnected from a network\n*   **Force** - Force the container to disconnect from a network\n\n#### [Remove a network](#remove-a-network)\n\n`DELETE /networks/(id or name)`\n\nInstruct the driver to remove the network (`id`).\n\n**Example request**:\n\n```\nDELETE /v1.24/networks/22be93d5babb089c5aab8dbc369042fad48ff791584ca2da2100db837a1c7c30 HTTP/1.1\n```\n\n**Example response**:\n\n```\nHTTP/1.1 204 No Content\n```\n\n**Status codes**:\n\n*   **204** - no error\n*   **403** - operation not supported for pre-defined networks\n*   **404** - no such network\n*   **500** - server error\n\n### [3.6 Plugins (experimental)](#36-plugins-experimental)\n\n#### [List plugins](#list-plugins)\n\n`GET /plugins`\n\nReturns information about installed plugins.\n\n**Example request**:\n\n```\nGET /v1.24/plugins HTTP/1.1\n```\n\n**Example response**:\n\n**Status codes**:\n\n*   **200** - no error\n*   **500** - server error\n\n#### [Install a plugin](#install-a-plugin)\n\n`POST /plugins/pull?name=<plugin name>`\n\nPulls and installs a plugin. After the plugin is installed, it can be enabled using the [`POST /plugins/(plugin name)/enable` endpoint](#enable-a-plugin).\n\n**Example request**:\n\nThe `:latest` tag is optional, and is used as default if omitted. When using this endpoint to pull a plugin from the registry, the `X-Registry-Auth` header can be used to include a base64-encoded AuthConfig object. Refer to the [create an image](#create-an-image) section for more details.\n\n**Example response**:\n\n**Query parameters**:\n\n*   **name** - Name of the plugin to pull. The name may include a tag or digest. This parameter is required.\n\n**Status codes**:\n\n*   **200** - no error\n*   **500** - error parsing reference / not a valid repository/tag: repository name must have at least one component\n*   **500** - plugin already exists\n\n#### [Inspect a plugin](#inspect-a-plugin)\n\n`GET /plugins/(plugin name)`\n\nReturns detailed information about an installed plugin.\n\n**Example request**:\n\nThe `:latest` tag is optional, and is used as default if omitted.\n\n**Example response**:\n\n**Status codes**:\n\n*   **200** - no error\n*   **404** - plugin not installed\n\n#### [Enable a plugin](#enable-a-plugin)\n\n`POST /plugins/(plugin name)/enable`\n\nEnables a plugin\n\n**Example request**:\n\nThe `:latest` tag is optional, and is used as default if omitted.\n\n**Example response**:\n\n**Status codes**:\n\n*   **200** - no error\n*   **404** - plugin not installed\n*   **500** - plugin is already enabled\n\n#### [Disable a plugin](#disable-a-plugin)\n\n`POST /plugins/(plugin name)/disable`\n\nDisables a plugin\n\n**Example request**:\n\nThe `:latest` tag is optional, and is used as default if omitted.\n\n**Example response**:\n\n**Status codes**:\n\n*   **200** - no error\n*   **404** - plugin not installed\n*   **500** - plugin is already disabled\n\n#### [Remove a plugin](#remove-a-plugin)\n\n`DELETE /plugins/(plugin name)`\n\nRemoves a plugin\n\n**Example request**:\n\nThe `:latest` tag is optional, and is used as default if omitted.\n\n**Example response**:\n\n**Status codes**:\n\n*   **200** - no error\n*   **404** - plugin not installed\n*   **500** - plugin is active\n\n### [3.7 Nodes](#37-nodes)\n\n**Note**: Node operations require the engine to be part of a swarm.\n\n#### [List nodes](#list-nodes)\n\n`GET /nodes`\n\nList nodes\n\n**Example request**:\n\n```\nGET /v1.24/nodes HTTP/1.1\n```\n\n**Example response**:\n\n```\nHTTP/1.1 200 OK\nContent-Type: application/json\n\n[\n  {\n    \"ID\": \"24ifsmvkjbyhk\",\n    \"Version\": {\n      \"Index\": 8\n    },\n    \"CreatedAt\": \"2016-06-07T20:31:11.853781916Z\",\n    \"UpdatedAt\": \"2016-06-07T20:31:11.999868824Z\",\n    \"Spec\": {\n      \"Name\": \"my-node\",\n      \"Role\": \"manager\",\n      \"Availability\": \"active\"\n      \"Labels\": {\n          \"foo\": \"bar\"\n      }\n    },\n    \"Description\": {\n      \"Hostname\": \"bf3067039e47\",\n      \"Platform\": {\n        \"Architecture\": \"x86_64\",\n        \"OS\": \"linux\"\n      },\n      \"Resources\": {\n        \"NanoCPUs\": 4000000000,\n        \"MemoryBytes\": 8272408576\n      },\n      \"Engine\": {\n        \"EngineVersion\": \"1.12.0\",\n        \"Labels\": {\n            \"foo\": \"bar\",\n        }\n        \"Plugins\": [\n          {\n            \"Type\": \"Volume\",\n            \"Name\": \"local\"\n          },\n          {\n            \"Type\": \"Network\",\n            \"Name\": \"bridge\"\n          }\n          {\n            \"Type\": \"Network\",\n            \"Name\": \"null\"\n          }\n          {\n            \"Type\": \"Network\",\n            \"Name\": \"overlay\"\n          }\n        ]\n      }\n    },\n    \"Status\": {\n      \"State\": \"ready\"\n    },\n    \"ManagerStatus\": {\n      \"Leader\": true,\n      \"Reachability\": \"reachable\",\n      \"Addr\": \"172.17.0.2:2377\"\"\n    }\n  }\n]\n```\n\n**Query parameters**:\n\n*   **filters** – a JSON encoded value of the filters (a `map[string][]string`) to process on the nodes list. Available filters:\n    *   `id=<node id>`\n    *   `label=<engine label>`\n    *   `membership=`(`accepted`|`pending`)\\`\n    *   `name=<node name>`\n    *   `role=`(`manager`|`worker`)\\`\n\n**Status codes**:\n\n*   **200** – no error\n*   **406** - node is not part of a swarm\n*   **500** – server error\n\n#### [Inspect a node](#inspect-a-node)\n\n`GET /nodes/(id or name)`\n\nReturn low-level information on the node `id`\n\n**Example request**:\n\n```\n  GET /v1.24/nodes/24ifsmvkjbyhk HTTP/1.1\n```\n\n**Example response**:\n\n```\nHTTP/1.1 200 OK\nContent-Type: application/json\n\n{\n  \"ID\": \"24ifsmvkjbyhk\",\n  \"Version\": {\n    \"Index\": 8\n  },\n  \"CreatedAt\": \"2016-06-07T20:31:11.853781916Z\",\n  \"UpdatedAt\": \"2016-06-07T20:31:11.999868824Z\",\n  \"Spec\": {\n    \"Name\": \"my-node\",\n    \"Role\": \"manager\",\n    \"Availability\": \"active\"\n    \"Labels\": {\n        \"foo\": \"bar\"\n    }\n  },\n  \"Description\": {\n    \"Hostname\": \"bf3067039e47\",\n    \"Platform\": {\n      \"Architecture\": \"x86_64\",\n      \"OS\": \"linux\"\n    },\n    \"Resources\": {\n      \"NanoCPUs\": 4000000000,\n      \"MemoryBytes\": 8272408576\n    },\n    \"Engine\": {\n      \"EngineVersion\": \"1.12.0\",\n      \"Labels\": {\n          \"foo\": \"bar\",\n      }\n      \"Plugins\": [\n        {\n          \"Type\": \"Volume\",\n          \"Name\": \"local\"\n        },\n        {\n          \"Type\": \"Network\",\n          \"Name\": \"bridge\"\n        }\n        {\n          \"Type\": \"Network\",\n          \"Name\": \"null\"\n        }\n        {\n          \"Type\": \"Network\",\n          \"Name\": \"overlay\"\n        }\n      ]\n    }\n  },\n  \"Status\": {\n    \"State\": \"ready\"\n  },\n  \"ManagerStatus\": {\n    \"Leader\": true,\n    \"Reachability\": \"reachable\",\n    \"Addr\": \"172.17.0.2:2377\"\"\n  }\n}\n```\n\n**Status codes**:\n\n*   **200** – no error\n*   **404** – no such node\n*   **406** – node is not part of a swarm\n*   **500** – server error\n\n#### [Remove a node](#remove-a-node)\n\n`DELETE /nodes/(id or name)`\n\nRemove a node from the swarm.\n\n**Example request**:\n\n```\nDELETE /v1.24/nodes/24ifsmvkjbyhk HTTP/1.1\n```\n\n**Example response**:\n\n```\nHTTP/1.1 200 OK\nContent-Length: 0\nContent-Type: text/plain; charset=utf-8\n```\n\n**Query parameters**:\n\n*   **force** - 1/True/true or 0/False/false, Force remove a node from the swarm. Default `false`.\n\n**Status codes**:\n\n*   **200** – no error\n*   **404** – no such node\n*   **406** – node is not part of a swarm\n*   **500** – server error\n\n#### [Update a node](#update-a-node)\n\n`POST /nodes/(id)/update`\n\nUpdate a node.\n\nThe payload of the `POST` request is the new `NodeSpec` and overrides the current `NodeSpec` for the specified node.\n\nIf `Availability` or `Role` are omitted, this returns an error. Any other field omitted resets the current value to either an empty value or the default cluster-wide value.\n\n**Example Request**\n\n```\nPOST /v1.24/nodes/24ifsmvkjbyhk/update?version=8 HTTP/1.1\nContent-Type: application/json\nContent-Length: 12345\n\n{\n  \"Availability\": \"active\",\n  \"Name\": \"node-name\",\n  \"Role\": \"manager\",\n  \"Labels\": {\n    \"foo\": \"bar\"\n  }\n}\n```\n\n**Example response**:\n\n```\nHTTP/1.1 200 OK\nContent-Length: 0\nContent-Type: text/plain; charset=utf-8\n```\n\n**Query parameters**:\n\n*   **version** – The version number of the node object being updated. This is required to avoid conflicting writes.\n\nJSON Parameters:\n\n*   **Annotations** – Optional medata to associate with the node.\n    *   **Name** – User-defined name for the node.\n    *   **Labels** – A map of labels to associate with the node (e.g., `{\"key\":\"value\", \"key2\":\"value2\"}`).\n*   **Role** - Role of the node (worker|manager).\n*   **Availability** - Availability of the node (active|pause|drain).\n\n**Status codes**:\n\n*   **200** – no error\n*   **404** – no such node\n*   **406** – node is not part of a swarm\n*   **500** – server error\n\n### [3.8 Swarm](#38-swarm)\n\n#### [Inspect swarm](#inspect-swarm)\n\n`GET /swarm`\n\nInspect swarm\n\n**Example response**:\n\n```\nHTTP/1.1 200 OK\nContent-Type: application/json\n\n{\n  \"CreatedAt\" : \"2016-08-15T16:00:20.349727406Z\",\n  \"Spec\" : {\n    \"Dispatcher\" : {\n      \"HeartbeatPeriod\" : 5000000000\n    },\n    \"Orchestration\" : {\n     \"TaskHistoryRetentionLimit\" : 10\n    },\n    \"CAConfig\" : {\n      \"NodeCertExpiry\" : 7776000000000000\n    },\n    \"Raft\" : {\n      \"LogEntriesForSlowFollowers\" : 500,\n      \"HeartbeatTick\" : 1,\n      \"SnapshotInterval\" : 10000,\n      \"ElectionTick\" : 3\n    },\n    \"TaskDefaults\" : {},\n    \"Name\" : \"default\"\n  },\n \"JoinTokens\" : {\n    \"Worker\" : \"SWMTKN-1-1h8aps2yszaiqmz2l3oc5392pgk8e49qhx2aj3nyv0ui0hez2a-6qmn92w6bu3jdvnglku58u11a\",\n    \"Manager\" : \"SWMTKN-1-1h8aps2yszaiqmz2l3oc5392pgk8e49qhx2aj3nyv0ui0hez2a-8llk83c4wm9lwioey2s316r9l\"\n },\n \"ID\" : \"70ilmkj2f6sp2137c753w2nmt\",\n \"UpdatedAt\" : \"2016-08-15T16:32:09.623207604Z\",\n \"Version\" : {\n   \"Index\" : 51\n}\n```\n\n}\n\n**Status codes**:\n\n*   **200** - no error\n*   **406** – node is not part of a swarm\n*   **500** - sever error\n\n#### [Initialize a new swarm](#initialize-a-new-swarm)\n\n`POST /swarm/init`\n\nInitialize a new swarm. The body of the HTTP response includes the node ID.\n\n**Example request**:\n\n```\nPOST /v1.24/swarm/init HTTP/1.1\nContent-Type: application/json\nContent-Length: 12345\n\n{\n  \"ListenAddr\": \"0.0.0.0:2377\",\n  \"AdvertiseAddr\": \"192.168.1.1:2377\",\n  \"ForceNewCluster\": false,\n  \"Spec\": {\n    \"Orchestration\": {},\n    \"Raft\": {},\n    \"Dispatcher\": {},\n    \"CAConfig\": {}\n  }\n}\n```\n\n**Example response**:\n\n```\nHTTP/1.1 200 OK\nContent-Length: 28\nContent-Type: application/json\nDate: Thu, 01 Sep 2016 21:49:13 GMT\nServer: Docker/1.12.0 (linux)\n\n\"7v2t30z9blmxuhnyo6s4cpenp\"\n```\n\n**Status codes**:\n\n*   **200** – no error\n*   **400** – bad parameter\n*   **406** – node is already part of a swarm\n*   **500** - server error\n\nJSON Parameters:\n\n*   **ListenAddr** – Listen address used for inter-manager communication, as well as determining the networking interface used for the VXLAN Tunnel Endpoint (VTEP). This can either be an address/port combination in the form `192.168.1.1:4567`, or an interface followed by a port number, like `eth0:4567`. If the port number is omitted, the default swarm listening port is used.\n*   **AdvertiseAddr** – Externally reachable address advertised to other nodes. This can either be an address/port combination in the form `192.168.1.1:4567`, or an interface followed by a port number, like `eth0:4567`. If the port number is omitted, the port number from the listen address is used. If `AdvertiseAddr` is not specified, it will be automatically detected when possible.\n*   **ForceNewCluster** – Force creation of a new swarm.\n*   **Spec** – Configuration settings for the new swarm.\n    *   **Orchestration** – Configuration settings for the orchestration aspects of the swarm.\n        *   **TaskHistoryRetentionLimit** – Maximum number of tasks history stored.\n    *   **Raft** – Raft related configuration.\n        *   **SnapshotInterval** – Number of logs entries between snapshot.\n        *   **KeepOldSnapshots** – Number of snapshots to keep beyond the current snapshot.\n        *   **LogEntriesForSlowFollowers** – Number of log entries to keep around to sync up slow followers after a snapshot is created.\n        *   **HeartbeatTick** – Amount of ticks (in seconds) between each heartbeat.\n        *   **ElectionTick** – Amount of ticks (in seconds) needed without a leader to trigger a new election.\n    *   **Dispatcher** – Configuration settings for the task dispatcher.\n        *   **HeartbeatPeriod** – The delay for an agent to send a heartbeat to the dispatcher.\n    *   **CAConfig** – Certificate authority configuration.\n        *   **NodeCertExpiry** – Automatic expiry for nodes certificates.\n        *   **ExternalCA** - Configuration for forwarding signing requests to an external certificate authority.\n            *   **Protocol** - Protocol for communication with the external CA (currently only \"cfssl\" is supported).\n            *   **URL** - URL where certificate signing requests should be sent.\n            *   **Options** - An object with key/value pairs that are interpreted as protocol-specific options for the external CA driver.\n\n#### [Join an existing swarm](#join-an-existing-swarm)\n\n`POST /swarm/join`\n\nJoin an existing swarm\n\n**Example request**:\n\n```\nPOST /v1.24/swarm/join HTTP/1.1\nContent-Type: application/json\n\n{\n  \"ListenAddr\": \"0.0.0.0:2377\",\n  \"AdvertiseAddr\": \"192.168.1.1:2377\",\n  \"RemoteAddrs\": [\"node1:2377\"],\n  \"JoinToken\": \"SWMTKN-1-3pu6hszjas19xyp7ghgosyx9k8atbfcr8p2is99znpy26u2lkl-7p73s1dx5in4tatdymyhg9hu2\"\n}\n```\n\n**Example response**:\n\n```\nHTTP/1.1 200 OK\nContent-Length: 0\nContent-Type: text/plain; charset=utf-8\n```\n\n**Status codes**:\n\n*   **200** – no error\n*   **400** – bad parameter\n*   **406** – node is already part of a swarm\n*   **500** - server error\n\nJSON Parameters:\n\n*   **ListenAddr** – Listen address used for inter-manager communication if the node gets promoted to manager, as well as determining the networking interface used for the VXLAN Tunnel Endpoint (VTEP).\n*   **AdvertiseAddr** – Externally reachable address advertised to other nodes. This can either be an address/port combination in the form `192.168.1.1:4567`, or an interface followed by a port number, like `eth0:4567`. If the port number is omitted, the port number from the listen address is used. If `AdvertiseAddr` is not specified, it will be automatically detected when possible.\n*   **RemoteAddr** – Address of any manager node already participating in the swarm.\n*   **JoinToken** – Secret token for joining this swarm.\n\n#### [Leave a swarm](#leave-a-swarm)\n\n`POST /swarm/leave`\n\nLeave a swarm\n\n**Example request**:\n\n```\nPOST /v1.24/swarm/leave HTTP/1.1\n```\n\n**Example response**:\n\n```\nHTTP/1.1 200 OK\nContent-Length: 0\nContent-Type: text/plain; charset=utf-8\n```\n\n**Query parameters**:\n\n*   **force** - Boolean (0/1, false/true). Force leave swarm, even if this is the last manager or that it will break the cluster.\n\n**Status codes**:\n\n*   **200** – no error\n*   **406** – node is not part of a swarm\n*   **500** - server error\n\n#### [Update a swarm](#update-a-swarm)\n\n`POST /swarm/update`\n\nUpdate a swarm\n\n**Example request**:\n\n```\nPOST /v1.24/swarm/update HTTP/1.1\nContent-Length: 12345\n\n{\n  \"Name\": \"default\",\n  \"Orchestration\": {\n    \"TaskHistoryRetentionLimit\": 10\n  },\n  \"Raft\": {\n    \"SnapshotInterval\": 10000,\n    \"LogEntriesForSlowFollowers\": 500,\n    \"HeartbeatTick\": 1,\n    \"ElectionTick\": 3\n  },\n  \"Dispatcher\": {\n    \"HeartbeatPeriod\": 5000000000\n  },\n  \"CAConfig\": {\n    \"NodeCertExpiry\": 7776000000000000\n  },\n  \"JoinTokens\": {\n    \"Worker\": \"SWMTKN-1-3pu6hszjas19xyp7ghgosyx9k8atbfcr8p2is99znpy26u2lkl-1awxwuwd3z9j1z3puu7rcgdbx\",\n    \"Manager\": \"SWMTKN-1-3pu6hszjas19xyp7ghgosyx9k8atbfcr8p2is99znpy26u2lkl-7p73s1dx5in4tatdymyhg9hu2\"\n  }\n}\n```\n\n**Example response**:\n\n```\nHTTP/1.1 200 OK\nContent-Length: 0\nContent-Type: text/plain; charset=utf-8\n```\n\n**Query parameters**:\n\n*   **version** – The version number of the swarm object being updated. This is required to avoid conflicting writes.\n*   **rotateWorkerToken** - Set to `true` (or `1`) to rotate the worker join token.\n*   **rotateManagerToken** - Set to `true` (or `1`) to rotate the manager join token.\n\n**Status codes**:\n\n*   **200** – no error\n*   **400** – bad parameter\n*   **406** – node is not part of a swarm\n*   **500** - server error\n\nJSON Parameters:\n\n*   **Orchestration** – Configuration settings for the orchestration aspects of the swarm.\n    *   **TaskHistoryRetentionLimit** – Maximum number of tasks history stored.\n*   **Raft** – Raft related configuration.\n    *   **SnapshotInterval** – Number of logs entries between snapshot.\n    *   **KeepOldSnapshots** – Number of snapshots to keep beyond the current snapshot.\n    *   **LogEntriesForSlowFollowers** – Number of log entries to keep around to sync up slow followers after a snapshot is created.\n    *   **HeartbeatTick** – Amount of ticks (in seconds) between each heartbeat.\n    *   **ElectionTick** – Amount of ticks (in seconds) needed without a leader to trigger a new election.\n*   **Dispatcher** – Configuration settings for the task dispatcher.\n    *   **HeartbeatPeriod** – The delay for an agent to send a heartbeat to the dispatcher.\n*   **CAConfig** – CA configuration.\n    *   **NodeCertExpiry** – Automatic expiry for nodes certificates.\n    *   **ExternalCA** - Configuration for forwarding signing requests to an external certificate authority.\n        *   **Protocol** - Protocol for communication with the external CA (currently only \"cfssl\" is supported).\n        *   **URL** - URL where certificate signing requests should be sent.\n        *   **Options** - An object with key/value pairs that are interpreted as protocol-specific options for the external CA driver.\n*   **JoinTokens** - Tokens that can be used by other nodes to join the swarm.\n    *   **Worker** - Token to use for joining as a worker.\n    *   **Manager** - Token to use for joining as a manager.\n\n### [3.9 Services](#39-services)\n\n**Note**: Service operations require to first be part of a swarm.\n\n#### [List services](#list-services)\n\n`GET /services`\n\nList services\n\n**Example request**:\n\n```\nGET /v1.24/services HTTP/1.1\n```\n\n**Example response**:\n\n```\nHTTP/1.1 200 OK\nContent-Type: application/json\n\n[\n  {\n    \"ID\": \"9mnpnzenvg8p8tdbtq4wvbkcz\",\n    \"Version\": {\n      \"Index\": 19\n    },\n    \"CreatedAt\": \"2016-06-07T21:05:51.880065305Z\",\n    \"UpdatedAt\": \"2016-06-07T21:07:29.962229872Z\",\n    \"Spec\": {\n      \"Name\": \"hopeful_cori\",\n      \"TaskTemplate\": {\n        \"ContainerSpec\": {\n          \"Image\": \"redis\"\n        },\n        \"Resources\": {\n          \"Limits\": {},\n          \"Reservations\": {}\n        },\n        \"RestartPolicy\": {\n          \"Condition\": \"any\",\n          \"MaxAttempts\": 0\n        },\n        \"Placement\": {\n          \"Constraints\": [\n            \"node.role == worker\"\n          ]\n        }\n      },\n      \"Mode\": {\n        \"Replicated\": {\n          \"Replicas\": 1\n        }\n      },\n      \"UpdateConfig\": {\n        \"Parallelism\": 1,\n        \"FailureAction\": \"pause\"\n      },\n      \"EndpointSpec\": {\n        \"Mode\": \"vip\",\n        \"Ports\": [\n          {\n            \"Protocol\": \"tcp\",\n            \"TargetPort\": 6379,\n            \"PublishedPort\": 30001\n          }\n        ]\n      }\n    },\n    \"Endpoint\": {\n      \"Spec\": {\n        \"Mode\": \"vip\",\n        \"Ports\": [\n          {\n            \"Protocol\": \"tcp\",\n            \"TargetPort\": 6379,\n            \"PublishedPort\": 30001\n          }\n        ]\n      },\n      \"Ports\": [\n        {\n          \"Protocol\": \"tcp\",\n          \"TargetPort\": 6379,\n          \"PublishedPort\": 30001\n        }\n      ],\n      \"VirtualIPs\": [\n        {\n          \"NetworkID\": \"4qvuz4ko70xaltuqbt8956gd1\",\n          \"Addr\": \"10.255.0.2/16\"\n        },\n        {\n          \"NetworkID\": \"4qvuz4ko70xaltuqbt8956gd1\",\n          \"Addr\": \"10.255.0.3/16\"\n        }\n      ]\n    }\n  }\n]\n```\n\n**Query parameters**:\n\n*   **filters** – a JSON encoded value of the filters (a `map[string][]string`) to process on the services list. Available filters:\n    *   `id=<service id>`\n    *   `label=<service label>`\n    *   `name=<service name>`\n\n**Status codes**:\n\n*   **200** – no error\n*   **406** – node is not part of a swarm\n*   **500** – server error\n\n#### [Create a service](#create-a-service)\n\n`POST /services/create`\n\nCreate a service. When using this endpoint to create a service using a private repository from the registry, the `X-Registry-Auth` header must be used to include a base64-encoded AuthConfig object. Refer to the [create an image](#create-an-image) section for more details.\n\n**Example request**:\n\n```\nPOST /v1.24/services/create HTTP/1.1\nContent-Type: application/json\nContent-Length: 12345\n\n{\n  \"Name\": \"web\",\n  \"TaskTemplate\": {\n    \"ContainerSpec\": {\n      \"Image\": \"nginx:alpine\",\n      \"Mounts\": [\n        {\n          \"ReadOnly\": true,\n          \"Source\": \"web-data\",\n          \"Target\": \"/usr/share/nginx/html\",\n          \"Type\": \"volume\",\n          \"VolumeOptions\": {\n            \"DriverConfig\": {\n            },\n            \"Labels\": {\n              \"com.example.something\": \"something-value\"\n            }\n          }\n        }\n      ],\n      \"User\": \"33\"\n    },\n    \"Networks\": [\n        {\n          \"Target\": \"overlay1\"\n        }\n    ],\n    \"LogDriver\": {\n      \"Name\": \"json-file\",\n      \"Options\": {\n        \"max-file\": \"3\",\n        \"max-size\": \"10M\"\n      }\n    },\n    \"Placement\": {\n      \"Constraints\": [\n        \"node.role == worker\"\n      ]\n    },\n    \"Resources\": {\n      \"Limits\": {\n        \"MemoryBytes\": 104857600\n      },\n      \"Reservations\": {\n      }\n    },\n    \"RestartPolicy\": {\n      \"Condition\": \"on-failure\",\n      \"Delay\": 10000000000,\n      \"MaxAttempts\": 10\n    }\n  },\n  \"Mode\": {\n    \"Replicated\": {\n      \"Replicas\": 4\n    }\n  },\n  \"UpdateConfig\": {\n    \"Delay\": 30000000000,\n    \"Parallelism\": 2,\n    \"FailureAction\": \"pause\"\n  },\n  \"EndpointSpec\": {\n    \"Ports\": [\n      {\n        \"Protocol\": \"tcp\",\n        \"PublishedPort\": 8080,\n        \"TargetPort\": 80\n      }\n    ]\n  },\n  \"Labels\": {\n    \"foo\": \"bar\"\n  }\n}\n```\n\n**Example response**:\n\n```\nHTTP/1.1 201 Created\nContent-Type: application/json\n\n{\n  \"ID\":\"ak7w3gjqoa3kuz8xcpnyy0pvl\"\n}\n```\n\n**Status codes**:\n\n*   **201** – no error\n*   **403** - network is not eligible for services\n*   **406** – node is not part of a swarm\n*   **409** – name conflicts with an existing object\n*   **500** - server error\n\n**JSON Parameters**:\n\n*   **Name** – User-defined name for the service.\n*   **Labels** – A map of labels to associate with the service (e.g., `{\"key\":\"value\", \"key2\":\"value2\"}`).\n*   **TaskTemplate** – Specification of the tasks to start as part of the new service.\n    *   **ContainerSpec** - Container settings for containers started as part of this task.\n        *   **Image** – A string specifying the image name to use for the container.\n        *   **Command** – The command to be run in the image.\n        *   **Args** – Arguments to the command.\n        *   **Env** – A list of environment variables in the form of `[\"VAR=value\"[,\"VAR2=value2\"]]`.\n        *   **Dir** – A string specifying the working directory for commands to run in.\n        *   **User** – A string value specifying the user inside the container.\n        *   **Labels** – A map of labels to associate with the service (e.g., `{\"key\":\"value\", \"key2\":\"value2\"}`).\n        *   **Mounts** – Specification for mounts to be added to containers created as part of the service.\n            *   **Target** – Container path.\n            *   **Source** – Mount source (e.g. a volume name, a host path).\n            *   **Type** – The mount type (`bind`, or `volume`).\n            *   **ReadOnly** – A boolean indicating whether the mount should be read-only.\n            *   **BindOptions** - Optional configuration for the `bind` type.\n                *   **Propagation** – A propagation mode with the value `[r]private`, `[r]shared`, or `[r]slave`.\n            *   **VolumeOptions** – Optional configuration for the `volume` type.\n                *   **NoCopy** – A boolean indicating if volume should be populated with the data from the target. (Default false)\n                *   **Labels** – User-defined name and labels for the volume.\n                *   **DriverConfig** – Map of driver-specific options.\n                    *   **Name** - Name of the driver to use to create the volume.\n                    *   **Options** - key/value map of driver specific options.\n        *   **StopGracePeriod** – Amount of time to wait for the container to terminate before forcefully killing it.\n    *   **LogDriver** - Log configuration for containers created as part of the service.\n        *   **Name** - Name of the logging driver to use (`json-file`, `syslog`, `journald`, `gelf`, `fluentd`, `awslogs`, `splunk`, `etwlogs`, `none`).\n        *   **Options** - Driver-specific options.\n    *   **Resources** – Resource requirements which apply to each individual container created as part of the service.\n        *   **Limits** – Define resources limits.\n            *   **NanoCPUs** – CPU limit in units of 10\\-9 CPU shares.\n            *   **MemoryBytes** – Memory limit in Bytes.\n        *   **Reservation** – Define resources reservation.\n            *   **NanoCPUs** – CPU reservation in units of 10\\-9 CPU shares.\n            *   **MemoryBytes** – Memory reservation in Bytes.\n    *   **RestartPolicy** – Specification for the restart policy which applies to containers created as part of this service.\n        *   **Condition** – Condition for restart (`none`, `on-failure`, or `any`).\n        *   **Delay** – Delay between restart attempts.\n        *   **MaxAttempts** – Maximum attempts to restart a given container before giving up (default value is 0, which is ignored).\n        *   **Window** – Windows is the time window used to evaluate the restart policy (default value is 0, which is unbounded).\n    *   **Placement** – Restrictions on where a service can run.\n        *   **Constraints** – An array of constraints, e.g. `[ \"node.role == manager\" ]`.\n*   **Mode** – Scheduling mode for the service (`replicated` or `global`, defaults to `replicated`).\n*   **UpdateConfig** – Specification for the update strategy of the service.\n    *   **Parallelism** – Maximum number of tasks to be updated in one iteration (0 means unlimited parallelism).\n    *   **Delay** – Amount of time between updates.\n    *   **FailureAction** - Action to take if an updated task fails to run, or stops running during the update. Values are `continue` and `pause`.\n*   **Networks** – Array of network names or IDs to attach the service to.\n*   **EndpointSpec** – Properties that can be configured to access and load balance a service.\n    *   **Mode** – The mode of resolution to use for internal load balancing between tasks (`vip` or `dnsrr`). Defaults to `vip` if not provided.\n    *   **Ports** – List of exposed ports that this service is accessible on from the outside, in the form of: `{\"Protocol\": <\"tcp\"|\"udp\">, \"PublishedPort\": <port>, \"TargetPort\": <port>}`. Ports can only be provided if `vip` resolution mode is used.\n\n**Request Headers**:\n\n*   **Content-type** – Set to `\"application/json\"`.\n*   **X-Registry-Auth** – base64-encoded AuthConfig object, containing either login information, or a token. Refer to the [create an image](#create-an-image) section for more details.\n\n#### [Remove a service](#remove-a-service)\n\n`DELETE /services/(id or name)`\n\nStop and remove the service `id`\n\n**Example request**:\n\n```\nDELETE /v1.24/services/16253994b7c4 HTTP/1.1\n```\n\n**Example response**:\n\n```\nHTTP/1.1 200 OK\nContent-Length: 0\nContent-Type: text/plain; charset=utf-8\n```\n\n**Status codes**:\n\n*   **200** – no error\n*   **404** – no such service\n*   **406** - node is not part of a swarm\n*   **500** – server error\n\n#### [Inspect one or more services](#inspect-one-or-more-services)\n\n`GET /services/(id or name)`\n\nReturn information on the service `id`.\n\n**Example request**:\n\n```\nGET /v1.24/services/1cb4dnqcyx6m66g2t538x3rxha HTTP/1.1\n```\n\n**Example response**:\n\n```\n{\n  \"ID\": \"ak7w3gjqoa3kuz8xcpnyy0pvl\",\n  \"Version\": {\n    \"Index\": 95\n  },\n  \"CreatedAt\": \"2016-06-07T21:10:20.269723157Z\",\n  \"UpdatedAt\": \"2016-06-07T21:10:20.276301259Z\",\n  \"Spec\": {\n    \"Name\": \"redis\",\n    \"TaskTemplate\": {\n      \"ContainerSpec\": {\n        \"Image\": \"redis\"\n      },\n      \"Resources\": {\n        \"Limits\": {},\n        \"Reservations\": {}\n      },\n      \"RestartPolicy\": {\n        \"Condition\": \"any\",\n        \"MaxAttempts\": 0\n      },\n      \"Placement\": {}\n    },\n    \"Mode\": {\n      \"Replicated\": {\n        \"Replicas\": 1\n      }\n    },\n    \"UpdateConfig\": {\n      \"Parallelism\": 1,\n      \"FailureAction\": \"pause\"\n    },\n    \"EndpointSpec\": {\n      \"Mode\": \"vip\",\n      \"Ports\": [\n        {\n          \"Protocol\": \"tcp\",\n          \"TargetPort\": 6379,\n          \"PublishedPort\": 30001\n        }\n      ]\n    }\n  },\n  \"Endpoint\": {\n    \"Spec\": {\n      \"Mode\": \"vip\",\n      \"Ports\": [\n        {\n          \"Protocol\": \"tcp\",\n          \"TargetPort\": 6379,\n          \"PublishedPort\": 30001\n        }\n      ]\n    },\n    \"Ports\": [\n      {\n        \"Protocol\": \"tcp\",\n        \"TargetPort\": 6379,\n        \"PublishedPort\": 30001\n      }\n    ],\n    \"VirtualIPs\": [\n      {\n        \"NetworkID\": \"4qvuz4ko70xaltuqbt8956gd1\",\n        \"Addr\": \"10.255.0.4/16\"\n      }\n    ]\n  }\n}\n```\n\n**Status codes**:\n\n*   **200** – no error\n*   **404** – no such service\n*   **406** - node is not part of a swarm\n*   **500** – server error\n\n#### [Update a service](#update-a-service)\n\n`POST /services/(id)/update`\n\nUpdate a service. When using this endpoint to create a service using a private repository from the registry, the `X-Registry-Auth` header can be used to update the authentication information for that is stored for the service. The header contains a base64-encoded AuthConfig object. Refer to the [create an image](#create-an-image) section for more details.\n\n**Example request**:\n\n```\nPOST /v1.24/services/1cb4dnqcyx6m66g2t538x3rxha/update?version=23 HTTP/1.1\nContent-Type: application/json\nContent-Length: 12345\n\n{\n  \"Name\": \"top\",\n  \"TaskTemplate\": {\n    \"ContainerSpec\": {\n      \"Image\": \"busybox\",\n      \"Args\": [\n        \"top\"\n      ]\n    },\n    \"Resources\": {\n      \"Limits\": {},\n      \"Reservations\": {}\n    },\n    \"RestartPolicy\": {\n      \"Condition\": \"any\",\n      \"MaxAttempts\": 0\n    },\n    \"Placement\": {}\n  },\n  \"Mode\": {\n    \"Replicated\": {\n      \"Replicas\": 1\n    }\n  },\n  \"UpdateConfig\": {\n    \"Parallelism\": 1\n  },\n  \"EndpointSpec\": {\n    \"Mode\": \"vip\"\n  }\n}\n```\n\n**Example response**:\n\n```\nHTTP/1.1 200 OK\nContent-Length: 0\nContent-Type: text/plain; charset=utf-8\n```\n\n**JSON Parameters**:\n\n*   **Name** – User-defined name for the service. Note that renaming services is not supported.\n*   **Labels** – A map of labels to associate with the service (e.g., `{\"key\":\"value\", \"key2\":\"value2\"}`).\n*   **TaskTemplate** – Specification of the tasks to start as part of the new service.\n    *   **ContainerSpec** - Container settings for containers started as part of this task.\n        *   **Image** – A string specifying the image name to use for the container.\n        *   **Command** – The command to be run in the image.\n        *   **Args** – Arguments to the command.\n        *   **Env** – A list of environment variables in the form of `[\"VAR=value\"[,\"VAR2=value2\"]]`.\n        *   **Dir** – A string specifying the working directory for commands to run in.\n        *   **User** – A string value specifying the user inside the container.\n        *   **Labels** – A map of labels to associate with the service (e.g., `{\"key\":\"value\", \"key2\":\"value2\"}`).\n        *   **Mounts** – Specification for mounts to be added to containers created as part of the new service.\n            *   **Target** – Container path.\n            *   **Source** – Mount source (e.g. a volume name, a host path).\n            *   **Type** – The mount type (`bind`, or `volume`).\n            *   **ReadOnly** – A boolean indicating whether the mount should be read-only.\n            *   **BindOptions** - Optional configuration for the `bind` type\n                *   **Propagation** – A propagation mode with the value `[r]private`, `[r]shared`, or `[r]slave`.\n            *   **VolumeOptions** – Optional configuration for the `volume` type.\n                *   **NoCopy** – A boolean indicating if volume should be populated with the data from the target. (Default false)\n                *   **Labels** – User-defined name and labels for the volume.\n                *   **DriverConfig** – Map of driver-specific options.\n                    *   **Name** - Name of the driver to use to create the volume\n                    *   **Options** - key/value map of driver specific options\n        *   **StopGracePeriod** – Amount of time to wait for the container to terminate before forcefully killing it.\n    *   **Resources** – Resource requirements which apply to each individual container created as part of the service.\n        *   **Limits** – Define resources limits.\n            *   **CPU** – CPU limit\n            *   **Memory** – Memory limit\n        *   **Reservation** – Define resources reservation.\n            *   **CPU** – CPU reservation\n            *   **Memory** – Memory reservation\n    *   **RestartPolicy** – Specification for the restart policy which applies to containers created as part of this service.\n        *   **Condition** – Condition for restart (`none`, `on-failure`, or `any`).\n        *   **Delay** – Delay between restart attempts.\n        *   **MaxAttempts** – Maximum attempts to restart a given container before giving up (default value is 0, which is ignored).\n        *   **Window** – Windows is the time window used to evaluate the restart policy (default value is 0, which is unbounded).\n    *   **Placement** – Restrictions on where a service can run.\n        *   **Constraints** – An array of constraints, e.g. `[ \"node.role == manager\" ]`.\n*   **Mode** – Scheduling mode for the service (`replicated` or `global`, defaults to `replicated`).\n*   **UpdateConfig** – Specification for the update strategy of the service.\n    *   **Parallelism** – Maximum number of tasks to be updated in one iteration (0 means unlimited parallelism).\n    *   **Delay** – Amount of time between updates.\n*   **Networks** – Array of network names or IDs to attach the service to.\n*   **EndpointSpec** – Properties that can be configured to access and load balance a service.\n    *   **Mode** – The mode of resolution to use for internal load balancing between tasks (`vip` or `dnsrr`). Defaults to `vip` if not provided.\n    *   **Ports** – List of exposed ports that this service is accessible on from the outside, in the form of: `{\"Protocol\": <\"tcp\"|\"udp\">, \"PublishedPort\": <port>, \"TargetPort\": <port>}`. Ports can only be provided if `vip` resolution mode is used.\n\n**Query parameters**:\n\n*   **version** – The version number of the service object being updated. This is required to avoid conflicting writes.\n\n**Request Headers**:\n\n*   **Content-type** – Set to `\"application/json\"`.\n*   **X-Registry-Auth** – base64-encoded AuthConfig object, containing either login information, or a token. Refer to the [create an image](#create-an-image) section for more details.\n\n**Status codes**:\n\n*   **200** – no error\n*   **404** – no such service\n*   **406** - node is not part of a swarm\n*   **500** – server error\n\n### [3.10 Tasks](#310-tasks)\n\n**Note**: Task operations require the engine to be part of a swarm.\n\n#### [List tasks](#list-tasks)\n\n`GET /tasks`\n\nList tasks\n\n**Example request**:\n\n```\nGET /v1.24/tasks HTTP/1.1\n```\n\n**Example response**:\n\n```\n[\n  {\n    \"ID\": \"0kzzo1i0y4jz6027t0k7aezc7\",\n    \"Version\": {\n      \"Index\": 71\n    },\n    \"CreatedAt\": \"2016-06-07T21:07:31.171892745Z\",\n    \"UpdatedAt\": \"2016-06-07T21:07:31.376370513Z\",\n    \"Spec\": {\n      \"ContainerSpec\": {\n        \"Image\": \"redis\"\n      },\n      \"Resources\": {\n        \"Limits\": {},\n        \"Reservations\": {}\n      },\n      \"RestartPolicy\": {\n        \"Condition\": \"any\",\n        \"MaxAttempts\": 0\n      },\n      \"Placement\": {}\n    },\n    \"ServiceID\": \"9mnpnzenvg8p8tdbtq4wvbkcz\",\n    \"Slot\": 1,\n    \"NodeID\": \"60gvrl6tm78dmak4yl7srz94v\",\n    \"Status\": {\n      \"Timestamp\": \"2016-06-07T21:07:31.290032978Z\",\n      \"State\": \"running\",\n      \"Message\": \"started\",\n      \"ContainerStatus\": {\n        \"ContainerID\": \"e5d62702a1b48d01c3e02ca1e0212a250801fa8d67caca0b6f35919ebc12f035\",\n        \"PID\": 677\n      }\n    },\n    \"DesiredState\": \"running\",\n    \"NetworksAttachments\": [\n      {\n        \"Network\": {\n          \"ID\": \"4qvuz4ko70xaltuqbt8956gd1\",\n          \"Version\": {\n            \"Index\": 18\n          },\n          \"CreatedAt\": \"2016-06-07T20:31:11.912919752Z\",\n          \"UpdatedAt\": \"2016-06-07T21:07:29.955277358Z\",\n          \"Spec\": {\n            \"Name\": \"ingress\",\n            \"Labels\": {\n              \"com.docker.swarm.internal\": \"true\"\n            },\n            \"DriverConfiguration\": {},\n            \"IPAMOptions\": {\n              \"Driver\": {},\n              \"Configs\": [\n                {\n                  \"Subnet\": \"10.255.0.0/16\",\n                  \"Gateway\": \"10.255.0.1\"\n                }\n              ]\n            }\n          },\n          \"DriverState\": {\n            \"Name\": \"overlay\",\n            \"Options\": {\n              \"com.docker.network.driver.overlay.vxlanid_list\": \"256\"\n            }\n          },\n          \"IPAMOptions\": {\n            \"Driver\": {\n              \"Name\": \"default\"\n            },\n            \"Configs\": [\n              {\n                \"Subnet\": \"10.255.0.0/16\",\n                \"Gateway\": \"10.255.0.1\"\n              }\n            ]\n          }\n        },\n        \"Addresses\": [\n          \"10.255.0.10/16\"\n        ]\n      }\n    ],\n  },\n  {\n    \"ID\": \"1yljwbmlr8er2waf8orvqpwms\",\n    \"Version\": {\n      \"Index\": 30\n    },\n    \"CreatedAt\": \"2016-06-07T21:07:30.019104782Z\",\n    \"UpdatedAt\": \"2016-06-07T21:07:30.231958098Z\",\n    \"Name\": \"hopeful_cori\",\n    \"Spec\": {\n      \"ContainerSpec\": {\n        \"Image\": \"redis\"\n      },\n      \"Resources\": {\n        \"Limits\": {},\n        \"Reservations\": {}\n      },\n      \"RestartPolicy\": {\n        \"Condition\": \"any\",\n        \"MaxAttempts\": 0\n      },\n      \"Placement\": {}\n    },\n    \"ServiceID\": \"9mnpnzenvg8p8tdbtq4wvbkcz\",\n    \"Slot\": 1,\n    \"NodeID\": \"60gvrl6tm78dmak4yl7srz94v\",\n    \"Status\": {\n      \"Timestamp\": \"2016-06-07T21:07:30.202183143Z\",\n      \"State\": \"shutdown\",\n      \"Message\": \"shutdown\",\n      \"ContainerStatus\": {\n        \"ContainerID\": \"1cf8d63d18e79668b0004a4be4c6ee58cddfad2dae29506d8781581d0688a213\"\n      }\n    },\n    \"DesiredState\": \"shutdown\",\n    \"NetworksAttachments\": [\n      {\n        \"Network\": {\n          \"ID\": \"4qvuz4ko70xaltuqbt8956gd1\",\n          \"Version\": {\n            \"Index\": 18\n          },\n          \"CreatedAt\": \"2016-06-07T20:31:11.912919752Z\",\n          \"UpdatedAt\": \"2016-06-07T21:07:29.955277358Z\",\n          \"Spec\": {\n            \"Name\": \"ingress\",\n            \"Labels\": {\n              \"com.docker.swarm.internal\": \"true\"\n            },\n            \"DriverConfiguration\": {},\n            \"IPAMOptions\": {\n              \"Driver\": {},\n              \"Configs\": [\n                {\n                  \"Subnet\": \"10.255.0.0/16\",\n                  \"Gateway\": \"10.255.0.1\"\n                }\n              ]\n            }\n          },\n          \"DriverState\": {\n            \"Name\": \"overlay\",\n            \"Options\": {\n              \"com.docker.network.driver.overlay.vxlanid_list\": \"256\"\n            }\n          },\n          \"IPAMOptions\": {\n            \"Driver\": {\n              \"Name\": \"default\"\n            },\n            \"Configs\": [\n              {\n                \"Subnet\": \"10.255.0.0/16\",\n                \"Gateway\": \"10.255.0.1\"\n              }\n            ]\n          }\n        },\n        \"Addresses\": [\n          \"10.255.0.5/16\"\n        ]\n      }\n    ]\n  }\n]\n```\n\n**Query parameters**:\n\n*   **filters** – a JSON encoded value of the filters (a `map[string][]string`) to process on the services list. Available filters:\n    *   `id=<task id>`\n    *   `name=<task name>`\n    *   `service=<service name>`\n    *   `node=<node id or name>`\n    *   `label=key` or `label=\"key=value\"`\n    *   `desired-state=(running | shutdown | accepted)`\n\n**Status codes**:\n\n*   **200** – no error\n*   **406** - node is not part of a swarm\n*   **500** – server error\n\n#### [Inspect a task](#inspect-a-task)\n\n`GET /tasks/(id)`\n\nGet details on the task `id`\n\n**Example request**:\n\n```\nGET /v1.24/tasks/0kzzo1i0y4jz6027t0k7aezc7 HTTP/1.1\n```\n\n**Example response**:\n\n```\n{\n  \"ID\": \"0kzzo1i0y4jz6027t0k7aezc7\",\n  \"Version\": {\n    \"Index\": 71\n  },\n  \"CreatedAt\": \"2016-06-07T21:07:31.171892745Z\",\n  \"UpdatedAt\": \"2016-06-07T21:07:31.376370513Z\",\n  \"Spec\": {\n    \"ContainerSpec\": {\n      \"Image\": \"redis\"\n    },\n    \"Resources\": {\n      \"Limits\": {},\n      \"Reservations\": {}\n    },\n    \"RestartPolicy\": {\n      \"Condition\": \"any\",\n      \"MaxAttempts\": 0\n    },\n    \"Placement\": {}\n  },\n  \"ServiceID\": \"9mnpnzenvg8p8tdbtq4wvbkcz\",\n  \"Slot\": 1,\n  \"NodeID\": \"60gvrl6tm78dmak4yl7srz94v\",\n  \"Status\": {\n    \"Timestamp\": \"2016-06-07T21:07:31.290032978Z\",\n    \"State\": \"running\",\n    \"Message\": \"started\",\n    \"ContainerStatus\": {\n      \"ContainerID\": \"e5d62702a1b48d01c3e02ca1e0212a250801fa8d67caca0b6f35919ebc12f035\",\n      \"PID\": 677\n    }\n  },\n  \"DesiredState\": \"running\",\n  \"NetworksAttachments\": [\n    {\n      \"Network\": {\n        \"ID\": \"4qvuz4ko70xaltuqbt8956gd1\",\n        \"Version\": {\n          \"Index\": 18\n        },\n        \"CreatedAt\": \"2016-06-07T20:31:11.912919752Z\",\n        \"UpdatedAt\": \"2016-06-07T21:07:29.955277358Z\",\n        \"Spec\": {\n          \"Name\": \"ingress\",\n          \"Labels\": {\n            \"com.docker.swarm.internal\": \"true\"\n          },\n          \"DriverConfiguration\": {},\n          \"IPAMOptions\": {\n            \"Driver\": {},\n            \"Configs\": [\n              {\n                \"Subnet\": \"10.255.0.0/16\",\n                \"Gateway\": \"10.255.0.1\"\n              }\n            ]\n          }\n        },\n        \"DriverState\": {\n          \"Name\": \"overlay\",\n          \"Options\": {\n            \"com.docker.network.driver.overlay.vxlanid_list\": \"256\"\n          }\n        },\n        \"IPAMOptions\": {\n          \"Driver\": {\n            \"Name\": \"default\"\n          },\n          \"Configs\": [\n            {\n              \"Subnet\": \"10.255.0.0/16\",\n              \"Gateway\": \"10.255.0.1\"\n            }\n          ]\n        }\n      },\n      \"Addresses\": [\n        \"10.255.0.10/16\"\n      ]\n    }\n  ]\n}\n```\n\n**Status codes**:\n\n*   **200** – no error\n*   **404** – unknown task\n*   **406** - node is not part of a swarm\n*   **500** – server error\n\n### [4.1 Inside `docker run`](#41-inside-docker-run)\n\nAs an example, the `docker run` command line makes the following API calls:\n\n*   Create the container\n    \n*   If the status code is 404, it means the image doesn't exist:\n    \n    *   Try to pull it.\n    *   Then, retry to create the container.\n*   Start the container.\n    \n*   If you are not in detached mode:\n    \n*   Attach to the container, using `logs=1` (to have `stdout` and `stderr` from the container's start) and `stream=1`\n    \n*   If in detached mode or only `stdin` is attached, display the container's id.\n    \n\n### [4.2 Hijacking](#42-hijacking)\n\nIn this version of the API, `/attach`, uses hijacking to transport `stdin`, `stdout`, and `stderr` on the same socket.\n\nTo hint potential proxies about connection hijacking, Docker client sends connection upgrade headers similarly to websocket.\n\n```\nUpgrade: tcp\nConnection: Upgrade\n```\n\nWhen Docker daemon detects the `Upgrade` header, it switches its status code from **200 OK** to **101 UPGRADED** and resends the same headers.\n\n### [4.3 CORS Requests](#43-cors-requests)\n\nTo set cross origin requests to the Engine API please give values to `--api-cors-header` when running Docker in daemon mode. Set \\* (asterisk) allows all, default or blank means CORS disabled\n\n```\n$ dockerd -H=\"192.168.1.9:2375\" --api-cors-header=\"http://foo.bar\"\n```",
  "title": "Engine API v1.24 | Docker Docs\n",
  "description": "API Documentation for Docker",
  "languageCode": "en"
},
{
  "url": "https://docs.docker.com/build/guide/multi-stage/",
  "markdown": "# Multi-stage | Docker Docs\n\nThis section explores multi-stage builds. There are two main reasons for why you’d want to use multi-stage builds:\n\n*   They allow you to run build steps in parallel, making your build pipeline faster and more efficient.\n*   They allow you to create a final image with a smaller footprint, containing only what's needed to run your program.\n\nIn a Dockerfile, a build stage is represented by a `FROM` instruction. The Dockerfile from the previous section doesn’t leverage multi-stage builds. It’s all one build stage. That means that the final image is bloated with resources used to compile the program.\n\nThe program compiles to executable binaries, so you don’t need Go language utilities to exist in the final image.\n\nUsing multi-stage builds, you can choose to use different base images for your build and runtime environments. You can copy build artifacts from the build stage over to the runtime stage.\n\nModify the Dockerfile as follows. This change creates another stage using a minimal `scratch` image as a base. In the final `scratch` stage, the binaries built in the previous stage are copied over to the filesystem of the new stage.\n\nNow if you build the image and inspect it, you should see a significantly smaller number:\n\nThe image went from 150MB to only just 8.45MB in size. That’s because the resulting image only contains the binaries, and nothing else.\n\nYou've reduced the footprint of the image. The following step shows how you can improve build speed with multi-stage builds, using parallelism. The build currently produces the binaries one after the other. There is no reason why you need to build the client before building the server, or vice versa.\n\nYou can split the binary-building steps into separate stages. In the final `scratch` stage, copy the binaries from each corresponding build stage. By segmenting these builds into separate stages, Docker can run them in parallel.\n\nThe stages for building each binary both require the Go compilation tools and application dependencies. Define these common steps as a reusable base stage. You can do that by assigning a name to the stage using the pattern `FROM image AS stage_name`. This allows you to reference the stage name in a `FROM` instruction of another stage (`FROM stage_name`).\n\nYou can also assign a name to the binary-building stages, and reference the stage name in the `COPY --from=stage_name` instruction when copying the binaries to the final `scratch` image.\n\nNow, instead of first building the binaries one after the other, the `build-client` and `build-server` stages are executed simultaneously.\n\n![Stages executing in parallel](https://docs.docker.com/build/guide/images/parallelism.gif)\n\nThe final image is now small, and you’re building it efficiently using parallelism. But this image is slightly strange, in that it contains both the client and the server binary in the same image. Shouldn’t these be two different images?\n\nIt’s possible to create multiple different images using a single Dockerfile. You can specify a target stage of a build using the `--target` flag. Replace the unnamed `FROM scratch` stage with two separate stages named `client` and `server`.\n\nAnd now you can build the client and server programs as separate Docker images (tags):\n\nThe images are now even smaller, about 4 MB each.\n\nThis change also avoids having to build both binaries each time. When selecting to build the `client` target, Docker only builds the stages leading up to that target. The `build-server` and `server` stages are skipped if they’re not needed. Likewise, building the `server` target skips the `build-client` and `client` stages.\n\nMulti-stage builds are useful for creating images with less bloat and a smaller footprint, and also helps to make builds run faster.\n\nRelated information:\n\n*   [Multi-stage builds](https://docs.docker.com/build/building/multi-stage/)\n*   [Base images](https://docs.docker.com/build/building/base-images/)\n\nThe next section describes how you can use file mounts to further improve build speeds.",
  "title": "Multi-stage | Docker Docs\n",
  "description": "Faster and smaller builds with multi-stage builds",
  "languageCode": "en"
},
{
  "url": "https://docs.docker.com/build/guide/export/",
  "markdown": "# Export binaries | Docker Docs\n\nDid you know that you can use Docker to build your application to standalone binaries? Sometimes, you don’t want to package and distribute your application as a Docker image. Use Docker to build your application, and use exporters to save the output to disk.\n\nThe default output format for `docker build` is a container image. That image is automatically loaded to your local image store, where you can run a container from that image, or push it to a registry. Under the hood, this uses the default exporter, called the `docker` exporter.\n\nTo export your build results as files instead, you can use the `local` exporter. The `local` exporter saves the filesystem of the build container to the specified directory on the host machine.\n\nTo use the `local` exporter, pass the `--output` option to the `docker build` command. The `--output` flag takes one argument: the destination on the host machine where you want to save the files.\n\nThe following commands exports the files from of the `server` target to the current working directory on the host filesystem:\n\nRunning this command creates a binary at `./bin/server`. It’s created under the `bin/` directory because that’s where the file was located inside the build container.\n\nIf you want to create a build that exports both binaries, you can create another build stage in the Dockerfile that copies both of the binaries from each build stage:\n\nNow you can build the `binaries` target using the `--output` option to export both the client and server binaries.\n\nThis section has demonstrated how you can use Docker to build and export standalone binaries. These binaries can be distributed freely, and don’t require a container runtime like the Docker daemon.\n\nThe binaries you've generated so far are Linux binaries. That's because the build environment is Linux. If your host OS is Linux, you can run these files. Building binaries that work on Mac or Windows machines requires cross-compilation. This is explored later on in this guide.\n\nRelated information:\n\n*   [`docker build --output` CLI reference](https://docs.docker.com/reference/cli/docker/image/build/#output)\n*   [Build exporters](https://docs.docker.com/build/exporters/)\n\nThe next topic of this guide is testing: how you can use Docker to run application tests.",
  "title": "Export binaries | Docker Docs\n",
  "description": "Using Docker builds to create and export executable binaries",
  "languageCode": "en"
},
{
  "url": "https://docs.docker.com/build/guide/multi-platform/",
  "markdown": "# Multi-platform | Docker Docs\n\nUp until this point in the guide, you've built Linux binaries. This section describes how you can support other operating systems, and architectures, using multi-platform builds via emulation and cross-compilation.\n\nThe easiest way to get started with building for multiple platforms is using emulation. With emulation, you can build your app to multiple architectures without having to make any changes to your Dockerfile. All you need to do is to pass the `--platform` flag to the build command, specifying the OS and architecture you want to build for.\n\nThe following command builds the server image for the `linux/arm/v7` platform:\n\nYou can also use emulation to produce outputs for multiple platforms at once. However, the default image store in Docker Engine doesn't support building and loading multi-platform images. You need to enable the containerd image store which supports concurrent multi-platform builds.\n\n* * *\n\nTo enable the containerd image store in Docker Desktop, go to **Settings** and select **Use containerd for pulling and storing images** in the **General** tab.\n\nNote that changing the image store means you'll temporarily lose access to images and containers in the classic image store. Those resources still exist, but to view them, you'll need to disable the containerd image store.\n\nIf you're not using Docker Desktop, enable the containerd image store by adding the following feature configuration to your `/etc/docker/daemon.json` configuration file.\n\nRestart the daemon after updating the configuration file.\n\n* * *\n\nTo run multi-platform builds, invoke the `docker build` command, and pass it the same arguments as you did before. Only this time, also add a `--platform` flag specifying multiple architectures.\n\nThis command uses emulation to run the same build three times, once for each platform. The build results are exported to a `bin` directory.\n\nWhen you build for multiple platforms concurrently, BuildKit runs all of the build steps under emulation for each platform that you specify. Effectively forking the build into multiple concurrent processes.\n\n![Build pipelines using emulation](https://docs.docker.com/build/guide/images/emulation.png)\n\nThere are, however, a few downsides to running multi-platform builds using emulation:\n\n*   If you tried running the command above, you may have noticed that it took a long time to finish. Emulation can be much slower than native execution for CPU-intensive tasks.\n*   Emulation only works when the architecture is supported by the base image you’re using. The example in this guide uses the Alpine Linux version of the `golang` image, which means you can only build Linux images this way, for a limited set of CPU architectures, without having to change the base image.\n\nAs an alternative to emulation, the next step explores cross-compilation. Cross-compiling makes multi-platform builds much faster and versatile.\n\nUsing cross-compilation means leveraging the capabilities of a compiler to build for multiple platforms, without the need for emulation.\n\nThe first thing you'll need to do is pinning the builder to use the node’s native architecture as the build platform. This is to prevent emulation. Then, from the node's native architecture, the builder cross-compiles the application to a number of other target platforms.\n\n### [Platform build arguments](#platform-build-arguments)\n\nThis approach involves using a few pre-defined build arguments that you have access to in your Docker builds: `BUILDPLATFORM` and `TARGETPLATFORM` (and derivatives, like `TARGETOS`). These build arguments reflect the values you pass to the `--platform` flag.\n\nFor example, if you invoke a build with `--platform=linux/amd64`, then the build arguments resolve to:\n\n*   `TARGETPLATFORM=linux/amd64`\n*   `TARGETOS=linux`\n*   `TARGETARCH=amd64`\n\nWhen you pass more than one value to the platform flag, build stages that use the pre-defined platform arguments are forked automatically for each platform. This is in contrast to builds running under emulation, where the entire build pipeline runs per platform.\n\n![Build pipelines using cross-compilation](https://docs.docker.com/build/guide/images/cross-compilation.png)\n\n### [Update the Dockerfile](#update-the-dockerfile)\n\nTo build the app using the cross-compilation technique, update the Dockerfile as follows:\n\n*   Add `--platform=$BUILDPLATFORM` to the `FROM` instruction for the initial `base` stage, pinning the platform of the `golang` image to match the architecture of the host machine.\n*   Add `ARG` instructions for the Go compilation stages, making the `TARGETOS` and `TARGETARCH` build arguments available to the commands in this stage.\n*   Set the `GOOS` and `GOARCH` environment variables to the values of `TARGETOS` and `TARGETARCH`. The Go compiler uses these variables to do cross-compilation.\n\nThe only thing left to do now is to run the actual build. To run a multi-platform build, set the `--platform` option, and specify a CSV string of the OS and architectures that you want to build for. The following command illustrates how to build, and export, binaries for Mac (ARM64), Windows, and Linux:\n\nWhen the build finishes, you’ll find client and server binaries for all of the selected platforms in the `bin` directory:\n\nThis section has demonstrated how you can get started with multi-platform builds using emulation and cross-compilation.\n\nRelated information:\n\n*   [Multi-platfom images](https://docs.docker.com/build/building/multi-platform/)\n*   [containerd image store (Docker Desktop)](https://docs.docker.com/desktop/containerd/)\n*   [containerd image store (Docker Engine)](https://docs.docker.com/storage/containerd/)\n\nYou may also want to consider checking out [xx - Dockerfile cross-compilation helpers](https://github.com/tonistiigi/xx). `xx` is a Docker image containing utility scripts that make cross-compiling with Docker builds easier.\n\nThis section is the final part of the Build with Docker guide. The following page contains some pointers for where to go next.",
  "title": "Multi-platform | Docker Docs\n",
  "description": "Building for multiple operating systems and architectures",
  "languageCode": "en"
},
{
  "url": "https://docs.docker.com/build/guide/test/",
  "markdown": "# Test | Docker Docs\n\nThis section focuses on testing. The example in this section focuses on linting, but the same principles apply for other kinds of tests as well, such as unit tests. Code linting is a static analysis of code that helps you detect errors, style violations, and anti-patterns.\n\nThe exact steps for how to test your code can vary a lot depending on the programming language or framework that you use. The example application used in this guide is written in Go. You will add a build step that uses `golangci-lint`, a popular linters runner for Go.\n\nThe `golangci-lint` tool is available as an image on Docker Hub. Before you add the lint step to the Dockerfile, you can try it out using a `docker run` command.\n\nYou will notice that `golangci-lint` works: it finds an issue in the code where there's a missing error check.\n\nNow you can add this as a step to the Dockerfile.\n\nThe added `lint` stage uses the `golangci/golangci-lint` image from Docker Hub to invoke the `golangci-lint run` command with a bind-mount for the build context.\n\nThe lint stage is independent of any of the other stages in the Dockerfile. Therefore, running a regular build won’t cause the lint step to run. To lint the code, you must specify the `lint` stage:\n\nIn addition to running tests, it's sometimes useful to be able to export the results of a test to a test report.\n\nExporting test results is no different to exporting binaries, as shown in the previous section of this guide:\n\n1.  Save the test results to a file.\n2.  Create a new stage in your Dockerfile using the `scratch` base image.\n3.  Export that stage using the `local` exporter.\n\nThe exact steps on how to do this is left as a reader's exercise :-)\n\nThis section has shown an example on how you can use Docker builds to run tests (or as shown in this section, linters).\n\nThe next topic in this guide is multi-platform builds, using emulation and cross-compilation.",
  "title": "Test | Docker Docs\n",
  "description": "Running tests with Docker Build",
  "languageCode": "en"
},
{
  "url": "https://docs.docker.com/build/guide/next-steps/",
  "markdown": "# Next steps | Docker Docs\n\nThis guide has demonstrated some of the build features and capabilities that Docker provides.\n\nIf you would like to continue learning about Docker build, consider exploring the following resources:\n\n*   [BuildKit](https://docs.docker.com/build/buildkit/): deep-dive into the open source build engine that powers your Docker builds\n*   [Drivers](https://docs.docker.com/build/drivers/): configure for how and where your Docker builds run\n*   [Exporters](https://docs.docker.com/build/exporters/): save your build results to different output formats\n*   [Bake](https://docs.docker.com/build/bake/): orchestrate your build workflows\n*   [Attestations](https://docs.docker.com/build/attestations/): annotate your build artifacts with metadata\n*   [Continuous integration](https://docs.docker.com/build/ci/): run Docker builds in CI\n\nIf you have suggestions for improving the content of this guide, you can use the feedback widget to submit your feedback.\n\nIf you don't see the feedback widget, try turning off your content filtering extension or ad blocker, if you use one.\n\nYou can also submit an issue on [the docs GitHub repository](https://github.com/docker/docs/issues/new), if you prefer.",
  "title": "Next steps | Docker Docs\n",
  "description": "Next steps following the Docker Build guide",
  "languageCode": "en"
},
{
  "url": "https://docs.docker.com/engine/reference/commandline/dockerd/",
  "markdown": "# dockerd | Docker Docs\n\nOptions with \\[\\] may be specified multiple times.\n\n`dockerd` is the persistent process that manages containers. Docker uses different binaries for the daemon and client. To run the daemon you type `dockerd`.\n\nTo run the daemon with debug output, use `dockerd --debug` or add `\"debug\": true` to [the `daemon.json` file](#daemon-configuration-file).\n\n> **Enabling experimental features**\n> \n> Enable experimental features by starting `dockerd` with the `--experimental` flag or adding `\"experimental\": true` to the `daemon.json` file.\n\n### [Environment variables](#environment-variables)\n\nThe following list of environment variables are supported by the `dockerd` daemon. Some of these environment variables are supported both by the Docker Daemon and the `docker` CLI. Refer to [Environment variables](https://docs.docker.com/engine/reference/commandline/cli/#environment-variables) in the CLI section to learn about environment variables supported by the `docker` CLI.\n\n| Variable | Description |\n| --- | --- |\n| `DOCKER_CERT_PATH` | Location of your authentication keys. This variable is used both by the [`docker` CLI](https://docs.docker.com/engine/reference/commandline/cli/) and the `dockerd` daemon. |\n| `DOCKER_DRIVER` | The storage driver to use. |\n| `DOCKER_RAMDISK` | If set this disables `pivot_root`. |\n| `DOCKER_TLS_VERIFY` | When set Docker uses TLS and verifies the remote. This variable is used both by the [`docker` CLI](https://docs.docker.com/engine/reference/commandline/cli/) and the `dockerd` daemon. |\n| `DOCKER_TMPDIR` | Location for temporary files created by the daemon. |\n| `HTTP_PROXY` | Proxy URL for HTTP requests unless overridden by NoProxy. See the [Go specification](https://pkg.go.dev/golang.org/x/net/http/httpproxy#Config) for details. |\n| `HTTPS_PROXY` | Proxy URL for HTTPS requests unless overridden by NoProxy. See the [Go specification](https://pkg.go.dev/golang.org/x/net/http/httpproxy#Config) for details. |\n| `MOBY_DISABLE_PIGZ` | Disables the use of [`unpigz`](https://linux.die.net/man/1/pigz) to decompress layers in parallel when pulling images, even if it is installed. |\n| `NO_PROXY` | Comma-separated values specifying hosts that should be excluded from proxying. See the [Go specification](https://pkg.go.dev/golang.org/x/net/http/httpproxy#Config) for details. |\n\n### [Proxy configuration](#proxy-configuration)\n\n> **Note**\n> \n> Refer to the [Docker Desktop manual](https://docs.docker.com/desktop/networking/#httphttps-proxy-support) if you are running [Docker Desktop](https://docs.docker.com/desktop/).\n\nIf you are behind an HTTP proxy server, for example in corporate settings, you may have to configure the Docker daemon to use the proxy server for operations such as pulling and pushing images. The daemon can be configured in three ways:\n\n1.  Using environment variables (`HTTP_PROXY`, `HTTPS_PROXY`, and `NO_PROXY`).\n2.  Using the `http-proxy`, `https-proxy`, and `no-proxy` fields in the [daemon configuration file](#daemon-configuration-file) (Docker Engine version 23.0 or later).\n3.  Using the `--http-proxy`, `--https-proxy`, and `--no-proxy` command-line options. (Docker Engine version 23.0 or later).\n\nThe command-line and configuration file options take precedence over environment variables. Refer to [control and configure Docker with systemd](https://docs.docker.com/config/daemon/systemd/#httphttps-proxy) to set these environment variables on a host using `systemd`.\n\n### [Daemon socket option](#daemon-socket-option)\n\nThe Docker daemon can listen for [Docker Engine API](https://docs.docker.com/engine/api/) requests via three different types of Socket: `unix`, `tcp`, and `fd`.\n\nBy default, a `unix` domain socket (or IPC socket) is created at `/var/run/docker.sock`, requiring either `root` permission, or `docker` group membership.\n\nIf you need to access the Docker daemon remotely, you need to enable the tcp Socket. When using a TCP socket, the Docker daemon provides un-encrypted and un-authenticated direct access to the Docker daemon by default. You should secure the daemon either using the [built in HTTPS encrypted socket](https://docs.docker.com/engine/security/protect-access/), or by putting a secure web proxy in front of it. You can listen on port `2375` on all network interfaces with `-H tcp://0.0.0.0:2375`, or on a particular network interface using its IP address: `-H tcp://192.168.59.103:2375`. It is conventional to use port `2375` for un-encrypted, and port `2376` for encrypted communication with the daemon.\n\n> **Note**\n> \n> If you're using an HTTPS encrypted socket, keep in mind that only TLS version 1.0 and higher is supported. Protocols SSLv3 and below are not supported for security reasons.\n\nOn systemd based systems, you can communicate with the daemon via [systemd socket activation](https://0pointer.de/blog/projects/socket-activation.html), with `dockerd -H fd://`. Using `fd://` works for most setups, but you can also specify individual sockets: `dockerd -H fd://3`. If the specified socket activated files aren't found, the daemon exits. You can find examples of using systemd socket activation with Docker and systemd in the [Docker source tree](https://github.com/docker/docker/tree/master/contrib/init/systemd/).\n\nYou can configure the Docker daemon to listen to multiple sockets at the same time using multiple `-H` options:\n\nThe example below runs the daemon listening on the default Unix socket, and on 2 specific IP addresses on this host:\n\nThe Docker client honors the `DOCKER_HOST` environment variable to set the `-H` flag for the client. Use **one** of the following commands:\n\nSetting the `DOCKER_TLS_VERIFY` environment variable to any value other than the empty string is equivalent to setting the `--tlsverify` flag. The following are equivalent:\n\nThe Docker client honors the `HTTP_PROXY`, `HTTPS_PROXY`, and `NO_PROXY` environment variables (or the lowercase versions thereof). `HTTPS_PROXY` takes precedence over `HTTP_PROXY`.\n\nThe Docker client supports connecting to a remote daemon via SSH:\n\nTo use SSH connection, you need to set up `ssh` so that it can reach the remote host with public key authentication. Password authentication is not supported. If your key is protected with passphrase, you need to set up `ssh-agent`.\n\n#### [Bind Docker to another host/port or a Unix socket](#bind-docker-to-another-hostport-or-a-unix-socket)\n\n> **Warning**\n> \n> Changing the default `docker` daemon binding to a TCP port or Unix `docker` user group introduces security risks, as it may allow non-root users to gain root access on the host. Make sure you control access to `docker`. If you are binding to a TCP port, anyone with access to that port has full Docker access; so it's not advisable on an open network.\n\nWith `-H` it's possible to make the Docker daemon to listen on a specific IP and port. By default, it listens on `unix:///var/run/docker.sock` to allow only local connections by the root user. You could set it to `0.0.0.0:2375` or a specific host IP to give access to everybody, but that isn't recommended because someone could gain root access to the host where the daemon is running.\n\nSimilarly, the Docker client can use `-H` to connect to a custom port. The Docker client defaults to connecting to `unix:///var/run/docker.sock` on Linux, and `tcp://127.0.0.1:2376` on Windows.\n\n`-H` accepts host and port assignment in the following format:\n\nFor example:\n\n*   `tcp://` -> TCP connection to `127.0.0.1` on either port `2376` when TLS encryption is on, or port `2375` when communication is in plain text.\n*   `tcp://host:2375` -> TCP connection on host:2375\n*   `tcp://host:2375/path` -> TCP connection on host:2375 and prepend path to all requests\n*   `unix://path/to/socket` -> Unix socket located at `path/to/socket`\n\n`-H`, when empty, defaults to the same value as when no `-H` was passed in.\n\n`-H` also accepts short form for TCP bindings: `host:` or `host:port` or `:port`\n\nRun Docker in daemon mode:\n\nDownload an `ubuntu` image:\n\nYou can use multiple `-H`, for example, if you want to listen on both TCP and a Unix socket\n\n### [Daemon storage-driver](#daemon-storage-driver)\n\nOn Linux, the Docker daemon has support for several different image layer storage drivers: `overlay2`, `fuse-overlayfs`, `btrfs`, and `zfs`.\n\n`overlay2` is the preferred storage driver for all currently supported Linux distributions, and is selected by default. Unless users have a strong reason to prefer another storage driver, `overlay2` should be used.\n\nYou can find out more about storage drivers and how to select one in [Select a storage driver](https://docs.docker.com/storage/storagedriver/select-storage-driver/).\n\nOn Windows, the Docker daemon only supports the `windowsfilter` storage driver.\n\n### [Options per storage driver](#options-per-storage-driver)\n\nParticular storage-driver can be configured with options specified with `--storage-opt` flags. Options for `zfs` start with `zfs`, and options for `btrfs` start with `btrfs`.\n\n#### [ZFS options](#zfs-options)\n\n##### [`zfs.fsname`](#zfsfsname)\n\nSpecifies the ZFS filesystem that the daemon should use to create its datasets. By default, the ZFS filesystem in `/var/lib/docker` is used.\n\n###### [Example](#example)\n\n#### [Btrfs options](#btrfs-options)\n\n##### [`btrfs.min_space`](#btrfsmin_space)\n\nSpecifies the minimum size to use when creating the subvolume which is used for containers. If user uses disk quota for btrfs when creating or running a container with **\\--storage-opt size** option, Docker should ensure the **size** can't be smaller than **btrfs.min\\_space**.\n\n###### [Example](#example-1)\n\n#### [Overlay2 options](#overlay2-options)\n\n##### [`overlay2.size`](#overlay2size)\n\nSets the default max size of the container. It is supported only when the backing filesystem is `xfs` and mounted with `pquota` mount option. Under these conditions the user can pass any size less than the backing filesystem size.\n\n###### [Example](#example-2)\n\n#### [Windowsfilter options](#windowsfilter-options)\n\n##### [`size`](#size)\n\nSpecifies the size to use when creating the sandbox which is used for containers. Defaults to 20G.\n\n###### [Example](#example-3)\n\n### [Runtime options](#runtime-options)\n\nThe Docker daemon relies on a [OCI](https://github.com/opencontainers/runtime-spec) compliant runtime (invoked via the `containerd` daemon) as its interface to the Linux kernel `namespaces`, `cgroups`, and `SELinux`.\n\n#### [Configure container runtimes](#configure-container-runtimes)\n\nBy default, the Docker daemon uses runc as a container runtime. You can configure the daemon to add additional runtimes.\n\ncontainerd shims installed on `PATH` can be used directly, without the need to edit the daemon's configuration. For example, if you install the Kata Containers shim (`containerd-shim-kata-v2`) on `PATH`, then you can select that runtime with `docker run` without having to edit the daemon's configuration:\n\nContainer runtimes that don't implement containerd shims, or containerd shims installed outside of `PATH`, must be registered with the daemon, either via the configuration file or using the `--add-runtime` command line flag.\n\nFor examples on how to use other container runtimes, see [Alternative container runtimes](https://docs.docker.com/engine/alternative-runtimes/)\n\n##### [Configure runtimes using `daemon.json`](#configure-runtimes-using-daemonjson)\n\nTo register and configure container runtimes using the daemon's configuration file, add the runtimes as entries under `runtimes`:\n\nThe key of the entry (`<runtime>` in the previous example) represents the name of the runtime. This is the name that you reference when you run a container, using `docker run --runtime <runtime>`.\n\nThe runtime entry contains an object specifying the configuration for your runtime. The properties of the object depends on what kind of runtime you're looking to register:\n\n*   If the runtime implements its own containerd shim, the object shall contain a `runtimeType` field and an optional `options` field.\n    \n    See [Configure shims](#configure-containerd-shims).\n    \n*   If the runtime is designed to be a drop-in replacement for runc, the object contains a `path` field, and an optional `runtimeArgs` field.\n    \n    See [Configure runc drop-in replacements](#configure-runc-drop-in-replacements).\n    \n\nAfter changing the runtimes configuration in the configuration file, you must reload or restart the daemon for changes to take effect:\n\n##### [Configure containerd shims](#configure-containerd-shims)\n\nIf the runtime that you want to register implements a containerd shim, or if you want to register a runtime which uses the runc shim, use the following format for the runtime entry:\n\n`runtimeType` refers to either:\n\n*   A fully qualified name of a containerd shim.\n    \n    The fully qualified name of a shim is the same as the `runtime_type` used to register the runtime in containerd's CRI configuration. For example, `io.containerd.runsc.v1`.\n    \n*   The path of a containerd shim binary.\n    \n    This option is useful if you installed the containerd shim binary outside of `PATH`.\n    \n\n`options` is optional. It lets you specify the runtime configuration that you want to use for the shim. The configuration parameters that you can specify in `options` depends on the runtime you're registering. For most shims, the supported configuration options are `TypeUrl` and `ConfigPath`. For example:\n\nYou can configure multiple runtimes using the same runtimeType. For example:\n\nThe `options` field takes a special set of configuration parameters when used with `\"runtimeType\": \"io.containerd.runc.v2\"`. For more information about runc parameters, refer to the runc configuration section in [CRI Plugin Config Guide](https://github.com/containerd/containerd/blob/v1.7.2/docs/cri/config.md#full-configuration).\n\n##### [Configure runc drop-in replacements](#configure-runc-drop-in-replacements)\n\nIf the runtime that you want to register can act as a drop-in replacement for runc, you can register the runtime either using the daemon configuration file, or using the `--add-runtime` flag for the `dockerd` cli.\n\nWhen you use the configuration file, the entry uses the following format:\n\nWhere `path` is either the absolute path to the runtime executable, or the name of an executable installed on `PATH`:\n\nAnd `runtimeArgs` lets you optionally pass additional arguments to the runtime. Entries with this format use the containerd runc shim to invoke a custom runtime binary.\n\nWhen you use the `--add-runtime` CLI flag, use the following format:\n\nDefining runtime arguments via the command line is not supported.\n\nFor an example configuration for a runc drop-in replacment, see [Alternative container runtimes > youki](https://docs.docker.com/engine/alternative-runtimes/#youki)\n\n##### [Configure the default container runtime](#configure-the-default-container-runtime)\n\nYou can specify either the name of a fully qualified containerd runtime shim, or the name of a registered runtime. You can specify the default runtime either using the daemon configuration file, or using the `--default-runtime` flag for the `dockerd` cli.\n\nWhen you use the configuration file, the entry uses the following format:\n\nWhen you use the `--default-runtime` CLI flag, use the following format:\n\n#### [Run containerd standalone](#run-containerd-standalone)\n\nBy default, the Docker daemon automatically starts `containerd`. If you want to control `containerd` startup, manually start `containerd` and pass the path to the `containerd` socket using the `--containerd` flag. For example:\n\n#### [Configure cgroup driver](#configure-cgroup-driver)\n\nYou can configure how the runtime should manage container cgroups, using the `--exec-opt native.cgroupdriver` CLI flag.\n\nYou can only specify `cgroupfs` or `systemd`. If you specify `systemd` and it is not available, the system errors out. If you omit the `native.cgroupdriver` option, `cgroupfs` is used on cgroup v1 hosts, `systemd` is used on cgroup v2 hosts with systemd available.\n\nThis example sets the `cgroupdriver` to `systemd`:\n\nSetting this option applies to all containers the daemon launches.\n\n#### [Configure container isolation technology (Windows)](#configure-container-isolation-technology-windows)\n\nFor Windows containers, you can specify the default container isolation technology to use, using the `--exec-opt isolation` flag.\n\nThe following example makes `hyperv` the default isolation technology:\n\nIf no isolation value is specified on daemon start, on Windows client, the default is `hyperv`, and on Windows server, the default is `process`.\n\n### [Daemon DNS options](#daemon-dns-options)\n\nTo set the DNS server for all Docker containers, use:\n\nTo set the DNS search domain for all Docker containers, use:\n\n### [Allow push of non-distributable artifacts](#allow-push-of-non-distributable-artifacts)\n\nSome images (e.g., Windows base images) contain artifacts whose distribution is restricted by license. When these images are pushed to a registry, restricted artifacts are not included.\n\nTo override this behavior for specific registries, use the `--allow-nondistributable-artifacts` option in one of the following forms:\n\n*   `--allow-nondistributable-artifacts myregistry:5000` tells the Docker daemon to push non-distributable artifacts to myregistry:5000.\n*   `--allow-nondistributable-artifacts 10.1.0.0/16` tells the Docker daemon to push non-distributable artifacts to all registries whose resolved IP address is within the subnet described by the CIDR syntax.\n\nThis option can be used multiple times.\n\nThis option is useful when pushing images containing non-distributable artifacts to a registry on an air-gapped network so hosts on that network can pull the images without connecting to another server.\n\n> **Warning**\n> \n> Non-distributable artifacts typically have restrictions on how and where they can be distributed and shared. Only use this feature to push artifacts to private registries and ensure that you are in compliance with any terms that cover redistributing non-distributable artifacts.\n\n### [Insecure registries](#insecure-registries)\n\nIn this section, \"registry\" refers to a private registry, and `myregistry:5000` is a placeholder example of a private registry.\n\nDocker considers a private registry either secure or insecure. A secure registry uses TLS and a copy of its CA certificate is placed on the Docker host at `/etc/docker/certs.d/myregistry:5000/ca.crt`. An insecure registry is either not using TLS (i.e., listening on plain text HTTP), or is using TLS with a CA certificate not known by the Docker daemon. The latter can happen when the certificate wasn't found under `/etc/docker/certs.d/myregistry:5000/`, or if the certificate verification failed (i.e., wrong CA).\n\nBy default, Docker assumes all registries to be secure, except for local registries. Communicating with an insecure registry isn't possible if Docker assumes that registry is secure. In order to communicate with an insecure registry, the Docker daemon requires `--insecure-registry` in one of the following two forms:\n\n*   `--insecure-registry myregistry:5000` tells the Docker daemon that myregistry:5000 should be considered insecure.\n*   `--insecure-registry 10.1.0.0/16` tells the Docker daemon that all registries whose domain resolve to an IP address is part of the subnet described by the CIDR syntax, should be considered insecure.\n\nThe flag can be used multiple times to allow multiple registries to be marked as insecure.\n\nIf an insecure registry isn't marked as insecure, `docker pull`, `docker push`, and `docker search` result in error messages, prompting the user to either secure or pass the `--insecure-registry` flag to the Docker daemon as described above.\n\nLocal registries, whose IP address falls in the 127.0.0.0/8 range, are automatically marked as insecure as of Docker 1.3.2. It isn't recommended to rely on this, as it may change in the future.\n\nEnabling `--insecure-registry`, i.e., allowing un-encrypted and/or untrusted communication, can be useful when running a local registry. However, because its use creates security vulnerabilities it should only be enabled for testing purposes. For increased security, users should add their CA to their system's list of trusted CAs instead of enabling `--insecure-registry`.\n\n#### [Legacy Registries](#legacy-registries)\n\nOperations against registries supporting only the legacy v1 protocol are no longer supported. Specifically, the daemon doesn't attempt to push, pull or sign in to v1 registries. The exception to this is `search` which can still be performed on v1 registries.\n\n### [Running a Docker daemon behind an HTTPS\\_PROXY](#running-a-docker-daemon-behind-an-https_proxy)\n\nWhen running inside a LAN that uses an `HTTPS` proxy, the proxy's certificates replace Docker Hub's certificates. These certificates must be added to your Docker host's configuration:\n\n1.  Install the `ca-certificates` package for your distribution\n2.  Ask your network admin for the proxy's CA certificate and append them to `/etc/pki/tls/certs/ca-bundle.crt`\n3.  Then start your Docker daemon with `HTTPS_PROXY=http://username:password@proxy:port/ dockerd`. The `username:` and `password@` are optional - and are only needed if your proxy is set up to require authentication.\n\nThis only adds the proxy and authentication to the Docker daemon's requests. To use the proxy when building images and running containers, see [Configure Docker to use a proxy server](https://docs.docker.com/network/proxy/)\n\n### [Default `ulimit` settings](#default-ulimit-settings)\n\nThe `--default-ulimit` flag lets you set the default `ulimit` options to use for all containers. It takes the same options as `--ulimit` for `docker run`. If these defaults aren't set, `ulimit` settings are inherited from the Docker daemon. Any `--ulimit` options passed to `docker run` override the daemon defaults.\n\nBe careful setting `nproc` with the `ulimit` flag, as `nproc` is designed by Linux to set the maximum number of processes available to a user, not to a container. For details, see [`docker run` reference](https://docs.docker.com/reference/cli/docker/container/run/#ulimit).\n\nDocker's access authorization can be extended by authorization plugins that your organization can purchase or build themselves. You can install one or more authorization plugins when you start the Docker `daemon` using the `--authorization-plugin=PLUGIN_ID` option.\n\nThe `PLUGIN_ID` value is either the plugin's name or a path to its specification file. The plugin's implementation determines whether you can specify a name or path. Consult with your Docker administrator to get information about the plugins available to you.\n\nOnce a plugin is installed, requests made to the `daemon` through the command line or Docker's Engine API are allowed or denied by the plugin. If you have multiple plugins installed, each plugin, in order, must allow the request for it to complete.\n\nFor information about how to create an authorization plugin, refer to the [authorization plugin](https://docs.docker.com/engine/extend/plugins_authorization/) section.\n\n### [Daemon user namespace options](#daemon-user-namespace-options)\n\nThe Linux kernel [user namespace support](https://man7.org/linux/man-pages/man7/user_namespaces.7.html) provides additional security by enabling a process, and therefore a container, to have a unique range of user and group IDs which are outside the traditional user and group range utilized by the host system. One of the most important security improvements is that, by default, container processes running as the `root` user have expected administrative privileges it expects (with some restrictions) inside the container, but are effectively mapped to an unprivileged `uid` on the host.\n\nFor details about how to use this feature, as well as limitations, see [Isolate containers with a user namespace](https://docs.docker.com/engine/security/userns-remap/).\n\n### [Configure host gateway IP](#configure-host-gateway-ip)\n\nThe Docker daemon supports a special `host-gateway` value for the `--add-host` flag for the `docker run` and `docker build` commands. This value resolves to the host's gateway IP and lets containers connect to services running on the host.\n\nBy default, `host-gateway` resolves to the IP address of the default bridge. You can configure this to resolve to a different IP using the `--host-gateway-ip` flag for the dockerd command line interface, or the `host-gateway-ip` key in the daemon configuration file.\n\n### [Enable CDI devices](#enable-cdi-devices)\n\n> **Note**\n> \n> This is experimental feature and as such doesn't represent a stable API.\n> \n> This feature isn't enabled by default. To this feature, set `features.cdi` to `true` in the `daemon.json` configuration file.\n\nContainer Device Interface (CDI) is a [standardized](https://github.com/cncf-tags/container-device-interface/blob/main/SPEC.md) mechanism for container runtimes to create containers which are able to interact with third party devices.\n\nThe Docker daemon supports running containers with CDI devices if the requested device specifications are available on the filesystem of the daemon.\n\nThe default specification directors are:\n\n*   `/etc/cdi/` for static CDI Specs\n*   `/var/run/cdi` for generated CDI Specs\n\nAlternatively, you can set custom locations for CDI specifications using the `cdi-spec-dirs` option in the `daemon.json` configuration file, or the `--cdi-spec-dir` flag for the `dockerd` CLI.\n\nWhen CDI is enabled for a daemon, you can view the configured CDI specification directories using the `docker info` command.\n\n### [Miscellaneous options](#miscellaneous-options)\n\nIP masquerading uses address translation to allow containers without a public IP to talk to other machines on the internet. This may interfere with some network topologies, and can be disabled with `--ip-masq=false`.\n\nDocker supports soft links for the Docker data directory (`/var/lib/docker`) and for `/var/lib/docker/tmp`. The `DOCKER_TMPDIR` and the data directory can be set like this:\n\n#### [Default cgroup parent](#default-cgroup-parent)\n\nThe `--cgroup-parent` option lets you set the default cgroup parent for containers. If this option isn't set, it defaults to `/docker` for the cgroupfs driver, and `system.slice` for the systemd cgroup driver.\n\nIf the cgroup has a leading forward slash (`/`), the cgroup is created under the root cgroup, otherwise the cgroup is created under the daemon cgroup.\n\nAssuming the daemon is running in cgroup `daemoncgroup`, `--cgroup-parent=/foobar` creates a cgroup in `/sys/fs/cgroup/memory/foobar`, whereas using `--cgroup-parent=foobar` creates the cgroup in `/sys/fs/cgroup/memory/daemoncgroup/foobar`\n\nThe systemd cgroup driver has different rules for `--cgroup-parent`. systemd represents hierarchy by slice and the name of the slice encodes the location in the tree. So `--cgroup-parent` for systemd cgroups should be a slice name. A name can consist of a dash-separated series of names, which describes the path to the slice from the root slice. For example, `--cgroup-parent=user-a-b.slice` means the memory cgroup for the container is created in `/sys/fs/cgroup/memory/user.slice/user-a.slice/user-a-b.slice/docker-<id>.scope`.\n\nThis setting can also be set per container, using the `--cgroup-parent` option on `docker create` and `docker run`, and takes precedence over the `--cgroup-parent` option on the daemon.\n\n#### [Daemon metrics](#daemon-metrics)\n\nThe `--metrics-addr` option takes a TCP address to serve the metrics API. This feature is still experimental, therefore, the daemon must be running in experimental mode for this feature to work.\n\nTo serve the metrics API on `localhost:9323` you would specify `--metrics-addr 127.0.0.1:9323`, allowing you to make requests on the API at `127.0.0.1:9323/metrics` to receive metrics in the [prometheus](https://prometheus.io/docs/instrumenting/exposition_formats/) format.\n\nPort `9323` is the [default port associated with Docker metrics](https://github.com/prometheus/prometheus/wiki/Default-port-allocations) to avoid collisions with other Prometheus exporters and services.\n\nIf you are running a Prometheus server you can add this address to your scrape configs to have Prometheus collect metrics on Docker. For more information, see [Collect Docker metrics with Prometheus](https://docs.docker.com/config/daemon/prometheus/).\n\n#### [Node generic resources](#node-generic-resources)\n\nThe `--node-generic-resources` option takes a list of key-value pair (`key=value`) that allows you to advertise user defined resources in a Swarm cluster.\n\nThe current expected use case is to advertise NVIDIA GPUs so that services requesting `NVIDIA-GPU=[0-16]` can land on a node that has enough GPUs for the task to run.\n\nExample of usage:\n\n### [Daemon configuration file](#daemon-configuration-file)\n\nThe `--config-file` option allows you to set any configuration option for the daemon in a JSON format. This file uses the same flag names as keys, except for flags that allow several entries, where it uses the plural of the flag name, e.g., `labels` for the `label` flag.\n\nThe options set in the configuration file must not conflict with options set using flags. The Docker daemon fails to start if an option is duplicated between the file and the flags, regardless of their value. This is intentional, and avoids silently ignore changes introduced in configuration reloads. For example, the daemon fails to start if you set daemon labels in the configuration file and also set daemon labels via the `--label` flag. Options that are not present in the file are ignored when the daemon starts.\n\nThe `--validate` option allows to validate a configuration file without starting the Docker daemon. A non-zero exit code is returned for invalid configuration files.\n\n##### [On Linux](#on-linux)\n\nThe default location of the configuration file on Linux is `/etc/docker/daemon.json`. Use the `--config-file` flag to specify a non-default location.\n\nThe following is a full example of the allowed configuration options on Linux:\n\n> **Note**\n> \n> You can't set options in `daemon.json` that have already been set on daemon startup as a flag. On systems that use systemd to start the Docker daemon, `-H` is already set, so you can't use the `hosts` key in `daemon.json` to add listening addresses. See [custom Docker daemon options](https://docs.docker.com/config/daemon/systemd/#custom-docker-daemon-options) for an example on how to configure the daemon using systemd drop-in files.\n\n##### [On Windows](#on-windows)\n\nThe default location of the configuration file on Windows is `%programdata%\\docker\\config\\daemon.json`. Use the `--config-file` flag to specify a non-default location.\n\nThe following is a full example of the allowed configuration options on Windows:\n\nThe `default-runtime` option is by default unset, in which case dockerd automatically detects the runtime. This detection is based on if the `containerd` flag is set.\n\nAccepted values:\n\n*   `com.docker.hcsshim.v1` - This is the built-in runtime that Docker has used since Windows supported was first added and uses the v1 HCS API's in Windows.\n*   `io.containerd.runhcs.v1` - This is uses the containerd `runhcs` shim to run the container and uses the v2 HCS API's in Windows.\n\n#### [Feature options](#feature-options)\n\nThe optional field `features` in `daemon.json` lets you enable or disable specific daemon features.\n\nThe list of feature options include:\n\n*   `containerd-snapshotter`: when set to `true`, the daemon uses containerd snapshotters instead of the classic storage drivers for storing image and container data. For more information, see [containerd storage](https://docs.docker.com/storage/containerd/).\n    \n*   `windows-dns-proxy`: when set to `true`, the daemon's internal DNS resolver will forward requests to external servers. Without this, most applications running in the container will still be able to use secondary DNS servers configured in the container itself, but `nslookup` won't be able to resolve external names. The current default is `false`, it will change to `true` in a future release. This option is only allowed on Windows.\n    \n    > **Warning** The `windows-dns-proxy` feature flag will be removed in a future release.\n    \n\n#### [Configuration reload behavior](#configuration-reload-behavior)\n\nSome options can be reconfigured when the daemon is running without requiring to restart the process. The daemon uses the `SIGHUP` signal in Linux to reload, and a global event in Windows with the key `Global\\docker-daemon-config-$PID`. You can modify the options in the configuration file, but the daemon still checks for conflicting settings with the specified CLI flags. The daemon fails to reconfigure itself if there are conflicts, but it won't stop execution.\n\nThe list of currently supported options that can be reconfigured is this:\n\n| Option | Description |\n| --- | --- |\n| `debug` | Toggles debug mode of the daemon. |\n| `labels` | Replaces the daemon labels with a new set of labels. |\n| `live-restore` | Toggles [live restore](https://docs.docker.com/config/containers/live-restore/). |\n| `max-concurrent-downloads` | Configures the max concurrent downloads for each pull. |\n| `max-concurrent-uploads` | Configures the max concurrent uploads for each push. |\n| `max-download-attempts` | Configures the max download attempts for each pull. |\n| `default-runtime` | Configures the runtime to be used if not is specified at container creation. |\n| `runtimes` | Configures the list of available OCI runtimes that can be used to run containers. |\n| `authorization-plugin` | Specifies the authorization plugins to use. |\n| `allow-nondistributable-artifacts` | Specifies a list of registries to which the daemon will push non-distributable artifacts. |\n| `insecure-registries` | Specifies a list of registries that the daemon should consider insecure. |\n| `registry-mirrors` | Specifies a list of registry mirrors. |\n| `shutdown-timeout` | Configures the daemon's existing configuration timeout with a new timeout for shutting down all containers. |\n| `features` | Enables or disables specific features. |\n\n### [Run multiple daemons](#run-multiple-daemons)\n\n> **Note**\n> \n> Running multiple daemons on a single host is considered experimental. You may encounter unsolved problems, and things may not work as expected in some cases.\n\nThis section describes how to run multiple Docker daemons on a single host. To run multiple daemons, you must configure each daemon so that it doesn't conflict with other daemons on the same host. You can set these options either by providing them as flags, or by using a [daemon configuration file](#daemon-configuration-file).\n\nThe following daemon options must be configured for each daemon:\n\nWhen your daemons use different values for these flags, you can run them on the same host without any problems. It is important that you understand the meaning of these options and to use them correctly.\n\n*   The `-b, --bridge=` flag is set to `docker0` as default bridge network. It is created automatically when you install Docker. If you aren't using the default, you must create and configure the bridge manually, or set it to 'none': `--bridge=none`\n*   `--exec-root` is the path where the container state is stored. The default value is `/var/run/docker`. Specify the path for your running daemon here.\n*   `--data-root` is the path where persisted data such as images, volumes, and cluster state are stored. The default value is `/var/lib/docker`. To avoid any conflict with other daemons, set this parameter separately for each daemon.\n*   `-p, --pidfile=/var/run/docker.pid` is the path where the process ID of the daemon is stored. Specify the path for your PID file here.\n*   `--host=[]` specifies where the Docker daemon listens for client connections. If unspecified, it defaults to `/var/run/docker.sock`.\n*   `--iptables=false` prevents the Docker daemon from adding iptables rules. If multiple daemons manage iptables rules, they may overwrite rules set by another daemon. Be aware that disabling this option requires you to manually add iptables rules to expose container ports. If you prevent Docker from adding iptables rules, Docker also doesn't add IP masquerading rules, even if you set `--ip-masq` to `true`. Without IP masquerading rules, Docker containers can't connect to external hosts or the internet when using network other than default bridge.\n*   `--config-file=/etc/docker/daemon.json` is the path where configuration file is stored. You can use it instead of daemon flags. Specify the path for each daemon.\n*   `--tls*` Docker daemon supports `--tlsverify` mode that enforces encrypted and authenticated remote connections. The `--tls*` options enable use of specific certificates for individual daemons.\n\nExample script for a separate “bootstrap” instance of the Docker daemon without network:\n\n### [Default network options](#default-network-options)\n\nThe `default-network-opts` key in the `daemon.json` configuration file, and the equivalent `--default-network-opt` CLI flag, let you specify default values for driver network driver options for new networks.\n\nThe following example shows how to configure options for the `bridge` driver using the `daemon.json` file.\n\nThis example uses the `bridge` network driver. Refer to the [bridge network driver page](https://docs.docker.com/network/drivers/bridge/#options) for an overview of available driver options.\n\nAfter changing the configuration and restarting the daemon, new networks that you create use these option configurations as defaults.\n\nNote that changing this daemon configuration doesn't affect pre-existing networks.\n\nUsing the `--default-network-opt` CLI flag is useful for testing and debugging purposes, but you should prefer using the `daemon.json` file for persistent daemon configuration. The CLI flag expects a value with the following format: `driver=opt=value`, for example:",
  "title": "dockerd | Docker Docs\n",
  "description": "The daemon command description and usage",
  "languageCode": "en"
},
{
  "url": "https://docs.docker.com/engine/reference/builder/",
  "markdown": "# Dockerfile reference | Docker Docs\n\nDocker can build images automatically by reading the instructions from a Dockerfile. A Dockerfile is a text document that contains all the commands a user could call on the command line to assemble an image. This page describes the commands you can use in a Dockerfile.\n\nThe Dockerfile supports the following instructions:\n\n| Instruction | Description |\n| --- | --- |\n| [`ADD`](#add) | Add local or remote files and directories. |\n| [`ARG`](#arg) | Use build-time variables. |\n| [`CMD`](#cmd) | Specify default commands. |\n| [`COPY`](#copy) | Copy files and directories. |\n| [`ENTRYPOINT`](#entrypoint) | Specify default executable. |\n| [`ENV`](#env) | Set environment variables. |\n| [`EXPOSE`](#expose) | Describe which ports your application is listening on. |\n| [`FROM`](#from) | Create a new build stage from a base image. |\n| [`HEALTHCHECK`](#healthcheck) | Check a container's health on startup. |\n| [`LABEL`](#label) | Add metadata to an image. |\n| [`MAINTAINER`](#maintainer-deprecated) | Specify the author of an image. |\n| [`ONBUILD`](#onbuild) | Specify instructions for when the image is used in a build. |\n| [`RUN`](#run) | Execute build commands. |\n| [`SHELL`](#shell) | Set the default shell of an image. |\n| [`STOPSIGNAL`](#stopsignal) | Specify the system call signal for exiting a container. |\n| [`USER`](#user) | Set user and group ID. |\n| [`VOLUME`](#volume) | Create volume mounts. |\n| [`WORKDIR`](#workdir) | Change working directory. |\n\nHere is the format of the Dockerfile:\n\nThe instruction is not case-sensitive. However, convention is for them to be UPPERCASE to distinguish them from arguments more easily.\n\nDocker runs instructions in a Dockerfile in order. A Dockerfile **must begin with a `FROM` instruction**. This may be after [parser directives](#parser-directives), [comments](#format), and globally scoped [ARGs](#arg). The `FROM` instruction specifies the [parent image](https://docs.docker.com/glossary/#parent-image) from which you are building. `FROM` may only be preceded by one or more `ARG` instructions, which declare arguments that are used in `FROM` lines in the Dockerfile.\n\nBuildKit treats lines that begin with `#` as a comment, unless the line is a valid [parser directive](#parser-directives). A `#` marker anywhere else in a line is treated as an argument. This allows statements like:\n\nComment lines are removed before the Dockerfile instructions are executed. The comment in the following example is removed before the shell executes the `echo` command.\n\nThe following examples is equivalent.\n\nComments don't support line continuation characters.\n\n> **Note on whitespace**\n> \n> For backward compatibility, leading whitespace before comments (`#`) and instructions (such as `RUN`) are ignored, but discouraged. Leading whitespace is not preserved in these cases, and the following examples are therefore equivalent:\n> \n> Whitespace in instruction arguments, however, isn't ignored. The following example prints `hello world` with leading whitespace as specified:\n\nParser directives are optional, and affect the way in which subsequent lines in a Dockerfile are handled. Parser directives don't add layers to the build, and don't show up as build steps. Parser directives are written as a special type of comment in the form `# directive=value`. A single directive may only be used once.\n\nOnce a comment, empty line or builder instruction has been processed, BuildKit no longer looks for parser directives. Instead it treats anything formatted as a parser directive as a comment and doesn't attempt to validate if it might be a parser directive. Therefore, all parser directives must be at the top of a Dockerfile.\n\nParser directives aren't case-sensitive, but they're lowercase by convention. It's also conventional to include a blank line following any parser directives. Line continuation characters aren't supported in parser directives.\n\nDue to these rules, the following examples are all invalid:\n\nInvalid due to line continuation:\n\nInvalid due to appearing twice:\n\nTreated as a comment because it appears after a builder instruction:\n\nTreated as a comment because it appears after a comment that isn't a parser directive:\n\nThe following `unknowndirective` is treated as a comment because it isn't recognized. The known `syntax` directive is treated as a comment because it appears after a comment that isn't a parser directive.\n\nNon line-breaking whitespace is permitted in a parser directive. Hence, the following lines are all treated identically:\n\nThe following parser directives are supported:\n\n*   `syntax`\n*   `escape`\n\n### [syntax](#syntax)\n\nUse the `syntax` parser directive to declare the Dockerfile syntax version to use for the build. If unspecified, BuildKit uses a bundled version of the Dockerfile frontend. Declaring a syntax version lets you automatically use the latest Dockerfile version without having to upgrade BuildKit or Docker Engine, or even use a custom Dockerfile implementation.\n\nMost users will want to set this parser directive to `docker/dockerfile:1`, which causes BuildKit to pull the latest stable version of the Dockerfile syntax before the build.\n\nFor more information about how the parser directive works, see [Custom Dockerfile syntax](https://docs.docker.com/build/buildkit/dockerfile-frontend/).\n\n### [escape](#escape)\n\nOr\n\nThe `escape` directive sets the character used to escape characters in a Dockerfile. If not specified, the default escape character is `\\`.\n\nThe escape character is used both to escape characters in a line, and to escape a newline. This allows a Dockerfile instruction to span multiple lines. Note that regardless of whether the `escape` parser directive is included in a Dockerfile, escaping is not performed in a `RUN` command, except at the end of a line.\n\nSetting the escape character to `` ` `` is especially useful on `Windows`, where `\\` is the directory path separator. `` ` `` is consistent with [Windows PowerShell](https://technet.microsoft.com/en-us/library/hh847755.aspx).\n\nConsider the following example which would fail in a non-obvious way on Windows. The second `\\` at the end of the second line would be interpreted as an escape for the newline, instead of a target of the escape from the first `\\`. Similarly, the `\\` at the end of the third line would, assuming it was actually handled as an instruction, cause it be treated as a line continuation. The result of this Dockerfile is that second and third lines are considered a single instruction:\n\nResults in:\n\nOne solution to the above would be to use `/` as the target of both the `COPY` instruction, and `dir`. However, this syntax is, at best, confusing as it is not natural for paths on Windows, and at worst, error prone as not all commands on Windows support `/` as the path separator.\n\nBy adding the `escape` parser directive, the following Dockerfile succeeds as expected with the use of natural platform semantics for file paths on Windows:\n\nResults in:\n\nEnvironment variables (declared with [the `ENV` statement](#env)) can also be used in certain instructions as variables to be interpreted by the Dockerfile. Escapes are also handled for including variable-like syntax into a statement literally.\n\nEnvironment variables are notated in the Dockerfile either with `$variable_name` or `${variable_name}`. They are treated equivalently and the brace syntax is typically used to address issues with variable names with no whitespace, like `${foo}_bar`.\n\nThe `${variable_name}` syntax also supports a few of the standard `bash` modifiers as specified below:\n\n*   `${variable:-word}` indicates that if `variable` is set then the result will be that value. If `variable` is not set then `word` will be the result.\n*   `${variable:+word}` indicates that if `variable` is set then `word` will be the result, otherwise the result is the empty string.\n\nThe following variable replacements are supported in a pre-release version of Dockerfile syntax, when using the `# syntax=docker/dockerfile-upstream:master` syntax directive in your Dockerfile:\n\n*   `${variable#pattern}` removes the shortest match of `pattern` from `variable`, seeking from the start of the string.\n    \n*   `${variable##pattern}` removes the longest match of `pattern` from `variable`, seeking from the start of the string.\n    \n*   `${variable%pattern}` removes the shortest match of `pattern` from `variable`, seeking backwards from the end of the string.\n    \n*   `${variable%%pattern}` removes the longest match of `pattern` from `variable`, seeking backwards from the end of the string.\n    \n*   `${variable/pattern/replacement}` replace the first occurrence of `pattern` in `variable` with `replacement`\n    \n*   `${variable//pattern/replacement}` replaces all occurrences of `pattern` in `variable` with `replacement`\n    \n\nIn all cases, `word` can be any string, including additional environment variables.\n\n`pattern` is a glob pattern where `?` matches any single character and `*` any number of characters (including zero). To match literal `?` and `*`, use a backslash escape: `\\?` and `\\*`.\n\nYou can escape whole variable names by adding a `\\` before the variable: `\\$foo` or `\\${foo}`, for example, will translate to `$foo` and `${foo}` literals respectively.\n\nExample (parsed representation is displayed after the `#`):\n\nEnvironment variables are supported by the following list of instructions in the Dockerfile:\n\n*   `ADD`\n*   `COPY`\n*   `ENV`\n*   `EXPOSE`\n*   `FROM`\n*   `LABEL`\n*   `STOPSIGNAL`\n*   `USER`\n*   `VOLUME`\n*   `WORKDIR`\n*   `ONBUILD` (when combined with one of the supported instructions above)\n\nYou can also use environment variables with `RUN`, `CMD`, and `ENTRYPOINT` instructions, but in those cases the variable substitution is handled by the command shell, not the builder. Note that instructions using the exec form don't invoke a command shell automatically. See [Variable substitution](#variable-substitution).\n\nEnvironment variable substitution use the same value for each variable throughout the entire instruction. Changing the value of a variable only takes effect in subsequent instructions. Consider the following example:\n\n*   The value of `def` becomes `hello`\n*   The value of `ghi` becomes `bye`\n\nYou can use `.dockerignore` file to exclude files and directories from the build context. For more information, see [.dockerignore file](https://docs.docker.com/build/building/context/#dockerignore-files).\n\nThe `RUN`, `CMD`, and `ENTRYPOINT` instructions all have two possible forms:\n\n*   `INSTRUCTION [\"executable\",\"param1\",\"param2\"]` (exec form)\n*   `INSTRUCTION command param1 param2` (shell form)\n\nThe exec form makes it possible to avoid shell string munging, and to invoke commands using a specific command shell, or any other executable. It uses a JSON array syntax, where each element in the array is a command, flag, or argument.\n\nThe shell form is more relaxed, and emphasizes ease of use, flexibility, and readability. The shell form automatically uses a command shell, whereas the exec form does not.\n\n### [Exec form](#exec-form)\n\nThe exec form is parsed as a JSON array, which means that you must use double-quotes (\") around words, not single-quotes (').\n\nThe exec form is best used to specify an `ENTRYPOINT` instruction, combined with `CMD` for setting default arguments that can be overridden at runtime. For more information, see [ENTRYPOINT](#entrypoint).\n\n#### [Variable substitution](#variable-substitution)\n\nUsing the exec form doesn't automatically invoke a command shell. This means that normal shell processing, such as variable substitution, doesn't happen. For example, `RUN [ \"echo\", \"$HOME\" ]` won't handle variable substitution for `$HOME`.\n\nIf you want shell processing then either use the shell form or execute a shell directly with the exec form, for example: `RUN [ \"sh\", \"-c\", \"echo $HOME\" ]`. When using the exec form and executing a shell directly, as in the case for the shell form, it's the shell that's doing the environment variable substitution, not the builder.\n\n#### [Backslashes](#backslashes)\n\nIn exec form, you must escape backslashes. This is particularly relevant on Windows where the backslash is the path separator. The following line would otherwise be treated as shell form due to not being valid JSON, and fail in an unexpected way:\n\nThe correct syntax for this example is:\n\n### [Shell form](#shell-form)\n\nUnlike the exec form, instructions using the shell form always use a command shell. The shell form doesn't use the JSON array format, instead it's a regular string. The shell form string lets you escape newlines using the [escape character](#escape) (backslash by default) to continue a single instruction onto the next line. This makes it easier to use with longer commands, because it lets you split them up into multiple lines. For example, consider these two lines:\n\nThey're equivalent to the following line:\n\nYou can also use heredocs with the shell form to break up a command:\n\nFor more information about heredocs, see [Here-documents](#here-documents).\n\n### [Use a different shell](#use-a-different-shell)\n\nYou can change the default shell using the `SHELL` command. For example:\n\nFor more information, see [SHELL](#shell).\n\nOr\n\nOr\n\nThe `FROM` instruction initializes a new build stage and sets the [base image](https://docs.docker.com/glossary/#base-image) for subsequent instructions. As such, a valid Dockerfile must start with a `FROM` instruction. The image can be any valid image.\n\n*   `ARG` is the only instruction that may precede `FROM` in the Dockerfile. See [Understand how ARG and FROM interact](#understand-how-arg-and-from-interact).\n*   `FROM` can appear multiple times within a single Dockerfile to create multiple images or use one build stage as a dependency for another. Simply make a note of the last image ID output by the commit before each new `FROM` instruction. Each `FROM` instruction clears any state created by previous instructions.\n*   Optionally a name can be given to a new build stage by adding `AS name` to the `FROM` instruction. The name can be used in subsequent `FROM <name>`, [`COPY --from=<name>`](#copy---from), and [`RUN --mount=type=bind,from=<name>`](#run---mounttypebind) instructions to refer to the image built in this stage.\n*   The `tag` or `digest` values are optional. If you omit either of them, the builder assumes a `latest` tag by default. The builder returns an error if it can't find the `tag` value.\n\nThe optional `--platform` flag can be used to specify the platform of the image in case `FROM` references a multi-platform image. For example, `linux/amd64`, `linux/arm64`, or `windows/amd64`. By default, the target platform of the build request is used. Global build arguments can be used in the value of this flag, for example [automatic platform ARGs](#automatic-platform-args-in-the-global-scope) allow you to force a stage to native build platform (`--platform=$BUILDPLATFORM`), and use it to cross-compile to the target platform inside the stage.\n\n### [Understand how ARG and FROM interact](#understand-how-arg-and-from-interact)\n\n`FROM` instructions support variables that are declared by any `ARG` instructions that occur before the first `FROM`.\n\nAn `ARG` declared before a `FROM` is outside of a build stage, so it can't be used in any instruction after a `FROM`. To use the default value of an `ARG` declared before the first `FROM` use an `ARG` instruction without a value inside of a build stage:\n\nThe `RUN` instruction will execute any commands to create a new layer on top of the current image. The added layer is used in the next step in the Dockerfile. `RUN` has two forms:\n\nFor more information about the differences between these two forms, see [shell or exec forms](#shell-and-exec-form).\n\nThe shell form is most commonly used, and lets you break up longer instructions into multiple lines, either using newline [escapes](#escape), or with [heredocs](#here-documents):\n\nThe available `[OPTIONS]` for the `RUN` instruction are:\n\n*   [`--mount`](#run---mount)\n*   [`--network`](#run---network)\n*   [`--security`](#run---security)\n\n### [Cache invalidation for RUN instructions](#cache-invalidation-for-run-instructions)\n\nThe cache for `RUN` instructions isn't invalidated automatically during the next build. The cache for an instruction like `RUN apt-get dist-upgrade -y` will be reused during the next build. The cache for `RUN` instructions can be invalidated by using the `--no-cache` flag, for example `docker build --no-cache`.\n\nSee the [Dockerfile Best Practices guide](https://docs.docker.com/engine/userguide/eng-image/dockerfile_best-practices/) for more information.\n\nThe cache for `RUN` instructions can be invalidated by [`ADD`](#add) and [`COPY`](#copy) instructions.\n\n### [RUN --mount](#run---mount)\n\n`RUN --mount` allows you to create filesystem mounts that the build can access. This can be used to:\n\n*   Create bind mount to the host filesystem or other build stages\n*   Access build secrets or ssh-agent sockets\n*   Use a persistent package management cache to speed up your build\n\nThe supported mount types are:\n\n| Type | Description |\n| --- | --- |\n| [`bind`](#run---mounttypebind) (default) | Bind-mount context directories (read-only). |\n| [`cache`](#run---mounttypecache) | Mount a temporary directory to cache directories for compilers and package managers. |\n| [`tmpfs`](#run---mounttypetmpfs) | Mount a `tmpfs` in the build container. |\n| [`secret`](#run---mounttypesecret) | Allow the build container to access secure files such as private keys without baking them into the image. |\n| [`ssh`](#run---mounttypessh) | Allow the build container to access SSH keys via SSH agents, with support for passphrases. |\n\n### [RUN --mount=type=bind](#run---mounttypebind)\n\nThis mount type allows binding files or directories to the build container. A bind mount is read-only by default.\n\n| Option | Description |\n| --- | --- |\n| `target`[1](#fn:1) | Mount path. |\n| `source` | Source path in the `from`. Defaults to the root of the `from`. |\n| `from` | Build stage or image name for the root of the source. Defaults to the build context. |\n| `rw`,`readwrite` | Allow writes on the mount. Written data will be discarded. |\n\n### [RUN --mount=type=cache](#run---mounttypecache)\n\nThis mount type allows the build container to cache directories for compilers and package managers.\n\n| Option | Description |\n| --- | --- |\n| `id` | Optional ID to identify separate/different caches. Defaults to value of `target`. |\n| `target`[1](#fn:1) | Mount path. |\n| `ro`,`readonly` | Read-only if set. |\n| `sharing` | One of `shared`, `private`, or `locked`. Defaults to `shared`. A `shared` cache mount can be used concurrently by multiple writers. `private` creates a new mount if there are multiple writers. `locked` pauses the second writer until the first one releases the mount. |\n| `from` | Build stage to use as a base of the cache mount. Defaults to empty directory. |\n| `source` | Subpath in the `from` to mount. Defaults to the root of the `from`. |\n| `mode` | File mode for new cache directory in octal. Default `0755`. |\n| `uid` | User ID for new cache directory. Default `0`. |\n| `gid` | Group ID for new cache directory. Default `0`. |\n\nContents of the cache directories persists between builder invocations without invalidating the instruction cache. Cache mounts should only be used for better performance. Your build should work with any contents of the cache directory as another build may overwrite the files or GC may clean it if more storage space is needed.\n\n#### [Example: cache Go packages](#example-cache-go-packages)\n\n#### [Example: cache apt packages](#example-cache-apt-packages)\n\nApt needs exclusive access to its data, so the caches use the option `sharing=locked`, which will make sure multiple parallel builds using the same cache mount will wait for each other and not access the same cache files at the same time. You could also use `sharing=private` if you prefer to have each build create another cache directory in this case.\n\n### [RUN --mount=type=tmpfs](#run---mounttypetmpfs)\n\nThis mount type allows mounting `tmpfs` in the build container.\n\n| Option | Description |\n| --- | --- |\n| `target`[1](#fn:1) | Mount path. |\n| `size` | Specify an upper limit on the size of the filesystem. |\n\n### [RUN --mount=type=secret](#run---mounttypesecret)\n\nThis mount type allows the build container to access secure files such as private keys without baking them into the image.\n\n| Option | Description |\n| --- | --- |\n| `id` | ID of the secret. Defaults to basename of the target path. |\n| `target` | Mount path. Defaults to `/run/secrets/` + `id`. |\n| `required` | If set to `true`, the instruction errors out when the secret is unavailable. Defaults to `false`. |\n| `mode` | File mode for secret file in octal. Default `0400`. |\n| `uid` | User ID for secret file. Default `0`. |\n| `gid` | Group ID for secret file. Default `0`. |\n\n#### [Example: access to S3](#example-access-to-s3)\n\n### [RUN --mount=type=ssh](#run---mounttypessh)\n\nThis mount type allows the build container to access SSH keys via SSH agents, with support for passphrases.\n\n| Option | Description |\n| --- | --- |\n| `id` | ID of SSH agent socket or key. Defaults to \"default\". |\n| `target` | SSH agent socket path. Defaults to `/run/buildkit/ssh_agent.${N}`. |\n| `required` | If set to `true`, the instruction errors out when the key is unavailable. Defaults to `false`. |\n| `mode` | File mode for socket in octal. Default `0600`. |\n| `uid` | User ID for socket. Default `0`. |\n| `gid` | Group ID for socket. Default `0`. |\n\n#### [Example: access to GitLab](#example-access-to-gitlab)\n\nYou can also specify a path to `*.pem` file on the host directly instead of `$SSH_AUTH_SOCK`. However, pem files with passphrases are not supported.\n\n### [RUN --network](#run---network)\n\n`RUN --network` allows control over which networking environment the command is run in.\n\nThe supported network types are:\n\n| Type | Description |\n| --- | --- |\n| [`default`](#run---networkdefault) (default) | Run in the default network. |\n| [`none`](#run---networknone) | Run with no network access. |\n| [`host`](#run---networkhost) | Run in the host's network environment. |\n\n### [RUN --network=default](#run---networkdefault)\n\nEquivalent to not supplying a flag at all, the command is run in the default network for the build.\n\n### [RUN --network=none](#run---networknone)\n\nThe command is run with no network access (`lo` is still available, but is isolated to this process)\n\n#### [Example: isolating external effects](#example-isolating-external-effects)\n\n`pip` will only be able to install the packages provided in the tarfile, which can be controlled by an earlier build stage.\n\n### [RUN --network=host](#run---networkhost)\n\nThe command is run in the host's network environment (similar to `docker build --network=host`, but on a per-instruction basis)\n\n> **Warning**\n> \n> The use of `--network=host` is protected by the `network.host` entitlement, which needs to be enabled when starting the buildkitd daemon with `--allow-insecure-entitlement network.host` flag or in [buildkitd config](https://github.com/moby/buildkit/blob/master/docs/buildkitd.toml.md), and for a build request with [`--allow network.host` flag](https://docs.docker.com/engine/reference/commandline/buildx_build/#allow).\n\n### [RUN --security](#run---security)\n\n> **Note**\n> \n> Not yet available in stable syntax, use [`docker/dockerfile:1-labs`](#syntax) version.\n\nThe default security mode is `sandbox`. With `--security=insecure`, the builder runs the command without sandbox in insecure mode, which allows to run flows requiring elevated privileges (e.g. containerd). This is equivalent to running `docker run --privileged`.\n\n> **Warning**\n> \n> In order to access this feature, entitlement `security.insecure` should be enabled when starting the buildkitd daemon with `--allow-insecure-entitlement security.insecure` flag or in [buildkitd config](https://github.com/moby/buildkit/blob/master/docs/buildkitd.toml.md), and for a build request with [`--allow security.insecure` flag](https://docs.docker.com/engine/reference/commandline/buildx_build/#allow).\n\nDefault sandbox mode can be activated via `--security=sandbox`, but that is no-op.\n\n#### [Example: check entitlements](#example-check-entitlements)\n\nThe `CMD` instruction sets the command to be executed when running a container from an image.\n\nYou can specify `CMD` instructions using [shell or exec forms](#shell-and-exec-form):\n\n*   `CMD [\"executable\",\"param1\",\"param2\"]` (exec form)\n*   `CMD [\"param1\",\"param2\"]` (exec form, as default parameters to `ENTRYPOINT`)\n*   `CMD command param1 param2` (shell form)\n\nThere can only be one `CMD` instruction in a Dockerfile. If you list more than one `CMD`, only the last one takes effect.\n\nThe purpose of a `CMD` is to provide defaults for an executing container. These defaults can include an executable, or they can omit the executable, in which case you must specify an `ENTRYPOINT` instruction as well.\n\nIf you would like your container to run the same executable every time, then you should consider using `ENTRYPOINT` in combination with `CMD`. See [`ENTRYPOINT`](#entrypoint). If the user specifies arguments to `docker run` then they will override the default specified in `CMD`, but still use the default `ENTRYPOINT`.\n\nIf `CMD` is used to provide default arguments for the `ENTRYPOINT` instruction, both the `CMD` and `ENTRYPOINT` instructions should be specified in the [exec form](#exec-form).\n\n> **Note**\n> \n> Don't confuse `RUN` with `CMD`. `RUN` actually runs a command and commits the result; `CMD` doesn't execute anything at build time, but specifies the intended command for the image.\n\nThe `LABEL` instruction adds metadata to an image. A `LABEL` is a key-value pair. To include spaces within a `LABEL` value, use quotes and backslashes as you would in command-line parsing. A few usage examples:\n\nAn image can have more than one label. You can specify multiple labels on a single line. Prior to Docker 1.10, this decreased the size of the final image, but this is no longer the case. You may still choose to specify multiple labels in a single instruction, in one of the following two ways:\n\n> **Note**\n> \n> Be sure to use double quotes and not single quotes. Particularly when you are using string interpolation (e.g. `LABEL example=\"foo-$ENV_VAR\"`), single quotes will take the string as is without unpacking the variable's value.\n\nLabels included in base or parent images (images in the `FROM` line) are inherited by your image. If a label already exists but with a different value, the most-recently-applied value overrides any previously-set value.\n\nTo view an image's labels, use the `docker image inspect` command. You can use the `--format` option to show just the labels;\n\n## [MAINTAINER (deprecated)](#maintainer-deprecated)\n\nThe `MAINTAINER` instruction sets the _Author_ field of the generated images. The `LABEL` instruction is a much more flexible version of this and you should use it instead, as it enables setting any metadata you require, and can be viewed easily, for example with `docker inspect`. To set a label corresponding to the `MAINTAINER` field you could use:\n\nThis will then be visible from `docker inspect` with the other labels.\n\nThe `EXPOSE` instruction informs Docker that the container listens on the specified network ports at runtime. You can specify whether the port listens on TCP or UDP, and the default is TCP if you don't specify a protocol.\n\nThe `EXPOSE` instruction doesn't actually publish the port. It functions as a type of documentation between the person who builds the image and the person who runs the container, about which ports are intended to be published. To publish the port when running the container, use the `-p` flag on `docker run` to publish and map one or more ports, or the `-P` flag to publish all exposed ports and map them to high-order ports.\n\nBy default, `EXPOSE` assumes TCP. You can also specify UDP:\n\nTo expose on both TCP and UDP, include two lines:\n\nIn this case, if you use `-P` with `docker run`, the port will be exposed once for TCP and once for UDP. Remember that `-P` uses an ephemeral high-ordered host port on the host, so TCP and UDP doesn't use the same port.\n\nRegardless of the `EXPOSE` settings, you can override them at runtime by using the `-p` flag. For example\n\nTo set up port redirection on the host system, see [using the -P flag](https://docs.docker.com/engine/reference/run/#expose-incoming-ports). The `docker network` command supports creating networks for communication among containers without the need to expose or publish specific ports, because the containers connected to the network can communicate with each other over any port. For detailed information, see the [overview of this feature](https://docs.docker.com/engine/userguide/networking/).\n\nThe `ENV` instruction sets the environment variable `<key>` to the value `<value>`. This value will be in the environment for all subsequent instructions in the build stage and can be [replaced inline](#environment-replacement) in many as well. The value will be interpreted for other environment variables, so quote characters will be removed if they are not escaped. Like command line parsing, quotes and backslashes can be used to include spaces within values.\n\nExample:\n\nThe `ENV` instruction allows for multiple `<key>=<value> ...` variables to be set at one time, and the example below will yield the same net results in the final image:\n\nThe environment variables set using `ENV` will persist when a container is run from the resulting image. You can view the values using `docker inspect`, and change them using `docker run --env <key>=<value>`.\n\nA stage inherits any environment variables that were set using `ENV` by its parent stage or any ancestor. Refer [here](https://docs.docker.com/build/building/multi-stage/) for more on multi-staged builds.\n\nEnvironment variable persistence can cause unexpected side effects. For example, setting `ENV DEBIAN_FRONTEND=noninteractive` changes the behavior of `apt-get`, and may confuse users of your image.\n\nIf an environment variable is only needed during build, and not in the final image, consider setting a value for a single command instead:\n\nOr using [`ARG`](#arg), which is not persisted in the final image:\n\n> **Alternative syntax**\n> \n> The `ENV` instruction also allows an alternative syntax `ENV <key> <value>`, omitting the `=`. For example:\n> \n> This syntax does not allow for multiple environment-variables to be set in a single `ENV` instruction, and can be confusing. For example, the following sets a single environment variable (`ONE`) with value `\"TWO= THREE=world\"`:\n> \n> The alternative syntax is supported for backward compatibility, but discouraged for the reasons outlined above, and may be removed in a future release.\n\nADD has two forms. The latter form is required for paths containing whitespace.\n\nThe available `[OPTIONS]` are:\n\n*   [`--keep-git-dir`](#add---keep-git-dir)\n*   [`--checksum`](#add---checksum)\n*   [`--chown`](#add---chown---chmod)\n*   [`--chmod`](#add---chown---chmod)\n*   [`--link`](#add---link)\n*   [`--exclude`](#add---exclude)\n\nThe `ADD` instruction copies new files, directories or remote file URLs from `<src>` and adds them to the filesystem of the image at the path `<dest>`.\n\nMultiple `<src>` resources may be specified but if they are files or directories, their paths are interpreted as relative to the source of the context of the build.\n\nEach `<src>` may contain wildcards and matching will be done using Go's [filepath.Match](https://golang.org/pkg/path/filepath#Match) rules. For example:\n\nTo add all files in the root of the build context starting with \"hom\":\n\nIn the following example, `?` is a single-character wildcard, matching e.g. \"home.txt\".\n\nThe `<dest>` is an absolute path, or a path relative to `WORKDIR`, into which the source will be copied inside the destination container.\n\nThe example below uses a relative path, and adds \"test.txt\" to `<WORKDIR>/relativeDir/`:\n\nWhereas this example uses an absolute path, and adds \"test.txt\" to `/absoluteDir/`\n\nWhen adding files or directories that contain special characters (such as `[` and `]`), you need to escape those paths following the Golang rules to prevent them from being treated as a matching pattern. For example, to add a file named `arr[0].txt`, use the following;\n\nIn the case where `<src>` is a remote file URL, the destination will have permissions of 600. If the remote file being retrieved has an HTTP `Last-Modified` header, the timestamp from that header will be used to set the `mtime` on the destination file. However, like any other file processed during an `ADD`, `mtime` isn't included in the determination of whether or not the file has changed and the cache should be updated.\n\n> **Note**\n> \n> If you build by passing a Dockerfile through STDIN (`docker build - < somefile`), there is no build context, so the Dockerfile can only contain a URL based `ADD` instruction. You can also pass a compressed archive through STDIN: (`docker build - < archive.tar.gz`), the Dockerfile at the root of the archive and the rest of the archive will be used as the context of the build.\n\nIf your URL files are protected using authentication, you need to use `RUN wget`, `RUN curl` or use another tool from within the container as the `ADD` instruction doesn't support authentication.\n\n> **Note**\n> \n> The first encountered `ADD` instruction will invalidate the cache for all following instructions from the Dockerfile if the contents of `<src>` have changed. This includes invalidating the cache for `RUN` instructions. See the [Dockerfile Best Practices guide – Leverage build cache](https://docs.docker.com/develop/develop-images/dockerfile_best-practices/#leverage-build-cache) for more information.\n\n`ADD` obeys the following rules:\n\n*   The `<src>` path must be inside the build context; you can't use `ADD ../something /something`, because the builder can only access files from the context, and `../something` specifies a parent file or directory of the build context root.\n    \n*   If `<src>` is a URL and `<dest>` does end with a trailing slash, then the filename is inferred from the URL and the file is downloaded to `<dest>/<filename>`. For instance, `ADD http://example.com/foobar /` would create the file `/foobar`. The URL must have a nontrivial path so that an appropriate filename can be discovered in this case (`http://example.com` doesn't work).\n    \n*   If `<src>` is a directory, the entire contents of the directory are copied, including filesystem metadata.\n    \n    > **Note**\n    > \n    > The directory itself isn't copied, only its contents.\n    \n*   If `<src>` is a local `tar` archive in a recognized compression format (`identity`, `gzip`, `bzip2` or `xz`) then it's unpacked as a directory. Resources from remote URLs aren't decompressed. When a directory is copied or unpacked, it has the same behavior as `tar -x`. The result is the union of:\n    \n    1.  Whatever existed at the destination path and\n    2.  The contents of the source tree, with conflicts resolved in favor of \"2.\" on a file-by-file basis.\n    \n    > **Note**\n    > \n    > Whether a file is identified as a recognized compression format or not is done solely based on the contents of the file, not the name of the file. For example, if an empty file happens to end with `.tar.gz` this isn't recognized as a compressed file and doesn't generate any kind of decompression error message, rather the file will simply be copied to the destination.\n    \n*   If `<src>` is any other kind of file, it's copied individually along with its metadata. In this case, if `<dest>` ends with a trailing slash `/`, it will be considered a directory and the contents of `<src>` will be written at `<dest>/base(<src>)`.\n    \n*   If multiple `<src>` resources are specified, either directly or due to the use of a wildcard, then `<dest>` must be a directory, and it must end with a slash `/`.\n    \n*   If `<src>` is a file, and `<dest>` doesn't end with a trailing slash, the contents of `<src>` will be written as filename `<dest>`.\n    \n*   If `<dest>` doesn't exist, it's created, along with all missing directories in its path.\n    \n\n### [Adding private Git repositories](#adding-private-git-repositories)\n\nTo add a private repository via SSH, create a Dockerfile with the following form:\n\nThis Dockerfile can be built with `docker build --ssh` or `buildctl build --ssh`, e.g.,\n\n### [ADD --keep-git-dir](#add---keep-git-dir)\n\nWhen `<src>` is the HTTP or SSH address of a remote Git repository, BuildKit adds the contents of the Git repository to the image excluding the `.git` directory by default.\n\nThe `--keep-git-dir=true` flag lets you preserve the `.git` directory.\n\n### [ADD --checksum](#add---checksum)\n\nThe `--checksum` flag lets you verify the checksum of a remote resource:\n\nThe `--checksum` flag only supports HTTP sources currently.\n\n### [ADD --chown --chmod](#add---chown---chmod)\n\nSee [`COPY --chown --chmod`](#copy---chown---chmod).\n\n### [ADD --link](#add---link)\n\nSee [`COPY --link`](#copy---link).\n\n### [ADD --exclude](#add---exclude)\n\nSee [`COPY --exclude`](#copy---exclude).\n\nCOPY has two forms. The latter form is required for paths containing whitespace.\n\nThe available `[OPTIONS]` are:\n\n*   [`--from`](#copy---from)\n*   [`--chown`](#copy---chown---chmod)\n*   [`--chmod`](#copy---chown---chmod)\n*   [`--link`](#copy---link)\n*   [`--parents`](#copy---parents)\n*   [`--exclude`](#copy---exclude)\n\nThe `COPY` instruction copies new files or directories from `<src>` and adds them to the filesystem of the container at the path `<dest>`.\n\nMultiple `<src>` resources may be specified but the paths of files and directories will be interpreted as relative to the source of the context of the build.\n\nEach `<src>` may contain wildcards and matching will be done using Go's [filepath.Match](https://golang.org/pkg/path/filepath#Match) rules. For example:\n\nTo add all files in the root of the build context starting with \"hom\":\n\nIn the following example, `?` is a single-character wildcard, matching e.g. \"home.txt\".\n\nThe `<dest>` is an absolute path, or a path relative to `WORKDIR`, into which the source will be copied inside the destination container.\n\nThe example below uses a relative path, and adds \"test.txt\" to `<WORKDIR>/relativeDir/`:\n\nWhereas this example uses an absolute path, and adds \"test.txt\" to `/absoluteDir/`\n\nWhen copying files or directories that contain special characters (such as `[` and `]`), you need to escape those paths following the Golang rules to prevent them from being treated as a matching pattern. For example, to copy a file named `arr[0].txt`, use the following;\n\n> **Note**\n> \n> If you build using STDIN (`docker build - < somefile`), there is no build context, so `COPY` can't be used.\n\nOptionally `COPY` accepts a flag `--from=<name>` that can be used to set the source location to a previous build stage (created with `FROM .. AS <name>`) that will be used instead of a build context sent by the user. In case a build stage with a specified name can't be found an image with the same name is attempted to be used instead.\n\n`COPY` obeys the following rules:\n\n*   The `<src>` path is resolved relative to the build context. If you specify a relative path leading outside of the build context, such as `COPY ../something /something`, parent directory paths are stripped out automatically. The effective source path in this example becomes `COPY something /something`\n    \n*   If `<src>` is a directory, the entire contents of the directory are copied, including filesystem metadata.\n    \n    > **Note**\n    > \n    > The directory itself isn't copied, only its contents.\n    \n*   If `<src>` is any other kind of file, it's copied individually along with its metadata. In this case, if `<dest>` ends with a trailing slash `/`, it will be considered a directory and the contents of `<src>` will be written at `<dest>/base(<src>)`.\n    \n*   If multiple `<src>` resources are specified, either directly or due to the use of a wildcard, then `<dest>` must be a directory, and it must end with a slash `/`.\n    \n*   If `<src>` is a file, and `<dest>` doesn't end with a trailing slash, the contents of `<src>` will be written as filename `<dest>`.\n    \n*   If `<dest>` doesn't exist, it's created, along with all missing directories in its path.\n    \n\n> **Note**\n> \n> The first encountered `COPY` instruction will invalidate the cache for all following instructions from the Dockerfile if the contents of `<src>` have changed. This includes invalidating the cache for `RUN` instructions. See the [Dockerfile Best Practices guide – Leverage build cache](https://docs.docker.com/develop/develop-images/dockerfile_best-practices/#leverage-build-cache) for more information.\n\n### [COPY --from](#copy---from)\n\nBy default, the `COPY` instruction copies files from the build context. The `COPY --from` flag lets you copy files from an image, a build stage, or a named context instead.\n\nTo copy from a build stage in a [multi-stage build](https://docs.docker.com/build/building/multi-stage/), specify the name of the stage you want to copy from. You specify stage names using the `AS` keyword with the `FROM` instruction.\n\nYou can also copy files directly from other images. The following example copies an `nginx.conf` file from the official Nginx image.\n\nThe source path of `COPY --from` is always resolved from filesystem root of the image or stage that you specify.\n\n### [COPY --chown --chmod](#copy---chown---chmod)\n\n> **Note**\n> \n> Only octal notation is currently supported. Non-octal support is tracked in [moby/buildkit#1951](https://github.com/moby/buildkit/issues/1951).\n\nThe `--chown` and `--chmod` features are only supported on Dockerfiles used to build Linux containers, and doesn't work on Windows containers. Since user and group ownership concepts do not translate between Linux and Windows, the use of `/etc/passwd` and `/etc/group` for translating user and group names to IDs restricts this feature to only be viable for Linux OS-based containers.\n\nAll files and directories copied from the build context are created with a UID and GID of `0` unless the optional `--chown` flag specifies a given username, groupname, or UID/GID combination to request specific ownership of the copied content. The format of the `--chown` flag allows for either username and groupname strings or direct integer UID and GID in any combination. Providing a username without groupname or a UID without GID will use the same numeric UID as the GID. If a username or groupname is provided, the container's root filesystem `/etc/passwd` and `/etc/group` files will be used to perform the translation from name to integer UID or GID respectively. The following examples show valid definitions for the `--chown` flag:\n\nIf the container root filesystem doesn't contain either `/etc/passwd` or `/etc/group` files and either user or group names are used in the `--chown` flag, the build will fail on the `COPY` operation. Using numeric IDs requires no lookup and does not depend on container root filesystem content.\n\n### [COPY --link](#copy---link)\n\nEnabling this flag in `COPY` or `ADD` commands allows you to copy files with enhanced semantics where your files remain independent on their own layer and don't get invalidated when commands on previous layers are changed.\n\nWhen `--link` is used your source files are copied into an empty destination directory. That directory is turned into a layer that is linked on top of your previous state.\n\nIs equivalent of doing two builds:\n\nand\n\nand merging all the layers of both images together.\n\n#### [Benefits of using `--link`](#benefits-of-using---link)\n\nUse `--link` to reuse already built layers in subsequent builds with `--cache-from` even if the previous layers have changed. This is especially important for multi-stage builds where a `COPY --from` statement would previously get invalidated if any previous commands in the same stage changed, causing the need to rebuild the intermediate stages again. With `--link` the layer the previous build generated is reused and merged on top of the new layers. This also means you can easily rebase your images when the base images receive updates, without having to execute the whole build again. In backends that support it, BuildKit can do this rebase action without the need to push or pull any layers between the client and the registry. BuildKit will detect this case and only create new image manifest that contains the new layers and old layers in correct order.\n\nThe same behavior where BuildKit can avoid pulling down the base image can also happen when using `--link` and no other commands that would require access to the files in the base image. In that case BuildKit will only build the layers for the `COPY` commands and push them to the registry directly on top of the layers of the base image.\n\n#### [Incompatibilities with `--link=false`](#incompatibilities-with---linkfalse)\n\nWhen using `--link` the `COPY/ADD` commands are not allowed to read any files from the previous state. This means that if in previous state the destination directory was a path that contained a symlink, `COPY/ADD` can not follow it. In the final image the destination path created with `--link` will always be a path containing only directories.\n\nIf you don't rely on the behavior of following symlinks in the destination path, using `--link` is always recommended. The performance of `--link` is equivalent or better than the default behavior and, it creates much better conditions for cache reuse.\n\n### [COPY --parents](#copy---parents)\n\n> **Note**\n> \n> Not yet available in stable syntax, use [`docker/dockerfile:1.7-labs`](#syntax) version.\n\nThe `--parents` flag preserves parent directories for `src` entries. This flag defaults to `false`.\n\nThis behavior is similar to the [Linux `cp` utility's](https://www.man7.org/linux/man-pages/man1/cp.1.html) `--parents` or [`rsync`](https://man7.org/linux/man-pages/man1/rsync.1.html) `--relative` flag.\n\nAs with Rsync, it is possible to limit which parent directories are preserved by inserting a dot and a slash (`./`) into the source path. If such point exists, only parent directories after it will be preserved. This may be especially useful copies between stages with `--from` where the source paths need to be absolute.\n\nNote that, without the `--parents` flag specified, any filename collision will fail the Linux `cp` operation with an explicit error message (`cp: will not overwrite just-created './x/a.txt' with './y/a.txt'`), where the Buildkit will silently overwrite the target file at the destination.\n\nWhile it is possible to preserve the directory structure for `COPY` instructions consisting of only one `src` entry, usually it is more beneficial to keep the layer count in the resulting image as low as possible. Therefore, with the `--parents` flag, the Buildkit is capable of packing multiple `COPY` instructions together, keeping the directory structure intact.\n\n### [COPY --exclude](#copy---exclude)\n\n> **Note**\n> \n> Not yet available in stable syntax, use [`docker/dockerfile:1.7-labs`](#syntax) version.\n\nThe `--exclude` flag lets you specify a path expression for files to be excluded.\n\nThe path expression follows the same format as `<src>`, supporting wildcards and matching using Go's [filepath.Match](https://golang.org/pkg/path/filepath#Match) rules. For example, to add all files starting with \"hom\", excluding files with a `.txt` extension:\n\nYou can specify the `--exclude` option multiple times for a `COPY` instruction. Multiple `--excludes` are files matching its patterns not to be copied, even if the files paths match the pattern specified in `<src>`. To add all files starting with \"hom\", excluding files with either `.txt` or `.md` extensions:\n\n## [ENTRYPOINT](#entrypoint)\n\nAn `ENTRYPOINT` allows you to configure a container that will run as an executable.\n\n`ENTRYPOINT` has two possible forms:\n\n*   The exec form, which is the preferred form:\n    \n*   The shell form:\n    \n\nFor more information about the different forms, see [Shell and exec form](#shell-and-exec-form).\n\nThe following command starts a container from the `nginx` with its default content, listening on port 80:\n\nCommand line arguments to `docker run <image>` will be appended after all elements in an exec form `ENTRYPOINT`, and will override all elements specified using `CMD`.\n\nThis allows arguments to be passed to the entry point, i.e., `docker run <image> -d` will pass the `-d` argument to the entry point. You can override the `ENTRYPOINT` instruction using the `docker run --entrypoint` flag.\n\nThe shell form of `ENTRYPOINT` prevents any `CMD` command line arguments from being used. It also starts your `ENTRYPOINT` as a subcommand of `/bin/sh -c`, which does not pass signals. This means that the executable will not be the container's `PID 1`, and will not receive Unix signals. In this case, your executable doesn't receive a `SIGTERM` from `docker stop <container>`.\n\nOnly the last `ENTRYPOINT` instruction in the Dockerfile will have an effect.\n\n### [Exec form ENTRYPOINT example](#exec-form-entrypoint-example)\n\nYou can use the exec form of `ENTRYPOINT` to set fairly stable default commands and arguments and then use either form of `CMD` to set additional defaults that are more likely to be changed.\n\nWhen you run the container, you can see that `top` is the only process:\n\nTo examine the result further, you can use `docker exec`:\n\nAnd you can gracefully request `top` to shut down using `docker stop test`.\n\nThe following Dockerfile shows using the `ENTRYPOINT` to run Apache in the foreground (i.e., as `PID 1`):\n\nIf you need to write a starter script for a single executable, you can ensure that the final executable receives the Unix signals by using `exec` and `gosu` commands:\n\nLastly, if you need to do some extra cleanup (or communicate with other containers) on shutdown, or are co-ordinating more than one executable, you may need to ensure that the `ENTRYPOINT` script receives the Unix signals, passes them on, and then does some more work:\n\nIf you run this image with `docker run -it --rm -p 80:80 --name test apache`, you can then examine the container's processes with `docker exec`, or `docker top`, and then ask the script to stop Apache:\n\n> **Note**\n> \n> You can override the `ENTRYPOINT` setting using `--entrypoint`, but this can only set the binary to exec (no `sh -c` will be used).\n\n### [Shell form ENTRYPOINT example](#shell-form-entrypoint-example)\n\nYou can specify a plain string for the `ENTRYPOINT` and it will execute in `/bin/sh -c`. This form will use shell processing to substitute shell environment variables, and will ignore any `CMD` or `docker run` command line arguments. To ensure that `docker stop` will signal any long running `ENTRYPOINT` executable correctly, you need to remember to start it with `exec`:\n\nWhen you run this image, you'll see the single `PID 1` process:\n\nWhich exits cleanly on `docker stop`:\n\nIf you forget to add `exec` to the beginning of your `ENTRYPOINT`:\n\nYou can then run it (giving it a name for the next step):\n\nYou can see from the output of `top` that the specified `ENTRYPOINT` is not `PID 1`.\n\nIf you then run `docker stop test`, the container will not exit cleanly - the `stop` command will be forced to send a `SIGKILL` after the timeout:\n\n### [Understand how CMD and ENTRYPOINT interact](#understand-how-cmd-and-entrypoint-interact)\n\nBoth `CMD` and `ENTRYPOINT` instructions define what command gets executed when running a container. There are few rules that describe their co-operation.\n\n1.  Dockerfile should specify at least one of `CMD` or `ENTRYPOINT` commands.\n    \n2.  `ENTRYPOINT` should be defined when using the container as an executable.\n    \n3.  `CMD` should be used as a way of defining default arguments for an `ENTRYPOINT` command or for executing an ad-hoc command in a container.\n    \n4.  `CMD` will be overridden when running the container with alternative arguments.\n    \n\nThe table below shows what command is executed for different `ENTRYPOINT` / `CMD` combinations:\n\n|     | No ENTRYPOINT | ENTRYPOINT exec\\_entry p1\\_entry | ENTRYPOINT \\[\"exec\\_entry\", \"p1\\_entry\"\\] |\n| --- | --- | --- | --- |\n| **No CMD** | error, not allowed | /bin/sh -c exec\\_entry p1\\_entry | exec\\_entry p1\\_entry |\n| **CMD \\[\"exec\\_cmd\", \"p1\\_cmd\"\\]** | exec\\_cmd p1\\_cmd | /bin/sh -c exec\\_entry p1\\_entry | exec\\_entry p1\\_entry exec\\_cmd p1\\_cmd |\n| **CMD exec\\_cmd p1\\_cmd** | /bin/sh -c exec\\_cmd p1\\_cmd | /bin/sh -c exec\\_entry p1\\_entry | exec\\_entry p1\\_entry /bin/sh -c exec\\_cmd p1\\_cmd |\n\n> **Note**\n> \n> If `CMD` is defined from the base image, setting `ENTRYPOINT` will reset `CMD` to an empty value. In this scenario, `CMD` must be defined in the current image to have a value.\n\nThe `VOLUME` instruction creates a mount point with the specified name and marks it as holding externally mounted volumes from native host or other containers. The value can be a JSON array, `VOLUME [\"/var/log/\"]`, or a plain string with multiple arguments, such as `VOLUME /var/log` or `VOLUME /var/log /var/db`. For more information/examples and mounting instructions via the Docker client, refer to [_Share Directories via Volumes_](https://docs.docker.com/storage/volumes/) documentation.\n\nThe `docker run` command initializes the newly created volume with any data that exists at the specified location within the base image. For example, consider the following Dockerfile snippet:\n\nThis Dockerfile results in an image that causes `docker run` to create a new mount point at `/myvol` and copy the `greeting` file into the newly created volume.\n\n### [Notes about specifying volumes](#notes-about-specifying-volumes)\n\nKeep the following things in mind about volumes in the Dockerfile.\n\n*   **Volumes on Windows-based containers**: When using Windows-based containers, the destination of a volume inside the container must be one of:\n    \n    *   a non-existing or empty directory\n    *   a drive other than `C:`\n*   **Changing the volume from within the Dockerfile**: If any build steps change the data within the volume after it has been declared, those changes will be discarded.\n    \n*   **JSON formatting**: The list is parsed as a JSON array. You must enclose words with double quotes (`\"`) rather than single quotes (`'`).\n    \n*   **The host directory is declared at container run-time**: The host directory (the mountpoint) is, by its nature, host-dependent. This is to preserve image portability, since a given host directory can't be guaranteed to be available on all hosts. For this reason, you can't mount a host directory from within the Dockerfile. The `VOLUME` instruction does not support specifying a `host-dir` parameter. You must specify the mountpoint when you create or run the container.\n    \n\nor\n\nThe `USER` instruction sets the user name (or UID) and optionally the user group (or GID) to use as the default user and group for the remainder of the current stage. The specified user is used for `RUN` instructions and at runtime, runs the relevant `ENTRYPOINT` and `CMD` commands.\n\n> Note that when specifying a group for the user, the user will have _only_ the specified group membership. Any other configured group memberships will be ignored.\n\n> **Warning**\n> \n> When the user doesn't have a primary group then the image (or the next instructions) will be run with the `root` group.\n> \n> On Windows, the user must be created first if it's not a built-in account. This can be done with the `net user` command called as part of a Dockerfile.\n\nThe `WORKDIR` instruction sets the working directory for any `RUN`, `CMD`, `ENTRYPOINT`, `COPY` and `ADD` instructions that follow it in the Dockerfile. If the `WORKDIR` doesn't exist, it will be created even if it's not used in any subsequent Dockerfile instruction.\n\nThe `WORKDIR` instruction can be used multiple times in a Dockerfile. If a relative path is provided, it will be relative to the path of the previous `WORKDIR` instruction. For example:\n\nThe output of the final `pwd` command in this Dockerfile would be `/a/b/c`.\n\nThe `WORKDIR` instruction can resolve environment variables previously set using `ENV`. You can only use environment variables explicitly set in the Dockerfile. For example:\n\nThe output of the final `pwd` command in this Dockerfile would be `/path/$DIRNAME`\n\nIf not specified, the default working directory is `/`. In practice, if you aren't building a Dockerfile from scratch (`FROM scratch`), the `WORKDIR` may likely be set by the base image you're using.\n\nTherefore, to avoid unintended operations in unknown directories, it's best practice to set your `WORKDIR` explicitly.\n\nThe `ARG` instruction defines a variable that users can pass at build-time to the builder with the `docker build` command using the `--build-arg <varname>=<value>` flag.\n\n> **Warning**\n> \n> It isn't recommended to use build arguments for passing secrets such as user credentials, API tokens, etc. Build arguments are visible in the `docker history` command and in `max` mode provenance attestations, which are attached to the image by default if you use the Buildx GitHub Actions and your GitHub repository is public.\n> \n> Refer to the [`RUN --mount=type=secret`](#run---mounttypesecret) section to learn about secure ways to use secrets when building images.\n\nA Dockerfile may include one or more `ARG` instructions. For example, the following is a valid Dockerfile:\n\n### [Default values](#default-values)\n\nAn `ARG` instruction can optionally include a default value:\n\nIf an `ARG` instruction has a default value and if there is no value passed at build-time, the builder uses the default.\n\n### [Scope](#scope)\n\nAn `ARG` variable definition comes into effect from the line on which it is defined in the Dockerfile not from the argument's use on the command-line or elsewhere. For example, consider this Dockerfile:\n\nA user builds this file by calling:\n\nThe `USER` at line 2 evaluates to `some_user` as the `username` variable is defined on the subsequent line 3. The `USER` at line 4 evaluates to `what_user`, as the `username` argument is defined and the `what_user` value was passed on the command line. Prior to its definition by an `ARG` instruction, any use of a variable results in an empty string.\n\nAn `ARG` instruction goes out of scope at the end of the build stage where it was defined. To use an argument in multiple stages, each stage must include the `ARG` instruction.\n\n### [Using ARG variables](#using-arg-variables)\n\nYou can use an `ARG` or an `ENV` instruction to specify variables that are available to the `RUN` instruction. Environment variables defined using the `ENV` instruction always override an `ARG` instruction of the same name. Consider this Dockerfile with an `ENV` and `ARG` instruction.\n\nThen, assume this image is built with this command:\n\nIn this case, the `RUN` instruction uses `v1.0.0` instead of the `ARG` setting passed by the user:`v2.0.1` This behavior is similar to a shell script where a locally scoped variable overrides the variables passed as arguments or inherited from environment, from its point of definition.\n\nUsing the example above but a different `ENV` specification you can create more useful interactions between `ARG` and `ENV` instructions:\n\nUnlike an `ARG` instruction, `ENV` values are always persisted in the built image. Consider a docker build without the `--build-arg` flag:\n\nUsing this Dockerfile example, `CONT_IMG_VER` is still persisted in the image but its value would be `v1.0.0` as it is the default set in line 3 by the `ENV` instruction.\n\nThe variable expansion technique in this example allows you to pass arguments from the command line and persist them in the final image by leveraging the `ENV` instruction. Variable expansion is only supported for [a limited set of Dockerfile instructions.](#environment-replacement)\n\n### [Predefined ARGs](#predefined-args)\n\nDocker has a set of predefined `ARG` variables that you can use without a corresponding `ARG` instruction in the Dockerfile.\n\n*   `HTTP_PROXY`\n*   `http_proxy`\n*   `HTTPS_PROXY`\n*   `https_proxy`\n*   `FTP_PROXY`\n*   `ftp_proxy`\n*   `NO_PROXY`\n*   `no_proxy`\n*   `ALL_PROXY`\n*   `all_proxy`\n\nTo use these, pass them on the command line using the `--build-arg` flag, for example:\n\nBy default, these pre-defined variables are excluded from the output of `docker history`. Excluding them reduces the risk of accidentally leaking sensitive authentication information in an `HTTP_PROXY` variable.\n\nFor example, consider building the following Dockerfile using `--build-arg HTTP_PROXY=http://user:pass@proxy.lon.example.com`\n\nIn this case, the value of the `HTTP_PROXY` variable is not available in the `docker history` and is not cached. If you were to change location, and your proxy server changed to `http://user:pass@proxy.sfo.example.com`, a subsequent build does not result in a cache miss.\n\nIf you need to override this behaviour then you may do so by adding an `ARG` statement in the Dockerfile as follows:\n\nWhen building this Dockerfile, the `HTTP_PROXY` is preserved in the `docker history`, and changing its value invalidates the build cache.\n\n### [Automatic platform ARGs in the global scope](#automatic-platform-args-in-the-global-scope)\n\nThis feature is only available when using the [BuildKit](https://docs.docker.com/build/buildkit/) backend.\n\nBuildKit supports a predefined set of `ARG` variables with information on the platform of the node performing the build (build platform) and on the platform of the resulting image (target platform). The target platform can be specified with the `--platform` flag on `docker build`.\n\nThe following `ARG` variables are set automatically:\n\n*   `TARGETPLATFORM` - platform of the build result. Eg `linux/amd64`, `linux/arm/v7`, `windows/amd64`.\n*   `TARGETOS` - OS component of TARGETPLATFORM\n*   `TARGETARCH` - architecture component of TARGETPLATFORM\n*   `TARGETVARIANT` - variant component of TARGETPLATFORM\n*   `BUILDPLATFORM` - platform of the node performing the build.\n*   `BUILDOS` - OS component of BUILDPLATFORM\n*   `BUILDARCH` - architecture component of BUILDPLATFORM\n*   `BUILDVARIANT` - variant component of BUILDPLATFORM\n\nThese arguments are defined in the global scope so are not automatically available inside build stages or for your `RUN` commands. To expose one of these arguments inside the build stage redefine it without value.\n\nFor example:\n\n### [BuildKit built-in build args](#buildkit-built-in-build-args)\n\n| Arg | Type | Description |\n| --- | --- | --- |\n| `BUILDKIT_CACHE_MOUNT_NS` | String | Set optional cache ID namespace. |\n| `BUILDKIT_CONTEXT_KEEP_GIT_DIR` | Bool | Trigger Git context to keep the `.git` directory. |\n| `BUILDKIT_INLINE_CACHE`[2](#fn:2) | Bool | Inline cache metadata to image config or not. |\n| `BUILDKIT_MULTI_PLATFORM` | Bool | Opt into deterministic output regardless of multi-platform output or not. |\n| `BUILDKIT_SANDBOX_HOSTNAME` | String | Set the hostname (default `buildkitsandbox`) |\n| `BUILDKIT_SYNTAX` | String | Set frontend image |\n| `SOURCE_DATE_EPOCH` | Int | Set the Unix timestamp for created image and layers. More info from [reproducible builds](https://reproducible-builds.org/docs/source-date-epoch/). Supported since Dockerfile 1.5, BuildKit 0.11 |\n\n#### [Example: keep `.git` dir](#example-keep-git-dir)\n\nWhen using a Git context, `.git` dir is not kept on checkouts. It can be useful to keep it around if you want to retrieve git information during your build:\n\n### [Impact on build caching](#impact-on-build-caching)\n\n`ARG` variables are not persisted into the built image as `ENV` variables are. However, `ARG` variables do impact the build cache in similar ways. If a Dockerfile defines an `ARG` variable whose value is different from a previous build, then a \"cache miss\" occurs upon its first usage, not its definition. In particular, all `RUN` instructions following an `ARG` instruction use the `ARG` variable implicitly (as an environment variable), thus can cause a cache miss. All predefined `ARG` variables are exempt from caching unless there is a matching `ARG` statement in the Dockerfile.\n\nFor example, consider these two Dockerfile:\n\nIf you specify `--build-arg CONT_IMG_VER=<value>` on the command line, in both cases, the specification on line 2 doesn't cause a cache miss; line 3 does cause a cache miss. `ARG CONT_IMG_VER` causes the `RUN` line to be identified as the same as running `CONT_IMG_VER=<value> echo hello`, so if the `<value>` changes, you get a cache miss.\n\nConsider another example under the same command line:\n\nIn this example, the cache miss occurs on line 3. The miss happens because the variable's value in the `ENV` references the `ARG` variable and that variable is changed through the command line. In this example, the `ENV` command causes the image to include the value.\n\nIf an `ENV` instruction overrides an `ARG` instruction of the same name, like this Dockerfile:\n\nLine 3 doesn't cause a cache miss because the value of `CONT_IMG_VER` is a constant (`hello`). As a result, the environment variables and values used on the `RUN` (line 4) doesn't change between builds.\n\nThe `ONBUILD` instruction adds to the image a trigger instruction to be executed at a later time, when the image is used as the base for another build. The trigger will be executed in the context of the downstream build, as if it had been inserted immediately after the `FROM` instruction in the downstream Dockerfile.\n\nAny build instruction can be registered as a trigger.\n\nThis is useful if you are building an image which will be used as a base to build other images, for example an application build environment or a daemon which may be customized with user-specific configuration.\n\nFor example, if your image is a reusable Python application builder, it will require application source code to be added in a particular directory, and it might require a build script to be called after that. You can't just call `ADD` and `RUN` now, because you don't yet have access to the application source code, and it will be different for each application build. You could simply provide application developers with a boilerplate Dockerfile to copy-paste into their application, but that's inefficient, error-prone and difficult to update because it mixes with application-specific code.\n\nThe solution is to use `ONBUILD` to register advance instructions to run later, during the next build stage.\n\nHere's how it works:\n\n1.  When it encounters an `ONBUILD` instruction, the builder adds a trigger to the metadata of the image being built. The instruction doesn't otherwise affect the current build.\n2.  At the end of the build, a list of all triggers is stored in the image manifest, under the key `OnBuild`. They can be inspected with the `docker inspect` command.\n3.  Later the image may be used as a base for a new build, using the `FROM` instruction. As part of processing the `FROM` instruction, the downstream builder looks for `ONBUILD` triggers, and executes them in the same order they were registered. If any of the triggers fail, the `FROM` instruction is aborted which in turn causes the build to fail. If all triggers succeed, the `FROM` instruction completes and the build continues as usual.\n4.  Triggers are cleared from the final image after being executed. In other words they aren't inherited by \"grand-children\" builds.\n\nFor example you might add something like this:\n\n### [ONBUILD limitations](#onbuild-limitations)\n\n*   Chaining `ONBUILD` instructions using `ONBUILD ONBUILD` isn't allowed.\n*   The `ONBUILD` instruction may not trigger `FROM` or `MAINTAINER` instructions.\n*   `ONBUILD COPY --from` is [not supported](https://github.com/moby/buildkit/issues/816).\n\nThe `STOPSIGNAL` instruction sets the system call signal that will be sent to the container to exit. This signal can be a signal name in the format `SIG<NAME>`, for instance `SIGKILL`, or an unsigned number that matches a position in the kernel's syscall table, for instance `9`. The default is `SIGTERM` if not defined.\n\nThe image's default stopsignal can be overridden per container, using the `--stop-signal` flag on `docker run` and `docker create`.\n\nThe `HEALTHCHECK` instruction has two forms:\n\n*   `HEALTHCHECK [OPTIONS] CMD command` (check container health by running a command inside the container)\n*   `HEALTHCHECK NONE` (disable any healthcheck inherited from the base image)\n\nThe `HEALTHCHECK` instruction tells Docker how to test a container to check that it's still working. This can detect cases such as a web server stuck in an infinite loop and unable to handle new connections, even though the server process is still running.\n\nWhen a container has a healthcheck specified, it has a health status in addition to its normal status. This status is initially `starting`. Whenever a health check passes, it becomes `healthy` (whatever state it was previously in). After a certain number of consecutive failures, it becomes `unhealthy`.\n\nThe options that can appear before `CMD` are:\n\n*   `--interval=DURATION` (default: `30s`)\n*   `--timeout=DURATION` (default: `30s`)\n*   `--start-period=DURATION` (default: `0s`)\n*   `--start-interval=DURATION` (default: `5s`)\n*   `--retries=N` (default: `3`)\n\nThe health check will first run **interval** seconds after the container is started, and then again **interval** seconds after each previous check completes.\n\nIf a single run of the check takes longer than **timeout** seconds then the check is considered to have failed.\n\nIt takes **retries** consecutive failures of the health check for the container to be considered `unhealthy`.\n\n**start period** provides initialization time for containers that need time to bootstrap. Probe failure during that period will not be counted towards the maximum number of retries. However, if a health check succeeds during the start period, the container is considered started and all consecutive failures will be counted towards the maximum number of retries.\n\n**start interval** is the time between health checks during the start period. This option requires Docker Engine version 25.0 or later.\n\nThere can only be one `HEALTHCHECK` instruction in a Dockerfile. If you list more than one then only the last `HEALTHCHECK` will take effect.\n\nThe command after the `CMD` keyword can be either a shell command (e.g. `HEALTHCHECK CMD /bin/check-running`) or an exec array (as with other Dockerfile commands; see e.g. `ENTRYPOINT` for details).\n\nThe command's exit status indicates the health status of the container. The possible values are:\n\n*   0: success - the container is healthy and ready for use\n*   1: unhealthy - the container isn't working correctly\n*   2: reserved - don't use this exit code\n\nFor example, to check every five minutes or so that a web-server is able to serve the site's main page within three seconds:\n\nTo help debug failing probes, any output text (UTF-8 encoded) that the command writes on stdout or stderr will be stored in the health status and can be queried with `docker inspect`. Such output should be kept short (only the first 4096 bytes are stored currently).\n\nWhen the health status of a container changes, a `health_status` event is generated with the new status.\n\nThe `SHELL` instruction allows the default shell used for the shell form of commands to be overridden. The default shell on Linux is `[\"/bin/sh\", \"-c\"]`, and on Windows is `[\"cmd\", \"/S\", \"/C\"]`. The `SHELL` instruction must be written in JSON form in a Dockerfile.\n\nThe `SHELL` instruction is particularly useful on Windows where there are two commonly used and quite different native shells: `cmd` and `powershell`, as well as alternate shells available including `sh`.\n\nThe `SHELL` instruction can appear multiple times. Each `SHELL` instruction overrides all previous `SHELL` instructions, and affects all subsequent instructions. For example:\n\nThe following instructions can be affected by the `SHELL` instruction when the shell form of them is used in a Dockerfile: `RUN`, `CMD` and `ENTRYPOINT`.\n\nThe following example is a common pattern found on Windows which can be streamlined by using the `SHELL` instruction:\n\nThe command invoked by the builder will be:\n\nThis is inefficient for two reasons. First, there is an unnecessary `cmd.exe` command processor (aka shell) being invoked. Second, each `RUN` instruction in the shell form requires an extra `powershell -command` prefixing the command.\n\nTo make this more efficient, one of two mechanisms can be employed. One is to use the JSON form of the `RUN` command such as:\n\nWhile the JSON form is unambiguous and does not use the unnecessary `cmd.exe`, it does require more verbosity through double-quoting and escaping. The alternate mechanism is to use the `SHELL` instruction and the shell form, making a more natural syntax for Windows users, especially when combined with the `escape` parser directive:\n\nResulting in:\n\nThe `SHELL` instruction could also be used to modify the way in which a shell operates. For example, using `SHELL cmd /S /C /V:ON|OFF` on Windows, delayed environment variable expansion semantics could be modified.\n\nThe `SHELL` instruction can also be used on Linux should an alternate shell be required such as `zsh`, `csh`, `tcsh` and others.\n\nHere-documents allow redirection of subsequent Dockerfile lines to the input of `RUN` or `COPY` commands. If such command contains a [here-document](https://pubs.opengroup.org/onlinepubs/9699919799/utilities/V3_chap02.html#tag_18_07_04) the Dockerfile considers the next lines until the line only containing a here-doc delimiter as part of the same command.\n\n### [Example: Running a multi-line script](#example-running-a-multi-line-script)\n\nIf the command only contains a here-document, its contents is evaluated with the default shell.\n\nAlternatively, shebang header can be used to define an interpreter.\n\nMore complex examples may use multiple here-documents.\n\n### [Example: Creating inline files](#example-creating-inline-files)\n\nWith `COPY` instructions, you can replace the source parameter with a here-doc indicator to write the contents of the here-document directly to a file. The following example creates a `greeting.txt` file containing `hello world` using a `COPY` instruction.\n\nRegular here-doc [variable expansion and tab stripping rules](https://pubs.opengroup.org/onlinepubs/9699919799/utilities/V3_chap02.html#tag_18_07_04) apply. The following example shows a small Dockerfile that creates a `hello.sh` script file using a `COPY` instruction with a here-document.\n\nIn this case, file script prints \"hello bar\", because the variable is expanded when the `COPY` instruction gets executed.\n\nIf instead you were to quote any part of the here-document word `EOT`, the variable would not be expanded at build-time.\n\nNote that `ARG FOO=bar` is excessive here, and can be removed. The variable gets interpreted at runtime, when the script is invoked:\n\nFor examples of Dockerfiles, refer to:\n\n*   The [\"build images\" section](https://docs.docker.com/develop/develop-images/dockerfile_best-practices/)\n*   The [\"get started\" tutorial](https://docs.docker.com/get-started/)\n*   The [language-specific getting started guides](https://docs.docker.com/language/)\n*   The [build guide](https://docs.docker.com/build/guide/)",
  "title": "Dockerfile reference | Docker Docs\n",
  "description": "Find all the available commands you can use in a Dockerfile and learn how to use them, including COPY, ARG, ENTRYPOINT, and more.",
  "languageCode": "en"
},
{
  "url": "https://docs.docker.com/engine/reference/commandline/buildx_build/",
  "markdown": "# docker buildx build | Docker Docs\n\n|     |     |\n| --- | --- |\n| Description | Start a build |\n| Usage | `docker buildx build [OPTIONS] PATH \\| URL \\| -` |\n| Aliases<br><br>An alias is a short or memorable alternative for a longer command. | `docker buildx b` |\n\nThe `buildx build` command starts a build using BuildKit. This command is similar to the UI of `docker build` command and takes the same flags and arguments.\n\nFor documentation on most of these flags, refer to the [`docker build` documentation](https://docs.docker.com/reference/cli/docker/image/build/). This page describes a subset of the new flags.\n\n| Option | Default | Description |\n| --- | --- | --- |\n| [`--add-host`](https://docs.docker.com/reference/cli/docker/image/build/#add-host) |     | Add a custom host-to-IP mapping (format: `host:ip`) |\n| [`--allow`](#allow) |     | Allow extra privileged entitlement (e.g., `network.host`, `security.insecure`) |\n| [`--annotation`](#annotation) |     | Add annotation to the image |\n| [`--attest`](#attest) |     | Attestation parameters (format: `type=sbom,generator=image`) |\n| [`--build-arg`](#build-arg) |     | Set build-time variables |\n| [`--build-context`](#build-context) |     | Additional build contexts (e.g., name=path) |\n| [`--cache-from`](#cache-from) |     | External cache sources (e.g., `user/app:cache`, `type=local,src=path/to/dir`) |\n| [`--cache-to`](#cache-to) |     | Cache export destinations (e.g., `user/app:cache`, `type=local,dest=path/to/dir`) |\n| `--call` | `build` | Set method for evaluating build (`check`, `outline`, `targets`) |\n| [`--cgroup-parent`](https://docs.docker.com/reference/cli/docker/image/build/#cgroup-parent) |     | Set the parent cgroup for the `RUN` instructions during build |\n| `--check` |     | Shorthand for `--call=check` |\n| `--detach` |     | experimental (CLI) Detach buildx server (supported only on linux) |\n| [`-f, --file`](https://docs.docker.com/reference/cli/docker/image/build/#file) |     | Name of the Dockerfile (default: `PATH/Dockerfile`) |\n| `--iidfile` |     | Write the image ID to a file |\n| `--label` |     | Set metadata for an image |\n| [`--load`](#load) |     | Shorthand for `--output=type=docker` |\n| [`--metadata-file`](#metadata-file) |     | Write build result metadata to a file |\n| `--network` |     | Set the networking mode for the `RUN` instructions during build |\n| `--no-cache` |     | Do not use cache when building the image |\n| [`--no-cache-filter`](#no-cache-filter) |     | Do not cache specified stages |\n| [`-o, --output`](#output) |     | Output destination (format: `type=local,dest=path`) |\n| [`--platform`](#platform) |     | Set target platform for build |\n| [`--progress`](#progress) | `auto` | Set type of progress output (`auto`, `plain`, `tty`, `rawjson`). Use plain to show container output |\n| [`--provenance`](#provenance) |     | Shorthand for `--attest=type=provenance` |\n| `--pull` |     | Always attempt to pull all referenced images |\n| [`--push`](#push) |     | Shorthand for `--output=type=registry` |\n| `-q, --quiet` |     | Suppress the build output and print image ID on success |\n| `--root` |     | experimental (CLI) Specify root directory of server to connect |\n| [`--sbom`](#sbom) |     | Shorthand for `--attest=type=sbom` |\n| [`--secret`](#secret) |     | Secret to expose to the build (format: `id=mysecret[,src=/local/secret]`) |\n| `--server-config` |     | experimental (CLI) Specify buildx server config file (used only when launching new server) |\n| [`--shm-size`](#shm-size) |     | Shared memory size for build containers |\n| [`--ssh`](#ssh) |     | SSH agent socket or keys to expose to the build (format: `default\\|<id>[=<socket>\\|<key>[,<key>]]`) |\n| [`-t, --tag`](https://docs.docker.com/reference/cli/docker/image/build/#tag) |     | Name and optionally a tag (format: `name:tag`) |\n| [`--target`](https://docs.docker.com/reference/cli/docker/image/build/#target) |     | Set the target build stage to build |\n| [`--ulimit`](#ulimit) |     | Ulimit options |\n\n### [Create annotations (--annotation)](#annotation)\n\nAdd OCI annotations to the image index, manifest, or descriptor. The following example adds the `foo=bar` annotation to the image manifests:\n\nYou can optionally add a type prefix to specify the level of the annotation. By default, the image manifest is annotated. The following example adds the `foo=bar` annotation the image index instead of the manifests:\n\nYou can specify multiple types, separated by a comma (,) to add the annotation to multiple image components. The following example adds the `foo=bar` annotation to image index, descriptors, manifests:\n\nYou can also specify a platform qualifier in square brackets (`[os/arch]`) in the type prefix, to apply the annotation to a subset of manifests with the matching platform. The following example adds the `foo=bar` annotation only to the manifest with the `linux/amd64` platform:\n\nWildcards are not supported in the platform qualifier; you can't specify a type prefix like `manifest[linux/*]` to add annotations only to manifests which has `linux` as the OS platform.\n\nFor more information about annotations, see [Annotations](https://docs.docker.com/build/building/annotations/).\n\n### [Create attestations (--attest)](#attest)\n\nCreate [image attestations](https://docs.docker.com/build/attestations/). BuildKit currently supports:\n\n*   `sbom` - Software Bill of Materials.\n    \n    Use `--attest=type=sbom` to generate an SBOM for an image at build-time. Alternatively, you can use the [`--sbom` shorthand](#sbom).\n    \n    For more information, see [here](https://docs.docker.com/build/attestations/sbom/).\n    \n*   `provenance` - SLSA Provenance\n    \n    Use `--attest=type=provenance` to generate provenance for an image at build-time. Alternatively, you can use the [`--provenance` shorthand](#provenance).\n    \n    By default, a minimal provenance attestation will be created for the build result, which will only be attached for images pushed to registries.\n    \n    For more information, see [here](https://docs.docker.com/build/attestations/slsa-provenance/).\n    \n\n### [Allow extra privileged entitlement (--allow)](#allow)\n\nAllow extra privileged entitlement. List of entitlements:\n\n*   `network.host` - Allows executions with host networking.\n*   `security.insecure` - Allows executions without sandbox. See [related Dockerfile extensions](https://docs.docker.com/reference/dockerfile/#run---security).\n\nFor entitlements to be enabled, the BuildKit daemon also needs to allow them with `--allow-insecure-entitlement` (see [`create --buildkitd-flags`](https://docs.docker.com/reference/cli/docker/buildx/create/#buildkitd-flags)).\n\n### [Set build-time variables (--build-arg)](#build-arg)\n\nSame as [`docker build` command](https://docs.docker.com/reference/cli/docker/image/build/#build-arg).\n\nThere are also useful built-in build arguments, such as:\n\n*   `BUILDKIT_CONTEXT_KEEP_GIT_DIR=<bool>`: trigger git context to keep the `.git` directory\n*   `BUILDKIT_INLINE_CACHE=<bool>`: inline cache metadata to image config or not\n*   `BUILDKIT_MULTI_PLATFORM=<bool>`: opt into deterministic output regardless of multi-platform output or not\n\nLearn more about the built-in build arguments in the [Dockerfile reference docs](https://docs.docker.com/reference/dockerfile/#buildkit-built-in-build-args).\n\n### [Additional build contexts (--build-context)](#build-context)\n\nDefine additional build context with specified contents. In Dockerfile the context can be accessed when `FROM name` or `--from=name` is used. When Dockerfile defines a stage with the same name it is overwritten.\n\nThe value can be a local source directory, [local OCI layout compliant directory](https://github.com/opencontainers/image-spec/blob/main/image-layout.md), container image (with docker-image:// prefix), Git or HTTP URL.\n\nReplace `alpine:latest` with a pinned one:\n\nExpose a secondary local source directory:\n\n#### [Use an OCI layout directory as build context](#source-oci-layout)\n\nSource an image from a local [OCI layout compliant directory](https://github.com/opencontainers/image-spec/blob/main/image-layout.md), either by tag, or by digest:\n\nThe OCI layout directory must be compliant with the [OCI layout specification](https://github.com/opencontainers/image-spec/blob/main/image-layout.md). You can reference an image in the layout using either tags, or the exact digest.\n\n### [Override the configured builder instance (--builder)](#builder)\n\nSame as [`buildx --builder`](https://docs.docker.com/reference/cli/docker/buildx/#builder).\n\n### [Use an external cache source for a build (--cache-from)](#cache-from)\n\nUse an external cache source for a build. Supported types are `registry`, `local`, `gha` and `s3`.\n\n*   [`registry` source](https://github.com/moby/buildkit#registry-push-image-and-cache-separately) can import cache from a cache manifest or (special) image configuration on the registry.\n*   [`local` source](https://github.com/moby/buildkit#local-directory-1) can import cache from local files previously exported with `--cache-to`.\n*   [`gha` source](https://github.com/moby/buildkit#github-actions-cache-experimental) can import cache from a previously exported cache with `--cache-to` in your GitHub repository\n*   [`s3` source](https://github.com/moby/buildkit#s3-cache-experimental) can import cache from a previously exported cache with `--cache-to` in your S3 bucket\n\nIf no type is specified, `registry` exporter is used with a specified reference.\n\n`docker` driver currently only supports importing build cache from the registry.\n\nMore info about cache exporters and available attributes: [https://github.com/moby/buildkit#export-cache](https://github.com/moby/buildkit#export-cache)\n\n### [Export build cache to an external cache destination (--cache-to)](#cache-to)\n\nExport build cache to an external cache destination. Supported types are `registry`, `local`, `inline`, `gha` and `s3`.\n\n*   [`registry` type](https://github.com/moby/buildkit#registry-push-image-and-cache-separately) exports build cache to a cache manifest in the registry.\n*   [`local` type](https://github.com/moby/buildkit#local-directory-1) exports cache to a local directory on the client.\n*   [`inline` type](https://github.com/moby/buildkit#inline-push-image-and-cache-together) writes the cache metadata into the image configuration.\n*   [`gha` type](https://github.com/moby/buildkit#github-actions-cache-experimental) exports cache through the [GitHub Actions Cache service API](https://github.com/tonistiigi/go-actions-cache/blob/master/api.md#authentication).\n*   [`s3` type](https://github.com/moby/buildkit#s3-cache-experimental) exports cache to a S3 bucket.\n\nThe `docker` driver only supports cache exports using the `inline` and `local` cache backends.\n\nAttribute key:\n\n*   `mode` - Specifies how many layers are exported with the cache. `min` on only exports layers already in the final build stage, `max` exports layers for all stages. Metadata is always exported for the whole build.\n\nMore info about cache exporters and available attributes: [https://github.com/moby/buildkit#export-cache](https://github.com/moby/buildkit#export-cache)\n\n### [Load the single-platform build result to `docker images` (--load)](#load)\n\nShorthand for [`--output=type=docker`](#docker). Will automatically load the single-platform build result to `docker images`.\n\n### [Write build result metadata to a file (--metadata-file)](#metadata-file)\n\nTo output build metadata such as the image digest, pass the `--metadata-file` flag. The metadata will be written as a JSON object to the specified file. The directory of the specified file must already exist and be writable.\n\n> **Note**\n> \n> Build record [provenance](https://docs.docker.com/build/attestations/slsa-provenance/#provenance-attestation-example) (`buildx.build.provenance`) includes minimal provenance by default. Set the `BUILDX_METADATA_PROVENANCE` environment variable to customize this behavior:\n> \n> *   `min` sets minimal provenance (default).\n> *   `max` sets full provenance.\n> *   `disabled`, `false` or `0` does not set any provenance.\n\n### [Ignore build cache for specific stages (--no-cache-filter)](#no-cache-filter)\n\nThe `--no-cache-filter` lets you specify one or more stages of a multi-stage Dockerfile for which build cache should be ignored. To specify multiple stages, use a comma-separated syntax:\n\nFor example, the following Dockerfile contains four stages:\n\n*   `base`\n*   `install`\n*   `test`\n*   `release`\n\nTo ignore the cache for the `install` stage:\n\nTo ignore the cache the `install` and `release` stages:\n\nThe arguments for the `--no-cache-filter` flag must be names of stages.\n\n### [Set the export action for the build result (-o, --output)](#output)\n\nSets the export action for the build result. In `docker build` all builds finish by creating a container image and exporting it to `docker images`. `buildx` makes this step configurable allowing results to be exported directly to the client, OCI image tarballs, registry etc.\n\nBuildx with `docker` driver currently only supports local, tarball exporter and image exporter. `docker-container` driver supports all the exporters.\n\nIf just the path is specified as a value, `buildx` will use the local exporter with this path as the destination. If the value is \"-\", `buildx` will use `tar` exporter and write to `stdout`.\n\n> \\*\\*Note \\*\\*\n> \n> Since BuildKit v0.13.0 multiple outputs can be specified by repeating the flag.\n\nSupported exported types are:\n\n#### [`local`](#local)\n\nThe `local` export type writes all result files to a directory on the client. The new files will be owned by the current user. On multi-platform builds, all results will be put in subdirectories by their platform.\n\nAttribute key:\n\n*   `dest` - destination directory where files will be written\n\n#### [`tar`](#tar)\n\nThe `tar` export type writes all result files as a single tarball on the client. On multi-platform builds all results will be put in subdirectories by their platform.\n\nAttribute key:\n\n*   `dest` - destination path where tarball will be written. “-” writes to stdout.\n\n#### [`oci`](#oci)\n\nThe `oci` export type writes the result image or manifest list as an [OCI image layout](https://github.com/opencontainers/image-spec/blob/v1.0.1/image-layout.md) tarball on the client.\n\nAttribute key:\n\n*   `dest` - destination path where tarball will be written. “-” writes to stdout.\n\n#### [`docker`](#docker)\n\nThe `docker` export type writes the single-platform result image as a [Docker image specification](https://github.com/docker/docker/blob/v20.10.2/image/spec/v1.2.md) tarball on the client. Tarballs created by this exporter are also OCI compatible.\n\nThe default image store in Docker Engine doesn't support loading multi-platform images. You can enable the containerd image store, or push multi-platform images is to directly push to a registry, see [`registry`](#registry).\n\nAttribute keys:\n\n*   `dest` - destination path where tarball will be written. If not specified, the tar will be loaded automatically to the local image store.\n*   `context` - name for the Docker context where to import the result\n\n#### [`image`](#image)\n\nThe `image` exporter writes the build result as an image or a manifest list. When using `docker` driver the image will appear in `docker images`. Optionally, image can be automatically pushed to a registry by specifying attributes.\n\nAttribute keys:\n\n*   `name` - name (references) for the new image.\n*   `push` - Boolean to automatically push the image.\n\n#### [`registry`](#registry)\n\nThe `registry` exporter is a shortcut for `type=image,push=true`.\n\n### [Set the target platforms for the build (--platform)](#platform)\n\nSet the target platform for the build. All `FROM` commands inside the Dockerfile without their own `--platform` flag will pull base images for this platform and this value will also be the platform of the resulting image.\n\nThe default value is the platform of the BuildKit daemon where the build runs. The value takes the form of `os/arch` or `os/arch/variant`. For example, `linux/amd64` or `linux/arm/v7`. Additionally, the `--platform` flag also supports a special `local` value, which tells BuildKit to use the platform of the BuildKit client that invokes the build.\n\nWhen using `docker-container` driver with `buildx`, this flag can accept multiple values as an input separated by a comma. With multiple values the result will be built for all of the specified platforms and joined together into a single manifest list.\n\nIf the `Dockerfile` needs to invoke the `RUN` command, the builder needs runtime support for the specified platform. In a clean setup, you can only execute `RUN` commands for your system architecture. If your kernel supports [`binfmt_misc`](https://en.wikipedia.org/wiki/Binfmt_misc) launchers for secondary architectures, buildx will pick them up automatically. Docker desktop releases come with `binfmt_misc` automatically configured for `arm64` and `arm` architectures. You can see what runtime platforms your current builder instance supports by running `docker buildx inspect --bootstrap`.\n\nInside a `Dockerfile`, you can access the current platform value through `TARGETPLATFORM` build argument. Refer to the [`docker build` documentation](https://docs.docker.com/reference/dockerfile/#automatic-platform-args-in-the-global-scope) for the full description of automatic platform argument variants .\n\nYou can find the formatting definition for the platform specifier in the [containerd source code](https://github.com/containerd/containerd/blob/v1.4.3/platforms/platforms.go#L63).\n\n### [Set type of progress output (--progress)](#progress)\n\nSet type of progress output (`auto`, `plain`, `tty`, `rawjson`). Use `plain` to show container output (default `auto`).\n\n> **Note**\n> \n> You can also use the `BUILDKIT_PROGRESS` environment variable to set its value.\n\nThe following example uses `plain` output during the build:\n\n> **Note**\n> \n> Check also the [`BUILDKIT_COLORS`](https://docs.docker.com/build/building/variables/#buildkit_colors) environment variable for modifying the colors of the terminal output.\n\nThe `rawjson` output marshals the solve status events from BuildKit to JSON lines. This mode is designed to be read by an external program.\n\n### [Create provenance attestations (--provenance)](#provenance)\n\nShorthand for [`--attest=type=provenance`](#attest), used to configure provenance attestations for the build result. For example, `--provenance=mode=max` can be used as an abbreviation for `--attest=type=provenance,mode=max`.\n\nAdditionally, `--provenance` can be used with Boolean values to enable or disable provenance attestations. For example, `--provenance=false` disables all provenance attestations, while `--provenance=true` enables all provenance attestations.\n\nBy default, a minimal provenance attestation will be created for the build result. Note that the default image store in Docker Engine doesn't support attestations. Provenance attestations only persist for images pushed directly to a registry if you use the default image store. Alternatively, you can switch to using the containerd image store.\n\nFor more information about provenance attestations, see [here](https://docs.docker.com/build/attestations/slsa-provenance/).\n\n### [Push the build result to a registry (--push)](#push)\n\nShorthand for [`--output=type=registry`](#registry). Will automatically push the build result to registry.\n\n### [Create SBOM attestations (--sbom)](#sbom)\n\nShorthand for [`--attest=type=sbom`](#attest), used to configure SBOM attestations for the build result. For example, `--sbom=generator=<user>/<generator-image>` can be used as an abbreviation for `--attest=type=sbom,generator=<user>/<generator-image>`.\n\nAdditionally, `--sbom` can be used with Boolean values to enable or disable SBOM attestations. For example, `--sbom=false` disables all SBOM attestations.\n\nNote that the default image store in Docker Engine doesn't support attestations. Provenance attestations only persist for images pushed directly to a registry if you use the default image store. Alternatively, you can switch to using the containerd image store.\n\nFor more information, see [here](https://docs.docker.com/build/attestations/sbom/).\n\n### [Secret to expose to the build (--secret)](#secret)\n\nExposes secrets (authentication credentials, tokens) to the build. A secret can be mounted into the build using a `RUN --mount=type=secret` mount in the [Dockerfile](https://docs.docker.com/reference/dockerfile/#run---mounttypesecret). For more information about how to use build secrets, see [Build secrets](https://docs.docker.com/build/building/secrets/).\n\nSupported types are:\n\n*   [`file`](#file)\n*   [`env`](#env)\n\nBuildx attempts to detect the `type` automatically if unset.\n\n#### [`file`](#file)\n\nAttribute keys:\n\n*   `id` - ID of the secret. Defaults to base name of the `src` path.\n*   `src`, `source` - Secret filename. `id` used if unset.\n\n#### [`env`](#env)\n\nAttribute keys:\n\n*   `id` - ID of the secret. Defaults to `env` name.\n*   `env` - Secret environment variable. `id` used if unset, otherwise will look for `src`, `source` if `id` unset.\n\n### [Shared memory size for build containers (--shm-size)](#shm-size)\n\nSets the size of the shared memory allocated for build containers when using `RUN` instructions.\n\nThe format is `<number><unit>`. `number` must be greater than `0`. Unit is optional and can be `b` (bytes), `k` (kilobytes), `m` (megabytes), or `g` (gigabytes). If you omit the unit, the system uses bytes.\n\n> **Note**\n> \n> In most cases, it is recommended to let the builder automatically determine the appropriate configurations. Manual adjustments should only be considered when specific performance tuning is required for complex build scenarios.\n\n### [SSH agent socket or keys to expose to the build (--ssh)](#ssh)\n\nThis can be useful when some commands in your Dockerfile need specific SSH authentication (e.g., cloning a private repository).\n\n`--ssh` exposes SSH agent socket or keys to the build and can be used with the [`RUN --mount=type=ssh` mount](https://docs.docker.com/reference/dockerfile/#run---mounttypessh).\n\nExample to access Gitlab using an SSH agent socket:\n\n### [Set ulimits (--ulimit)](#ulimit)\n\n`--ulimit` overrides the default ulimits of build's containers when using `RUN` instructions and are specified with a soft and hard limit as such: `<type>=<soft limit>[:<hard limit>]`, for example:\n\n> **Note**\n> \n> If you don't provide a `hard limit`, the `soft limit` is used for both values. If no `ulimits` are set, they're inherited from the default `ulimits` set on the daemon.\n\n> **Note**\n> \n> In most cases, it is recommended to let the builder automatically determine the appropriate configurations. Manual adjustments should only be considered when specific performance tuning is required for complex build scenarios.",
  "title": "docker buildx build | Docker Docs\n",
  "description": "",
  "languageCode": "en"
}]