[
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.12/get-started/profile-builder/",
    "markdown": "# Profile Builder CLI | RudderStack Docs\n\nCreate a Profiles project using the Profile Builder (PB) tool.\n\n* * *\n\n*     10 minute read  \n    \n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> While creating a Profiles project, you can choose either of the below:\n> \n> *   **Profile Builder (PB) CLI** which gives you the flexibility to create, develop, and debug your Profiles project using various commands in fine detail. You can explore and implement the exhaustive list of features and functionalities offered by Profiles.\n> *   **Profiles UI** which provides a step-by-step intuitive workflow in the RudderStack dashboard. You can configure your project, schedule its run, explore the outputs and the user profiles.\n\n**Profile Builder (PB)** is a command-line interface (CLI) tool that simplifies data transformation within your warehouse. It generates customer profiles by stitching data together from multiple sources.\n\nThis guide lists the detailed steps to install and use the Profile Builder (PB) tool to create, configure, and run a new project.\n\n## Prerequisites\n\nYou must have:\n\n*   [Python 3](https://www.python.org/downloads/) installed on your machine.\n*   Admin privileges on your machine.\n\n## Steps\n\nTo set up a project using the PB tool, follow these steps:\n\n### 1: Install PB\n\nInstall the Profile Builder tool by running the following command:\n\n```\npip3 install profiles-rudderstack\n```\n\nIf you have already installed PB, use the following command to update its version:\n\n```\npip3 install profiles-rudderstack -U\n```\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> RudderStack recommends using a Python virtual environment to maintain an isolated and clean environment.\n> \n> ```\n> pipx install profiles-rudderstack\n> ```\n\nValidate Profile Builder’s version after install using:\n\nSee also: [Setup and installation FAQ](https://www.rudderstack.com/docs/archive/profiles/0.12/faq/#setup-and-installation)\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> If you are an existing user, migrate your project to the new schema. See [Migrate your existing project](#migrate-your-existing-project) for more information.\n\n### 2: Create warehouse connection\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> RudderStack supports **Snowflake**, **Redshift**, **BigQuery**, and **Databricks** warehouses for Profiles. You must grant certain [warehouse permissions](https://www.rudderstack.com/docs/archive/profiles/0.12/permissions/) to let RudderStack read from schema having the source tables (for example, `tracks` and `identifies` tables generated via Event Stream sources), and write data in a new schema created for Profiles.\n\nCreate a warehouse connection to allow PB to access your data:\n\nThen, follow the prompts to enter details about your warehouse connection.\n\nA sample connection for a Snowflake account is as follows:\n\n```\nEnter Connection Name: test\nEnter target:  (default:dev):  # Press enter, leaving it to default\nEnter account: ina13147.us-east-1\nEnter warehouse: rudder_warehouse\nEnter dbname: your_rudderstack_db \nEnter schema: rs_profiles # A segregated schema for storing tables/views created by Profiles\nEnter user: profiles_test_user\nEnter password: <password>\nEnter role: profiles_role\nAppend to /Users/<user_name>/.pb/siteconfig.yaml? [y/N]\ny\n```\n\n*   **Connection Name**: Name of the connection in the project file.\n*   **Target**: Environment name, such as `dev`, `prod`, `test`, etc. You can specify any target name and create a separate connection for the same.\n*   **Account**: See [Snowflake documentation](https://docs.snowflake.com/en/user-guide/admin-account-identifier.html) for more information.\n*   **Warehouse**: Name of the warehouse.\n*   **Database name**: Name of the database inside warehouse where model outputs will be written.\n*   **Schema**: Name of the schema inside database where you’ll store identity stitcher and entity features.\n*   **User**: Name of the user in data warehouse.\n*   **Password**: Password for the above user.\n*   **Role**: Name of the user role.\n\nRudderStack supports various user authentication mechanisms for Redshift:\n\n```\nEnter Connection Name: test\nEnter target:  (default:dev):  # Press enter, leaving it to default\nEnter dbname: your_rudderstack_db \nEnter schema: rs_profiles # A segregated schema for storing tables/views created by Profiles\nEnter user: profiles_test_user\nEnter sslmode: options - [disable require]: disable # Enter \"require\" in case your Redshift connection mandates sslmode. \nHow would you like to authenticate with the warehouse service? Please select your method by entering the corresponding number:\ny\n[1] Warehouse Credentials: Log in using your username and password.\n        Format: [Username, Password]\n[2] AWS Programmatic Credentials: Authenticate using one of the following AWS credentials methods:\n        a) Direct input of AWS Access Key ID, Secret Access Key, and an optional Session Token.\n                Format: [AWS Access Key ID, Secret Access Key, Session Token (Optional)]\n        b) AWS configuration profile stored on your system.\n                Format: [AWS Configuration Profile Name]\n        c) Use an AWS Secrets Manager ARN to securely retrieve credentials.\n                Format: [Secret ARN]\n```\n\n*   **Connection Name**: Name of the connection in the project file.\n*   **Target**: Environment name, such as `dev`, `prod`, `test`, etc. You can specify any target name and create a separate connection for the same.\n*   **Host**: Log in to AWS Console and go to **Clusters** to know about host.\n*   **Port**: Port number to connect to the warehouse.\n*   **Database name**: Name of the database inside warehouse where model outputs will be written.\n*   **Schema**: Name of the schema inside database where you’ll store identity stitcher and entity features.\n*   **User**: Name of the user in data warehouse.\n*   **Password**: Password for the above user.\n\n### Warehouse credentials\n\nIf you choose to use the warehouse credentials (option 1), enter the following details:\n\n```\nEnter host: warehouseabc.us-west-1.redshift.amazonaws.com\nEnter port: 5439\nEnter password: <password>\nEnter tunnel_info: Do you want to use SSH Tunnel to connect with the warehouse? (y/n): y\nEnter ssh user: user\nEnter ssh host: 1234\nEnter ssh port: 12345\nEnter ssh private key file path: /Users/alex/.ssh/id_ed25519.pub\nAppend to /Users/alex/.pb/siteconfig.yaml? [y/N] y\n```\n\n### AWS Programmatic Credentials\n\nIf you choose to use AWS Access Key ID, Secret Access Key, and an optional Session Token from AWS Programmatic Credentials (option 2a), enter the following details:\n\n**For Redshift Cluster**\n\n```\nWhich Redshift Service you want to connect with?\n        [c]Redshift Cluster\n        [s]Redshift Serverless\nc\nEnter Redshift Cluster Identifier: cluster-id\nEnter access_key_id: aid\nEnter secret_access_key: ***\nEnter session_token: If you are using temporary security credentials, please specify the session token, otherwise leave it empty.\nstoken\nAppend to /Users/alex/.pb/siteconfig.yaml? [y/N] y\n```\n\n**For Redshift Serverless**\n\n```\nWhich Redshift Service you want to connect with?\n        [c]Redshift Cluster\n        [s]Redshift Serverless\ns\nEnter Redshift Serverless Workgroup Name: wg-name\nEnter access_key_id: aid\nEnter secret_access_key: ***\nEnter session_token: If you are using temporary security credentials, please specify the session token, otherwise leave it empty.\nAppend to /Users/alex/.pb/siteconfig.yaml? [y/N] y\n```\n\nIf you choose to use AWS configuration profile stored on your system from AWS Programmatic Credentials (option 2b), enter the following details:\n\n**For Redshift Cluster**\n\n```\nWhich Redshift Service you want to connect with?\n        [c]Redshift Cluster\n        [s]Redshift Serverless\nc\nEnter Redshift Cluster Identifier: ci   \nEnter shared_profile: default\nEnter region: us-east-1\nAppend to /Users/alex/.pb/siteconfig.yaml? [y/N] y\n```\n\n**For Redshift Serverless**\n\n```\nWhich Redshift Service you want to connect with?\n        [c]Redshift Cluster\n        [s]Redshift Serverless\ns\nEnter Redshift Serverless Workgroup Name: serverless-wg\nEnter shared_profile: default\nAppend to /Users/alex/.pb/siteconfig.yaml? [y/N] y\n```\n\nIf you choose to use an AWS Secrets Manager ARN to securely retrieve credentials from AWS Programmatic Credentials (option 2c), enter the following details:\n\n**For Redshift Cluster**\n\n```\nWhich Redshift Service you want to connect with?\n        [c]Redshift Cluster\n        [s]Redshift Serverless\nc\nEnter Redshift Cluster Identifier: cluster-identifier\nEnter secrets_arn: ************************\nAppend to /Users/alex/.pb/siteconfig.yaml? [y/N] y\n```\n\nA sample connection for a Databricks account is as follows:\n\n```\nEnter Connection Name: test\nEnter target:  (default:dev):  # Press enter, leaving it to default\nEnter host: a1.8.azuredatabricks.net # The hostname or URL of your Databricks cluster\nEnter port: 443 # The port number used for establishing the connection. Usually it is 443 for https connections.\nEnter http_endpoint: /sql/1.0/warehouses/919uasdn92h # The path or specific endpoint you wish to connect to.\nEnter access_token: <password> # The access token created for authenticating the instance.\nEnter user: profiles_test_user # Username of your Databricks account.\nEnter schema: rs_profiles # A segregated schema for storing tables/views created by Profiles\nEnter catalog: your_rudderstack_db # The database or catalog having data that you’ll be accessing.\nAppend to /Users/<user_name>/.pb/siteconfig.yaml? [y/N]\ny\n```\n\n*   **Connection Name**: Name of the connection in the project file.\n*   **Target**: Environment name, such as `dev`, `prod`, `test`, etc. You can specify any target name and create a separate connection for the same.\n*   **Host**: Host name or URL of your Databricks cluster.\n*   **Port**: Port number for establishing the connection, usually `443` for `https` connections.\n*   **http\\_endpoint**: Path or specific endpoint you wish to connect to.\n*   **access\\_token**: Access token for authenticating the instance.\n*   **User**: Username of your Databricks account.\n*   **Schema**: Name of the schema to store your output tables/views.\n*   **Catalog**: Name of the database or catalog from where you want to access the data.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> RudderStack currently supports Databricks on Azure. To get the Databricks connection details:\n> \n> 1.  Log in to your Azure’s Databricks Web UI.\n> 2.  Click on **SQL Warehouses** on the left.\n> 3.  Select the warehouse to connect to.\n> 4.  Select the **Connection Details** tab.\n\nA sample connection for a BigQuery account is as follows:\n\n```\nEnter Connection Name: test\nEnter target:  (default:dev):  # Press enter, leaving it to default\nEnter credentials: json file path: # File path of your BQ JSON file, for example, /Users/alexm/Downloads/big.json. Entering an incorrect path will exit the program.\nEnter project_id: profiles121\nEnter schema: rs_profiles\nAppend to /Users/<user_name>/.pb/siteconfig.yaml? [y/N]\ny\n```\n\nThis creates a [site configuration file](https://www.rudderstack.com/docs/archive/profiles/0.12/cli-user-guide/structure/) inside your home directory: `~/.pb/siteconfig.yaml`. If you don’t see the file, enable the **View hidden files** option.\n\n### 3: Create project\n\nRun the following command to create a sample project:\n\n```\npb init pb-project -o MyProfilesProject\n```\n\nThe above command creates a new project in the **MyProfilesProject** folder with the following structure:\n\n[![Project structure](https://www.rudderstack.com/docs/images/profiles/project-structure.webp)](https://www.rudderstack.com/docs/images/profiles/project-structure.webp)\n\nSee [Project structure](https://www.rudderstack.com/docs/archive/profiles/0.12/cli-user-guide/structure/) for more information on the PB project files.\n\nNavigate to the `pb_project.yaml` file and set the value of `connection:` to the connection name as defined in the previous step.\n\n### 4: Change input sources\n\n*   Navigate to your project and open the `models/inputs.yaml` file. Here, you will see a list of tables/views along with their respective ID types.\n*   Replace the placeholder table names with the actual table names in the `table` field.\n\nSee [Project structure](https://www.rudderstack.com/docs/archive/profiles/0.12/cli-user-guide/structure/#inputs) for more information on setting these values.\n\n### 5: Validate project\n\nNavigate to your project and validate your warehouse connection and [inputInputs refers to the input data sources used to create the material (output) tables in the warehouse.](https://www.rudderstack.com/docs/resources/glossary/#input) sources:\n\nIf there are no errors, proceed to the next step. In case of errors, check if your warehouse schemas and tables have the [required permissions](https://www.rudderstack.com/docs/archive/profiles/0.12/permissions/).\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> Currently, this command is not supported for BigQuery warehouse.\n\n### 6: Generate SQL files\n\nCompile the project:\n\nThis generates SQL files in the `output/` folder that you can run directly on the warehouse.\n\n### 7: Generate output tables\n\nRun the project and generate [material tables](https://www.rudderstack.com/docs/archive/profiles/0.12/resources/glossary/#material-tables):\n\nThis command generates and runs the SQL files in the warehouse, creating the material tables.\n\n### 8: View generated tables\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> The view `user_default_id_stitcher` will always point to the latest generated ID stitcher and `user_profile` to the latest feature table.\n\nYou can run the `pb show models` command to get the exact name and path of the generated ID stitcher/feature table. See [show](https://www.rudderstack.com/docs/archive/profiles/0.12/cli-user-guide/commands/#show) command for more information.\n\nThen, execute the below query to view the generated tables in the warehouse:\n\n```\nselect * from <table_name> limit 10;\n```\n\nHere’s what the columns imply:\n\n![ID Stitcher Table](https://www.rudderstack.com/docs/images/profiles/idstitcher-table.webp)\n\n*   **user\\_main\\_id**: Rudder ID generated by Profile Builder. Think of a 1-to-many relationship, with one Rudder ID connected to different IDs belonging to same user such as User ID, Anonymous ID, Email, Phone number, etc.\n*   **other\\_id**: ID in input source tables that is stitched to a Rudder ID.\n*   **other\\_id\\_type**: Type of the other ID to be stitched (User ID, Anonymous ID, Email, etc).\n*   **valid\\_at**: Date at which the corresponding ID value occurred in the source tables. For example, the date at which a customer was first browsing anonymously, or when they logged into the CRM with their email ID, etc.\n\n![Feature Table](https://www.rudderstack.com/docs/images/profiles/feature-table.webp)\n\n*   **user\\_main\\_id**: Rudder ID generated by Profile Builder.\n*   **valid\\_at**: Date when the feature table entry was created for this record.\n*   **first\\_seen, last\\_seen, country, first\\_name, etc.** - All features for which values are computed.\n\n## Migrate your existing project\n\nTo migrate an existing PB project to the [schema version](https://www.rudderstack.com/docs/archive/profiles/0.12/resources/glossary/#schema-versions) supported by your PB binary, navigate to your project’s folder. Then, run the following command to replace the contents of the existing folder with the new one:\n\n```\npb migrate auto --inplace\n```\n\nA confirmation message appears on screen indicating that the migration is complete. A sample message for a user migrating their project from version 25 to 44:\n\n```\n2023-10-17T17:48:33.104+0530\tINFO\tmigrate/migrate.go:161\t\nProject migrated from version 25 to version 44\n```\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Profile Builder CLI | RudderStack Docs",
    "description": "Create a Profiles project using the Profile Builder (PB) tool.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.12/get-started/quickstart-ui/",
    "markdown": "# Profiles UI | RudderStack Docs\n\nCreate your Profiles project from the RudderStack dashboard.\n\n* * *\n\n*     3 minute read  \n    \n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> While creating a Profiles project, you can choose either of the below:\n> \n> *   **Profile Builder (PB) CLI** which gives you the flexibility to create, develop, and debug your Profiles project using various commands in fine detail. You can explore and implement the exhaustive list of features and functionalities offered by Profiles.\n> *   **Profiles UI** which provides a step-by-step intuitive workflow in the RudderStack dashboard. You can configure your project, schedule its run, explore the outputs and the user profiles.\n\nThis guide lists the detailed steps to create a Profiles project in the RudderStack dashboard.\n\n## Create Profiles project\n\n1.  Log in to the [RudderStack dashboard](https://app.rudderstack.com/) and go to **Unify** > **Profiles** option in the left sidebar.\n2.  Click **Create project**.\n\n[![Choose predefined template](https://www.rudderstack.com/docs/images/profiles/create-project.webp)](https://www.rudderstack.com/docs/images/profiles/create-project.webp)\n\n3.  Select **Basic Entity Setup** on the next screen.\n\n[![Choose predefined template](https://www.rudderstack.com/docs/images/profiles/basic-setup-profiles.webp)](https://www.rudderstack.com/docs/images/profiles/basic-setup-profiles.webp)\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> See [Import from Git](https://www.rudderstack.com/docs/archive/profiles/0.12/get-started/import-from-git/) for more information on **Import project from Git Repo** option.\n\n4.  Enter a unique name and description for your Profiles project.\n5.  Select the data warehouse from the dropdown and the source(s) connected to the warehouse.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> RudderStack currently supports the [Snowflake](https://www.rudderstack.com/docs/sources/reverse-etl/snowflake/#configuring-the-connection-credentials), [Redshift](https://www.rudderstack.com/docs/sources/reverse-etl/amazon-redshift/#configuring-the-connection-credentials), [Databricks](https://www.rudderstack.com/docs/sources/reverse-etl/databricks/#configuring-the-connection-credentials), and [BigQuery](https://www.rudderstack.com/docs/sources/reverse-etl/google-bigquery/#configuring-the-connection-credentials) warehouses for creating a Profiles project. You need to set up the warehouse source in the first place to populate it here in the dropdown.\n\n[![Choose predefined template](https://www.rudderstack.com/docs/images/profiles/source.webp)](https://www.rudderstack.com/docs/images/profiles/source.webp)\n\n6.  Map the identifiers from above-selected source by selecting the source, event, property, and ID type by clicking **Add mapping**:\n\n[![Choose predefined template](https://www.rudderstack.com/docs/images/profiles/map-id.webp)](https://www.rudderstack.com/docs/images/profiles/map-id.webp)\n\n7.  Define features either by adding a custom feature or selecting a pre-defined feature as shown:\n\n[![Choose predefined template](https://www.rudderstack.com/docs/images/profiles/define-features.webp)](https://www.rudderstack.com/docs/images/profiles/define-features.webp)\n\n*   To add a custom feature, click **Add a custom feature** and enter the relevant feature details:\n\n[![Choose predefined template](https://www.rudderstack.com/docs/images/profiles/custom-feature.webp)](https://www.rudderstack.com/docs/images/profiles/custom-feature.webp)\n\n*   To use a pre-defined feature, select the required feature from the **Template features library**.\n\n8.  Select the [schedule type](https://www.rudderstack.com/docs/sources/reverse-etl/sync-schedule-settings/).\n9.  Enter the warehouse details where you want to store this Profiles project.\n10.  Finally, review all the provided details and click **Create Profiles project**.\n\n## Download project\n\nTo download the Profiles project, click **View** corresponding to your Profiles project and click **Download this project**:\n\n[![Download Profiles project](https://www.rudderstack.com/docs/images/profiles/download-profiles-project.webp)](https://www.rudderstack.com/docs/images/profiles/download-profiles-project.webp)\n\nOnce downloaded, you can view the [project folder structure](https://www.rudderstack.com/docs/archive/profiles/0.12/cli-user-guide/structure/), modify the files, or run various [commands](https://www.rudderstack.com/docs/archive/profiles/0.12/cli-user-guide/commands/) to execute the desired use-cases.\n\n## Profile details\n\nTo view the profile details, click **View** corresponding to your Profiles project:\n\n| Option | Description |\n| --- | --- |\n| **Overview** | Lists the features of your Profiles project. |\n| **History** | Displays the history of Profile runs. |\n| **Explorer** | Displays the preview of first 50 rows of the output tables (features and ID stitching) only after the successful run of the project. You can also search for all the records, by typing in a unique identifier such as user\\_id, email, etc. |\n| **Settings** | Displays your profile settings and lets you delete your Profiles project. You can edit the project by clicking the edit icon next to each section. |\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> Note that:\n> \n> *   It can take up to thirty minutes for the data preview to show up in your profile’s **History** tab.\n> *   If you keep getting a blank screen, it may be because you do not have sufficient access. Make sure you have a [Connections Admin](https://www.rudderstack.com/docs/dashboard-guides/user-management/#resource-roles) resource role with [access to PII](https://www.rudderstack.com/docs/dashboard-guides/data-management/#limiting-access-to-pii-related-features). In case the problem persists, contact [RudderStack support](mailto:support@rudderstack.com).\n\n## FAQ\n\n**When trying to fetch data for a lib project, then data/columns are shown as blank. What should I do?**\n\nYou’ll need to sync data from a source to a destination. If data is synced from the source you are using and not from some pre-existing tables in the destination, the missing column/data issues should not occur.\n\n**I am not able to see Unify tab on the web app though I have admin privileges. What should I do?**\n\nDisable any adblockers on your web browser.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Profiles UI | RudderStack Docs",
    "description": "Create your Profiles project from the RudderStack dashboard.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.12/get-started/",
    "markdown": "# Quickstart | RudderStack Docs\n\nPrivacy Overview\n\nThis site uses cookies to improve your experience while you navigate through the website. Out of these cookies, the cookies that are categorized as necessary are stored on your browser as they are as essential for the working of basic functionalities of the website. We also use third-party cookies that help us analyze and understand how you use this website. These cookies will be stored in your browser only with your consent. You also have the option to opt-out of these cookies. But opting out of some of these cookies may have an effect on your browsing experience.\n\nNecessary cookies are absolutely essential for the website to function properly. This category only includes cookies that ensures basic functionalities and security features of the website. These cookies do not store any personal information.",
    "title": "Quickstart | RudderStack Docs",
    "description": "Know the different ways to create a Profiles project.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.12/get-started/import-from-git/",
    "markdown": "# Import Profiles Project from Git\n\nPrivacy Overview\n\nThis site uses cookies to improve your experience while you navigate through the website. Out of these cookies, the cookies that are categorized as necessary are stored on your browser as they are as essential for the working of basic functionalities of the website. We also use third-party cookies that help us analyze and understand how you use this website. These cookies will be stored in your browser only with your consent. You also have the option to opt-out of these cookies. But opting out of some of these cookies may have an effect on your browsing experience.\n\nNecessary cookies are absolutely essential for the website to function properly. This category only includes cookies that ensures basic functionalities and security features of the website. These cookies do not store any personal information.",
    "title": "Import Profiles Project from Git | RudderStack Docs",
    "description": "Import an existing Profiles project from the Git repository in RudderStack dashboard.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.13/example/feature-table/",
    "markdown": "# Feature Table | RudderStack Docs\n\nStep-by-step tutorial on creating a feature table model.\n\n* * *\n\n*     9 minute read  \n    \n\nOnce you have done [identity stitching](https://www.rudderstack.com/docs/archive/profiles/0.13/core-concepts/identity-stitching/) to unify the identity of your users across all the cross-platforms, you can evaluate and maintain the required features/traits for each identified user in a feature table.\n\nThis guide provides a detailed walkthrough on how to use a PB project and create output tables in a warehouse for a feature table model.\n\n## Prerequisites\n\nFamiliarize yourself with:\n\n*   A basic Profile Builder project by following the [Profile Builder CLI](https://www.rudderstack.com/docs/archive/profiles/0.13/get-started/profile-builder/) steps.\n*   [Structure](https://www.rudderstack.com/docs/archive/profiles/0.13/cli-user-guide/structure/) of a Profile Builder project and the parameters used in different files.\n*   [Identity Stitching](https://www.rudderstack.com/docs/archive/profiles/0.13/example/id-stitcher/) model as feature table reuses its output to extract the required features/traits.\n\n## Sample project\n\nThis sample project uses the output of an identity stitching model as an input to create a feature table. The following sections describe how to define your PB project files:\n\n### Project detail\n\nThe [`pb_project.yaml`](https://www.rudderstack.com/docs/archive/profiles/0.13/cli-user-guide/structure/#project-details) file defines the project details such as name, schema version, connection name and the entities which represent different identifiers.\n\nYou can define all the identifiers from different input sources you want to stitch together as a `user_main_id`:\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> You need to add `main_id` to the list only if you have defined `main_id_type: main_id` in the ID stitcher buildspec.\n\n```\n# Project name\nname: sample_id_stitching\n# Project's yaml schema version\nschema_version: 63\n# Warehouse connection\nconnection: test\n# Folder containing models\nmodel_folders:\n  - models\n# Entities in this project and their ids.\nentities:\n  - name: user\n    id_types:\n      - main_id # You need to add ``main_id`` to the list only if you have defined ``main_id_type: main_id`` in the id stitcher buildspec.\n      - user_id # one of the identifier from your data source.\n      - email\n# lib packages can be imported in project signifying that this project inherits its properties from there\npackages:\n  - name: corelib\n    url: \"https://github.com/rudderlabs/profiles-corelib/tag/schema_{{best_schema_version}}\"\n    # if required then you can extend the package definition such as for ID types.\n```\n\n### Input\n\nThe [input file](https://www.rudderstack.com/docs/archive/profiles/0.13/cli-user-guide/structure/#inputs) file includes the input table references and corresponding SQL for the above-mentioned entities:\n\n```\ninputs:\n- name: rsIdentifies\n  contract: # constraints that a model adheres to\n    is_optional: false\n    is_event_stream: true\n    with_entity_ids:\n      - user\n    with_columns:\n      - name: timestamp\n      - name: user_id\n      - name: anonymous_id\n      - name: email\n  app_defaults:\n    table: rudder_events_production.web.identifies # one of the WH table RudderStack generates when processing identify or track events.\n    occurred_at_col: timestamp\n    ids:\n      - select: \"user_id\" # kind of identity sql to pick this column from above table.\n        type: user_id\n        entity: user # as defined in project file\n        to_default_stitcher: true\n      - select: \"anonymous_id\"\n        type: anonymous_id\n        entity: user\n        to_default_stitcher: true\n      - select: \"lower(email)\" # can use sql.\n        type: email\n        entity: user\n        to_default_stitcher: true\n- name: rsTracks\n  contract:\n    is_optional: false\n    is_event_stream: true\n    with_entity_ids:\n      - user\n    with_columns:\n      - name: timestamp\n      - name: user_id\n      - name: anonymous_id\n  app_defaults:\n    table: rudder_events_production.web.tracks # another table in WH maintained by RudderStack processing track events.\n    occurred_at_col: timestamp\n    ids:\n      - select: \"user_id\"\n        type: user_id\n        entity: user\n        to_default_stitcher: true\n      - select: \"anonymous_id\"\n        type: anonymous_id\n        entity: user\n        to_default_stitcher: true\n```\n\n### Model\n\nProfiles **Feature Table** model lets you define the specific features/traits you want to evaluate from the huge spread of scattered data in your warehouse tables.\n\nA sample `profiles.yaml` file specifying a feature table model (`user_profile`):\n\n```\nmodels:\n  - name: user_profile\n    model_type: feature_table_model\n    model_spec:\n      validity_time: 24h\n      entity_key: user\n      features:\n        - user_lifespan\n        - days_active\n        - min_num_c_rank_num_b_partition\nvar_groups:\n  - name: user_vars\n    entity_key: user\n    vars:\n      - entity_var:\n          name: first_seen\n          select: min(timestamp::date)\n          from: inputs/rsTracks\n          where: properties_country is not null and properties_country != ''\n      - entity_var:\n          name: last_seen\n          select: max(timestamp::date)\n          from: inputs/rsTracks\n      - entity_var:\n          name: user_lifespan\n          select: '{{user.Var(\"last_seen\")}} - {{user.Var(\"first_seen\")}}'\n          description: Life Time Value of a customer\n      - entity_var:\n          name: days_active\n          select: count(distinct timestamp::date)\n          from: inputs/rsTracks\n          description: No. of days a customer was active\n      - entity_var:\n          name: campaign_source\n          default: \"'organic'\"\n      - entity_var:\n          name: user_rank\n          default: -1\n      - entity_var:\n          name: campaign_source_first_touch\n          select: first_value(context_campaign_source)\n          window:\n            order_by:\n              - timestamp asc\n          from: inputs/rsIdentifies\n          where: context_campaign_source is not null and context_campaign_source != ''\n      - input_var:\n          name: num_c_rank_num_b_partition\n          select: rank()\n          from: inputs/tbl_c\n          default: -1\n          window:\n            partition_by:\n              - \"{{tbl_c}}.num_b\"\n            order_by:\n              - \"{{tbl_c}}.num_c asc\"\n          where: \"{{tbl_c}}.num_b >= 10\"\n      - entity_var:\n          name: min_num_c_rank_num_b_partition\n          select: min(num_c_rank_num_b_partition)\n          from: inputs/tbl_c\n      - entity_var:\n          name: first_bill\n          select: min({{tbl_billing.Var(\"payment\")}})\n          from: inputs/tbl_billing\n          column_data_type: '{{warehouse.DataType(\"float\")}}'\n```\n\n##### Model specification fields\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `validity_time` | Time | Specifies the validity of the model with respect to its timestamp. For example, a model run as part of a scheduled nightly job for 2009-10-23 00:00:00 UTC with `validity_time`: `24h` would still be considered potentially valid and usable for any run requests, which do not require precise timestamps between 2009-10-23 00:00:00 UTC and 2009-10-24 00:00:00 UTC. This specifies the validity of generated feature table. Once the validity is expired, scheduling takes care of generating new tables. For example: `24h` for 24 hours, `30m` for 30 minutes, `3d` for 3 days, and so on. |\n| `entity_key` | String | Specifies the relevant entity from your `input.yaml` file. |\n| `features` | String | Specifies the list of `name` in `entity_var`, that must act as a feature. |\n\n**`entity_var`**\n\nThe `entity_var` field defines the features which act as an input for the feature table model. This variable stores the data temporarily, however, you can choose to store its data permanently by specifying the `name` in it as a feature in the `features` key.\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `name` | String | Name of the `entity_var` to identify it uniquely. |\n| `select` | String | Column name/value you want to select from the table. This defines the actual value that will be stored in the variable. You can use simple SQL expressions or select an `entity_var` as `{{entityName.Var(\\\"entity_var\\\")}}`. It has to be an aggregate operation that ensures the output is a unique value for a given `main_id`. For example: min(timestamp), count(\\*), sum(amount) etc. This holds true even when a window function (optional) is used. For example:: first\\_value(), last\\_value() etc are valid while rank(), row\\_number(), etc. are not valid and give unpredictable results. |\n| `from` | List | Reference to the source table from where data is to be fetched. You can either refer to another model from the same YAML or some other table specified in input YAML. |\n| `where` | String | Any filters you want to apply on the input table before selecting a value. This must be SQL compatible and should consider the data type of the table. |\n| `default` | String | Default value in case no data matches the filter. When defining default values, make sure you enclose the string values in single quotes followed by double quotes to avoid SQL failure. However, you can use the non-string values without any quotes. |\n| `description` | String | Textual description of the `entity_var`. |\n| `window` | Object | Specifies the window function. Window functions in SQL usually have both `partition_by` and `order_by` properties. But for `entity_var`, `partition_by` is added with `main_id` as default; so, adding `partition_by` manually is not supported. If you need partitioning on other columns too, check out `input_var` where `partition_by` on arbitrary and multiple columns is supported. |\n| `column_data_type` | String | (Optional) Data type for the `entity_var`. Supported data types are: `integer`, `variant`, `float`, `varchar`, `text`, and `timestamp`. |\n\n**`input_var`**\n\nThe syntax of `input_var` is similar to `entity_var`, with the only difference that instead of each value being associated to a row of the feature table, it’s associated with a row of the specified input. While you can think of an `entity_var` as adding a helper column to the feature table, you can consider an `input_var` as adding a helper column to the input.\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `name` | String | Name to store the retrieved data. |\n| `select` | String | Data to be stored in the name. |\n| `from` | List | Reference to the source table from where data is to be fetched. |\n| `where` | String | (Optional) Applies conditions for fetching data. |\n| `default` | String | (Optional) Default value for any entity for which the calculated value would otherwise be NULL. |\n| `description` | String | (Optional) Textual description. |\n| `column_data_type` | String | (Optional) Data type for the `input_var`. Supported data types are: `integer`, `variant`, `float`, `varchar`, `text`, and `timestamp`. |\n| `window` | Object | (Optional) Specifies a window over which the value should be calculated. |\n\n**`window`**\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `partition_by` | String | (Optional) List of SQL expressions to use in partitioning the data. |\n| `order_by` | String | (Optional) List of SQL expressions to use in ordering the data. |\n\nIn window option, `main_id` is not added by default, it can be any arbitrary list of columns from the input table. So if a feature should be partitioned by `main_id`, you must add it in the `partition_by` key.\n\n### Output\n\nAfter [running the project](https://www.rudderstack.com/docs/archive/profiles/0.13/get-started/profile-builder/#7-generate-output-tables), you can view the generated material tables.\n\n1.  Log in to your Snowflake console.\n2.  Click **Worksheets** from the top navigation bar.\n3.  In the left sidebar, click **Database** and the corresponding **Schema** to view the list of all tables. You can hover over a table to see the full table name along with its creation date and time.\n4.  Write a SQL query like `select * from <table_name>` and execute it to see the results:\n\n1.  Open [Postico2](https://eggerapps.at/postico2/). If required, create a new connection by entering the relevant details. Click **Test Connection** followed by **Connect**.\n2.  Click the **+** icon next to **Queries** in the left sidebar.\n3.  You can click **Database** and the corresponding schema to view the list of all tables/views.\n4.  Double click on the appropriate view name to paste the name on an empty worksheet.\n5.  You can prefix `SELECT *` from the view name pasted previously and suffix `LIMIT 10;` at the end.\n6.  Press Cmd+Enter keys, or click the **Run** button to execute the query.\n\n1.  Enter your Databricks workspace URL in the web browser and log in with your username and password.\n2.  Click the **Catalog** icon in left sidebar.\n3.  Choose the appropriate catalog from the list and click on it to view contents.\n4.  You will see list of tables/views. Click the appropriate table/view name to paste the name on worksheet.\n5.  You can prefix `SELECT * FROM` before the pasted view name and suffix `LIMIT 10;` at the end.\n6.  Select the query text. Press Cmd+Enter, or click the **Run** button to execute the query.\n\n1.  Log in to your [Google Cloud Console](https://console.cloud.google.com/)\n2.  Search for Bigquery in the search bar.\n3.  Select Bigquery from **Product and Pages** to open the Bigquery **Explorer**.\n4.  Select the correct project from top left drop down menu.\n5.  In the left sidebar, click the project ID, then the corresponding dataset view list of all the tables and views.\n6.  Write a SQL query like `select * from <table_name> limit 10;` and execute it to see the results.\n\nA sample output containing the results in Snowflake:\n\n![Generated table (Snowflake)](https://www.rudderstack.com/docs/images/profiles/profiles-feature-table.webp)\n\n## Feature table for cohort\n\nTo create feature table for a specific cohort, you can pass the cohort’s path in the `entity_cohort` field:\n\n```\n- name: us_users_features\n  model_type: feature_table_model\n  model_spec:\n    entity_cohort: models/knownUsUsers\n    time_grain: \"day\"\n    validity_time: 24h # 1 day\n    features:\n      - has_credit_card\n```\n\nTo create feature tables for the entire set of an entity’s instance, specify the `entity_key`:\n\n```\n- name: all_users_features\n  model_type: feature_table_model\n  model_spec:\n    entity_key: user\n    time_grain: \"day\"\n    validity_time: 24h # 1 day\n    features:\n      - max_timestamp\n```\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Feature Table | RudderStack Docs",
    "description": "Step-by-step tutorial on creating a feature table model.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.13/example/feature-views/",
    "markdown": "# Feature Views | RudderStack Docs\n\nStep-by-step tutorial on creating an feature view models.\n\n* * *\n\n*     9 minute read  \n    \n\nOnce you have done [identity stitching](https://www.rudderstack.com/docs/archive/profiles/0.13/core-concepts/identity-stitching/) to unify the identity of your users across all the cross-platforms, you can evaluate and maintain the required features/traits for each identified user using a feature views model.\n\n## Prerequisites\n\n*   A basic Profile Builder project by following the [Profile Builder CLI](https://www.rudderstack.com/docs/archive/profiles/0.13/get-started/profile-builder/) steps.\n*   [Structure](https://www.rudderstack.com/docs/archive/profiles/0.13/cli-user-guide/structure/) of a Profile Builder project and the parameters used in different files.\n*   [Identity Stitching](https://www.rudderstack.com/docs/archive/profiles/0.13/example/id-stitcher/) model as Feature Views reuses its output to extract the required features/traits.\n\n## Feature Views model\n\nYou can define and extract the required features/traits for an entity from your data warehouse using the feature views model. Once done, you can send them to the downstream destinations. A destination could either be the [Activation API](https://www.rudderstack.com/docs/archive/profiles/0.13/activation-api/) or any [Reverse ETL destination](https://www.rudderstack.com/docs/destinations/warehouse-destinations/) that RudderStack supports. Each such destination requires data in the form of a table with an ID column and one or more feature columns.\n\nYou can use the Feature Views model to access the entity features based on any ID type and create a view having all or a specified set of entity features across the project. It also lets you unify the traits/features (defined using `entity_vars`) and ML models to generate a comprehensive customer 360 table.\n\nTo create a feature views model, you can add `feature_views` section under `entities` and provide a list of ID types under the `id_served` field. RudderStack assigns a default name to the model, if not provided, and adds all the available features on the entity into the view by default.\n\n### Default feature views model\n\nThe `pb_project.yaml` file for a default feature views model:\n\n```\n...\nentities:\n  - name: user\n    id_types:\n      - main_id\n      - user_id\n    feature_views:\n      using_ids:\n        - id: email\n          name: features_by_email\n        - id: salesforce_id\n          name: salesforce_id_stitched_features\n      features:\n        - from: models/feature_table_1 #include everything from `feature_table_1` (default behaviour if `include` is not specified)\n          include:\t[\"*\"]\n        - from: models/feature_table_2 #exclude 'middle_name' feature from `feature_table_2`\n          exclude:\t[\"middle_name\"] \n```\n\n### Custom feature views model\n\nYou can also define a custom feature views model by including/excluding features from any other model and adding their references to the `feature_views` section.\n\nThe `models/profiles.yaml` file for a custom feature views model:\n\n```\nmodels:\n  - name: cart_feature_views\n    model_type: feature_views\n    model_spec:\n      validity_time: 24h # 1 day\n      entity_key: user\n      id_served: user_id\n      feature_list:\n        - from: packages/pkg/models/cart_table # a table created by package\n          include: [\"*\"] # will include all the traits\n        - from: models/user_var_table\n          include: [\"*\"]\n          exclude: [cart_quantity, purchase_status] # except two, all the other traits will be included\n        - from: models/sql_model\n          include: [lifetime_value] # will include only one trait\n```\n\n## Sample project\n\nThis sample project uses the output of an identity stitching model as an input to create a feature views. The following sections describe how to define your PB project files:\n\n### Project detail\n\nThe [`pb_project.yaml`](https://www.rudderstack.com/docs/archive/profiles/0.13/cli-user-guide/structure/#project-details) file defines the project details such as name, schema version, connection name and the entities which represent different identifiers.\n\nYou can define all the identifiers from different input sources you want to stitch together as a `user_main_id`:\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> You need to add `main_id` to the list only if you have defined `main_id_type: main_id` in the ID stitcher spec.\n\n```\n# Project name\nname: sample_id_stitching\n# Project's yaml schema version\nschema_version: 63\n# Warehouse connection\nconnection: test\n# Folder containing models\nmodel_folders:\n  - models\n# Entities in this project and their ids.\nentities:\n  - name: user\n    id_types:\n      - main_id # You need to add `main_id` to the list only if you have defined `main_id_type: main_id` in the id stitcher spec.\n      - user_id # one of the identifier from your data source.\n      - email\n# lib packages can be imported in project signifying that this project inherits its properties from there\npackages:\n  - name: corelib\n    url: \"https://github.com/rudderlabs/profiles-corelib/tag/schema_{{best_schema_version}}\"\n    # if required then you can extend the package definition such as for ID types.\n```\n\n### Input\n\nThe [input file](https://www.rudderstack.com/docs/archive/profiles/0.13/cli-user-guide/structure/#inputs) file includes the input table references and corresponding SQL for the above-mentioned entities:\n\n```\ninputs:\n- name: rsIdentifies\n  contract: # constraints that a model adheres to\n    is_optional: false\n    is_event_stream: true\n    with_entity_ids:\n      - user\n    with_columns:\n      - name: timestamp\n      - name: user_id\n      - name: anonymous_id\n      - name: email\n  app_defaults:\n    table: rudder_events_production.web.identifies # one of the WH table RudderStack generates when processing identify or track events.\n    occurred_at_col: timestamp\n    ids:\n      - select: \"user_id\" # kind of identity sql to pick this column from above table.\n        type: user_id\n        entity: user # as defined in project file\n        to_default_stitcher: true\n      - select: \"anonymous_id\"\n        type: anonymous_id\n        entity: user\n        to_default_stitcher: true\n      - select: \"lower(email)\" # can use sql.\n        type: email\n        entity: user\n        to_default_stitcher: true\n- name: rsTracks\n  contract:\n    is_optional: false\n    is_event_stream: true\n    with_entity_ids:\n      - user\n    with_columns:\n      - name: timestamp\n      - name: user_id\n      - name: anonymous_id\n  app_defaults:\n    table: rudder_events_production.web.tracks # another table in WH maintained by RudderStack processing track events.\n    occurred_at_col: timestamp\n    ids:\n      - select: \"user_id\"\n        type: user_id\n        entity: user\n        to_default_stitcher: true\n      - select: \"anonymous_id\"\n        type: anonymous_id\n        entity: user\n        to_default_stitcher: true\n```\n\n### Model\n\nThe **feature views** model lets you define and extract the features/traits from your warehouse tables. Each feature is defined using an `entity_var`.\n\nA sample `profiles.yaml` file specifying a feature views model:\n\n```\nvar_groups:\n  - name: first_group\n    entity_key: user\n    vars:\n      - entity_var:\n          name: first_seen\n          select: min(timestamp::date)\n          from: inputs/rsTracks\n          where: properties_country is not null and properties_country != ''\n      - entity_var:\n          name: last_seen\n          select: max(timestamp::date)\n          from: inputs/rsTracks\n          is_feature: false # Specifies the entity_var is not a feature\n      - entity_var:\n          name: user_lifespan\n          select: '{{user.Var(\"last_seen\")}} - {{user.Var(\"first_seen\")}}'\n          description: Life Time Value of a customer\n      - entity_var:\n          name: days_active\n          select: count(distinct timestamp::date)\n          from: inputs/rsTracks\n          description: No. of days a customer was active\n      - entity_var:\n          name: campaign_source\n          default: \"'organic'\"\n      - entity_var:\n          name: user_rank\n          default: -1\n      - entity_var:\n          name: campaign_source_first_touch\n          select: first_value(context_campaign_source)\n          window:\n              order_by:\n                  - timestamp asc\n              partition_by:\n                  - main_id\n          from: inputs/rsIdentifies\n          where: context_campaign_source is not null and context_campaign_source != ''\n      - input_var:\n          name: num_c_rank_num_b_partition\n          select: rank()\n          from: inputs/tbl_c\n          default: -1\n          window:\n            partition_by:\n              - '{{tbl_c}}.num_b'\n            order_by:\n              - '{{tbl_c}}.num_c asc'\n          where: '{{tbl_c}}.num_b >= 10'\n      - entity_var:\n          name: min_num_c_rank_num_b_partition\n          select: min(num_c_rank_num_b_partition)\n          from: inputs/tbl_c\n```\n\n**`var_groups`**\n\nThe `var_groups` field groups all the `vars` under it and provides the provision to define any configuration keys that need to be shared across `vars`.\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `name` | String | Name to identify the `var_groups` uniquely. |\n| `entity_key` | String | Specifies the entity to be used. |\n| `vars` | List | Specifies the `entity_var` and `input_var` variables. |\n\n**`entity_var`**\n\nThe `entity_var` field provides inputs for the feature views model. This variable stores the data temporarily, however, you can choose to store its data permanently by specifying the `name` in it as a feature in the `features` key.\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `name` | String | Name of the `entity_var` to identify it uniquely. |\n| `select` | String | Column name/value you want to select from the table. This defines the actual value that will be stored in the variable. You can use simple SQL expressions or select an `entity_var` as `{{entityName.Var(\\\"entity_var\\\")}}`. It has to be an aggregate operation that ensures the output is a unique value for a given `main_id`. For example: min(timestamp), count(\\*), sum(amount) etc. This holds true even when a window function (optional) is used. For example:: first\\_value(), last\\_value() etc are valid while rank(), row\\_number(), etc. are not valid and give unpredictable results. |\n| `from` | List | Reference to the source table from where data is to be fetched. You can either refer to another model from the same YAML or some other table specified in input YAML. |\n| `where` | String | Any filters you want to apply on the input table before selecting a value. This must be SQL compatible and should consider the data type of the table. |\n| `default` | String | Default value in case no data matches the filter. When defining default values, make sure you enclose the string values in single quotes followed by double quotes to avoid SQL failure. However, you can use the non-string values without any quotes. |\n| `description` | String | Textual description of the `entity_var`. |\n| `is_feature` | Boolean | Determines whether the `entity_var` is a feature. The default value is true. |\n| `window` | Object | Specifies the window function. Window functions in SQL usually have both `partition_by` and `order_by` properties. But for `entity_var`, `partition_by` is added with `main_id` as default; so, adding `partition_by` manually is not supported. If you need partitioning on other columns too, check out `input_var` where `partition_by` on arbitrary and multiple columns is supported. |\n\n**`input_var`**\n\nThe syntax of `input_var` is similar to `entity_var`, with the only difference that instead of each value being associated to a row of the feature views, it’s associated with a row of the specified input. While you can think of an `entity_var` as adding a helper column to the feature views, you can consider an `input_var` as adding a helper column to the input.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> If more than one `input_var` are required to derive an `entity_var`, then all the `input_var` must be defined on the same table.\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `name` | String | Name to store the retrieved data. |\n| `select` | String | Data to be stored in the name. |\n| `from` | List | Reference to the source table from where data is to be fetched. |\n| `where` | String | (Optional) Applies conditions for fetching data. |\n| `default` | String | (Optional) Default value for any entity for which the calculated value would otherwise be NULL. |\n| `description` | String | (Optional) Textual description. |\n| `window` | Object | (Optional) Specifies a window over which the value should be calculated. |\n\n**`window`**\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `partition_by` | String | (Optional) List of SQL expressions to use in partitioning the data. |\n| `order_by` | String | (Optional) List of SQL expressions to use in ordering the data. |\n\nIn window option, `main_id` is not added by default, it can be any arbitrary list of columns from the input table. So if a feature should be partitioned by `main_id`, you must add it in the `partition_by` key.\n\n### Output\n\nAfter [running the project](https://www.rudderstack.com/docs/archive/profiles/0.13/get-started/profile-builder/#7-generate-output-tables), you can view the generated material tables.\n\n1.  Log in to your Snowflake console.\n2.  Click **Worksheets** from the top navigation bar.\n3.  In the left sidebar, click **Database** and the corresponding **Schema** to view the list of all tables. You can hover over a table to see the full table name along with its creation date and time.\n4.  Write a SQL query like `select * from <table_name>` and execute it to see the results:\n\n1.  Open [Postico2](https://eggerapps.at/postico2/). If required, create a new connection by entering the relevant details. Click **Test Connection** followed by **Connect**.\n2.  Click the **+** icon next to **Queries** in the left sidebar.\n3.  You can click **Database** and the corresponding schema to view the list of all tables/views.\n4.  Double click on the appropriate view name to paste the name on an empty worksheet.\n5.  You can prefix `SELECT *` from the view name pasted previously and suffix `LIMIT 10;` at the end.\n6.  Press Cmd+Enter keys, or click the **Run** button to execute the query.\n\n1.  Enter your Databricks workspace URL in the web browser and log in with your username and password.\n2.  Click the **Catalog** icon in left sidebar.\n3.  Choose the appropriate catalog from the list and click on it to view contents.\n4.  You will see list of tables/views. Click the appropriate table/view name to paste the name on worksheet.\n5.  You can prefix `SELECT * FROM` before the pasted view name and suffix `LIMIT 10;` at the end.\n6.  Select the query text. Press Cmd+Enter, or click the **Run** button to execute the query.\n\n1.  Log in to your [Google Cloud Console](https://console.cloud.google.com/)\n2.  Search for Bigquery in the search bar.\n3.  Select Bigquery from **Product and Pages** to open the Bigquery **Explorer**.\n4.  Select the correct project from top left drop down menu.\n5.  In the left sidebar, click the project ID, then the corresponding dataset view list of all the tables and views.\n6.  Write a SQL query like `select * from <table_name> limit 10;` and execute it to see the results.\n\nA sample output containing the results in Snowflake:\n\n![Generated table (Snowflake)](https://www.rudderstack.com/docs/images/profiles/profiles-feature-table.webp)\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Feature Views | RudderStack Docs",
    "description": "Step-by-step tutorial on creating an feature view models.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.13/example/sql-model/",
    "markdown": "# SQL Models | RudderStack Docs\n\nStep-by-step tutorial on how to create a SQL Template model.\n\n* * *\n\n*     6 minute read  \n    \n\nThis guide provides a detailed walkthrough on how to use a PB project and create SQL Template models using custom SQL queries.\n\n## Prerequisites\n\n*   Familiarize yourself with:\n    \n    *   A basic Profile Builder project by following the [Profile Builder CLI](https://www.rudderstack.com/docs/archive/profiles/0.13/get-started/profile-builder/) steps.\n    *   [Structure](https://www.rudderstack.com/docs/archive/profiles/0.13/cli-user-guide/structure/) of a Profile Builder project and the parameters used in different files.\n\n## Sample project\n\nThe following sections describe how to define your PB project files:\n\n### Project detail\n\nThe [`pb_project.yaml`](https://www.rudderstack.com/docs/archive/profiles/0.13/cli-user-guide/structure/#project-details) file defines the project details such as name, schema version, connection name and the entities which represent different identifiers.\n\nYou can define all the identifiers from different input sources you want to stitch together as a single ID (`main_id` in this example):\n\n```\nname: sample_test\nschema_version: 63\nconnection: test\nmodel_folders:\n  - models\nentities:\n  - name: user\n    id_stitcher: models/test_id__\n    id_types:\n      - test_id\n      - exclude_id\nid_types:\n  - name: test_id\n    filters:\n      - type: include\n        regex: \"([0-9a-z])*\"\n      - type: exclude\n        value: \"\"\n  - name: exclude_id\n```\n\n### Input\n\nThe [input file](https://www.rudderstack.com/docs/archive/profiles/0.13/cli-user-guide/structure/#inputs) (`models/inputs.yaml`) file includes the input table references and corresponding SQL for the above-mentioned entities:\n\n```\ninputs:\n  - name: tbl_a\n    app_defaults:\n      table: Temp_tbl_a\n    occurred_at_col: insert_ts\n    ids:\n      - select: TRIM(COALESCE(NULL, id1))\n        type: test_id\n        entity: user\n        to_default_stitcher: true\n      - select: \"id2\"\n        type: test_id\n        entity: user\n        to_default_stitcher: true\n      - select: \"id3\"\n        type: exclude_id\n        entity: user\n        to_default_stitcher: true\n  - name: tbl_b\n    app_defaults:\n      view: Temp_view_b\n    occurred_at_col: timestamp\n    ids:\n      - select: \"id1\"\n        type: test_id\n        entity: user\n        to_default_stitcher: true\n      - select: \"id2\"\n        type: test_id\n        entity: user\n        to_default_stitcher: true\n      - select: \"id3\"\n        type: test_id\n        entity: user\n        to_default_stitcher: true\n  - name: tbl_c\n    app_defaults:\n      table: Temp_tbl_c\n    ids:\n      - select: \"id1\"\n        type: test_id\n        entity: user\n        to_default_stitcher: true\n      - select: \"id2\"\n        type: test_id\n        entity: user\n        to_default_stitcher: true\n```\n\n### Model\n\nProfiles **SQL model** lets you write custom SQL queries to achieve advanced use-cases to create desired output tables.\n\nA sample `profiles.yaml` file specifying a `single_sql` type SQL model:\n\n```\nmodels:\n- name: test_sql\n  model_type: sql_template\n  model_spec:\n    validity_time: 24h# 1 day\n    materialization:                 // optional\n      run_type: discrete             // optional [discrete, incremental]\n    single_sql: |\n        {%- with input1 = this.DeRef(\"inputs/tbl_a\") -%}\n          SELECT \n              id1 AS new_id1, \n              id2 AS new_id2, \n              {{input1}}.*\n          FROM {{input1}}\n        {%- endwith -%}        \n    occurred_at_col: insert_ts        // optional\n    ids:\n      - select: \"new_id1\"\n        type: test_id\n        entity: user\n        to_default_stitcher: true\n      - select: \"new_id2\"\n        type: test_id\n        entity: user\n        to_default_stitcher: true\n      - select: \"id3\"\n        type: test_id\n        entity: user\n        to_default_stitcher: true\n```\n\nA sample `profiles.yaml` file specifying a `multi_sql` type SQL model:\n\n```\nmodels:\n- name: test_sql\n    model_type: sql_template\n    model_spec:\n      validity_time: 24h # 1 day\n      materialization:\n        output_type: table\n        run_type: discrete\n      multi_sql: |\n        {% with input_material1 = this.DeRef(\"models/test_sql1\") input_material2 = this.DeRef(\"inputs/tbl_a\") input_material3 = this.DeRef(\"inputs/tbl_c\") %}\n          create {{this.GetMaterialization().OutputType.ToSql()}} {{this}} as (\n            select b.id1, b.id2, b.id3, b.insert_ts, a.new_id1, a.num_a, c.num_b, c.num_c\n            from {{ input_material1 }} a\n            full outer join {{ input_material2 }} b\n            on a.id2 = b.id2\n            full outer join {{ input_material3 }} c\n            on c.id2 = a.id2\n          );\n        {% endwith  %}        \n```\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> A `multi_sql` type SQL model only creates a table as an output type in a warehouse whereas `single_sql` type SQL model supports all the output types (deafult is `ephemeral`). See [materialization](https://www.rudderstack.com/docs/archive/profiles/0.13/resources/glossary/#materialization) for more information.\n\n##### Model specification fields\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `name` | String | Name of the SQL model. You can also refer this as an input as `models/test_sql`. |\n| `model_type` | String | Defines the type of model. |\n| `model_spec` | Object | Contains the specifications for the target model. |\n| `validity_time` | Time | Time Specifies the validity of the model with respect to its timestamp. For example, a model run as part of a scheduled nightly job for 2009-10-23 00:00:00 UTC with `validity_time`: `24h` would still be considered potentially valid and usable for any run requests, which do not require precise timestamps between 2009-10-23 00:00:00 UTC and 2009-10-24 00:00:00 UTC. This specifies the validity of generated feature table. Once the validity is expired, scheduling takes care of generating new tables. For example: 24h for 24 hours, 30m for 30 minutes, 3d for 3 days. |\n| `materialization` | List | Adds the key `run_type`: `incremental` to run the project in incremental mode. This mode considers row inserts and updates from the edge\\_sources input. These are inferred by checking the timestamp column for the next run. One can provide buffer time to consider any lag in data in the warehouse for the next incremental run like if new rows are added during the time of its run. If you do not specify this key then it’ll default to `run_type`: `discrete`. |\n| `single_sql` | List | Specifies the SQL template which must evaluate to a single SELECT SQL statement. After execution, it should produce a dataset which will materialize based on the provided materialization. |\n| `multi-sql` | List | Specifies the SQL template which can evaluate to multiple SQL statements. One of these SQL statements (typically the last one) must be a CREATE statement which shall be responsible for materializing the model into a table.<br><br>**Note**: You should set only one of `single_sql` or `multi_sql`. |\n| `occurred_at_col` | List | Name of the column which contains the timestamp value in the output of SQL template. |\n| `ids` | List | Specifies the list of all IDs present in the source table along with their column names (or column SQL expressions). It is required in case you want to use SQL models as an input to the `input_var` or `entity_var` fields. |\n\n## SQL template\n\nYou can pass custom SQL queries to the `single_sql` or `multi_sql` fields, which is also known as a **SQL template**. It provides the flexibility to write custom SQL by refering to any of the input sources listed in the `inputs.yaml` or any model listed in `models/profiles.yaml`.\n\nThe SQL templates follow a set query syntax which serves the purpose of creating a model. Follow the below rules to write SQL templates:\n\n*   Write SQL templates in the [pongo2 template engine](https://pkg.go.dev/github.com/flosch/pongo2#readme-first-impression-of-a-template) syntax.\n*   Avoid circular referencing while referencing the models. For example, `sql_model_a` references `sql_model_b` and `sql_model_b` references `sql_model_a`.\n*   Use `timestamp` variable (refers to the start time of the current run) to filter new events.\n*   `this` refers to the current model’s material. You can use the following methods to access the material properties available for `this`:\n    *   `DeRef(\"path/to/model\")`: Use this syntax `{{ this.DeRef(\"path/to/model\") }}` to refer to any model and return a database object corresponding to that model. The database object, in return, gives the actual name of the table/view in the warehouse. Then, generate the output, for example:\n\n```\n{% with input_table = this.DeRef(\"inputs/tbl_a\") %}\n    SELECT\n        t.a AS new_a,\n        t.b AS new_b,\n        t.*\n    FROM {{input_table}} AS t\n{% endwith %}\n```\n\n*   `GetMaterialization()`: Returns a structure with two fields: `MaterializationSpec{OutputType, RunType}`.\n    *   `OutputType`: You must use `OutputType` with `ToSQL()` method:  \n        For example, `CREATE OR REPLACE {{this.GetMaterialization().OutputType.ToSQL()}} {{this.GetSelectTargetSQL()}} AS ...`\n    *   `RunType`: For example, `this.GetMaterialization().RunType`\n\n## Refer SQL contents from another file\n\nIf you want to edit a SQL query in a text editor and not as a field in a YAML file, you can use the `ReadFile` method. It refers to the SQL contents stored in another file:\n\n```\nmodels:\n- name: example_sql_model\n  model_type: sql_template\n  model_spec:\n    validity_time: 24h # 1 day\n    materialization:\n      output_type: view\n      run_type: discrete\n    single_sql: \"{{this.ReadFile('models/compute.sql')}}\" # for a SQL file named compute.sql in the models folder\n    occurred_at_col: insert_ts\n```\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "SQL Models | RudderStack Docs",
    "description": "Step-by-step tutorial on how to create a SQL Template model.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.12/get-started/sample-data/",
    "markdown": "# Snowflake Sample Data | RudderStack Docs\n\nSample data for Snowflake\n\n* * *\n\n*     2 minute read  \n    \n\nRudderStack provides a sample data set for the Snowflake warehouse, available in the [Snowflake marketplace](https://app.snowflake.com/marketplace/listing/GZT0Z856CMJ/rudderstack-inc-rudderstack-event-data-for-quickstart). You can use this data to run the Profiles project and [Predictive features](https://www.rudderstack.com/docs/archive/profiles/0.12/predictions/) through the UI or the CLI.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> The number of columns in this data set are intentionally limited to make the data set easily understandable. Also, all email addresses are generated randomly and no PII is used in the generation of this data set.\n\nThe following tables, properties, and user information is included in the data set:\n\n## Tables\n\nThis data set includes below-mentioned RudderStack event data tables:\n\n*   `PAGES` - Page view events from anonymous and known users.\n*   `TRACKS` - Summarized tracked user actions (like `login`, `signup`, `order_completed`, etc.).\n*   `IDENTIFIES` - Identify calls run when a user provides a unique identifier (i.e., upon `signup`).\n*   `ORDER_COMPLETED` - Detailed payloads from tracked `order_completed` events.\n\nAs of January 2023, here are the approximate number of rows in each table:\n\n*   `PAGES`: ~43k\n*   `TRACKS`: ~14k\n*   `IDENTIFIES`: ~4.8k\n*   `ORDER_COMPLETED`: ~2.2k\n\nThese volumes follow the pattern of a normal eCommerce conversion funnel (pageview, signup, order). Specifically, here’s a rough breakdown of the user journey by volume:\n\n*   30% - Never sign in\n*   10% - Sign in but never add an item to cart\n*   40% - Add to cart and abandon\n*   20% - Make purchases\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> Note that this data includes _future_ data until Apr 2024, and starts in June 2023. This is to ensure that future users can still run the project with ‘current’ data. RudderStack team will refresh the data periodically throughout the year.\n\n## Properties\n\nThis data set includes a _subset_ of the standard properties found in the [Warehouse schema spec](https://www.rudderstack.com/docs/destinations/warehouse-destinations/warehouse-schema/) for each table. The required columns for running Profiles and Predictions projects are also present.\n\n### User information\n\nThe user data includes a subset of our standard properties for `identify` calls.\n\nThis data set contains a total of ~10k unique users by `anonymousId`. About half of these unique users (~4.8k) are known users (with an associated `identify` call).\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Snowflake Sample Data | RudderStack Docs",
    "description": "Sample data for Snowflake",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.12/core-concepts/data-modeling/",
    "markdown": "# Data Modeling | RudderStack Docs\n\nModel your unorganized and scattered warehouse data using RudderStack’s Profiles.\n\n* * *\n\n*     3 minute read  \n    \n\nProfiles models your organizational data by analyzing all the data in your warehouse to create unified customer profiles and enrich them with features to help you scale your business efficiently and swiftly.\n\nWhen you run the Profiles project, it creates an identity graph and feature views as outputs. You can augment the graph and create new user features by writing simple definitions in a configuration file or via SQL models.\n\n[![Profiles data modeling](https://www.rudderstack.com/docs/images/profiles/data-modeling.webp)](https://www.rudderstack.com/docs/images/profiles/data-modeling.webp)\n\n## Highlights\n\n*   Flexibility to use event stream, ETL, or any external tools as input sources.\n*   Support to define various entities like user, product, organization, etc.\n*   Intelligent merging of entities with different identifiers, like stitching Salesforce IDs.\n*   Ease of creating features/traits for any entity and using them to deliver personalization.\n*   Deal with advanced use-case scenarios using custom SQL queries.\n\n## Use varied input sources\n\nRudderStack Profiles gives you the flexibility of using a variety of input sources. These sources are essentially the tables or views which you can create using:\n\n*   [Event Stream](https://www.rudderstack.com/docs/sources/event-streams/) (loaded from event data)\n*   ETL extract (loaded from [Cloud Extract](https://www.rudderstack.com/docs/sources/extract/) (loaded from event data))\n*   Existing tables in the warehouse (generated by external tools like DBT).\n\n## Define entities\n\nEntities refer to an object for which you can create a profile. RudderStack allows you to use the desired object as an entity. For example, user, customer, product, or any other object that requires profiling.\n\nYou can define the entities in `pb_project.yaml` file and use them declaratively while describing the columns of your input sources.\n\n## Unify entities\n\nOnce you define the entities, you can resolve different identities for an entity using the process of [identity stitching](https://www.rudderstack.com/docs/archive/profiles/0.12/core-concepts/identity-stitching/). It matches the different identifiers across multiple devices, digital touchpoints, and other data (like offline point-of-sale interactions) to build a comprehensive identity graph. The identity graph includes nodes (identifiers) and their relationships (edges), and it is generated as a transparent table in the warehouse.\n\nFor example, you can stitch Salesforce IDs or other ID types.\n\n## Enrich with features\n\nOnce you map all the available identifiers to an individual user or entity, it is easier to collect their traits and compute the user features you want in your customer 360 table.\n\nUsing the identity graph as a map, the Profiles **entity var** models let you define or perform calculations over the customer data in your warehouse. Each `var` materialises as a column in the `entity_var` table and represents a distinct feature. In addition, ML models can also use the identify graph as well as other entity vars, to create new features. Finally, **feature view** model lets you unify entity vars as well as ML faetures into a single view.\n\nYou don’t need any other tool or deep technical/SQL expertise to create these features. Trait definition is in a single unified framework and there is no need to move data across silos.\n\nTo implement advanced use cases, you can use [custom SQL queries](https://www.rudderstack.com/docs/archive/profiles/0.12/example/sql-model/) to define user features.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Data Modeling | RudderStack Docs",
    "description": "Model your unorganized and scattered warehouse data using RudderStack's Profiles.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.12/core-concepts/identity-stitching/",
    "markdown": "# Identity Stitching | RudderStack Docs\n\nStitch multiple identifiers together to create a unified and comprehensive profile.\n\n* * *\n\n*     4 minute read  \n    \n\nIdentity stitching combines unique identifiers across your digital touchpoints to identify users and create a unified, omnichannel view of your customers. Using this feature, you can:\n\n*   **Understand user behavior**: Consolidate and connect customer data from various sources to better understand customers’ preferences, behaviors, and interactions across multiple touchpoints.\n*   **Provide personalized support**: Deliver personalized marketing messages and experiences to your customers. Ensures that the right message reaches the right person at the right time, increasing the effectiveness of marketing campaigns.\n*   **Enrich user profile with features**: Enhance user profiles with additional data points and features. These features can include demographic information, preferences, purchase history, browsing behavior, or any other static or computed data points.\n\n## Problem of multiple identities\n\nCompanies gather user data across digital touchpoints like websites, mobile apps, enterprise systems like CRMs, marketing platforms, etc. During this process, a single user is identified with multiple identifiers across their product journey, like their email ID, phone number, device ID, anonymous ID, and more. Also, the user information is spread across dozens of devices, accounts, or products as they often change their devices and use work and personal emails together.\n\nThe user data stored in the warehouse contains unstructured objects that represent one or more user (or entity) identities. Competitive businesses need to clarify this mess of data points into an accurate model of customer behavior and build personalized relationships.\n\nTo create a unified user profile, it is essential to correlate all of the different user identifiers into one canonical identifier so that all the data related to a particular user or entity can be associated with that user or entity.\n\nThis unification step, called **Identity Stitching**, ties all the user data from these tables into a unified view, giving you a 360-degree view of the user.\n\n## Perform identity stitching\n\nYou can use the RudderStack’s [identity stitching model](https://www.rudderstack.com/docs/archive/profiles/0.12/example/id-stitcher/) to define the identifiers you want to combine together. You can also define the input sources, like [Event Stream](https://www.rudderstack.com/docs/sources/event-streams/), [Cloud Extract](https://www.rudderstack.com/docs/sources/extract/) sources, which automatically produce an identity graph because all the schemas and unique identifiers are known.\n\n## Unify identities across all platforms and devices\n\nIdentity stitching is the process of matching different identifiers across multiple devices, digital touchpoints, and other data (like offline point-of-sale interactions) to build a comprehensive identity graph. This identity graph includes nodes (identifiers) and their relationships (edges), and it is generated as a transparent table in the warehouse.\n\nRudderStack performs identity stitching by mapping all the unique identifiers into a single canonical ID (for example, `rudder_id`), then uses that ID to make user feature development easier (for example, summing the payment events against a single `rudder_id`).\n\n[![single identity created from different identities](https://www.rudderstack.com/docs/images/profiles/id-stitching.webp)](https://www.rudderstack.com/docs/images/profiles/id-stitching.webp)\n\n## Identity graph\n\nIdentity stitching starts with the creation of an identity graph. The identity graph is a database housing the entity identifiers where you can identify and connect details related to your customer journeys. Further, it stitches them together in one customer profile representing their whole identity.\n\nThe most fundamental data in an identity graph is the ID tag associated with a device, account, network, session, transaction, or any other anonymous identifier that can engage with your company. Once you’ve collected this data and associated it with a single customer identity (wherever possible), your customer data becomes more reliable, and you can move on to achieve higher goals.\n\nAn identity graph incorporates models that help it in ingesting new information. As you add a new data point with whatever connections are immediately known, the graph database determines if it fits into any existing customer identifier. If there is a clear link - such as a matching device ID or conclusive biographical data like a credit card number - the graph incorporates the data into a relevant user node.\n\n[![identity graph](https://www.rudderstack.com/docs/images/profiles/identity-graph.webp)](https://www.rudderstack.com/docs/images/profiles/identity-graph.webp)\n\n## Notable features\n\n*   Use different input sources like RudderStack’s [Event Stream](https://www.rudderstack.com/docs/sources/event-streams/), [Cloud Extract](https://www.rudderstack.com/docs/sources/extract/) sources, or any existing tables in the warehouse.\n*   Merge identities for entities like a user, customer, product, business account, etc.\n*   Stitch identities from all the required channels like web, mobile, marketing platforms, etc.\n*   Use identity stitching results to develop user features and deliver personalized campaigns.\n\n#### See also\n\n*   [Sample identity stitching project](https://www.rudderstack.com/docs/archive/profiles/0.12/example/id-stitcher/)\n*   [Problem of Identity resolution](https://www.rudderstack.com/blog/the-tale-of-identity-graph-and-identity-resolution/)\n*   [How to achieve ID mapping in a data warehouse](https://www.rudderstack.com/blog/identity-graph-and-identity-resolution-in-sql/)\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Identity Stitching | RudderStack Docs",
    "description": "Stitch multiple identifiers together to create a unified and comprehensive profile.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.13/example/predictive-features-snowflake/prerequisite/",
    "markdown": "# Prerequisites | RudderStack Docs\n\nPrerequisites to generate predictive features in Snowflake using RudderStack Predictions\n\n* * *\n\n*     8 minute read  \n    \n\nTo follow this guide, you will need access to both RudderStack and Snowflake. If you do not have access, follow these links to create a free [RudderStack account](https://app.rudderstack.com/signup?type=freetrial) and [Snowflake account](https://signup.snowflake.com/).\n\nOnce you set up your RudderStack account, [reach out to our support team](mailto:support@rudderstack.com?subject=I%20would%20like%20access%20to%20your%20Predictions%20feature) to request access to our Predictions feature.\n\n## Set up Snowflake for Event Stream data\n\nBecause Predictions is designed to run in a production environment, you need to perform some basic set up in Snowflake (and later, your RudderStack workspace) to simulate the pipelines you would run when collecting user event data.\n\n### Create a new role and user in Snowflake\n\nIn your Snowflake console, run the following commands to create the role `QUICKSTART`.\n\nVerify the role `QUICKSTART` was successfully created.\n\nCreate a new user QUICKSTART\\_USER with a password `<strong_unique_password>`.\n\n```\nCREATE USER QUICKSTART_USER PASSWORD = '<strong_unique_password>' DEFAULT_ROLE = 'QUICKSTART';\n```\n\nVerify the user `QUICKSTART_USER` was successfully created.\n\n### Create RudderStack schema and grant permissions to role\n\nCreate a dedicated schema `_RUDDERSTACK` in your database.\n\n**Replace `<YOUR_DATABASE>` in all queries with your actual database name.**\n\n```\nCREATE SCHEMA \"<YOUR_DATABASE>\".\"_RUDDERSTACK\";\n```\n\nGrant full access to the schema \\_RUDDERSTACK for the previously created role `QUICKSTART`.\n\n```\nGRANT ALL PRIVILEGES ON SCHEMA \"<YOUR_DATABASE>\".\"_RUDDERSTACK\" TO ROLE QUICKSTART;\n```\n\n### Grant permissions on the warehouse, database, schema, and table\n\nEnable the user `QUICKSTART_USER` to perform all operations allowed for the role `QUICKSTART` (via the privileges granted to it).\n\n```\nGRANT ROLE QUICKSTART TO USER QUICKSTART_USER;\n```\n\nRun the following commands to allow the role `QUICKSTART` to look up the objects within your warehouse, database, schema, and the specific table or view:\n\n```\nGRANT USAGE ON WAREHOUSE \"<YOUR_WAREHOUSE>\" TO ROLE QUICKSTART;\nGRANT USAGE ON DATABASE \"<YOUR_DATABASE>\" TO ROLE QUICKSTART;\nGRANT USAGE ON SCHEMA \"<YOUR_DATABASE>\".\"_RUDDERSTACK\" TO ROLE QUICKSTART;\nGRANT SELECT ON ALL TABLES IN SCHEMA \"<YOUR_DATABASE>\".\"_RUDDERSTACK\" TO ROLE  QUICKSTART;\nGRANT SELECT ON FUTURE TABLES IN SCHEMA \"<YOUR_DATABASE>\".\"_RUDDERSTACK\" TO ROLE QUICKSTART;\nGRANT SELECT ON ALL VIEWS IN SCHEMA \"<YOUR_DATABASE>\".\"_RUDDERSTACK\" TO ROLE QUICKSTART;\nGRANT SELECT ON FUTURE VIEWS IN SCHEMA \"<YOUR_DATABASE>\".\"_RUDDERSTACK\" TO ROLE QUICKSTART;\n```\n\n**Replace `<YOUR_DATABASE>` with the exact Snowflake database name.**\n\n## Import RudderStack event data from the Snowflake marketplace\n\nTo set up automated features, you will need the RudderStack event data in your Snowflake warehouse. If you already use RudderStack and have the following tables and fields (see below), skip to the [Profiles Schema and Permissions](#profiles-schema-and-permissions) section. For this guide, using the provided sample data is recommended.\n\n*   `TRACKS`\n*   `IDENTIFIES`\n    *   `user_id`\n    *   `anonymous_id`\n    *   `email`\n*   `PAGES`\n*   `ORDER_COMPLETED`\n\n**NOTE:** You must have all the three identity types in your `INDENTIFIES` table. If you are using your own data and don’t normally track email, you can send the following `identify` call to add the column:\n\n```\nrudderanalytics.identify('userId', {\n    email:'email@address.com',\n    name:'name'\n})\n```\n\n### Get sample data\n\nIf you are setting up RudderStack for the first time go to the [Snowflake Marketplace](https://app.snowflake.com/marketplace/listing/GZT0Z856CMJ/rudderstack-inc-rudderstack-event-data-for-quickstart) and add RudderStack Event Data for Quickstart to your Snowflake account for free. This will add a database with the needed tables to your Snowflake warehouse with no additional storage cost for you.\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/Snowflake-marketplace.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/Snowflake-marketplace.webp)\n\nAt the next screen, open **Options** and add role `QUICKSTART` to have access to this database.\n\n### Create schema for sample data\n\nThe database with the sample data is read-only so you will need to copy it to a new schema to be able to create a valid event stream pipeline (and run a Predictions job on the data).\n\nCreate a new schema in the database you already set up. Name the schema “EVENTS”.\n\n```\nCREATE SCHEMA \"<YOUR_DATABASE>\".\"EVENTS\";\n```\n\nGive permission to the `QUICKSTART` role to create new tables in the above schema.\n\n```\nGRANT ALL PRIVILEGES ON SCHEMA \"<YOUR_DATABASE>\".\"EVENTS\" FOR ROLE QUICKSTART;\n```\n\nCopy the sample data into the newly create schema.\n\n```\nCREATE TABLE \"<YOUR_DATABASE>\".\"EVENTS\".\"TRACKS\" AS SELECT * FROM \"SNOWFLAKE_QUICKSTART\".\"PUBLIC\".\"TRACKS\";\nCREATE TABLE \"<YOUR_DATABASE>\".\"EVENTS\".\"IDENTIFIES\" AS SELECT * FROM \"SNOWFLAKE_QUICKSTART\".\"PUBLIC\".\"IDENTIFIES\";\nCREATE TABLE \"<YOUR_DATABASE>\".\"EVENTS\".\"PAGES\" AS SELECT * FROM \"SNOWFLAKE_QUICKSTART\".\"PUBLIC\".\"PAGES\";\nCREATE TABLE \"<YOUR_DATABASE>\".\"EVENTS\".\"ORDER_COMPLETED\" AS SELECT * FROM \"SNOWFLAKE_QUICKSTART\".\"PUBLIC\".\"ORDER_COMPLETED\";\n```\n\nNow you are ready to create a pipeline connection in RudderStack.\n\n## Create JavaScript source\n\nRudderStack’s Profiles and Predictions products require a warehouse destination with an active sync from a source (a data pipeline). Therefore we will create a JavaScript source that can send a test event to Snowflake.\n\nAfter logging into RudderStack, navigate to the **Directory** from the sidebar on the left, then select the JavaScript source from the list of sources.\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/select-js-source.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/select-js-source.webp)\n\nEnter “QuickStart Test Site” for the source name and click `Continue`. You have successfully added a source!\n\nNote at the bottom of the JavaScript Source page is a `Write Key`. You will need this for sending a test event after connecting the Snowflake destination.\n\n## Create Snowflake destination\n\nNavigate to the **Overview** tab in the JavaScript source view and click on **Add Destination**.\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/add-destination.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/add-destination.webp)\n\nSelect the Snowflake destination from the list, then on the next page give it the name “Snowflake QuickStart” and click **Continue**.\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/select-snowflake.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/select-snowflake.webp)\n\nAdd in your Snowflake connection credentials:\n\n*   **Account**: Your account name.\n*   **Database**: Your database name that you used in the previous steps for `QUICKSTART`.\n*   **Warehouse**: Your warehouse that you granted usage to `QUICKSTART`.\n*   **User**: `QUICKSTART_USER`\n*   **Role**: `QUICKSTART`\n*   **Password**: Password for `QUICKSTART_USER`.\n*   **Namespace**: `EVENTS`\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/snowflake-config.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/snowflake-config.webp)\n\nAt the bottom under **Object Storage Configuration** toggle **Use RudderStack managed object storage** ON.\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/object-storage-toggle.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/object-storage-toggle.webp)\n\nLeave the defaults for all other settings and click **Continue**. RudderStack will verify credentials and that it has the needed permissions.\n\nYou have now created a pipeline connection in RudderStack!\n\n## Send test event\n\nYou can use a test site to send a `connection_setup` event. This will not effect the sample data tables. But first, get the following configuration data from RudderStack:\n\n*   RudderStack Data Plane URL\n*   JavaScript Source Write Key\n\n### Data Plane URL\n\nGo to the **Connections** page in the RudderStack app and copy the **Data Plane** URL from the top of the page.\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/data-plane-url.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/data-plane-url.webp)\n\n### Write key\n\nGo to your JavaScript source in RudderStack and in the **Setup** tab scroll down and copy the **Write key**.\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/write-key.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/write-key.webp)\n\n### Test event\n\nGo to RudderStack’s [test website](https://ryanmccrary.github.io/rudderstackdemo/) and copy your Data Plane URL and Write Key into the top fields and press **Submit**.\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/test-site-setup.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/test-site-setup.webp)\n\nEnter `connection_setup` into the `event_name` field next to **Send Custom Event** and then click on **Send Custom Event**.\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/test-site-event.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/test-site-event.webp)\n\nYou can check the event using RudderStack’s [**Live events**](https://www.rudderstack.com/docs/dashboard-guides/live-events/) view or check the **Syncs** tab in the Snowflake destination.\n\n**Note that the test event needs to be delivered to Snowflake to validate the pipeline.** If needed, you can run a manual sync by clicking **Sync now** in the **Syncs** tab of the Snowflake destination view in RudderStack.\n\n## Profiles schema and permissions\n\nRemember that Predictions automatically runs a Profiles job to create an identity graph. In this step, create a new schema where the identity graph and the related tables and views will be generated.\n\n```\nCREATE SCHEMA \"<YOUR_DATABASE>\".\"PROFILES\";\n```\n\nNow we need to grant permissions to the `QUICKSTART` role.\n\nProfiles will need the following permissions to run:\n\n*   Read access to all input tables to the model (already complete if you followed the previous setup steps)\n*   Write access to the schemas and common tables that the Profiles project creates.\n\nFor the write access run the following statements:\n\n```\nGRANT ALL PRIVILEGES ON SCHEMA PROFILES_QUICKSTART.PROFILES TO ROLE QUICKSTART;\nGRANT SELECT ON ALL TABLES IN SCHEMA PROFILES_QUICKSTART.PROFILES TO ROLE QUICKSTART;\nGRANT SELECT ON FUTURE TABLES IN SCHEMA PROFILES_QUICKSTART.PROFILES TO ROLE QUICKSTART;\nGRANT SELECT ON ALL VIEWS IN SCHEMA PROFILES_QUICKSTART.PROFILES TO ROLE QUICKSTART;\nGRANT SELECT ON FUTURE VIEWS IN SCHEMA PROFILES_QUICKSTART.PROFILES TO ROLE QUICKSTART;\n```\n\nYou are now ready to run Profiles and Predictions projects in the RudderStack UI!\n\n## Profiles CLI setup\n\nBefore you start building automated features, you need to perform some additional setup steps so that you can transition seamlessly from the UI-based workflow to the code-based workflow in the [code your own custom predictions](https://www.rudderstack.com/docs/archive/profiles/0.13/example/predictive-features-snowflake/custom-code/) section.\n\nTo build custom features with code, you will need Python3 and the RudderStack Profiles CLI tool (`PB`, for Profiles Builder) installed on your machine. If you do not have `PB` installed, follow the instructions below. This includes authentication for your Snowflake environment. **Use the warehouse, database, and schema setup in the previous steps.** This authentication will be used for accessing your Snowflake warehouse and running Snowpark. For more information about Profiles CLI tool, see [documentation](https://www.rudderstack.com/docs/profiles/get-started/profile-builder/).\n\n### Install Profile Builder tool\n\nOpen a console window and install the Profile Builder `PB` tool.\n\n```\npip3 install profiles-rudderstack\n```\n\nCheck the version to make sure it is at least `0.10.5`\n\n### Install ML dependency\n\nIn order to run ML models you will need to install the python package `profiles-multieventstream-features`. Run the following command to install it.\n\n```\npip install git+https://github.com/rudderlabs/profiles-pycorelib\n```\n\nEnsure you have the following python packages installed. These are required to use the `rudderstack-profiles-classifier` package to train classification models for predictive features.\n\n```\ncachetools>=4.2.2\nhyperopt>=0.2.7\njoblib>=1.2.0\nmatplotlib>=3.7.1\nseaborn>=0.12.0\nnumpy>=1.23.1\npandas>=1.4.3\nPyYAML>=6.0.1\nsnowflake_connector_python>=3.1.0\nsnowflake-snowpark-python[pandas]>=0.10.0\nscikit_learn>=1.1.1\nscikit_plot>=0.3.7\nshap>=0.41.0\nplatformdirs>=3.8.1\nxgboost>=1.5.0\nredshift-connector\n```\n\n### Create warehouse connection\n\nInitiate a warehouse connection:\n\nFollow the prompts and enter the details for your Snowflake warehouse/database/schema/user.\n\n```\nEnter Connection Name: quickstart\nEnter target:  (default:dev)  # Press enter, leaving it to default\nEnter account: <YOUR_ACCOUNT>\nEnter warehouse: <YOUR_WAREHOUSE>\nEnter dbname: <YOUR_DATABASE>\nEnter schema: PROFILES\nEnter user: QUICKSTART_USER\nEnter password: <password>\nEnter role: QUICKSTART\nAppend to /Users/<user_name>/.pb/siteconfig.yaml? [y/N]\ny\n```\n\n### Enable ML models\n\nFinally, enable ML models within `siteconfig.yaml`.\n\nOpen the file `/Users/<user_name>/.pb/siteconfig.yaml` in a text editor.\n\nAt the bottom of the file there is a `py_models` section. Update it to look like this:\n\n```\npy_models:\n    enabled: true\n    python_path: $(which python3)\n    credentials_presets: null\n    allowed_git_urls_regex: \"\"\n```\n\n## Snowpark\n\nPredictive features utilizes Snowpark within your Snowflake environment. It uses the same authentication as Snowflake and is able to run jobs within Snowflake.\n\nThis will run python code in a virtual warehouse in Snowflake and will incur compute costs. These costs vary depending on the type of model and the quantity of data used in training and prediction. For more general information on Snowflake compute costs, see [Understanding Compute Costs](https://docs.snowflake.com/en/user-guide/cost-understanding-compute).\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Prerequisites | RudderStack Docs",
    "description": "Prerequisites to generate predictive features in Snowflake using RudderStack Predictions",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.13/example/predictive-features-snowflake/",
    "markdown": "# Predictive features | RudderStack Docs\n\nCreate predictive features with RudderStack and Snowflake without using MLOps\n\n* * *\n\n*     2 minute read  \n    \n\nPredictive features like future LTV and churn propensity can be game changing for a business. If your marketing, customer success, and other teams want to use them, though, your company often faces a binary choice: use a one-size-fits-all solution within an existing SaaS platform (i.e., marketing automation tool), or build out ML and MLOps capabilities internally.\n\nBoth options have significant drawbacks. First, templated SaaS-based solutions can’t leverage all of your customer data and aren’t configurable, which results in low accuracy and impact. On the other hand, hiring data scientists and setting up MLOps is expensive and complex.\n\nModern data teams need an option in the middle: the ability to deploy model templates on all of their customer data, but without additional tooling, processes and headcount.\n\nWith RudderStack Predictions and Snowflake, you can create predictive features directly in your warehouse, without the need to set up MLOps processes and infrastructure. Predictions leverages the full power of Snowpark to run ML models within your existing data engineering workflow.\n\nIn this section, you will learn about two ways to build predictive features in RudderStack Predictions:\n\n1.  [Set up automated features in the RudderStack UI](https://www.rudderstack.com/docs/archive/profiles/0.13/example/predictive-features-snowflake/setup-automated-features/) - You can setup and run the jobs within the RudderStack UI. This process makes it easy for less technical users to implement basic predictive features.\n2.  [Code your own custom predictions](https://www.rudderstack.com/docs/archive/profiles/0.13/example/predictive-features-snowflake/custom-code/) - Predictions also supports a code-based approach that gives technical users full control to define custom predictive features that match their unique business logic.\n\nIt’s important to note that Predictions runs on top of RudderStack [Profiles](https://www.rudderstack.com/docs/profiles/overview/), a product that automates identity resolution and user feature development in Snowflake.\n\nPredictions leverages the Profiles identity graph to train and run ML models. Because Predictions is part of Profiles, project outputs include an identity graph, standard user featuers (i.e., `last_seen`) and predictive user features (i.e., `percentile_churn_score_30_days`). Both types of features are built using RudderStack data sources and standardized feature definitions.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Predictive features | RudderStack Docs",
    "description": "Create predictive features with RudderStack and Snowflake without using MLOps",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.12/core-concepts/feature-development/",
    "markdown": "# Feature Development | RudderStack Docs\n\nEnrich unified profiles with the required features/traits to drive targeted campaigns.\n\n* * *\n\n*     2 minute read  \n    \n\nOnce you have [performed identity stitching](https://www.rudderstack.com/docs/archive/profiles/0.12/core-concepts/identity-stitching/#how-to-perform-identity-stitching) to map the individual entities to known identifiers, you can use its output to enhance the unified profiles with additional data points and features.\n\nYou can define the features/traits in your warehouse tables and further perform calculations over this data to devise meaningful outcomes, which can help marketing teams to run effective campaigns.\n\n## Define features\n\nYou can use `var_group` to define features. Each `var_group` can have multiple entity vars which can be considered as features for that entity. The Profiles project generates and runs SQL in the background and automatically adds the resulting features to a [feature view](https://www.rudderstack.com/docs/archive/profiles/0.12/example/entity-traits-360/).\n\nYou can produce a customer [feature view](https://www.rudderstack.com/docs/archive/profiles/0.12/example/entity-traits-360/) or a feature view for specific projects like personalization, recommendations, or analytics.\n\nYou can combine the features to create even more features. You can also use [custom SQL queries](https://www.rudderstack.com/docs/archive/profiles/0.12/example/sql-model/) to enrich unified user profiles for advanced use cases.\n\nA sample `pb_project.yaml` file with a definition of a feature\\_view::\n\n```\n...\nentities:\n  - name: user\n    id_types:\n      - main_id\n      - user_id\n    feature_views:\n      using_ids:\n        - id: user_id\n```\n\nA sample configuration file to create `var_groups`:\n\n```\nvar_groups:\n  - name: user_vars\n    entity_key: user\n    vars:\n      - entity_var:\n          name: first_seen\n          select: min(timestamp::date)\n          from: inputs/rsTracks\n          is_feature: false\n      - entity_var:\n          name: last_seen\n          select: max(timestamp::date)\n          from: inputs/rsTracks\n          is_feature: false\n      - entity_var:\n          name: user_lifespan\n          select: '{{user.Var(\"last_seen\")}} - {{user.Var(\"first_seen\")}}'\n          description: Life Time Value of a customer\n      - entity_var:\n          name: days_active\n          select: count(distinct timestamp::date)\n          from: inputs/rsTracks\n          description: No. of days a customer was active\n# ID stitcher\nmodels:\n  - name: domain_profile_id_stitcher\n    model_type: id_stitcher\n    model_spec:\n      validity_time: 24h # 1 day\n      entity_key: user\n      materialization:\n        run_type: incremental\n      edge_sources:\n        - from: inputs/rsIdentifies\n        - from: inputs/rsTracks\n```\n\n## Benefits\n\n*   You can use the output of the identity graph to define or compute features based on given ID types. Feature views creates a view which will have all or a specified set of features on that entity from across the project based on the identifier column provided.\n*   As the number of features/traits increases, Profiles makes the maintenance process much easier by using a configuration file (as opposed to large and complex SQL queries).\n*   Profiles generates highly performant SQL to build feature views, which helps mitigate computing costs and engineering resources when the data sets become large, dependencies become complex, and features require data from multiple sources.\n\n### Use-cases\n\n*   Create analytics queries like demographic views, user activity views, etc.\n*   Send data using a [Reverse ETL](https://www.rudderstack.com/docs/sources/reverse-etl/) pipeline to various cloud destinations.\n*   Use RudderStack Audiences to send customer profiles to marketing tools (available for beta customers).\n\n#### See also\n\n*   [Sample feature table project](https://www.rudderstack.com/docs/archive/profiles/0.12/example/feature-table/)\n*   [Sample SQL model project](https://www.rudderstack.com/docs/archive/profiles/0.12/example/sql-model/)\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Feature Development | RudderStack Docs",
    "description": "Enrich unified profiles with the required features/traits to drive targeted campaigns.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.13/example/predictive-features-snowflake/setup-automated-features/",
    "markdown": "# Setup Automated Features | RudderStack Docs\n\nSet up RudderStack to build automated features in the RudderStack UI\n\n* * *\n\n*     6 minute read  \n    \n\nSetting up automated features in the RudderStack UI is a straight-forward process. Predictive features are configured within a Profiles project and automatically added to the feature table output when the project is run.\n\n## Project setup\n\nFollow the steps below to set up a project and build predictive features:\n\n### Log into RudderStack\n\nYou can log-in [here](https://app.rudderstack.com/login).\n\n### Navigate to Profiles screen\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/Navigation.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/Navigation.webp)\n\n### Enter a name and description\n\nEnter a unique name and description for the Profiles Project where you want to build the predictive features.\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/Profiles-Name.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/Profiles-Name.webp)\n\n### Select sources\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/Sources.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/Sources.webp)\n\nSelect your Snowflake warehouse. If you have not configured the Snowflake warehouse, set up an event stream connection to Snowflake in RudderStack ([see details here](https://www.rudderstack.com/docs/destinations/warehouse-destinations/snowflake/)) and refer to the setup steps above.\n\nOnce you select the warehouse, you will be able to choose from RudderStack event sources that are connected to Snowflake. In this example, the JavaScript source created above is used to write to the same schema as the sample data. Profiles will use the `PAGES`, `TRACKS`, `IDENTIFIES`, and `ORDER_COMPLETED` tables from that schema to build automated and predictive features.\n\n## Map ID fields\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/Map-ID.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/Map-ID.webp)\n\nMap the fields from the source table(s) to the correct type of ID. The standard ID types are:\n\n*   `user_id`\n*   `anonymous_id`\n*   `email`\n\n**Note that for RudderStack event sources, standard ID column names will be mapped for you automatically**. If you have included additional identifiers in your payloads, you can map those custom column names to standard identifiers by clicking **Add mapping** at the bottom of the table.\n\n### Map `Order_Completed` table\n\nClick on **Add mapping** and map the `USER_ID` and `ANONYMOUS_ID` columns to standard identifiers to include the `ORDER_COMPLETED` table as a source for the identity graph and user features.\n\n| Source | Event | Property | ID Type |\n| --- | --- | --- | --- |\n| QuickStart Test Site | ORDER\\_COMPLETED | USER\\_ID | user\\_id |\n| QuickStart Test Site | ORDER\\_COMPLETED | ANONYMOUS\\_ID | anonymous\\_id |\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/map-id-orders.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/map-id-orders.webp)\n\n## Create default features in the UI\n\nThere are two types of automated features you can define in the UI:\n\n*   Default features\n*   Custom features\n\nThis guide focuses on the default features that are automatically generated.\n\n### Set up default features\n\nDefault features are features commonly used in Profiles projects. RudderStack provides a template library for these features to make them easy to add to your project. Templated features give you access to over 40 different standard and predictive features, which are generated in Snowflake automatically.\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/feature-categories.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/feature-categories.webp)\n\nDefault features are divided into 4 categories:\n\n*   **Attribution** - campaign, source, and churn features\n*   **Demographics** - user trait features\n*   **Engagement** - user activity features\n*   **Predictive ML Features** - predictive features\n\nYou can open the drop down menu for each category and select as many as you would like for your project.\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/feature-attribution.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/feature-attribution.webp)\n\nFor this guide, select:\n\n*   Attribution\n    *   `first_source_name`\n    *   `is_churned_30_days`\n    *   `is_churned_90_days`\n*   Demographics\n    *   `first_name`\n    *   `last_name`\n    *   `state`\n*   Engagement\n    *   `first_date_seen`\n    *   `last_date_seen`\n    *   `total_sessions_90_days`\n    *   `total_sessions_last_week`\n*   Predictive ML Features\n    *   `percentile_churn_score_30_days`\n\nIt is important to remember that RudderStack runs all of the feature-generation code transparently in Snowflake. For any of the default features, other than Predictive ML Features, you can click on **Preview Code** and get a yaml code snippet defining that feature (the yaml definition is used to generate SQL). This is helpful for technical users who want a deeper understanding of feature logic (and a running start for coding their own features).\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/churn-code-snippet.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/churn-code-snippet.webp)\n\n#### Churn definition\n\nRudderStack Predictions automatically generates a binary churn value for every user based on inactivity over a 7, 30, or 90-day period.\n\nFor example, to calculate the `is_churned_7_days` value, RudderStack looks for any activity timestamp for a particular user in the `TRACKS` and `PAGES` tables over the previous 7 day period. Practically, this means that RudderStack executes a ‘max timestamp’ query against those tables to see if users have viewed a page or performed other tracked actions (like clicks, form submits, add to carts, etc.) and then calculates the difference from today. If the query returns 7 or more, that means they haven’t performed any activity over the last 7 days and their `is_churned_7_days` trait is set to `1`.\n\n#### How Predictions models percentile churn scores\n\nUsing the standard definition (no activity over a defined period), RudderStack Predictions automatically runs a python-based churn model in Snowpark that predicts whether users will become inactive (churn) over the next 7, 30, or 90-day period. This model is trained on existing user data, using the Profiles identity graph, so it is recommended that you have a minimum of 5,000-10,000 unique users to achieve accurate output for business use cases.\n\n**How Predictions automates ML with Snowpark**\n\nPredictions streamlines integration with Snowpark by using the authentication from your existing Snowflake integration in RudderStack.\n\nIn order to run models in Snowpark, there is one additional set of permissions required. To run Predictions jobs, you must have permission to create stages within your schema. For more information see the **CREATE STAGE** [documentation](https://docs.snowflake.com/en/sql-reference/sql/create-stage#access-control-requirements).\n\nOnce permissions are granted, you will be able to run jobs that produce predictive features. **If you have followed the steps in [Prerequisite](https://www.rudderstack.com/docs/archive/profiles/0.13/example/predictive-features-snowflake/prerequisite/) guide, that permission has already been granted.**\n\n## Create custom features in the UI\n\nIf a needed feature is not in the template library, you can define a custom feature in the UI. Custom features can be standard or predictive features.\n\n### Add Custom Features\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/custom-feature.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/custom-feature.webp)\n\nClick on **Add a custom feature** at the top of the page and build an `average_order` feature with the following values:\n\n| Field | Value |\n| --- | --- |\n| **Name** | average\\_order |\n| **Description** | Average Order Size including shipping, taxes, and discounts |\n| **Function Type** | AGGREGATE |\n| **Function** | AVG |\n| **Event** | EVENTS.ORDER\\_COMPLETED |\n| **Property or Trait** | TOTAL |\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/custom-feature-define.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/custom-feature-define.webp)\n\nOnce complete click **Save**. The custom feature will be added to the top of the page.\n\n## Set Schedule\n\nThere are three options to set a schedule for how often the feature generation job runs:\n\n*   Basic\n*   Cron\n*   Manual\n\n### Basic\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/schedule-basic.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/schedule-basic.webp)\n\nSchedule on a predetermined interval.\n\nThe frequency can be every:\n\n*   30 minutes\n*   1 hour\n*   3 hours\n*   6 hours\n*   12 hours\n*   24 hours\n\nThen select a starting time for the initial sync.\n\n### Cron\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/schedule-cron.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/schedule-cron.webp)\n\nSchedule using cron expressions for more specific scheduling (i.e. Daily on Tuesdays and Thursdays).\n\nIf you are not familiar with cron expressions, you can use the builder in the UI.\n\n### Manual\n\nOnly runs when manually triggered within the UI. For this guide, select **Manual**.\n\n## Save, review, and create project\n\n### Save project\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/save-project.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/save-project.webp)\n\nFill in the **Schema** field with `PROFILES` (to match what we created earlier). This is where the feature table will be written to in Snowflake.\n\n### Review and create project\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/review-create.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/review-create.webp)\n\nFinally, review all the settings and when ready, click `Create user 360`.\n\n## Review created features\n\nOnce the initial project run is initiated, it may take up to 25-30 minutes to complete. Once the job is done, you are able to explore the data in RudderStack’s UI, including model fit charts for predictive features and individual user records with all features.\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/ui-predictive.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/ui-predictive.webp)\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/ui-predictive-graphs.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/ui-predictive-graphs.webp)\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/ui-explorer.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/ui-explorer.webp)\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Setup Automated Features | RudderStack Docs",
    "description": "Set up RudderStack to build automated features in the RudderStack UI",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.13/example/predictive-features-snowflake/custom-code/",
    "markdown": "# Custom Predictive Features | RudderStack Docs\n\nCode your own custom predictive features\n\n* * *\n\n*     8 minute read  \n    \n\nWhile automated features are incredibly useful for quickly deploying activity-based churn scores, data teams inevitably want to go deeper and define custom predictions that match their unique business logic and KPIs.\n\nBasic customization is possible in the UI as we covered above, but Predictions also supports a code-based workflow that gives technical users full control and complete customizability, as well as the ability to integrate the process into their existing development workflow.\n\nFor example, if you are an eCommerce company, it can be helpful to predict whether or not a user will make a purchase over a certain dollar amount, over the next `n` days.\n\nRudderStack makes it easy to migrate from the UI-based workflow to the code-based workflow to build these more complex use cases.\n\n## Download project files\n\nOn the Profiles screen, find your project and click the **Download this Project** button on the top right side. This will download all the files for that Profiles project in a compressed (zip) file including the modeling files.\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/download-project.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/download-project.webp)\n\nInside the Profiles folder you will find `pb_project.yaml` and a `models` folder with `resources.yaml`.\n\n```\nRudderStack QuickStart\n├── pb_project.yaml\n├── models\n│   ├── resources.yaml\n```\n\n### pb\\_project.yaml\n\n`pb_project.yaml` is the main configuration file for the Profiles project. The top section defines the `name`, `schema_version`, `connection`, and `model_folder` (where the files that define the details of the Profiles project can be found).\n\nUpdate the following values:\n\n*   `name` to `Profile-Quickstart` to match the name in the UI.\n*   `connection` to `QUICKSTART` to match the database connection we made in the Prerequisites section.\n\n```\nname: Profile-QuickStart\nschema_version: 49 # Or most recent version\nconnection: QUICKSTART\nmodel_folders:\n    - models\n```\n\nBelow there is an `entities` section that defines the entities and the kinds of ID’s make up that entity. An entity is a business concept or unit that will be used to build the identity graph and features. Projects can contain multiple entities like user, household, and organization.\n\nFor this project, there is one entity called `user` with 5 different types of IDs. An ID type maps which ID fields can be joined together. For example, if you have two tables with `user_id` columns called `id` and `userid`, by giving each the type `user_id` Profiles knows to join those tables on those columns.\n\nThe ID fields are already mapped to these types in the UI. Nothing needs to be updated in this section.\n\n```\nentities:\n  - name: user\n    id_types:\n      - main_id\n      - user_id\n    feature_views:\n      using_ids:\n        - id: email\n          name: features_by_email\n        - id: salesforce_id\n          name: salesforce_id_stitched_features\n      features:\n        - from: models/cart_feature_table\n          include:\n            - \"*\"\n```\n\nFinally there is a `packages` section. This section allows you to import a Profiles project from GitHub and use the feature definitions from that project in this one. The imported project provides the definitions for the standard features selected in the UI. Nothing needs to be updated in this section.\n\n```\npackages:\n    - name: base_features\n      url: https://github.com/rudderlabs/rudderstack-profiles-multieventstream-features\n      inputsMap: # These are the tables automatically mapping in the UI (TRACKS, PAGES, IDENTIFIES)\n        rsIdentifies_1: inputs/rsIdentifies_1\n        rsIdentifies_2: nil\n        rsIdentifies_3: nil\n        rsPages_1: inputs/rsPages_1\n        rsPages_2: nil\n        rsPages_3: nil\n        rsTracks_1: inputs/rsTracks_1\n        rsTracks_2: nil\n        rsTracks_3: nil\n      overrides: # By default all features are imported from the project, therefore the features we did not select need to be disabled\n        - requested_enable_status: disabled\n          models:\n            - entity/user/active_days_in_past_365_days\n            - entity/user/active_days_in_past_7_days\n            - entity/user/avg_session_length_in_sec_365_days\n            - entity/user/avg_session_length_in_sec_last_week\n            - entity/user/avg_session_length_in_sec_overall\n            - entity/user/campaign_sources\n            - entity/user/campaigns_list\n            - models/churn_7_days_model\n            - models/churn_90_days_model\n            - entity/user/country\n            - entity/user/currency\n            - entity/user/days_since_account_creation\n            - entity/user/days_since_last_seen\n            - entity/user/first_campaign_name\n            - entity/user/is_churned_7_days\n            - entity/user/last_campaign_name\n            - entity/user/last_source_name\n            - entity/user/max_timestamp_bw_tracks_pages\n            - entity/user/mediums_list\n            - entity/user/sources_list\n            - entity/user/total_sessions_365_days\n            - entity/user/total_sessions_till_date\n```\n\n### resource.yaml\n\n`resources.yaml` contains two main sections: `inputs` and `var_groups`.\n\nThe `inputs` section defines what ID’s are in each table and their mapping. Currently these are all the tables and mappings that were defined in the UI. These tables are used for creating an identity graph and all features related to it.\n\nIf you want to add another table in the future, the table and ID mappings would be added here. Below is an example of the `ORDER_COMPLETED` table we manually mapped in the UI. It consists of the following fields:\n\n| Field | Description |\n| --- | --- |\n| name | alias for the table; the primary reference in the rest of the yaml files |\n| table | `<SCHEMA>.<TABLE_NAME>` |\n| select | column with ID |\n| type | kind of ID |\n| entity | what entity the ID should be mapped to |\n| to\\_default\\_stitcher | `true` unless you decide to use a different ID stitcher |\n| remapping | leave as `null` |\n\n```\n- name: rs_EVENTS_ORDER_COMPLETED\n  app_defaults:\n    table: EVENTS.ORDER_COMPLETED\n    ids:\n        - select: USER_ID\n          type: user_id\n          entity: user\n          to_default_stitcher: true\n        - select: ANONYMOUS_ID\n          type: anonymous_id\n          entity: user\n          to_default_stitcher: true\n  remapping: null\n```\n\nThe `var_groups` section is where custom features are defined, both custom features created in the UI and those added via code in this file. Custom features are organized into groups by entity (in our case only `user`). The entity is like the `group by` variable in a SQL query.\n\nBelow that custom features are defined in the `vars` subsection. Here is the `average_order` feature we created in the UI.\n\n```\n- entity_var:\n    is_feature: true\n    name: average_order\n    description: Average Order Size including shipping, taxes, and discounts\n    select: AVG(TOTAL)\n    from: inputs/rs_EVENTS_ORDER_COMPLETED\n```\n\nA name and description are required for the custom feature and then it is defined using declarative SQL syntax. This allows you to define the custom feature the same way you would if creating a new table with SQL.\n\n## Create a custom predictive feature\n\nJust like in the UI workflow, you must already have defined the feature you want to predict. Therefore we are going to add a new custom feature for large purchases in the last 90 days. **NOTE: Currently predictive features can only be binary (i.e. 1/0)**\n\nA large order is defined here as any order with a `TOTAL` of > $100.\n\nAt the bottom of the `resources.yaml`, add the name and definition for `large_purchase_last_90`.\n\n```\n- entity_var:\n  name: large_purchase_last_90\n  description: Customer that made a purchase of >$100 in the last 90 days.\n  select: CASE WHEN MAX(TOTAL) > 100 THEN 1 ELSE 0 END\n  from: inputs/re_EVENTS_ORDER_COMPLETED\n  where: DATEDIFF(days, TIMESTAMP, CURRENT_DATE) <= 90\n```\n\nYou can use SQL functions and keywords in the definition. FOr example, a `CASE` statement in the SELECT statement and add a `where` statement and use the `DATEDIFF` function. You can also use the alias for the `ORDER_COMPLETED` table in the `from` statement.\n\nFor more details on Profiles and project file structure, you can review the Profiles [documentation](https://www.rudderstack.com/docs/profiles/overview/).\n\n## Organize the project in two files (**OPTIONAL**)\n\nProfiles does not need specific yaml files in the `models` folder in order to run. That allows you to organize your code as you feel is best. You can keep it all in one file or can split it over multiple files.\n\nYou can split the `resources.yaml` file into `inputs.yaml` and `profiles.yaml` by creating the two yaml files. Then copy everything from the `inputs` section into `inputs.yaml` and `var_groups` into `profiles.yaml`.\n\nOnce done, you can delete the `resources.yaml`.\n\n## Add a custom predictive feature\n\nThis section explains how to create 2 new custom predictive features from `large_purchase_last_90` called `likelihood_large_purchase_90` (raw score) and `percentile_large_purchase_90`(percentile score).\n\n#### Add Python ML requirement\n\nIn order to add custom predictive features, add the `profiles-pycorelib` package to the project requirements. At the bottom of `pb_project.yaml` add the following code to `pb_project.yaml`.\n\n```\npython_requirements:\n  - profiles-pycorelib==0.2.1\n```\n\n#### Create ml\\_models.yaml\n\nNow, create a new file and name it `ml_models.yaml`. This file is where you can define 2 new custom predictive features and how to train the ML model. The code for these new predictive features is discussed below.\n\nThis file is organized by the predictive model created for predictive features, not the individual features. The top level consists of:\n\n| Field/Section | Description |\n| --- | --- |\n| `name` | Name of the model (not feature) |\n| `model_type` | `python_model` |\n| `model_spec` | All of the model specifications |\n\n* * *\n\n`model_spec` section:\n\n| Section | Description |\n| --- | --- |\n| `train` | Training configuration |\n| `predict` | Scoring configuration |\n\n```\nmodels:\n    - name: &model_name large_purchase_90_model\n      model_type: python_model\n      model_spec:\n        occurred_at_col: insert_ts\n        entity_key: user\n        validity_time: 24h\n        py_repo_url: git@github.com:rudderlabs/rudderstack-profiles-classifier.git # Model training and scoring repo\n\n        train:\n          file_extension: .json\n          file_validity: 2160h # 90 days; when the model will be retrained\n          inputs: &inputs\n            - packages/base_features/models/rudder_user_base_features # location of the base features created in the UI\n            - packages/large_purchase_last_90 # custom feature created in var_groups\n            - models/average_order # custom feature we created in the UI\n          config:\n            data: &model_data_config\n              package_name: feature_table\n              label_column: large_purchase_last_90 # target feature\n              label_value: 1 # target feature value predicting\n              prediction_horizon_days: 90 # how far into the future\n              features_profiles_model:  'rudder_user_base_features' #taken from inputs\n              output_profiles_ml_model: *model_name\n              eligible_users: 'large_purchase_last_90 is not null' # limit training data to those with non-null values\n              inputs: *inputs\n            preprocessing: &model_prep_config\n              ignore_features: [first_name, last_name, state] # features we do not used in a model\n\n        predict:\n          inputs: *inputs # copy from train\n          config:\n            data: *model_data_config # copy from train\n            preprocessing: *model_prep_config # copy from train\n            outputs:\n              column_names:\n                percentile: &percentile percentile_large_purchase_90 # name of percentile feature\n                score: &raw_score likelihood_large_purchase_90 # name of raw likelihood feature\n              feature_meta_data: &feature_meta_data\n                features:\n                  - name: *percentile\n                    description: 'Percentile of likelihood score. Higher the score the more likely to make a larger purchase'\n                  - name: *raw_score\n                    description: 'Raw likelihood score. Higher the score the more likely to make a larger purchase'\n\n        <<: *feature_meta_data\n```\n\n## Compile and run\n\nSave all files. Now compile the project, this will make sure all SQL and python files are able to be created.\n\nFinally, run the project. This will generate the same files as `compile` and then execute them in Snowflake. The first run can take at least 30 minutes because of training ML models.\n\n## Final table\n\nThe final predictive features can be found in your Snowflake environment together in the same table. The table will provide you with the unified user ID, created by RudderStack, when the features are valid as of (i.e. when the model was last run to create these features), and model ID, and your predictive features.\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/final-table.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/final-table.webp)\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Custom Predictive Features | RudderStack Docs",
    "description": "Code your own custom predictive features",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.12/cli-user-guide/structure/",
    "markdown": "# Project Structure | RudderStack Docs\n\nKnow the specifications of a site configuration file, PB project structure, configuration files, and their parameters.\n\n* * *\n\n*     8 minute read  \n    \n\nOnce you complete the [Profile Builder CLI](https://www.rudderstack.com/docs/archive/profiles/0.12/get-started/profile-builder/) steps, you will be able to see the Profiles project on your machine.\n\n## Site configuration file\n\nRudderStack creates a site configuration file (`~/.pb/siteconfig.yaml`) while [creating a warehouse connection](https://www.rudderstack.com/docs/archive/profiles/0.12/get-started/profile-builder/#2-create-warehouse-connection). It contains the following details including secrets (if any):\n\n*   Warehouse connection details and its credentials.\n*   Git repository connection credentials (if any).\n\n> ![success](https://www.rudderstack.com/docs/images/tick.svg)\n> \n> If you have multiple Profiles projects and they use different warehouse connections, you can store the details for multiple connections in the same site configuration file.\n\nA sample site configuration file containing multiple warehouse connection details is shown below:\n\n```\nconnections:\n  prod-db-profile:\n      target: dev\n      outputs:\n          dev:\n              account: inb828.us-west-3\n              dbname: MAT_STORE\n              password: password\n              role: PROFILES_ROLE\n              schema: AB_SCHEMA\n              type: snowflake\n              user: rik\n              warehouse: PROD_WAREHOUSE\n  test-db-profile:\n      target: test\n      outputs:\n          db:\n              access_token: dabasihasdho\n              catalog: rs_dev\n              host: adb-98.18.azuredatabricks.net\n              http_endpoint: /sql/1.0/warehouses/919uasdn92h\n              port: 443\n              schema: rs_profiles\n              type: databricks\n              user: johndoe@abc.onmicrosoft.com\n          dev:\n              account: uk12.us-west-1\n              dbname: RUDDERSTACK_DB\n              password: password\n              role: RS_ROLE\n              schema: RS_PROFILES\n              type: snowflake\n              user: johndoe\n              warehouse: RS_WAREHOUSE\n          redshift_v1:\n              dbname: warehouse_rs\n              host: warehouse.abc.us-east-3.redshift.amazonaws.com\n              password: password\n              port: 5419\n              schema: rs_profiles\n              type: redshift\n              user: redshift_user\n          redshift_v2:\n              workgroup_name: warehouse_workgroup\n              region: us-east-1\n              driver: v2\n              sslmode: require\n              dbname: warehouse_rs\n              schema: rs_profiles\n              type: redshift\n              access_key_id: ******************\n              secret_access_key: ******************************\n           big:\n              credentials:\n                auth_provider_x509_cert_url: https://www.googleapis.com/oauth2/v1/certs\n                auth_uri: https://accounts.google.com/o/oauth2/auth\n                client_email: johndoe@big-query-integration-poc.iam.gserviceaccount.com\n                client_id: \"123345678909872\"\n                client_x509_cert_url: https://www.googleapis.com/robot/v1/metadata/x509/johndoe%40big-query-integration-poc.iam.gserviceaccount.com\n                private_key: |\n                    -----BEGIN PRIVATE KEY-----                    \n                   ## private key\n                    -----END PRIVATE KEY-----\n                private_key_id: 5271368bhjbd72y278222e233w23e231e\n              project_id: big-query-integration-poc\n                token_uri: https://oauth2.googleapis.com/token\n                type: service_account\n                project_id: rs_profiles\n              schema: rs_profiles\n              type: bigquery\n              user: johndoe@big-query-integration-poc.iam.gserviceaccount.com\ngitcreds:\n - reporegex: \"git@github.com:REPO_OWNER/*\" # in case of ssh url\n   key: |\n       -----BEGIN OPENSSH PRIVATE KEY-----\n       **********************************************************************\n       **********************************************************************\n       **********************************************************************\n       **********************************************************************\n       ****************************************************************\n       -----END OPENSSH PRIVATE KEY-----       \n - reporegex: \"https://github.com/rudderlabs/*\" # https url\n   basic_auth:\n     username: oauth2\n     password: ... # your personal access token with read permission\npy_models:\n    enabled: true # in case you are using Python models in your project, else set it to false\n    python_path: /opt/anaconda3/bin/python # the path where Python is installed (run `which python` to get the full path). If `py_models` is not enabled, set it to `\"\"`. For Windows, you may pass the path value as: python.exe\n    credentials_presets: null\n    allowed_git_urls_regex: \"\"\ncache_dir: /Users/YOURNAME/.pb/WhtGitCache/ # For Windows, the directory path will have forward slash (\\)\nfilepath: /Users/YOURNAME/.pb/siteconfig.yaml # For Windows, the file path will have forward slash (\\)\n```\n\n## Profiles project structure\n\nThe following image shows the folder structure of the project:\n\n[![Project structure](https://www.rudderstack.com/docs/images/profiles/project-structure.webp)](https://www.rudderstack.com/docs/images/profiles/project-structure.webp)\n\n### `pb_project.yaml`\n\nThe `pb_project.yaml` file contains the project details like the name, schema version, warehouse connection, [entityEntity refers to a digital representation of a class of real world distinct objects for which you can create a profile.](https://www.rudderstack.com/docs/resources/glossary/#entity) names along with ID types, etc.\n\nA sample `pb_project.yaml` file with entity type as `user`:\n\n```\nname: sample_attribution\nschema_version: 54\nconnection: test\ninclude_untimed: true\nmodel_folders:\n  - models\nentities:\n  - name: user\n    id_types:\n      - main_id\n      - user_id\n      - anonymous_id\n      - email\npackages:\n  - name: corelib\n    url: \"https://github.com/rudderlabs/rudderstack-profiles-corelib/tag/schema_{{best_schema_version}}\"\n\n# Profiles can also use certain model types defined in Python.\n# Examples include ML models. Those dependencies are specified here.\npython_requirements:\n  - profiles-pycorelib==0.1.0\n```\n\nThe following table explains the fields used in the above file:\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `name` | String | Name of the project. |\n| `schema_version` | Integer | Project’s YAML version. Each new schema version comes with improvements and added functionalities. |\n| `connection` | String | Connection name from [`siteconfig.yaml`](https://www.rudderstack.com/docs/archive/profiles/0.12/cli-user-guide/structure/) used for connecting to the warehouse. |\n| `include_untimed` | Boolean | Determines if inputs having no timestamps should be allowed. If true, data without timestamps is included when running the models. |\n| `model_folders` | String | Names of folders where model files are stored. |\n| [`entities`](#entities) | List | Lists all the entities used in the project for which you can define models. Each entry for an entity here is a JSON object specifying entity’s name and attributes. |\n| `packages` | List | List of packages with their name and URL. Optionally, you can also extend ID types filters for including or excluding certain values from this list. |\n\n##### `entities`\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `name` | String | Name of the entity used in the project. |\n| [`id_types`](#id_types) | List | List of all identifier types associated with the current entity. |\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> The identifiers listed in `id_types` may have a many-to-one relationship with an entity but each ID must belong to a single entity.\n> \n> For example, a `user` entity might have `id_types` as the `salesforce_id`, `anonymous_id`, `email`, and `session_id` (a user may have many session IDs over time). However, it should not include something like `ip_address`, as a single IP can be used by different users at different times and it is not considered as a user identifier.\n\n##### `packages`\n\nYou can import library packages in a project signifying where the project inherits its properties from.\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `name` | String | Specify a name. |\n| `url` | String | HTTPS URL of the lib package, with a tag for the best schema version. |\n\n### `inputs.yaml`\n\nThe `inputs.yaml` file lists all the input sources (tables/views) which should be used to obtain values for [models](#models) and eventually create output tables.\n\nIt also specifies the table/view along with column name and SQL expression for retrieving values. The input specification may also include metadata, and the constraints on those columns.\n\nA sample `inputs.yaml` file:\n\n```\ninputs:\n  - name: salesforceTasks\n    contract:\n      is_optional: false\n      is_event_stream: true\n      with_entity_ids:\n        - user\n      with_columns:\n        - name: activitydate\n        - name: whoid\n    app_defaults:\n      table: salesforce.task\n      # For BigQuery, it is recommended to use view (view: _views_<view_name>) instead of table for event streaming data sets.\n      occurred_at_col: activitydate\n      ids:\n        # column name or sql expression\n        - select: \"whoid\" \n          type: salesforce_id\n          entity: user\n          to_default_stitcher: true\n  - name: salesforceContact\n    contract:\n      is_optional: false\n      is_event_stream: true\n      with_entity_ids:\n        - user\n      with_columns:\n        - name: createddate\n        - name: id\n        - name: email\n    app_defaults:\n      table: salesforce.contact\n      # For BigQuery, it is recommended to use view (view: _views_<view_name>) instead of table for event streaming data sets.\n      occurred_at_col: createddate\n      ids:\n        - select: \"id\"\n          type: salesforce_id\n          entity: user\n          to_default_stitcher: true\n        - select: \"case when lower(email) like any ('%gmail%', '%yahoo%') then lower(email)  else split_part(lower(email),'@',2) end\"\n          type: email\n          entity: user\n          to_default_stitcher: true\n  - name: websitePageVisits\n    contract:\n      is_optional: false\n      is_event_stream: true\n      with_entity_ids:\n        - user\n      with_columns:\n        - name: timestamp\n        - name: anonymous_id\n        - name: context_traits_email\n        - name: user_id\n    app_defaults:\n      table: autotrack.pages\n      # For BigQuery, it is recommended to use view (view: _views_<view_name>) instead of table for event streaming data sets.\n      occurred_at_col: timestamp\n      ids:\n        - select: \"anonymous_id\"\n          type: rudder_anon_id\n          entity: user\n          to_default_stitcher: true\n        # below sql expression check the email type, if it is gmail and yahoo return email otherwise spilt email return domain of email.  \n        - select: \"case when lower(coalesce(context_traits_email, user_id)) like any ('%gmail%', '%yahoo%') then lower(coalesce(context_traits_email, user_id))  \\\n              else split_part(lower(coalesce(context_traits_email, user_id)),'@',2) end\"\n          type: email\n          entity: user\n          to_default_stitcher: true\n```\n\nThe following table explains the fields used in the above file:\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `name` | String | Name of the input model. |\n| `contract` | Dictionary | A model contract provides essential information about the model like the necessary columns and entity IDs that it should contain. This is crucial for other models that depend on it, as it helps find errors early and closer to the point of their origin. |\n| `app_defaults` | Dictionary | Values that input defaults to when you run the project directly. For library projects, you can remap the inputs and override the app defaults while importing the library projects. |\n\n##### `contract`\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `is_optional` | Boolean | Whether the model’s existence in the warehouse is mandatory. |\n| `is_event_stream` | Boolean | Whether the table/view is a series/stream of events. A model that has a `timestamp` column is an event stream model. |\n| `with_entity_ids` | List | List of all entities with which the model is related. A model M1 is considered related to model M2 if there is an ID of model M2 in M1’s output columns. |\n| `with_columns` | List | List of all ID columns that this contract is applicable for. |\n\n##### `app_defaults`\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `table`/`view` | String | Name of the warehouse table/view containing the data. You can prefix the table/view with an external schema or database in the same warehouse, if applicable. Note that you can specify either a table or view but not both. |\n| `occurred_at_col` | String | Name of the column in table/view containing the timestamp. |\n| [`ids`](#ids) | List | Specifies the list of all IDs present in the source table along with their column names (or column SQL expressions).<br><br>**Note**: Some input columns may contain IDs of associated entities. By their presence, such ID columns associate the row with the entity of the ID. The ID Stitcher may use these declarations to automatically discover ID-to-ID edges. |\n\n##### `ids`\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `select` | String | Specifies the column name to be used as the identifier. You can also specify a SQL expression if some transformation is required.<br><br>**Note**: You can also refer table from another Database/Schema in the same data warehouse. For example, `table: <database_name>.<schema_name>.<table_name>`. |\n| `type` | String | Type of identifier. All the ID types of a project are declared in [`pb_project.yaml`](#project-details). You can specify additional filters on the column expression.<br><br>**Note**: Each ID type is linked only with a single entity. |\n| `entity` | String | Entity name defined in the [`pb_project.yaml`](#project-details) file to which the ID belongs. |\n| `to_default_stitcher` | Boolean | Set this **optional** field to `false` for the ID to be excluded from the default ID stitcher. |\n\n### `profiles.yaml`\n\nThe `profiles.yaml` file lists entity\\_vars / input\\_vars used to create the output tables under `var_groups:`.\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `name` | String | A unique name for the var\\_group. |\n| `entity_key` | String | The entity to which the var\\_group belongs to. |\n| `vars` | Object | This section is used to specify variables, with the help of `entity_var` and `input_var`. Aggregation on stitched ID type is done by default and is implicit. |\n\nOptionally, you can create models using the above vars. The following fields are common for all the model types:\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `name` | String | Name of the model. Note that a table with the same name is created in the data warehouse. For example, if you define the name as `user_table`, the output table will be named something like `Material_user_table_<rest-of-generated-hash>_<timestamp-number>`. |\n| `model_type` | String | Defines the type of model. Possible values are: `id_stitcher`, `feature_table_model`, and `sql_template`. |\n| `model_spec` | Object | Creates a detailed configuration specification for the target model. Different schema is applicable for different model types as explained in each section below. |\n\nRudderStack supports the following model types:\n\n*   [Feature Views/Feature Table](https://www.rudderstack.com/docs/archive/profiles/0.12/example/feature-table/)\n*   [SQL Template](https://www.rudderstack.com/docs/archive/profiles/0.12/example/sql-model/)\n*   [ID Stitcher](https://www.rudderstack.com/docs/archive/profiles/0.12/example/id-stitcher/)\n*   [ID Collator](https://www.rudderstack.com/docs/archive/profiles/0.12/example/id-collator/)\n*   [Python models](https://www.rudderstack.com/docs/archive/profiles/0.12/predictions/python-models/)\n*   [Packages](https://www.rudderstack.com/docs/archive/profiles/0.12/example/packages/)\n\n### `README.md`\n\nThe `README.md` file provides a quick overview on how to use PB along with SQL queries for data analysis.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Project Structure | RudderStack Docs",
    "description": "Know the specifications of a site configuration file, PB project structure, configuration files, and their parameters.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.13/example/packages/",
    "markdown": "# Additional Concepts | RudderStack Docs\n\nAdditional concepts related to Profiles like packages, best practices, partial feature tables, etc.\n\n* * *\n\n*     19 minute read  \n    \n\nThis guide explains some of the advanced concepts related to Profiles.\n\n## Packages\n\nProfiles gives you the flexibility to utilize models from existing library projects while defining your own models and inputs within the PB project. This approach allows for a seamless integration of library of pre-existing features, which are readily available and can be applied directly to data streamed into your warehouse.\n\nIn the absence of any explicitly defined models, the PB project is capable of compiling and running models from the library package given that inputs are present in the warehouse as assumed in the lib package.\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> Packages currently work only on Snowflake.\n\nThe following list of packages are currently available in Profiles. You can [contact the RudderStack team](mailto:support@rudderstack.com) to access these:\n\n*   [profiles-corelib](https://github.com/rudderlabs/profiles-corelib)\n*   [profiles-base-features](https://github.com/rudderlabs/rudderstack-profiles-base-features)\n*   profiles-shopify-features\n*   profiles-ecommerce-features\n*   profiles-stripe-features\n*   profiles-multieventstream-features\n\nGenerally, there will be some deviations in terms of the database name and schema name of input models - however, you can easily handle this by remapping inputs.\n\nA sample `pb_project.yaml` file may look as follows:\n\n```\nname: app_project\nschema_version: 54\nprofile: test\npackages:\n  - name: test_ft\n    gitUrl: \"https://github.com/rudderlabs/librs360-shopify-features/tree/main\"\n```\n\nIn this case, the PB project imports a single package. It does not require a separate `models` folder or entities as the input and output models will be sourced from the imported packages.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> Note that:\n> \n> *   If non-mandatory inputs required by the model are not present in the warehouse, you can still run the model.\n> *   If there is any deviation in the table/view name for input models, that is, if the inputs assumed in library package are present under some other name, make sure to do the remapping.\n> *   If some of the assumed inputs are not present at all, they should be remapped to `nil`. This way you can create and run imported packages with minimal set of inputs present.\n\nFor example, to import a library package with the name of `shopify_features`:\n\n```\npackages: \n  - name: shopify_features\n    url: https://github.com/rudderlabs/librs360-shopify-features/tree/main\n    inputsMap: \n      rsCartCreate: inputs/rsWarehouseCartCreate\n      rsCartUpdate: inputs/rsCartUpdate\n      rsIdentifies: inputs/rsIdentifies\n      rsOrderCancelled: inputs/rsOrderCancelled\n      rsOrderCreated: inputs/rsOrderCreated\n      rsPages: nil\n      rsTracks: nil\n```\n\nIn `models`/`inputs.yaml`, these inputs need to be defined with table names present in the warehouse.\n\n```\ninputs:\n  - name: rsWarehouseCartCreate\n    table: YOUR_DB.YOUR_SCHEMA.CART_CREATE_TABLE_NAME_IN_YOUR_WH\n    occurred_at_col: timestamp\n    ids:\n      - select: \"anonymous_id\"\n        type: anonymous_id\n        entity: user\n    source_metadata:\n      role: shopify\n      category: webhook\n  - name: rsIdentifies\n    table: YOUR_DB.YOUR_SCHEMA.IDENTIFIES\n    occurred_at_col: timestamp\n    ids:\n      - select: \"user_id\"\n        type: user_id\n        entity: user\n      - select: \"anonymous_id\"\n        type: anonymous_id\n        entity: user\n      - select: \"lower(email)\"\n        type: email\n        entity: user\n    source_metadata:\n      role: shopify\n      category: webhook\n```\n\nNote that the name of the table/view is changed to the appropriate name in your warehouse. If tables are present with the same name (including database name and schema name) then no remapping is required.\n\n### Modify ID types\n\n#### Extend existing package\n\nYou can add custom ID types to the default list or modify an existing one by extending the package to include your specifications.\n\nFor the corresponding `id_type`, add the key `extends:` followed by name of the same/different `id_type` that you wish to extend and the `filters` with `include`/`exclude` values.\n\n```\n---pb_project.yaml---\npackages:\n  - name: corelib\n    url: \"https://github.com/rudderlabs/rudderstack-profiles-corelib/tag/schema_{{best_schema_version}}\"\nid_types:\n  - name: user_id\n    extends: user_id\n    filters:\n      - type: exclude\n        value: 123456\nid_types:\n  - name: customer_id\n    extends: user_id\n    filters:\n      - type: include\n        regex: sample\n```\n\n*   **id\\_types**\n\nEnlists the type of identifiers to be used for creating ID stitcher/`entity_var`/`input_var`. For example, you can define anonymous IDs that do not include the value `undefined` or email addresses in proper format.\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `name` | String | Name of the ID type like email, user ID, etc. |\n| `extends` | List | (Optional) Name of the ID type you wish to extend. |\n| `filters` | List | Filter(s) the ID types to include/exclude specific values. The filters are processed in the defined order. |\n\n*   **filters**\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `type` | String | Type of filter. Allowed values are `include` or `exclude`. |\n| `value` | String | Value to match, for example, you can reject certain invalid ID values like `NaN`, `unknown`, `test@domain.com`, etc. |\n| `regex` | String | Regular expression with which to match the values. |\n| `sql` | List | SQL statement with `select` and `from` keys. |\n\n#### Custom list of ID types\n\nTo have custom list of ID types other than the provisions in the default package, you can remove and add your list as follows:\n\n```\nentities:\n  - name: user\n    id_types:\n      - user_id\n      - anonymous_id\n      - email\n\nid_types:\n  - name: user_id\n  - name: anonymous_id\n    filters:\n      - type: exclude\n        value: \"\"\n      - type: exclude\n        value: \"unknown\"\n      - type: exclude\n        value: \"NaN\"\n  - name: email\n    filters:\n    - type: include\n      regex: \"[A-Za-z0-9+_.-]+@(.+)\"\n```\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> Make sure that the ID types are also defined in the entity definition.\n\n## Model Contracts\n\nYou can use the `contract` field to specify the constraints your model should adhere to while using the warehouse data.\n\nSuppose a model (`M1`) is dependent on model (`M2`). Now, `M1` can specify a contract defining the columns and entities that it needs from `M2` to be executed successfully. Also, it becomes mandatory for `M2` to provide the required columns and entities for contract validation.\n\n**Example 1**\n\nThe following `inputs.yaml` file defines a contract:\n\n```\n- name: rsIdentifies\n  contract:\n    is_optional: false\n    is_event_stream: true\n    with_entity_ids:\n      - user\n    with_columns:\n      - name: timestamp\n      - name: user_id\n      - name: anonymous_id\n```\n\nHere, the contract specifies that:\n\n*   The input table (`rsIdentifies`) must exist in the warehouse.\n*   The model is an event stream model where every row in the table must be an event.\n*   There must be a column in the inputs table (`rsIdentifies`) which represents the user identifier for `user` entity.\n*   The input table (`rsIdentifies`) must have the `timestamp`, `user_id`, and `anonymous_id` columns.\n\n**Example 2**\n\nLet’s consider a SQL model, `rsSessionTable` which takes `shopify_session_features` as an input:\n\n```\nmodels:\n- name: rsSessionTable\n  model_type: sql_template\n  model_spec:\n    ... # model specifications\n    single_sql: |\n      {% set contract = BuildContract('{\"with_columns\":[{\"name\":\"user_id\"}, {\"name\":\"anonymous_id\"}]}') %}\n      {% with SessionFeature = this.DeRef(\"models/shopify_session_features\",contract)%}\n          select user_id as id1, anonymous_id as id2 from {{SessionFeature}}      \n    contract:\n      with_entity_ids:\n        - user\n      with_columns:\n        - name: user_id\n          type: string\n        - name: anonymous_id\n          type: string\n```\n\nThere are two contracts defined in the above example:\n\n*   Contract for `shopify_session_features` model which dictates that the `user_id`, and `anonymous_id` columns must be present.\n*   Contract for `rsSessionTable` model which dictates that it must have a column representing the user identifier for `user` entity. Also, the `user_id`, and `anonymous_id` columns must be present.\n\nThis helps in improving the data quality, error handling, and enables static and dynamic validation of the project.\n\n## Partial feature tables\n\nPartial feature tables are created when only a few input sources are available.\n\nFor example, lets say that you import a library package and some of the input models assumed in the package are not present in your warehouse.\n\nWhen you remap some of these input models to nil, those inputs and the features directly or indirectly dependent upon those inputs are disabled. In such cases, a partial feature table is created from the rest of the available inputs. Similarly, ID stitcher also runs even if few of the edge sources are not present in the warehouse or remapped to nil.\n\n## Pre and post hooks\n\nA pre hook enables you to execute an SQL before running a model, for example, if you want to change DB access, create a DB object, etc. Likewise, a post hook enables you to execute an SQL after running a model. The SQL can also be templatized. Here’s an example code snippet:\n\n```\nmodels:\n  - name: test_id_stitcher\n    model_type: id_stitcher\n    hooks:\n      pre_run: \"CREATE OR REPLACE VIEW {{warehouse.ObjRef('V1')}} AS (SELECT * from {{warehouse.ObjRef('Temp_tbl_a')}});\"\n      post_run: 'CREATE OR REPLACE VIEW {{warehouse.ObjRef(\"V2\")}} AS (SELECT * from {{warehouse.ObjRef(\"Temp_tbl_a\")}});'\n    model_spec:\n      - # rest of model specs go here\n```\n\n## Use Amazon S3 bucket as input\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> This is an experimental feature.\n\nIf you store data in your Amazon S3 bucket in a CSV file format, you can use it as an input for the Profiles models. The S3 URI path must be specified in the `app_defaults.s3`:\n\n```\nname: s3_table\ncontract:\n  is_optional: false\n  is_event_stream: true\n  with_entity_ids:\n    - user\n  with_columns:\n    - name: insert_ts\n      datatype: timestamp\n    - name: num_a\n      datatype: integer\napp_defaults:\n  s3: \"s3://bucket-name/prefix/example.csv\"\n  occurred_at_col: insert_ts\n  ids:\n    - select: \"id1\"\n      type: test_id\n      entity: user\n    - select: \"id2\"\n      type: test_id\n      entity: user\n```\n\nEnsure that the CSV file follows the standard format with the first row as the header containing column names, for example:\n\n```\nID1,ID2,ID3,INSERT_TS,NUM_A\na,b,ex,2000-01-01T00:00:01Z,1\nD,e,ex,2000-01-01T00:00:01Z,3\nb,c,ex,2000-01-01T00:00:01Z,2\nNULL,d,ex,2000-01-01T00:00:01Z,4\n```\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> Note that:\n> \n> *   To escape comma (`,`) from any cell of the CSV file, enclose that cell with double quotes `\" \"` .\n> *   Double quotes (`\" \"`) enclosing a cell are ignored.\n\nFollow the below steps to grant PB the required permissions to access the file in S3 Bucket:\n\n### Private S3 bucket\n\nAdd `region`, [`access key id`](#generate-access-key-id-and-secret-access-key), [`secret access key`](#generate-access-key-id-and-secret-access-key), and [`session token`](#generate-session-token) in your `siteconfig` file so that PB can access the private bucket. By default, the region is set to `us-east-1` unless specified otherwise.\n\n```\naws_credential:\n    region: us-east-1\n    access_key: **********\n    secret_access_key: **********\n    session_token: **********\n```\n\n#### Generate `access key id` and `secret access key`\n\n1.  Open the AWS IAM console in your AWS account.\n2.  Click **Policies**.\n3.  Click **Create policy**.\n4.  In the Policy editor section, click the JSON option.\n5.  Replace the existing JSON policy with the following policy and replace the <bucket\\_name> with your actual bucket name:\n\n```\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:GetObjectVersion\"\n            ],\n            \"Resource\": \"arn:aws:s3:::<bucket_name>/*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:ListBucket\",\n                \"s3:GetBucketLocation\"\n            ],\n            \"Resource\": \"arn:aws:s3:::<bucket_name>\",\n            \"Condition\": {\n                \"StringLike\": {\n                    \"s3:prefix\": [\n                        \"*\"\n                    ]\n                }\n            }\n        }\n    ]\n}\n```\n\n6.  Click **Review policy**.\n7.  Enter the policy name. Then, click **Create policy** to create the policy.\n\nFurther, create an IAM user by following the below steps:\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> An IAM user requires the following permissions on an S3 bucket and folder to access files in the folder (and sub-folders):\n> \n> *   s3:GetBucketLocation\n> *   s3:GetObject\n> *   s3:GetObjectVersion\n> *   s3:ListBucket\n\n1.  In AWS IAM console, Click **Users**.\n2.  Click **Create user**.\n3.  Enter a name for the user.\n4.  Select Programmatic access as the access type, then click **Next: Permissions**.\n5.  Click **Attach existing policies directly**, and select the policy you created earlier. Then click **Next**.\n6.  Review the user details, then click **Create user**.\n7.  Copy the access key ID and secret access key values.\n\n#### Generate `session token`\n\n1.  Use the AWS CLI to create a named profile with the AWS credentials that you copied in the previous step.\n2.  To get the session token, run the following command:\n\n```\n $ aws sts get-session-token --profile <named-profile>\n```\n\nSee [Snowflake](https://docs.snowflake.com/en/user-guide/data-load-s3-config-aws-iam-user), [Redshift](https://docs.aws.amazon.com/redshift/latest/dg/copy-usage_notes-access-permissions.html), and [Databricks](https://docs.databricks.com/en/ingestion/copy-into/generate-temporary-credentials.html) for more information.\n\n### Public S3 Bucket\n\nYou **must** have the following permissions on the S3 bucket and folder to access files in the folder (and sub-folders):\n\n*   s3:GetBucketLocation\n*   s3:GetObject\n*   s3:GetObjectVersion\n*   s3:ListBucket\n\nYou can use the following policy in your bucket to grant the above permissions:\n\n1.  Go to the **Permissions** tab of your S3 bucket.\n2.  Edit bucket policy in **Permissions** tab and add the following policy. Replace the <bucket\\_name> with your actual bucket name:\n\n```\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": \"*\",\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:GetObjectVersion\"\n            ],\n            \"Resource\": \"arn:aws:s3:::<bucket_name>/*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": \"*\",\n            \"Action\": [\n                \"s3:ListBucket\",\n                \"s3:GetBucketLocation\"\n            ],\n            \"Resource\": \"arn:aws:s3:::<bucket_name>\"\n        }\n    ]\n}\n```\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> In Redshift, you additionally need to set an IAM role as **default** for your cluster, unless access keys are provided. It is necessary because more than one IAM role can be associated with the cluster, and Redshift needs explicit permission granted through an IAM role to access the S3 bucket (Public or Private).\n> \n> Follow [Redshift Documentation](https://docs.aws.amazon.com/redshift/latest/mgmt/default-iam-role.html#set-default-iam) for setting an IAM role as default.\n\n## Use CSV file as input\n\nAn input file (`models/inputs.yaml`) contains details of input sources such as tables, views, or CSV files along with column name and SQL expression for retrieving values.\n\nYou can read data from a CSV file by using `csv: <path_to_filename>` under `app_defaults` in the input specs. CSV data is loaded internally as a single SQL select query, making it useful for seeding tests.\n\nA sample code is as shown:\n\n```\n    app_defaults:\n      csv: \"../common.xtra/Temp_tbl_a.csv\"\n      # remaining syntax is same for all input sources\n```\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> RudderStack does not support CSV files with more than a few hundred rows.\n\n## Filter data\n\nYou can filter out any data by using the `filters` field in your projects file:\n\nFor example, if you want to exclude all the blacklisted email addresses, you can create an input model (for example, `csv_email_blacklist`) with CSV file as a source, that contains all such email addresses:\n\n```\nid_types:\n  - name: email\n    filters:\n      - type: exclude\n        sql:\n          select: email\n          from: inputs/csv_email_blacklist\n```\n\nAnother example, if you want to exclude all the user\\_ids, you can create an SQL model (for example, `sql_exclusion_model`) that contains a specific logic to enlist all such IDs:\n\n```\nid_types:\n  - name: user_id\n    filters:\n      - type: exclude\n        sql:\n          select: user_id\n          from: inputs/models/sql_exclusion_model\n```\n\n## Associate SSH key to Git project\n\nTo add public SSH key to your Git project:\n\n1.  Open your Profile project’s Git repository in the browser and click **Settings**.\n2.  Click **SSH Keys**.\n3.  Assign a name (say `Sample Profiles Key`) and paste the key generated from the RudderStack dashboard or a public-key generated using CLI.\n4.  Click **Add Key**.\n\nFor more information, see:\n\n*   [GitHub documentation](https://docs.github.com/en/authentication/connecting-to-github-with-ssh/managing-deploy-keys#deploy-keys)\n*   [GitLab documentation](https://docs.gitlab.com/ee/user/project/deploy_keys/#create-a-project-deploy-key)\n*   [BitBucket documentation](https://confluence.atlassian.com/bitbucketserver/ssh-access-keys-for-system-use-776639781.html)\n\n## Use private Git repos via CLI\n\nFollow these steps:\n\n1.  [Generate the SSH Key](https://git-scm.com/book/en/v2/Git-on-the-Server-Generating-Your-SSH-Public-Key).\n2.  [Associate the SSH Key to your Git Project](#associate-ssh-key-to-git-project).\n3.  Add private keys as credentials in the `siteconfig.yaml` file:\n\n```\ngitcreds:\n  - reporegex: git@<provider-host>:<org-name>/*\n    key: |\n    -----BEGIN OPENSSH PRIVATE KEY-----\n    b3BlbnNzaC1rZXktdjEAAAAABG5vbmUAAAAEb..........\n    -----END OPENSSH PRIVATE KEY-----    \n```\n\n## Supported Git URLs\n\nProfiles supports Git URLs for packages and scheduling via UI. You can host the repos at:\n\n*   GitHub\n*   GitLab\n*   BitBucket\n\n[Contact the RudderStack team](mailto:support@rudderstack.com) if your preferred host isn’t included.\n\nFor private repos, RudderStack only supports SSH Git URLs. You need to add credentials to the `siteconfig.yaml` and public ssh key manually to the platforms. See [Use private Git repos via CLI](#use-private-git-repos-via-cli).\n\nThe URL scheme doesn’t depend on individual Git provider host. You can use the below-mentioned Git URLs:\n\n**1\\. URL for the default branch of a repository**\n\n*   **Syntax:**\n    \n    `https://<provider-host>/<org-name>/<repo-name>/path/to/project`\n    \n*   **Example:**\n    \n    `https://github.com/rudderlabs/librs360-shopify-features/shopify-features` `https://gitlab.com/rudderlabs/librs360-shopify-features/shopify-features` `https://bitbucket.org/rudderlabs/librs360-shopify-features/shopify-features`\n    \n\n**For private repos, RudderStack support SSH URLs:**\n\n*   **Syntax:**\n    \n    `git@<provider-host>:<org-name>/<repo-name>/path/to/project`\n    \n*   **Example:**\n    \n    `git@github.com:rudderlabs/librs360-shopify-features/shopify-features` `git@gitlab.com:rudderlabs/librs360-shopify-features/shopify-features` `git@gbitbucket.org:rudderlabs/librs360-shopify-features/shopify-features`\n    \n\n**2\\. URL for a specific branch of a repository**\n\n*   **Syntax:**\n    \n    `https://<provider-host>/<org-name>/<repo-name>/tree/<branch-name>/path/to/project`\n    \n*   **Example:**\n    \n    `https://github.com/rudderlabs/librs360-shopify-features/tree/main/shopify-features` `https://gitlab.com/rudderlabs/librs360-shopify-features/tree/main/shopify-features` `https://bitbucket.org/rudderlabs/librs360-shopify-features/tree/main/shopify-features`\n    \n\n**3\\. URL for a specific tag within the repository**\n\n*   **Syntax:**\n    \n    `https://<provider-host>/<org-name>/<repo-name>/tag/<tag-name>/path/to/project`\n    \n*   **Example:**\n    \n    `https://github.com/rudderlabs/librs360-shopify-features/tag/wht_test/shopify-features` `https://gitlab.com/rudderlabs/librs360-shopify-features/tag/wht_test/shopify-features` `https://bitbucket.org/rudderlabs/librs360-shopify-features/tag/wht_test/shopify-features`\n    \n\n**4\\. URL for a specific commit within the repository**\n\n*   **Syntax:**\n    \n    `https://<provider-host>/<org-name>/<repo-name>/commit/<commit-hash>/path/to/project`\n    \n*   **Example:**\n    \n    `https://github.com/rudderlabs/librs360-shopify-features/commit/b8d49/shopify-features` `https://gitlab.com/rudderlabs/librs360-shopify-features/commit/b8d49/shopify-features` `https://bitbucket.org/rudderlabs/librs360-shopify-features/commit/b8d49/shopify-features`\n    \n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> RudderStack supports the git SSH URL with following pattern only in the dashboard:\n> \n> *   `git@<provider-host>:<org-name>/<repo-name>/tree/<branch-name>`\n> *   `git@<provider-host>:<org-name>/<repo-name>`\n> *   `git@<provider-host>:<org-name>/<repo-name>/tree/main/path/to/project`\n> \n> RudderStack supports any subfolder in git project without .git extension.\n\nIt is recommended to use git-tags instead of the latest commit on main branch of your library projects. Also, you can use a specific tag, for example: `https://github.com/org-name/lib-name/tag/schema_<n>`.\n\nIf you want Profile Builder to figure out the best schema version for every run, you can use the placeholder {{best\\_schema\\_version}}, for example, `https://github.com/org-name/lib-name/tag/schema_{{best_schema_version}}`. The selection of compatible git tags is done by PB, that is, it will figure out the best compatible version for the lib package.\n\nA sample project file:\n\n```\npackages:\n  - name: shopify_features\n    url: https://github.com/org-name/lib-names/tag/schema_{{best_schema_version}}\n    inputsMap:\n      rsCartUpdate: inputs/rsCartUpdate\n      rsIdentifies: inputs/rsIdentifies\n```\n\nUsing this will make Profiles use the best compatible version of the library project in case of any schema updates.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> You don’t have to replace the placeholder `{{best_schema_version}}`. For instance, if `https://github.com/org-name/lib-names/tags/` has a tag for schema\\_44, then `https://github.com/org-name/lib-names/tag/schema_44` will be automatically used. In any case, if you replace the placeholder with actual tag name, the project will work without any issues.\n\n## View model dependencies\n\nYou can create a DAG to see all the model dependencies, that is, how a model is dependent on other models by using any one of the following commands:\n\n`pb show dataflow`  \nOR  \n`pb show dependencies`\n\nFurther, you can use the `pb show models` command to view information about the models in your project. See [show](https://www.rudderstack.com/docs/archive/profiles/0.13/cli-user-guide/commands/#show) command for more information.\n\n## Multi-version support\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> This feature is supported for the Profiles versions 0.10.8, 0.11.5, and 0.12.0 onwards.\n\nYou can constraint your Profiles project to run only on specific version(s) by specifying it in the `pb_project.yaml` file, under `python_requirements` key. For example, use the below snippet to run your project on v0.10.8:\n\n```\npython_requirements:\n  - profiles-rudderstack==0.10.8\n```\n\nUse the below snippet to stay on any minor version between 0.12.0 and 0.13.0. If a new minor version is released, your project will be auto-migrated to that version:\n\n```\npython_requirements:\n  - profiles-rudderstack>=0.12.0,<0.13.0\n```\n\nIf you do not specify any version in `pb_project.yaml`, the latest Profiles version is used by default. The version constraints follow the same syntax as those of [Python dependency specifiers](https://packaging.python.org/en/latest/specifications/dependency-specifiers/).\n\nMake sure that the version of Profiles project is the same in your environment and the `pb_project.yaml` file. Otherwise, RudderStack will throw an error.\n\n## Reuse models output\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> This is an experimental feature.\n\nYou can define the `time_grain` parameter for a model to ensure the model runs only once in that time period. It lets you reuse the output material of that model within the time period, preventing unnecessary recalculations⁠.\n\nFor example, if you set the `time_grain` value for an ID stitcher model to a `day` and run the feature table model (based on the ID stitcher) multiple times during a day, the feature table model will reuse the ID stitcher’s output to compute the features. This will save a large amount of time and computations whenever you run the feature table model within that day.\n\nSimilarly, you can choose to run a feature such as `weekly_spends` only once a week, or `last_active_day` on a daily basis.\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> Setting `time_grain` parameter does not mean that the model will run automatically at the specified time period. It just ensures that the model runs only once during that time period and its outputs are reused within that duration. To schedule your project run, you must use the RudderStack dashboard.\n> \n> For example, if `time_grain` for a model is set to a `week`, its outputs will be reused throughout the week and running it twice within the week won’t change its results.\n\nYou can define the `time_grain` parameter in the `profiles.yaml` file of your project:\n\n```\nmodels:\n  - name: user_id_graph\n    model_type: id_stitcher\n    model_spec:\n      entity_key: user\n      time_grain: \"hour\"\n      materialization:\n        run_type: incremental\n      edge_sources:\n        - from: inputs/rsIdentifies\n        - from: inputs/rsTracks\nvar_groups:\n  - name: user_daily_vars\n    time_grain: \"day\"\n    entity_key: user\n    vars:\n      - entity_var:\n          name: days_since_last_seen\n          select: \"{{macro_datediff('{{user.Var(\\\"max_timestamp\\\")}}')}}\"\n  - name: user_weekly_vars\n    time_grain: \"week\"\n    entity_key: user\n    vars:\n      - entity_var:\n         name: weekly_amt_spent\n         select: sum(total_price_usd) - coalesce(sum(total_price_usd_order_cancelled), 0)\n         from: models/rsOrderCreatedOrderCancelled\n         where: \"{{macro_datediff_n('timestamp','7')}}\"\n```\n\nIn the above example, suppose you schedule your Profiles project to run every hour. The output for `user_id_graph` model will be computed every hour, the output for `user_daily_vars` will be computed once a day, and the output for `user_weekly_vars` will be computed once a week.\n\nFor a default ID stitcher model, you can define the `time_grain` value in the `entities` section as shown below:\n\n```\nentities:\n  - name: user\n    id_types:\n      - test_id\n      - exclude_id\n    default_id_stitcher:\n      time_grain: \"day\"\n```\n\n#### Supported values\n\nYou can set the following values for the `time_grain` field:\n\n*   `tick`: Considers all the data up to the current moment (default value).\n*   `10minutes`: Considers data up to the last 10-minute interval.\n*   `hour`: Considers data up to the end of the previous hour.\n*   `day`: Considers data up to the end of the previous day.\n*   `week`: Considers data up to the end of the previous week.\n*   `month`: Considers data up to the end of the previous month.\n*   `year`: Considers data up to the end of the previous year.\n\n## Enable/disable model run\n\nWhen you have various interdependent models, you might want to run only the required ones for a specific output.\n\nRudderStack provides the `enable_status` parameter which lets you specify whether to run a model or not. Using it, you can exclude the unnecessary models from the execution process. You can assign the following values to the `enable_status` field in your `pb_project.yaml` file:\n\n*   `good_to_have` (default): It will not execute or cause a failure when it is not possible to execute the model.\n*   `must_have`: It will cause a failure when is not possible to execute the model.\n*   `not_needed`: The model gets disabled if it has no dependency on the final output. It is the default value for a default ID stitcher model and ensures that the model is not executed if it is not required.\n*   `disabled`: The model gets disabled and is not executed.\n\nA sample `pb_project.yaml` file with `enable_status` parameter:\n\n```\nname: app_project\nschema_version: 63\nconnection: test\nmodel_folders:\n  - models\nentities:\n  - name: user\n    id_types:\n      - user_id\n      - anonymous_id\n    default_id_stitcher:\n      validity_time: 24h # 1 day\n      materialization:\n        run_type: incremental\n        enable_status: good_to_have\n      incremental_timedelta: 12h # half a day\n\nid_types:\n  - name: user_id\n    filters:\n      - type: include\n        regex: \"([0-9a-z])*\"\n      - type: exclude\n        value: \"\"\n  - name: anonymous_id\n```\n\n**Use-case**\n\nConsider a scenario where an ID stitcher model (`ids`) is dependent on `tbl_a` and `tbl_b`, and a feature table model (`ft1`) depends on the ID Stitcher (`ids`) and an input model `tbl_a`.\n\n*   If the ID stitcher and feature table models are marked as `not_needed`, there is no need to execute either of them. However, if the feature table is marked as `good_to_have`/`must_have`, then all the models must run to create final output.\n*   If `tbl_a` is set to `disabled`, the ID stitcher and feature table will not run. If either of them is marked as `must_have`, the project will run into an error.\n\n## Window functions\n\nA window function operates on a window (group) of related rows. It performs calculation on a subset of table rows that are connected to the current row in some way. The window function has the ability to access more than just the current row in the query result.\n\nThe window function returns one output row for each input row. The values returned are calculated by using values from the sets of rows in that window. A window is defined using a window specification, and is based on three main concepts:\n\n*   Window partitioning, which forms the groups of rows (`PARTITION BY` clause)\n*   Window ordering, which defines an order or sequence of rows within each partition (`ORDER BY` clause)\n*   Window frames, which are defined relative to each row to further restrict the set of rows (`ROWS` specification). It is also known as the frame clause.\n\n**Snowflake** does not enforces users to define the cumulative or sliding frames, and considers `ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING` as the default cumulative window frame. However, you can override this by defining the frame manually.\n\nOn the **Redshift** aggregate window function list given below, specify the `frame_clause` while using any function from the list:\n\n*   `AVG`\n*   `COUNT`\n*   `CUME_DIST`\n*   `DENSE_RANK`\n*   `FIRST_VALUE`\n*   `LAG`\n*   `LAST_VALUE`\n*   `LEAD`\n*   `LISTAGG`\n*   `MAX`\n*   `MEDIAN`\n*   `MIN`\n*   `NTH_VALUE`\n*   `PERCENTILE_CONT`\n*   `PERCENTILE_DISC`\n*   `RATIO_TO_REPORT`\n*   `STDDEV_POP`\n*   `STDDEV_SAMP` (synonym for `STDDEV`)\n*   `SUM`\n*   `VAR_POP`\n*   `VAR_SAMP` (synonym for `VARIANCE`)\n\nIn the Redshift ranking window functions given below, **do not** specify the `frame_clause` while using any function from the list:\n\n*   `DENSE_RANK`\n*   `NTILE`\n*   `PERCENT_RANK`\n*   `RANK`\n*   `ROW_NUMBER`\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> Use `frame_clause` carefully when using a window function. While It is not very critical for Snowflake, using it incorrectly in Redshift can lead to errors.\n\nExample of using `frame_clause`:\n\n```\n- entity_var:\n    name: first_num_b_order_num_b\n    select: first_value(tbl_c.num_b) # Specify frame clause as aggregate window function is used\n    from: inputs/tbl_c\n    default: -1\n    where: tbl_c.num_b >= 10\n    window:\n        order_by:\n        - tbl_c.num_b desc\n        frame_clause: ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING\n- entity_var:\n    name: first_num_b_order_num_b_rank\n    select: rank() # DO NOT specify frame clause as ranking window function is used\n    window:\n        order_by:\n        - first_num_b_order_num_b asc\n```\n\nNote how `frame_clause` is specified in first `entity_var` and not in the second one.\n\n## Macros\n\nMacros are the reusable functions that encapsulate complex processing logic directly within the SQL expression. You can create macros to perform computations and use them in multiple models. For example:\n\n```\nmacros:\n  - name: subtract_range\n    inputs:\n      - first_date\n      - second_date\n    value: \"{{first_date}} - {{second_date}}\"\n  - name: multiplyBy10_add\n    inputs:\n      - first_number\n      - second_number\n    value: \"{{first_number}} * 10 + {{second_number}}\"\n```\n\nYou can create a file (say `macros.yaml`) file under the `models` folder and refer them using the macro name in the `profiles.yaml` file:\n\n```\n- entity_var:\n    name: days_since_first_sale\n    select: \"{{ subtract_range('{{user.Var(\\\"first_sale_time\\\")}}') }}\"\n```\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `name` | String | Name to uniquely identify the macro. |\n| `inputs` | List | Defines the input variables to be used by the macro function. |\n| `value` | String | Contains the reusable function in the form of a SQL expression. Any reference to the inputs must be encapsulated within double curly brackets. |\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Additional Concepts | RudderStack Docs",
    "description": "Additional concepts related to Profiles like packages, best practices, partial feature tables, etc.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.12/permissions/",
    "markdown": "# Warehouse Permissions | RudderStack Docs\n\nGrant RudderStack the required permissions on your data warehouse.\n\n* * *\n\n*     5 minute read  \n    \n\nRudderStack supports **Snowflake**, **Redshift**, **Databricks**, and **BigQuery** for creating unified user profiles.\n\nTo read and write data to the warehouse, RudderStack requires specific warehouse permissions as explained in the following sections.\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> Keeping separate schemas for projects running via CLI and web is recommended. This way projects run from the CLI will never risk overwriting your production data.\n\n## Snowflake\n\nSnowflake uses a combination of DAC and RBAC models for [access control](https://docs.snowflake.com/en/user-guide/security-access-control-overview.html). However, RudderStack chooses an RBAC-based access control mechanism as multiple users can launch the [Profile Builder CLI](https://www.rudderstack.com/docs/archive/profiles/0.12/get-started/profile-builder/).\n\nAlso, it is not ideal to tie the result of an individual user run with that user. Hence, it is recommended to create a generic role (for example, `PROFILES_ROLE`) with the following privileges:\n\n*   Read access to all the inputs to the model (can be shared in case of multiple schemas/tables).\n*   Write access to the schemas and common tables as the PB project creates material (output) tables.\n\nIf you want to access any material created from the project run, the role (`PROFILES_ROLE`) must also have read access to all of those schemas.\n\nBelow are some sample commands which grant the required privileges to the role (`PROFILES_ROLE`) in a Snowflake warehouse:\n\n```\n-- Create role\nCREATE ROLE PROFILES_ROLE;\nSHOW ROLES; -- To validate\n```\n\n```\n-- Create user\nCREATE USER PROFILES_TEST_USER PASSWORD='<StrongPassword>' DEFAULT_ROLE='PROFILES_ROLE';\nSHOW USERS; -- To validate\n```\n\n```\n-- Grant role to user and database\nGRANT ROLE PROFILES_ROLE TO USER PROFILES_TEST_USER;\nGRANT USAGE ON DATABASE YOUR_RUDDERSTACK_DB TO ROLE PROFILES_ROLE;\n```\n\n```\n-- Create separate schema for Profiles and grant privileges to role\nCREATE SCHEMA YOUR_RUDDERSTACK_DB.RS_PROFILES;\nGRANT ALL PRIVILEGES ON SCHEMA YOUR_RUDDERSTACK_DB.RS_PROFILES TO ROLE PROFILES_ROLE;\nGRANT USAGE ON WAREHOUSE RUDDER_WAREHOUSE TO ROLE PROFILES_ROLE;\nGRANT USAGE ON SCHEMA YOUR_RUDDERSTACK_DB.EVENTSSCHEMA TO ROLE PROFILES_ROLE;\n```\n\nFor accessing input sources, you can individually grant select on tables/views, or give blanket grant to all in a schema.\n\n```\n-- Assuming we want read access to tables/views in schema EVENTSSCHEMA\nGRANT SELECT ON ALL TABLES IN SCHEMA YOUR_RUDDERSTACK_DB.RS_PROFILES TO PROFILES_ROLE;\nGRANT SELECT ON FUTURE TABLES IN SCHEMA YOUR_RUDDERSTACK_DB.RS_PROFILES TO PROFILES_ROLE;\nGRANT SELECT ON ALL VIEWS IN SCHEMA YOUR_RUDDERSTACK_DB.RS_PROFILES TO PROFILES_ROLE;\nGRANT SELECT ON FUTURE VIEWS IN SCHEMA YOUR_RUDDERSTACK_DB.RS_PROFILES TO PROFILES_ROLE;\n```\n\n```\n-- Assuming we want read access to tracks and identifies tables in schema EVENTSSCHEMA\nGRANT SELECT ON TABLE YOUR_RUDDERSTACK_DB.RS_PROFILES.TRACKS TO PROFILES_ROLE;\nGRANT SELECT ON TABLE YOUR_RUDDERSTACK_DB.RS_PROFILES.IDENTIFIES TO PROFILES_ROLE;\n```\n\n## Redshift\n\nSuppose the inputs/edge sources are in a single schema `website_eventstream` and the name of the newly created Profiles user is `rudderstack_admin`. In this case, the requirements are as follows:\n\n*   A separate schema `rs_profiles` (to store all the common and output tables).\n*   The `rudderstack_admin` user should have all the privileges on the above schema and the associated tables.\n*   The `rudderstack_admin` user should have `USAGE` privilege on schemas that have the edge sources and input tables (`website_eventstream`) and read (`SELECT`) privileges on specific tables as well. This privilege can extend to the migration schema and other schemas from where data from warehouses comes in.\n*   The `rudderstack_admin` user should have privileges to use `plpythonu` to create some UDFs.\n\nThe sample commands are as follows:\n\n```\nCREATE USER rudderstack_admin WITH PASSWORD '<strong_unique_password>';\nCREATE SCHEMA rs_profiles;\nGRANT ALL ON SCHEMA \"rs_profiles\" TO rudderstack_admin;\nGRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA \"rs_profiles\" TO rudderstack_admin;\nGRANT USAGE ON SCHEMA \"website_eventstream\" TO rudderstack_admin;\nGRANT USAGE ON LANGUAGE plpythonu TO rudderstack_admin;\n```\n\n```\nGRANT SELECT ON ALL TABLES IN SCHEMA \"website_eventstream\" TO rudderstack_admin;\n```\n\nTo give access to only specific input tables/views referred in your Profiles project, use the below command:\n\n```\nGRANT SELECT ON TABLE \"<YOUR_SCHEMA>\".\"<YOUR_TABLE>\" TO rudderstack_admin;\n```\n\n### Supported inputs\n\nRudderStack supports the following input types for Redshift warehouse/serverless:\n\n*   Redshift cluster with DC2 type nodes with following types as inputs:\n    *   Redshift internal tables\n    *   External schema and tables only for inputs (not supported as output)\n    *   CSV files stored on S3 as inputs\n*   Redshift cluster with RA3 type nodes with following types as inputs:\n    *   Redshift internal tables\n    *   External schema and tables only for inputs (not supported as output)\n    *   CSV files stored on S3 as inputs\n    *   Cross DB input tables\n*   Redshift serverless with following types as inputs:\n    *   Redshift internal tables\n    *   External schema and tables (not supported as output)\n    *   CSV files stored on S3 as inputs\n    *   Cross DB input tables RudderStack also supports various authentication mechanisms to authenticate the user running the Profiles project. Refer [Redshift warehouse connection](https://www.rudderstack.com/docs/archive/profiles/0.12/get-started/profile-builder/#2-create-warehouse-connection) for more information.\n\n## Databricks\n\n1.  Open the Databricks UI.\n2.  Create a new user.\n3.  Reuse an existing catalog or create a new one by clicking **Create Catalog**.\n4.  Grant `USE SCHEMA` privilege on the catalog.\n5.  Create a separate schema to write objects created by RudderStack Profiles.\n6.  Grant all privileges on this schema.\n7.  Grant privileges to access relevant schemas for the input tables. For example, if an input schema is in a schema named `website_eventstream`, then you can run the following commands to assign a blanket grant to all schemas or only specific tables/views referred in your Profiles project:\n\n```\nCREATE USER rudderstack_admin WITH PASSWORD <strong_unique_password>;\nGRANT USE SCHEMA ON CATALOG <catalog name> TO rudderstack_admin;\nCREATE SCHEMA RS_PROFILES;\nGRANT ALL PRIVILEGES ON SCHEMA RS_PROFILES TO rudderstack_admin;\nGRANT SELECT ON SCHEMA website_eventstream TO rudderstack_admin;\n```\n\n```\nGRANT SELECT ON ALL TABLES IN SCHEMA \"website_eventstream\" TO rudderstack_admin;\n```\n\nTo give access to only specific input tables/views referred in your Profiles project, use the below command:\n\n```\nGRANT SELECT ON TABLE public.input_table TO rudderstack_admin; \n```\n\n## BigQuery\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> For BigQuery, RudderStack recommends you to use a view instead of table for streaming data sets.\n\n1.  Open the [BigQuery UI](https://console.cloud.google.com/) (Google Cloud Console) and select your project (for example, `rudderstack`).\n2.  Click **Query Editor** from the left sidebar.\n3.  Execute the following command to create a schema for a dataset (for example, `prod_dataset`):\n\n```\nCREATE SCHEMA rudderstack.prod_dataset.rs_profiles;\n```\n\n4.  Grant read access to all your input source tables:\n\n```\nGRANT SELECT ON ALL TABLES IN SCHEMA rudderstack.prod_dataset.rs_profiles;\n```\n\n5.  Grant permission to your user (for example, `rudder_user`) for creating tables/views in the schema:\n\n```\nGRANT CREATE ON SCHEMA rudderstack.prod_dataset.rs_profiles TO rudder_user;\n```\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Warehouse Permissions | RudderStack Docs",
    "description": "Grant RudderStack the required permissions on your data warehouse.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.13/changelog/",
    "markdown": "# Changelog | RudderStack Docs\n\nChangelog for all the Profiles versions.\n\n* * *\n\n*     44 minute read  \n    \n\n## Version 0.13.2\n\n_11 June 2024_\n\n**Bug Fixes**\n\n*   Fixed issues with auto-migrate on Windows.\n*   Resolved insufficient memory error encountered while adding `main_id` to `input_var` tables in Redshift.\n\n## Version 0.13.1\n\n_23 May 2024_\n\n**Bug Fixes**\n\n*   Resolved the issue with remapped models in entity projects in the RudderStack dashboard.\n*   Fixed timestamp casting issues on BigQuery.\n\n**Known Issues**\n\n*   RudderStack does not support accessing input sources in a different project for the BigQuery warehouse.\n*   Linux users might see this warning for all command runs - you can ignore it: `WARN[0000]log.go:228 gosnowflake.(*defaultLogger).Warn DBUS_SESSION_BUS_ADDRESS envvar looks to be not set, this can lead to runaway dbus-daemon processes. To avoid this, set envvar DBUS_SESSION_BUS_ADDRESS=$XDG_RUNTIME_DIR/bus (if it exists) or DBUS_SESSION_BUS_ADDRESS=/dev/null`.\n*   Redshift: If two different users create material objects on the same schema, RudderStack gives an error during cleanup when trying to drop views created by the other user, like `user_var_table`.\n*   `pb validate access` command does not work for BigQuery.\n*   `pb insert` does not work for Redshift, Databricks, and BigQuery.\n*   Cross database references can fail on Redshift for a few clusters.\n*   If you are referring a public package in the project and get `ssh: handshake failed error`, then you’ll have to manually remove the entire folder from _WhtGitCache_ to make it work.\n*   The code for `validity_time` is redundant and should be removed.\n*   In some cases, you may need to install the `profiles-rudderstack` and `profiles-rudderstack-bin` pip packages separately.\n*   You may have to execute the compile command once before executing validate access. Otherwise, you will get a `seq_no` error.\n*   Cohort features do not inherit the parent’s features.\n*   RudderStack dashboard lists a cohort only if it has features.\n*   Timegrains is an experimental feature. There might be some undiscovered issues.\n*   Activations does not work with Redshift warehouse.\n\n## Version 0.13\n\n_16 May 2024_\n\n**What’s New**\n\n*   Timegrains feature _(experimental)_ lets you reuse the output material of a model within the specified time period, preventing unnecessary recalculations⁠. You can define its value as a day, week, month etc. to compute features at the end of that particular period.\n*   Cohorts feature _(experimental)_ lets you define core customer segments within an entity based on some characteristics.\n*   Removed support for `rebase_incremental` in project\\_spec. However, you can specify it using the command-line tool.\n*   The `output` folder now generates artifacts for `run` and `compile` commands in separate subfolders with the same name.\n*   `pb init pb-project` now comes with a sample SQL model.\n*   `ReadFile` support introduced in YAML which lets you highlight syntax. In a SQL model, you can use `{{this.ReadFile(\"models/sql_file.sql\")}}` to refer SQL content from a file.\n*   Material registry is now at v5. It includes two new columns - `model_ref` and `registry_version`. The registry will be automatically migrated once you execute `pb run` command.\n\n**Improvements**\n\n*   Improved performance in compilation and ID stitching processes, resulting in faster operations.\n*   Improved error handling at some places.\n*   Few internal refactorings for enhanced working.\n\n**Bug Fixes**\n\n*   Removed `include_untimed` key from the `pb_project.yaml` file as it was redundant.\n*   Fixed sporadic lengthening of project runs in RudderStack dashboard.\n\n**Known Issues**\n\n*   Linux users might see this warning for all command runs - you can ignore it: `WARN[0000]log.go:228 gosnowflake.(*defaultLogger).Warn DBUS_SESSION_BUS_ADDRESS envvar looks to be not set, this can lead to runaway dbus-daemon processes. To avoid this, set envvar DBUS_SESSION_BUS_ADDRESS=$XDG_RUNTIME_DIR/bus (if it exists) or DBUS_SESSION_BUS_ADDRESS=/dev/null`.\n*   Redshift: If two different users create material objects on the same schema, RudderStack will throw error during cleanup when trying to drop views created by the other user, such as `user_var_table`.\n*   `pb validate access` command does not work for BigQuery.\n*   Some commands such as `pb insert` does not work for Redshift, Databricks, and BigQuery.\n*   For a few clusters, cross database references can fail on Redshift.\n*   If you are referring a public package in the project and get `ssh: handshake failed error`, then you’ll have to manually clear _WhtGitCache_ folder to make it work.\n*   The code for `validity_time` is redundant and should be removed.\n*   In some cases, you may need to install the `profiles-rudderstack` and `profiles-rudderstack-bin` pip packages separately.\n*   You may have to execute the compile command once before executing validate access. Otherwise, you will get a `seq_no` error.\n*   Cohort features do not inherit parent’s features.\n*   RudderStack dashboard lists a cohort only if it has features.\n*   Timegrains is an experimental feature. There might be some undiscovered issues.\n*   Activations does not work with Redshift warehouse.\n\n## Version 0.12.1\n\n_2 May 2024_\n\n**Improvements**\n\n*   If all the inputs of a model are disabled, the model is disabled by default.\n\n**Bug Fixes**\n\n*   Resolved bug where some projects were failing in case nested columns were missing.\n*   Updated the migration logic to preserve old view names. The reason being that some existing Activation API projects were failing due to the renaming of `serve_traits` to `feature_views`.\n\n**Known Issues**\n\n*   Linux users might see this warning for all command runs - you can ignore it: `WARN[0000]log.go:228 gosnowflake.(*defaultLogger).Warn DBUS_SESSION_BUS_ADDRESS envvar looks to be not set, this can lead to runaway dbus-daemon processes. To avoid this, set envvar DBUS_SESSION_BUS_ADDRESS=$XDG_RUNTIME_DIR/bus (if it exists) or DBUS_SESSION_BUS_ADDRESS=/dev/null`.\n*   Redshift: If two different users create material objects on the same schema, RudderStack will throw error during cleanup when trying to drop views created by the other user, such as `user_var_table`.\n*   `pb validate access` command does not work for BigQuery.\n*   Some commands such as `pb insert` does not work for Redshift, Databricks, and BigQuery.\n*   For a few clusters, cross database references can fail on Redshift.\n*   If you are referring a public package in the project and get `ssh: handshake failed error`, then you’ll have to manually clear _WhtGitCache_ folder to make it work.\n*   The code for `validity_time` is redundant and should be removed.\n*   In some cases, you may need to install the `profiles-rudderstack` and `profiles-rudderstack-bin` pip packages separately.\n*   You may have to execute the compile command once before executing validate access. Otherwise, you will get a `seq_no` error.\n\n## Version 0.12.0\n\n_25 April 2024_\n\n**What’s New**\n\n*   Support for external tables on Redshift Serverless.\n    \n*   Redshift users can now also connect via SSH Tunnel.\n    \n*   Support to specify the version on which you want to run your Profiles project. It is also backported to v0.10.8 and v0.11.5 onwards.\n    \n*   Feature views path refs look like below:\n    \n    *   user/all/feature\\_view _\\# By default, the id served is user `main_id`._\n    *   user/all/feature\\_view/using\\_email _\\# For non default IDs, the path ref has using\\_<idname>_\n    *   user/all/feature\\_view/salesforce\\_id\\_stitched\\_features _\\# The names can still be overridden._\n*   Ability to add description (optional) to feature tags in the project file.\n    \n    ```\n    available_tags:\n        - name: demographics\n          description: all tags related to user demographics\n        - name: billing\n    ```\n    \n*   Reorganized the overall flow when defining a Redshift connection. You can specify the region while using Redshift.\n    \n*   Creating a new project using `pb init pb-project` now comes with sample data in CSV format, in a folder named `csvs`. These are referenced in the input file.\n    \n*   Renamed `Entity Traits 360` model to `Feature Views`. Also, in the `pb_project.yaml` file, a Feature Views model is included by default under the entities definition.\n    \n*   Schema has been migrated from 54 -> 61 in the project file.\n    \n\n**Improvements**\n\n*   `pb init connection` now fetches connection name from the site configuration file, and asks you to specify one as well. If you don’t enter any value and there’s only one entry in the file, then it picks that value.\n*   Default ID stitcher now uses all package models for ID sources as well, in addition to local ones.\n*   Relevant error is now thrown, if ID types for an entity has not been specified in the `pb_project.yaml` file.\n*   An entity project in the RudderStack dashboard no longer fails in case a few columns are missing in the source tables.\n*   Debugging is improved and the correct line number is displayed.\n*   `pb show models` command now includes model dependencies along with some more statistics.\n*   Few retoolings from the ground up.\n\n**Bug Fixes**\n\n*   Resolved bug related to failure when a git repo couldn’t be found in the RudderStack dashboard.\n*   Resolved bug where default ID stitcher was getting created even if there were no ID edges.\n*   Resolved bug where Redshift Profiles projects were failing if names of `entity_var`/`input_var` were longer than 47 characters.\n*   Resolved bug where `pb run --rebase_incremental` command was taking edges from previous runs.\n*   Resolved the issue where a project wasn’t compiling in case it referenced a package having inputs that weren’t defined in the application with the same name.\n\n## Version 0.11.5\n\n_16 April 2024_\n\n**Bug Fixes**\n\n*   Fixed issue for Redshift where the driver version wasn’t getting populated correctly.\n*   Improved cleanup functionality for Redshift by dropping procedures that were used for creating `entity_vars`.\n*   Resolved the issue where migrated project folder’s files weren’t getting deleted.\n*   Fixed the bug where `pb run --rebase_incremental` command was taking edges from previous runs.\n*   Few internal refactorings while returning data types of columns.\n\n## Version 0.11.3\n\n_1 April 2024_\n\n**What’s New**\n\n*   An optional parameter `column_data_type` to specify the data type for an `entity_var`/`input_var`.\n*   Support for programmatic credentials for Redshift.\n*   Schema update in the project yaml file from 53 to 54.\n\n**Improvements**\n\n*   Better error propagation in case of concurrency.\n*   Few internal refactorings for improved overall working.\n\n**Bug Fixes**\n\n*   Resolved `relation still open` error when accessing external tables in Redshift.\n*   Fixed some bugs when getting the `latest seq_no` for a material in BigQuery.\n*   Resolved the issue of conflict in row-ID in case of very large datasets in BigQuery.\n*   Begin and end time of all models are now in UTC timezone. This fixes a few inconsistency issues in models.\n*   Resolved a concurrency issue which occurred on two different root models with the same name.\n\n## Version 0.11.2\n\n_15 March 2024_\n\n**What’s New**\n\n*   You can now do parallel processing while running a project using the `--concurrency` flag. Currently, this is supported only for Snowflake warehouse. It is recommended to use this option judiciously as applying a very large value can impact your system resources.\n*   RedShift users can now access external tables in their data catalog(s).\n\n**Improvements**\n\n*   Project created using `pb init pb-project` now works for all warehouses.\n\n**Bug Fixes**\n\n*   Fixed issues encountered while running BigQuery projects on Windows.\n*   Resolved errors for entity var names in case they match with input column name.\n*   Resolved bugs related to inserting `seq_no`.\n\n## Version 0.11.1\n\n_7 March 2024_\n\n*   Includes bug fixes related to creating vars on ID models and nil model remapping.\n\n## Version 0.11.0\n\n_1 March 2024_\n\n**What’s New**\n\n*   RudderStack now supports BigQuery (beta), offering the same seamless experience as on other data warehouses.\n*   CSV models _(Experimental)_: In the inputs specs, RudderStack has added the ability to read data from a CSV file, instead of a Database table/view. You can use files from local storage, or kept on S3. Under `app_defaults`, instead of `table`/`view`, use `csv` (local storage) or `s3` (kept on S3) followed by the path where the CSV file is kept. Note that this feature is experimental, and RudderStack currently supports S3 on Snowflake and Redshift. A sample code is as follows:\n\n```\n    app_defaults:\n      csv: \"../common.xtra/Temp_tbl_a.csv\"\n…\n    app_defaults:\n      s3: \"s3://s3-wht-input-test-bucket/test/Temp_tbl_d.csv\"\n```\n\n*   Filter IDs: You can now filter out a vast number of ID’s using SQL. For example, if you wish to exclude all blacklisted ID’s that are listed in an input model named `csv_email_blacklist` and user ID’s from an SQL model named `sql_exclusion_model`, then, you may edit your project file as:\n\n```\nid_types:\n  - name: email\n    filters:\n      - type: exclude\n        sql:\n          select: email\n          from: inputs/csv_email_blacklist\n  - name: user_id\n    filters:\n      - type: exclude\n        sql:\n          select: user_id\n          from: models/sql_exclusion_model\n```\n\n*   Pre and Post Hooks: A pre hook enables you to execute an SQL, before running a model, for example, if you want to change DB access, create a DB object, etc. Likewise, a post hook enables you to execute an SQL after running a model. The SQL can also be templatized. Here’s an example code snippet:\n\n```\nmodels:\n  - name: test_id_stitcher\n    model_type: id_stitcher\n    hooks:\n      pre_run: \"CREATE OR REPLACE VIEW {{warehouse.ObjRef('V1')}} AS (SELECT * from {{warehouse.ObjRef('Temp_tbl_a')}});\"\n      post_run: 'CREATE OR REPLACE VIEW {{warehouse.ObjRef(\"V2\")}} AS (SELECT * from {{warehouse.ObjRef(\"Temp_tbl_a\")}});'\n    model_spec:\n```\n\n*   `pb show models` - You can now view in JSON format by passing the flag –json.\n*   For Databricks, RudderStack now supports the `pb validate access` command.\n*   RudderStack has reverted to having a custom ID stitcher in a new project created using `pb init pb-project`.\n*   When creating a new connection in Redshift, you’ll now be asked to input sslmode. You can enter either `disable` (default) or `require`. This will help RudderStack’s tool to work with Redshift DB’s that require SSL mode to be enabled.\n*   RudderStack supports triggering tasks by using URL and also read the status back.\n*   RudderStack dashboard now supports Git Projects hosted on BitBucket and GitLab.\n*   In model specs, a materialization’s `enable_status` is changed to `snake_case`. That is, `enable_status`: `mustHave` -> `enable_status`: `must_have`.\n*   Schema version in the project file has been updated from 49 to 53.\n\n**Improvements**\n\n*   Better error messages are shown in case of incorrect/missing/duplicate entity-vars.\n*   Error handling has been improved at a few places in Python models.\n*   Model path refs are now case insensitive.\n*   The command `pb migrate auto` can now handle the case where model folders aren’t present.\n*   Specific messages are now shown, in case of errors in the material registry.\n*   Due to limitations of Databricks SQL, RudderStack has added restrictions on using catalog `hive_metastore`. So, in case a user on that catalog tries to use RudderStack’s tool, an error is thrown.\n\n**Bug Fixes**\n\n*   Resolved the intermittent issue in Redshift where it throws an error `ptr_to_latest_seqno_cache does not exist`.\n*   Bugs in `pb show idstitcher-report`, `pb show user-lookup`, and `pb cleanup materials` commands have been rectified. `pb show idstitcher-report` is still flaky, however, RudderStack team is working on improving it.\n*   Fixed bug in packages wherein `entityvars`/`inputvars` weren’t able to refer SQL models.\n*   Resolved erroneous queries for validate access command in case of missing privileges.\n*   Recsolved the issue where git repo wasn’t getting cloned in case `cache_dir` in siteconfig was written using tilde notation.\n*   Fixed some bugs related to `begin_time` of models.\n*   Resolved a few issues when cloning Git projects in the web app.\n*   Several fixes in gRPC, making it more stable.\n*   The remapping: key is removed (if exists) in `models/inputs.yaml` as it was redundant.\n*   Resolved some bugs in incremental ID stitcher.\n\n**Known Issues**\n\n*   `pb validate access` command does not work for BigQuery.\n\n## Version 0.10.6\n\n_19 January 2024_\n\nAn internal fix to address issues that arose from a recent update by Snowflake.\n\n## Version 0.10.5\n\n_9 January 2024_\n\nSome internal fixes to make py-native models more robust.\n\n## Version 0.10.4\n\n_15 December 2023_\n\nOur latest version has a plethora of features that makes our product more feature-rich and impactful.\n\n**What’s New**\n\n*   **Vars as models**: Earlier, Vars could only be defined inside the feature table under `vars:` section. Now, Vars are defined independent of feature tables. In the model specs file, we have created a new top level key called `var_groups`. We can create multiple groups of vars that can then be used in various models (eg. in feature table). All vars in a var-group need to have the same entity. So if you have 2 entities, you need at least 2 var groups. However, you can create multiple var\\_groups for every entity. For example, you can create churn\\_vars, revenue\\_vars, engagement\\_vars etc. So that it is easier to navigate and maintain the vars that you need. Each such model shall have name, entity\\_key and vars (list of objects). This is in line with Profiles design philosophy to see everything as a model.\n*   **User defined model types via Python \\[Experimental feature\\]**: Ever wondered what it would take to implement a new model type yourself? Custom model types can now be implemented in Python. Check out [this library](https://github.com/rudderlabs/profiles-pycorelib) for some officially supported model types with their Python source. Note that this is an experimental feature, so the implementation and interfaces can change significantly in the upcoming versions. To use a python package model in your project, simply specify it as a `python_requirement` in `pb_project.yaml`, similar to requirements.txt. The BuildSpec structure is defined using JSON schema within the Python package. Below code snippet shows how the requirements such as for training and config can be specified in the project:\n\n```\n    entities:\n      - name: user\n      python_requirements:\n      - profiles_rudderstack_pysql==0.2.0 #registers py_sql_model model type\n```\n\n```\n    models:\n      - name: test_py_native_model\n        model_type: py_sql_model\n        model_spec:\n          occurred_at_col: insert_ts\n          validity_time: 24h\n          train_config:\n            prop1: \"prop1\"\n            prop2: \"prop2\"\n```\n\n*   **Default ID stitcher**: Until now, when a new project was created using `pb init pb-project`, the file `profiles.yaml` had specifications for creating a custom ID stitcher. That has a few limitations, when edge sources are spanning across packages. Also, we observed that several of our users weren’t doing much changes to the ID stitcher, except for making it `incremental`. As a solution, we have a “default ID stitcher”, that is created by default for all projects. It runs on all the input sources and ID types defined. For quickstart purposes, users needn’t make any changes to the project, to get the ID stitcher working. In case any changes are to be made, then a user can create a custom ID stitcher, as was done in earlier versions.\n    \n*   **Default ID types**: Now, common concepts like ID types can be loaded from packages. So we needn’t define them in all new projects. Hence, we have moved the common ID type definitions into a library project called [`profiles-corelib`](https://github.com/rudderlabs/rudderstack-profiles-corelib). So when you create a new project, the key `id_types` is not created by default. In case you wish to create a custom list of ID types that is different from the default one, then you may do it as was the case in earlier versions.\n    \n*   **Override packages**: Continuing from previous point: packages now have `overrides` materialization spec. In case you wish to add custom ID types to the default list or modify an existing one, then you may extend the package to include your specifications. For the corresponding id\\_type, add the key `extends:` followed by name of the same/different id\\_type that you wish to extend, and corresponding `filters` with include/exclude values. Below is an example of the same:\n    \n\n```\n    packages:\n        - name: foo-bar\n        url: \"https://github.com/rudderlabs/package-555\"\n    id_types:\n        - name: user_id\n        extends: user_id\n        filters:\n            - type: exclude\n              value: 123456\n    id_types:\n        - name: customer_id\n        extends: user_id\n        filters:\n            - type: include\n              regex: sample\n```\n\n*   **entity\\_var tags**: You can now define a list of tags in the project file under `tags:` key. Then, you can add a tag to each entity\\_var.\n*   **Redshift**: We have added support for the RA3 node type. So now our users on that cluster can cross-reference objects in another database/schema.\n*   Schema version in the project file has been updated from 44 -> 49.\n\n**Improvements**\n\n*   Generated ID’s are now more stable. This means that they are unlikely to adapt to merging of ID Clusters, thereby creating a more accurate profile of your users.\n*   By default, every entity\\_var is a feature, unless specified otherwise using `is_feature: false`. So now, you need not explicitly add them to the `features:` list.\n*   You can now add escape characters to an entity\\_var’s description.\n*   Several internal refactorings to improve overall working of the application.\n\n**Bug Fixes**\n\n*   An entity\\_var having a description with special characters was failing during project re-runs. This has now been resolved.\n*   We have fixed the bug where two entity\\_vars across different entities in the same project couldn’t have the same name.\n*   Fixed some bugs related to vars as models, auto migration of projects, and ID lookup.\n\n**Known Issues**\n\n*   Redshift: If two different users create material objects on the same schema, then our tool will throw error when trying to drop views created by the other user, such as `user_var_table`.\n*   Some commands such as `insert` do not work on Redshift and Databricks.\n*   For a few clusters, cross DB references can fail on Redshift.\n*   If you are referring a public package in the project and get `ssh: handshake failed` error, then you’ll have to manually clear `WhtGitCache` folder to make it work.\n*   The code for `validity_time` is redundant and should be removed.\n*   Sometimes you may have sometimes install both the pip packages separately (`profiles-rudderstack` and `profiles-rudderstack-bin`).\n*   You may have to execute the `compile` command once, before executing `validate access`. Otherwise, you can get a `seq_no` error.\n\n## Version 0.9.4\n\n_8 November 2023_\n\nThis release includes the following bug fixes and improvements:\n\n*   `pb run --grep_var_dependencies` - we are now setting default values using the rule “if a project is migrated on load from a version older than 43, then grep\\_var\\_dependencies will default to true otherwise false”. Also, handled a null pointer case for non existent vars listed in dependencies.\n*   `pb migrate_on_load` / `migrate auto` - we have made the message clearer on curly braches in dot syntax message.\n*   `pb migrate manual` - we have removed compatibility-mode as it was no longer required.\n*   A few internal refactorings.\n\n## Version 0.9.3\n\n_2 November 2023_\n\nThis release addresses a few vulnerability fixes.\n\n## Version 0.9.2\n\n_26 October 2023_\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> In case you are unable to install then we recommend having Python3 versions from 3.8 to 3.10.\n\nThis release includes a bug fix on self dependency of vars, in case column has same name as entity-var.\n\n## Version 0.9.1\n\n_19 October 2023_\n\nOur latest release contains some useful features and improvements, as asked by our users.\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> After the auto-migration to v44, you might be shown some warnings to do changes in the YAML. Please check the Tutorials section. Or, you may contact our team and we will assist you with the same.\n\n**What’s New**\n\n*   We have added support for Databricks (beta). Now Databricks users can seamlessly create ID stitcher and feature table models, without writing complex SQL queries! If you’re using Databricks and want to try out Profiles, kindly get in touch with our team.\n*   **Vars as models** : Now, entity\\_vars and input\\_vars can be treated as independent models. Presently, they are tied to a feature table model. In SQL template text, for example in SQL model templates, please use `{{entity-name.Var(var-name)}}` going forward to refer to an entity-var or an input-var. For example, for entity\\_var `user_lifespan` in HelloPbProject, change `select: last_seen - first_seen` to `select: '{{user.Var(\"last_seen\")}} - {{user.Var(\"first_seen\")}}'`.\n*   `pb show dataflow` and `pb show dependencies` commands - A new flag `--include_disabled` flag is added to let disabled models be part of the generated image. Also, we now show the relative path from local root, instead of the full path.\n*   `pb run` command - Added flag `--ignore-model-errors` to let the project continue running in case of an erroneous model. So, the execution wouldn’t stop due to 1 bad model.\n*   `pb run` - Added flag `--grep_var_dependencies` (default: true) which searches for vars dependencies by using `grep` over fields from vars definition.\n*   `pb show idstitcher-report` - Added flag `--seq_no`, using which a specific run for an ID stitcher model can be specified.\n*   **Best schema version** - For a library project, in the `url` key of `packages`, we have introduced the concept of “best version tag”. That is, instead of specifying the specific Git URL of the library project, we give a URL with GIT tag `url: https://github.com/rudderlabs/librs360-shopify-features/tag/schema_{{best_schema_version}}`. Using this will make our tool use the best compatible version of the library project, in case of any schema updates.\n*   Schema has been migrated from version 42 -> 44.\n\n**Improvements**\n\n*   The command `pb show user-lookup` now includes more details including the count of rows created and total number of features.\n*   Commenting out features will ensure that the corresponding entity-var and any related entity-var/input-var being used only for computation of this commented feature wont run\n*   Several improvements done beneath the surface.\n\n**Bug Fixes**\n\n*   The flag `-- force` was having issues in dropping priorly created materialization models. This has now been resolved.\n*   Fixed bug where project was unable to run due to giving a custom name to the ID stitcher.\n*   Resolved an issue in the command `pb show idstitcher-report`, in the case if the hash of the ID Stitcher model has changed from that of the last run, rerunning the ID Stitcher model.\n*   Removed flag `-l` from the command `pb show idstitcher-report` as it was redundant.\n\n**Known Issues**\n\n*   Redshift: If two different users create material objects on the same schema, then our tool will throw error when trying to drop views created by the other user, such as `user_var_table`.\n*   Some commands such as `insert` do not work on Redshift and Databricks.\n*   For a few clusters, cross DB references can fail on Redshift.\n\n## Version 0.8.0\n\n_25 August 2023_\n\n**What’s New**\n\n*   Model Contracts - We have added support for model contracts and their validation. For every input or SQL model, there’s a new key `contract:` which contains the following keys: `is_optional` (boolean, to indicate if the model is optional), `is_event_stream` (boolean, in case the data is event stream and has timestamp), `with_entity_ids` (list of all entities model contains), `with_columns` (list of all column names model have). A contract can be passed along with the model path in `this.DeRef`. For more information, check out [Model Contracts](https://www.rudderstack.com/docs/archive/profiles/0.13/example/packages/#model-contracts).\n*   Inputs model - The keys `occurred_at_col` and `ids` are now a part of `app_defaults`, to reinforce that they can also be overridden.\n*   Schema has been migrated from 40 -> 42 in the project file.\n\n**Improvements**\n\n*   The command `pb cleanup materials` now removes tables generated by Python models also.\n*   `pb show user-lookup` now includes user traits from Python models as well.\n*   A few changes under the hood, for more efficient processing and execution.\n\n**Bug Fixes**\n\n*   Fixed issue in Python models where validity of the train file wasn’t working and it so was retraining the model(s) on every run.\n*   Resolved the bug where wrong credentials in siteconfig file was not printing the exact error.\n*   Queries for checking warehouse access (grant) were duplicated and therefore recursively checking grants on the same models again and again. This resulted in taking more time than what was required. It has now been fixed.\n*   `pb migrate auto` - There was an issue in migration of multi-line strings of SQL models, that has now been resolved.\n\n## Version 0.7.3\n\n_14 August 2023_\n\n**What’s New**\n\n*   `pb show idstitcher-report`:`pb show idstitcher-report`: By passing flag `--id_stitcher_model`, you can now create an HTML report with relevant results and graphics including largest cluster, ID graph, etc.\n*   Material Registry has been updated to version 4, as additional information is now stored for target (as defined in siteconfig), system username, and invocation metadata (hostname and the project’s invocation folder). So now, if anyone logs into the system and creates material objects using PB, then these details will be stored. This is based on a feature request from one of our customers. Note: make sure to execute `pb validate access` for migrating the registry.\n*   `pb discover materials`:`pb discover materials` - This command now shows a few additional columns - target, username, hostname, invocation folder.\n*   Default ID stitcher: In the inputs file, the key `to_default_stitcher` needs to be set to `true` explicitly for an ID to get picked in the default ID stitcher. This field is optional and by default set to false, without impacting if the project is using a custom ID stitcher. In your project file, if you remove the key `id_stitcher: models/<name of ID stitcher model>`, then it’ll use the default ID stitcher and create a material view of the name `<entity_name>_default_id_stitcher`.\n*   In the inputs.yaml file, table or view names now appear under a key named `app_defaults:`. This signifies that these are values that input defaults to, when the project is run directly. For library projects, inputs can be remapped and appdefaults overridden. when library projects are imported.\n*   Schema has been migrated from 38 -> 40 in the project file.\n\n**Improvements**\n\n*   `pb init pb-project`:`pb init pb-project`: Added keys on default ID stitcher.\n*   A few improvements behind the scenes, for enhancing the overall functionality.\n\n**Bug Fixes**\n\n*   Resolved the issue where projects migrated using `migrate_on_load` were referring to the location of the migrated project in the material registry. This was affecting the count of ID’s before and after stitching.\n*   Fixed bug where ID stitcher wouldn’t check whether a material was actually existing in the database, before running in incremental mode.\n*   When the material registry was on an unsupported common tables version, then the project environment loading would fail, thereby crashing the application. This has now been resolved.\n*   Features defined in Python models, now do appear in the list of features.\n*   Vars can still be specified in specs of a feature table model. However, the app ignores them. This is a bug and would be fixed in subsequent releases.\n\n## Version 0.7.2\n\n_24 July 2023_\n\nOur newest release brings enhanced functionality and a more efficient experience.\n\n**What’s New**\n\n*   **Model Enable/Disable**: You can now enable or disable specific models using the `materialization` key in model specifications. Use the `status` key to set values. For more information, refer to [Models enabling themselves](https://rudderlabs.github.io/pywht/source/120_tutorials.html#models-enabling-themselves).\n*   **Migrate Auto**: When migrating a project, the ordering of elements now remains the same as in the original files, preserving comments.\n*   **Graceful Application Exit**: You can now exit the application gracefully while it’s running. For example, if you’re generating material tables using the run command, you can exit using Ctrl+C.\n*   **Schema Migration**: The schema version in the project file has been updated from 37 to 38.\n\n**Improvements**\n\n*   Projects created using `init pb-project` now include dependencies.\n*   Instead of generating one big SQL file, we now create multiple files in a folder during SQL generation of a feature table model. This reduces the disk space requirements.\n*   Internal optimizations have been implemented to improve overall performance and efficiency.\n\n**Bug Fixes**\n\n*   An issue has been fixed where insufficient grants for accessing the warehouse would lead to duplicate suggested queries. Also, in some cases, incorrect queries were displayed, such as when a Redshift user was asked to grant a role.\n*   The project URL is now being stored in the material registry, instead of GitHub passkey.\n*   Fixed a bug where macros defined in a separate file as global macros were unable to access a common context.\n*   Resolved a bug where Python models were not appearing in the dependency graph.\n\n## Version 0.7.1\n\n_23 June 2023_\n\nOur latest release addresses some critical issues in the previous release. Therefore, if you’re on v0.7.0, then it’s highly recommended to update to the latest version.\n\n## Version 0.7.0\n\n_22 June 2023_\n\nOur newest release is quite significant in terms of new features and improvements offered. Be sure to try it out and share your feedback with us.\n\n**What’s New**\n\n*   `query` - A new command which displays output of tables/views from the warehouse. So you can view generated material tables from the CLI itself. For example, `pb query \"select * from {{this.DeRef(\"models/user_id_stitcher\")}}\"`.\n*   `show idstitcher-report` - A new sub command that creates report on an ID stitcher run. Such as, whether it converged, count of Pre-stitched ID’s before run, Post-stitched ID’s after run, etc. Usage: `pb show idstitcher-report` .\n*   `show user-lookup` - A new sub command that allows you to search a user by using any of the traits as ID types. E.g., `pb show user-lookup -v <trait value>`.\n*   If non-mandatory inputs required by the model are not present in the warehouse, you can still run the model. Applicable to packages and feature tables.\n*   Schema updated from 33 -> 37 in the project file. Please note that the material registry has been migrated to version 3, so you’ll have to execute `pb validate access` once in order to execute the `run` command.\n\n**Improvements**\n\n*   Added an optional field `source_metadata` in the model file inputs.yaml.\n*   Added EnableStatus field in materialization so that models can be enabled and disabled automatically based on whether it is required or not.\n*   Default ID stitcher now supports incremental mode as well.\n*   In macros, you can now specify timestamps in any format.\n\n**Bug Fixes**\n\n*   In case a project is migrated using flag `migrate_on_load`, then src\\_url in the material registry was pointing to the new folder. Now, that is fixed.\n*   Resolved bugs in generating edges for dependency graphs.\n*   Tons of several other improvements and bug fixes under the hood.\n\n## Version 0.6.0\n\n_26 May 2023_\n\nWe are excited to announce the release of PB Version 0.6.0: packed with new features, improvements, and enhanced user experience.\n\n**What’s New**\n\n*   **Support for begin time and end time**: Our latest release introduces the ability to specify time range for your operations, using two new flags `--begin_time` and `--end_time`. By default, the `--end_time` flag is set to `now`. For example, you can use the command `pb run --begin_time 2023-05-01T12:00:00Z` to fetch all data loaded after 1st May 2023. Note that the flag `--timestamp` is now deprecated.\n*   A new flag, `model_refs`, has been introduced which restricts the operation to a specified model. You can specify model references, such as `pb run --model_refs models/user_id_stitcher`.\n*   `seq_no` - Another new flag, using which you can Continue a previous run by specifying its sequence number. Models that already exist would not be rebuilt, unless `--force` is also specified.\n*   **Show command** - `pb show dependencies` has been added to generate a graph showcasing model dependencies. This visual representation will help you understand the relationships between various models in your project.\n*   **Show command** - `pb show dataflow`: Another new command which generates a graph with reversed edges, illustrating the dataflow within your project.\n*   **migrate\\_on\\_load** - A new flag, `migrate_on_load`, has been introduced. When executing the command `pb run --migrate_on_load`, by default this flag creates a `migrations` folder inside the project folder that has migrated version of your project to the latest schema version and runs it on the warehouse. This simplifies the process of upgrading your project to the latest schema version, without changing source files.\n*   **migrated\\_folder\\_path** - Continuing from previous command, you can use this flag to change folder location of the migrated project.\n*   Schema in the project file has been updated to version 33.\n\n**Improvements**\n\n*   SQL Models now provide more relevant and informative error messages instead of generic “not found” errors. This simplifies troubleshooting and debugging processes.\n*   Numerous improvements and optimizations have been made behind the scenes, enhancing the overall performance and stability of PB.\n\n## Version 0.5.2\n\n_5 May 2023_\n\nOur latest release offers significant performance improvements, enhancements, and bug fixes to provide a better experience.\n\n**What’s New:**\n\n*   A new command, `pb show models`, which displays various models and their specifications in the project.\n*   Ability to exit the application while the run command is being executed.\n*   Project schema version has been migrated to 30.\n\n**Improvements:**\n\n*   Major performance improvements for Redshift. In large data sets, it will reduce the time taken to create ID stitcher table to less than 1/4th of the time taken earlier.\n*   `insert` command now picks the connection specified in the current project folder. If not available, it picks “test” in the connection file.\n*   Siteconfig is now validated when project is loaded.\n*   The `cleanup materials` command now removes SQL models as well.\n\n**Bug Fixes:**\n\n*   Resolved the problem where values with null timestamps were excluded from incremental ID stitcher.\n*   The `insert` command was showing a success message even if no tables were inserted in the warehouse. This has been fixed.\n\n## Version 0.5.1\n\n_11 April 2023_\n\n**What’s New**\n\n*   Updated schema to version 28 in the project file.\n\n**Improvements**\n\n*   Changed project path parameter from `-w` to `-p` for improved usability.\n\n**Bug Fixes**\n\n*   Addressed a few reported bugs for an improved user experience.\n*   Implemented performance enhancements to optimize overall system performance.\n\n## Version 0.5.0\n\n_28 March 2023_\n\nThis release offers significant new additions and improvements, as well as bug fixes.\n\n**What’s New**\n\n*   **Cleanup materials** - You can now use the command `pb cleanup materials` to delete materials in the warehouse automatically, without the need for manual deletion. Just specify the retention time period in the number of days (default 180) and all tables/views created prior to that date will be deleted.\n    \n*   Schema has been migrated to 27. This includes the following changes:\n    \n    *   **pb\\_project.yaml** - The schema version has been updated from 25 → 27. Also, `main_id` is removed from `id_types` as main\\_id\\_type is now optional, rudder\\_id is the main\\_id\\_type by default.\n    *   **models/profiles.yaml** - To explicitly declare edge source ids, each value in `edge_sources` now requires a `from:` key to be appended. Also, if you didn’t define `main_id` in the project file, then no need to specify here.\n\n**Improvements**\n\n*   In the backend code we’ve enabled registry migration which flattens the registry, enabling incremental ID stitcher to operate on incomplete materials. It also introduces a mechanism for migrating common tables.\n*   We have implemented better error handling for cases where an incorrect model name is passed. Any errors related to incorrect model names are now properly identified and handled by the system.\n*   Based on feedback from our users, we have renamed default models from `domain_profile<>` to `user_profile<>`.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> Due to changes in registry, we will be depricating older versions of PB.\n\n**Bug Fixes**\n\n*   Fixed the bug where some experimental features, such as Discover, were not working for Redshift.\n*   Addressed the problem where validation errors were incorrectly being triggered when a connection had multiple targets, one of which was invalid. The system now only generates an error if the warehouse target that is being passed has errors.\n*   In addition to previous one, a few more bugs were fixed that were related to validation.\n*   Errors were coming for users who had initialized the GIT repository but had not added the remote origin. This issue has now been fixed.\n\n**Known Issues**\n\n*   Warning: While the run command is being executed, canceling it by pressing Ctrl+C doesn’t work as expected. Though it will stop the program’s execution on the CLI, the query will keep running on the data warehouse. This is a documented Snowflake behavior.\n*   In a model, an input can’t use columns named “MAIN\\_ID”, “OTHER\\_ID”, “OTHER\\_ID\\_TYPE”, or “VALID\\_AT” in its ID SQL.\n*   When creating a connection via `init` command, pressing the Ctrl+C command doesn’t exit the application.\n*   `migrate auto` jumbles up the order and removes comments.\n*   On Redshift, validate access passes all tests, but `run` command sometimes fail giving error “permission denied for language plpythonu”.\n*   Some commands such as `insert` do not work on Redshift.\n*   For a few clusters, cross DB references can fail on Redshift.\n*   The command `migrate auto` migrates siteconfig in your home directory but not any local one.\n*   While working with same type of data in Snowflake and Redshift you might encounter errors where it works on Snowflake but not on Redshift. This is due to the fact that implicit casting of different data types for different function or operator might not be supported on one data warehouse while supported on other.\n\n## Version 0.4.0\n\n_2 March 2023_\n\nWe are proud to announce the latest version of PB 0.4.0, which includes several new features and improvements.\n\n**What’s New**\n\n*   **Redshift** - We are excited to share that we now offer Redshift integration. With YAML, you can now effortlessly create ID stitched and feature table models on your Redshift warehouse, without any difficulty.\n    \n*   **Incremental ID stitching**: You can now stitch together data from multiple sources in incremental mode. When new data is added to your source tables, only that data will be fetched, without needing to reload the whole table each time! This shall result in significant performance improvements from earlier versions, especially if the delta of new data is much smaller compared to what’s already been stitched.\n    \n*   **Insert**: A new command, allowing users to add sample data to their warehouse, without having to manually add them.\n    \n*   Schema has been migrated to 25. This includes the following changes:\n    \n    *   **models/profiles.yaml** - Renamed entityvar to `entity_var` and inputvar to `input_var`\n    *   **pb\\_project.yaml** - Renamed profile to `connection`.\n    *   **siteconfig.yaml** - Renamed profiles to `connections`.\n\nBe sure to use the `migrate auto` command to upgrade your project and the connections file.\n\n**Improvements**\n\n*   The command `init profile` has been renamed to `init connection`.\n*   Lots of modifications under the hood.\n\n**Bug Fixes**\n\nResolved issue on default values in an entity var, ensuring that the values are properly set.\n\n**Known Issues**\n\n*   Warning: While the run command is being executed, canceling it by pressing Ctrl+C doesn’t work as expected. Though it will stop the program’s execution on the CLI, the query will keep running on the data warehouse. This is a documented Snowflake behavior.\n*   In a model, an input can’t use columns named “MAIN\\_ID”, “OTHER\\_ID”, “OTHER\\_ID\\_TYPE”, or “VALID\\_AT” in its ID SQL.\n*   When creating a connection via `init` command, pressing the Ctrl+C command doesn’t exit the application.\n*   `migrate auto` jumbles up the order and removes comments.\n*   On Redshift, some experimental commands such as discover do not work.\n*   The command `migrate auto` migrates siteconfig in your home directory but not any local one.\n*   While working with same type of data in Snowflake and Redshift you might encounter errors where it works on Snowflake but not on Redshift. This is due to the fact that implicit casting of different data types for different function or operator might not be supported on one data warehouse while supported on other.\n\n## Version 0.3.1\n\n_3 February 2023_\n\nThis version addresses a crucial defect, so please make sure to update your version. Note that you won’t have to update your schema for this release.\n\n## Version 0.3.0\n\n_25 January 2023_\n\nWe have got a new name! WHT is now called Profile Builder (PB), RS360 is now Profiles. Be sure to check out our newest release that comes with several new features for an enhanced user experience.\n\n**What’s New**\n\n*   **Migrate** - A new command that will enable you to migrate your project to a newer schema. It has two subcommands:\n    \n    *   **Manual** - You will get to know steps you need to follow to manually migrate the project yourself. It will include both breaking and non-breaking changes.\n    *   **Auto** - Automatically migrate from one version to another.\n*   We have made a few significant changes to YAML. The changes consist of:\n    \n    *   Bumping schema version from 9 → 18.\n    *   Entityvar (Feature Table) - We have renamed tablevar, tablefeature and feature to entityvar; as they all were adding columns to an entity with nearly identical YAML. A new `vars:` section of feature table YAML contains list of inputvars and entityvars. Whereas `features:` field same YAML is a list of entityvar names which should be part of the final output table.\n    *   ID Stitcher is now linked to an entity. As a result, all tables using that entity will use the linked ID Stitcher. Earlier, an ID stitcher was linked to a feature model.\n    *   Some of the terms in yaml spec are changed to make it closer to SQL terminologies. For entityvar and inputvar spec: value → select, filter → where , ref → from. In inputs spec: sql → select.\n    *   Project file has a new key named `include_untimed`. If set to `true`, data without timestamps are included when running models. This reduces data errors for timestamp materials. Also, we have deprecated the flag `require-time-clean` in the `run` command.\n    *   Id types can now be re-used between entities. In the project file, entities now have a list of id types names, instead of a list of definitions. In the inputs file, a required entity field is added to the ID list that specifies which entity this ID type is being extracted for.\n    *   Now an inputvar can also read from a macro, just like tablevar.\n    *   Global Macros - You can now define macros in a separate YAML file inside your models folder. They can then be used across different models in your project. Thus a macro becomes independent that can be reused across multiple models.\n    *   wht\\_project.yaml is renamed to pb\\_project.yaml and ml-features.yaml to profiles.yaml.\n*   **Cleanup Materials** - A new command that allows you to review all the created materials and then delete them (NOTE: experimental feature).\n    \n*   **Discover** - A new subcommand `discover materials` has been added. Using it, you can now discover all the materials associated with a project.\n    \n*   **Compile/Run** - GIT URL now supports tags. To use, execute the command `pb compile -w git@github.com:<orgname>/<repo>/tag/<tag_version>/<folderpath>`.\n    \n\n**Improvements**\n\n*   **Web app** - The UI is now more intuitive and user-friendly.\n*   Log tracing is now enabled by default for most commands. Log files are stored in logs/logfile.log of your current working directory. They store upto 10 MB data. Also, the logger file now stores more granular information for easier debugging in case of unexpected errors.\n*   Significant performance improvements in creating ID stitched tables, in case a lot of duplicates are present.\n*   Add extra columns (Hash, SeqNos) to differentiate between entries for commands to discover sources and entities.\n*   When you execute a profile via run command, then the generated SQL gets saved in the output folder.\n*   Added .gitignore file to init project command, to prevent unnecessary files being added to GIT Repo. Such as, .DS\\_Store, output and logs folders.\n*   Tonnes of changes under the hood.\n\n**Bug Fixes**\n\n*   Fixed the bug where window functions were creating multiple rows (duplicates) per main id.\n*   Resolved the bug in inputvars which was doing joins on main\\_id instead of row id.\n*   Executing the command init profile now inputs values in the same order as on the web app.\n*   Resolved the bug where extra gitcreds\\[\\] and warehouse lines were added on overwriting a profile that already existed.\n*   A few redundant parameters were being shown in the validate access command which have been removed.\n*   Removed a couple of redundant subcommands in the init project.\n\n**Known Issues:**\n\n*   Warning: While the run command is being executed, canceling it by pressing Ctrl+C doesn’t work as expected. Though it will stop the program’s execution on the CLI, the query will keep running on the data warehouse. This is a documented Snowflake behavior.\n*   In a model, an input can’t use columns named “MAIN\\_ID”, “OTHER\\_ID”, “OTHER\\_ID\\_TYPE”, or “VALID\\_AT” in its ID SQL.\n*   When creating a profile via `init` command, pressing the Ctrl+C command doesn’t exit the application.\n*   Web app doesn’t allow you to select a date older than 30 days.\n*   Migrate auto jumbles up the order and removes comments.\n\n## Version 0.2.2\n\n_12 November 2022_\n\nOur November release is significant as it has several fixes and improvements for an enhanced experience. Check it out and be sure to let us know your feedback.\n\n**What’s New**\n\n*   **ID Stitcher / Feature Table** - You can now define a view as source, in addition to table, in the inputs file. This is particularly of use when you need to support an sql query that’s complex or out of scope for PB. To use it, in your inputs file define the edge\\_source as `view: <view_name>` instead of `table: <table_name>`.\n*   **Inputvars** - A new identifier which adds temporary helper columns to an input table, for use in calculating a featuretable.\n*   **Window Functions** - In your model file, you can now add window function support to features, tablevars, tablefeatures and inputvars. Also, you can add filters to features.\n\n**Improvements**\n\n*   Schema version 9 makes it more streamlined to define the model. We welcome your feedback for further improvements on this.\n*   Compile command now show errors if the input SQL is buggy.\n*   Discover - subcommands `entities` and `features` now show a few more fields.\n*   Discover - Export to CSV works for subcommands and also generates files in the output folder.\n*   Init pb-project - Based on feedback, it now generates a README file and also has simpler YAML files with comments. It should now be easier for our users to create a model and get it running.\n*   Several internal refactorings on how the application works.\n*   Web app - Massive improvements under the hood related to UI elements, preserving state when entering data, showing correct data and validations, and displaying run time in user’s local time zone.\n\n**Bug Fixes**\n\n*   Fixed the issue where every time `pb run` was executed for a feature table, it was adding a new row to the output of `pb discover features`.\n*   Resolved the bug where error wasn’t shown if an unknown flag was used.\n*   There was an issue generating material tables on a new schema, which has now been resolved.\n*   Bug fix on generating empty SQL files from input models.\n*   Fixed bug where model names with \\_ in the name would sometimes fail to update the latest view pointer correctly.\n*   Web app - Artifacts list now shows different folders for different runs to isolate them.\n*   Web app - When the PB project is running, the screen now shows correct start timestamp.\n*   Web app - Date filters to find PB runs are now working.\n*   Web app - Scheduling UI is now fully responsive about when the run will take place.\n*   Web app - Resolved the issue where a project would run only once and was then showing error.\n\n**Known Issues:**\n\n*   Warning: While the run command is being executed, canceling it by pressing Ctrl+C doesn’t work as expected. Though it will stop the program’s execution on the CLI, the query will keep running on the data warehouse. This is a documented Snowflake behavior.\n*   In a model, an input can’t use columns named “MAIN\\_ID”, “OTHER\\_ID”, “OTHER\\_ID\\_TYPE”, or “VALID\\_AT” in its ID SQL.\n*   When creating a profile via `init` command, pressing the Ctrl+C command doesn’t exit the application.\n*   Logger file generation is disabled at the moment.\n*   Some no-op parameters are shown upon passing the help flag(-h) to `validate access` command.\n\n## Version 0.2.0\n\n_5 October 2022_\n\nThe September release is our largest update yet. We have added a lot of quality of life improvements and net new features to the PB product line. We plan on releasing even more features in our mid-October release to further improve the usability of the product as well as add additional features that will further help form the core of the product. A substantial amount of the features in this release were based directly off feedback from the first beta testing with external users and internal stakeholders. Please feel free to walk through our newest release. We welcome and encourage all constructive feedback on the product.\n\n**What’s New**\n\n*   **Feature Table** - After encouraging feedback from beta testing of the ID Stitcher, we are feeling more confident about sharing our C360 feature table functionality with beta customers. During testing of this release, we benchmarked ourselves against the feature set that our E-Commerce ML models expect. Many features were implemented successfully. Some needed functionality which could not be pushed through QA gates in this September release. Nevertheless, the feature table YAML is now ready for internal customers to explore.\n*   **Web App** - We are now ready to share the scheduling functionality within the web app. This will allow the user to schedule, and automatically run PB models from the Rudder backplane. Any artifacts and log files created during the execution of PB projects are also available for the user to explore. This critical functionality will enable users to debug their cloud PB runs.\n*   **Validate** - A new command, `pb validate` allows users to run various tests on the project related configurations and validate the privileges associated with the role used for running the project. For example, the subcommand `pb validate access` does an exhaustive test of required privileges before accessing the warehouse.\n*   **Version** - This is another new command that provides information on the current version of the app.\n*   **Logger** - When you execute the compile and run commands, all errors and success messages that were previously only displayed on screen, are now also logged in a file inside the project output folder.\n*   **Discover** - You can now export the output of the discover command in a CSV file. The ability to discover across all schemas in one’s warehouse is also added.\n\n**Improvements**\n\n*   We have made many changes to the way ID Stitcher config is written. We are forming a more complete opinion on the semantic model representation for customer’s data. Entities, IDs, and ID types are now defined in the PB project file. The model file syntax is also more organized and easier to write. To see examples of the new syntax check out the section on [Identity stitching](https://www.rudderstack.com/docs/archive/profiles/0.13/core-concepts/identity-stitching/) or sample files by executing command `pb init pb-project`. The sample project file also contains include and exclude filters, to illustrate their usage.\n*   In PB command invocation, whenever a file is written, its location is now shown on the console and in log files.\n*   Many enhancements on how errors are handled inside the application.\n*   Massive improvements under the hood.\n\n**Bug Fixes**\n\n*   Fixed the issue in ID stitching where it was not picking up singleton components (i.e. the ones with only 1 edge), due to which they were getting skipped in the final output table.\n*   In the `init` command, not entering any value for target wasn’t setting it to default value as “dev”.\n*   Pressing Ctrl+C wasn’t exiting the application.\n*   The command `init profile` now appends to an existing profile, instead of overwriting it.\n*   Fixed the issue in `discover` command where the material table name was being displayed instead of the model name.\n\n**Known Issues:**\n\n*   Warning: While the run command is being executed, canceling it by pressing Ctrl+C doesn’t work as expected. Though it will stop the program’s execution on the CLI, the query will keep running on the data warehouse. This is a documented Snowflake behavior.\n*   In a model, an input can’t use columns named “MAIN\\_ID”, “OTHER\\_ID”, “OTHER\\_ID\\_TYPE”, or “VALID\\_AT” in its ID SQL.\n*   The web app is not showing a description and last run on the landing page.\n*   In the web app, date filters to find PB runs aren’t working.\n*   In the web app, when the PB project is running, the screen shows an incorrect start timestamp.\n*   Artifacts list changes when a project is running versus when it completes execution. Since all runs on the same Kubernetes pod share the same project folder, we are creating artifacts of different runs under the same parent folder. So, the same folder is currently shown for different runs of the project. In the next release, we will configure different folders for different runs to isolate them.\n*   In case of feature table models, the compile command doesn’t always show error if the input SQL is buggy. Thise error may still be found when the model is run.\n*   When creating a profile via `init` command, pressing the Ctrl+C command doesn’t exit the application.\n*   Creating a PB Project doesn’t currently include a sample independent ID stitcher. Instead, it is a child model to the generated feature table model.\n*   We are working toward better readability of the logger file. We welcome any feedback here.\n*   The command `pb discover features` needs to show a few more fields.\n*   Every time `pb run` is executed for a feature table, it adds a new row to the output of pb discover features. Only one row should appear for each feature.\n*   Export to CSV for the discover command should work for subcommands and also generate files in an output folder.\n*   Some no-op parameters are shown upon passing the help flag(-h) to `validate access` command.\n*   In some cases, error isn’t shown if an unknown flag is used.\n*   Scheduling UI isn’t sometimes fully responsive about when the run will take place.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> The documentation for September release does not completely match with the current release. We are currently working on updating the documentation and will have new versions out soon. Please contact the Data Apps team if you are confused by some deviation.\n\n## Version 0.1.0\n\n_18 August 2022_\n\nWe are now in beta! Please do try out PB and share your feedback with us, so that we can make it better.\n\n**What’s New**\n\n*   **ID Stitcher** - ID Stitching solves the problem of tying different identities together, for the same user across different sessions/devices. With v0.1.0, we launch PB ID Stitching. It provides an easy to use and powerful interface to specify Id Stitching inputs.\n*   **Command Line Interface** - Our CLI tool works on Linux, Windows and Mac machines. Using it you can setup a profile having connection to your Database, make a PB project, create SQL from models, run ID stitcher models directly on the Warehouse, and discover all the created models/entities/sources on DW.\n\n**Improvements**\n\n*   We have enhanced the speed of Discover and Compile commands, from minutes to a few seconds.\n*   The description of a few commands in Help has been improved.\n\n**Bug Fixes**\n\n*   The command for discovering entities wasn’t working, which has now been resolved.\n*   Fixed the bug on init profile command where siteconfig wasn’t getting created on first-time installations.\n*   A few bugs resolved related to output of discover command.\n\n**Known Issues:**\n\n*   Warning: While the run command is being executed, please do not cancel it by pressing Ctrl+C. Though it will stop the program’s execution on CLI, the query will keep running on the data warehouse. This is a documented Snowflake behaviour.\n*   Null ID’s in ID stitcher. If first listed Id is null, the entire row may be ignored. That means, results are silently incorrect.\n*   If first listed ID is null, the entire row may be ignored. The first listed ID is assumed to be the key ID. If it is ever null the results may be incorrect.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Changelog | RudderStack Docs",
    "description": "Changelog for all the Profiles versions.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.12/cli-user-guide/commands/",
    "markdown": "# Commands | RudderStack Docs\n\nLearn about the Profiles commands and how to use them.\n\n* * *\n\n*     11 minute read  \n    \n\nThe Profile Builder tool supports specific commands, making executing the usual operations easier. The basic syntax of executing a command is:\n\n```\n$ pb <command> <subcommand> [parameters]\n```\n\n## Supported commands\n\nYou can use the following Profile Builder commands:\n\n### cleanup\n\nDisplays and removes materials, older than the retention time period specified by the user (default value is 180 days).\n\n```\npb cleanup materials -r <number of days>\n```\n\n**Optional Parameter**\n\n| Parameter | Description |\n| --- | --- |\n| `-r` | Retention time in number of days.<br><br>**Example**: If you pass 1, then all the materials created prior to one day (24 hours) are listed. This is followed by prompts asking you for confirmation, after which you can view the material names and delete them. |\n\n### compile\n\nGenerates SQL queries from models.\n\nIt creates SQL queries from the `models/profiles.yaml` file, storing the generated results in the **Output** subfolder in the project’s folder. With each run, a new folder is created inside it. You can manually execute these SQL files on the warehouse.\n\n**Optional parameters**\n\n| Parameter | Description |\n| --- | --- |\n| `clean_output` | Empties the output folder(s) before executing the command. |\n| `-c` | Uses a site configuration file other than the one in `.pb` directory.<br><br>**Example**: `$ pb compile -c MyOtherConnection/siteconfig.yaml` |\n| `-t` | Defines target name (mentioned in `siteconfig.yaml`) or timestamp in building the model.<br><br>**Example**: If your `siteconfig.yaml` has two targets, `dev` and `test`, and you want to use the `test` instance: `$ pb compile -t test` |\n| `--begin_time` | Timestamp to be used as a start time in building model. |\n| `--end_time` | Timestamp to be used as an end time in building model. |\n| `--migrate_on_load` | Whether to automatically migrate the project and packages to the latest version. Defaults to false. |\n| `--migrated_folder_path` | Folder location of the migrated project. Defaults to sub-directory of the project folder. |\n| `-p` | *   Uses a project file (`pb_project.yaml`) other than the one in current directory.  <br>    **Example**: `$ pb compile -p MyOtherProject`.<br>  <br>*   Fetches project from a URL such as GitHub.  <br>    **Example**:`$ pb compile -p git@github.com:<orgname>/<repo>`. You can also fetch a specific tag, like `$ pb compile -p git@github.com:<orgname>/<repo>/tag/<tag_version>/<folderpath>` |\n| `--rebase_incremental` | Rebases any incremental models (build afresh from their inputs) instead of starting from a previous run. You can do this every once in a while to address the stale data or migration/cleanup of an input table. |\n\n### discover\n\nDiscovers elements in the warehouse, such as models, entities, features and sources.\n\nIt allows you to discover all the registered elements in the warehouse.\n\n**Subcommands**\n\nDiscover all the `models`, `entities`, `features`, `sources`, and `materials` in the warehouse.\n\n```\n$ pb discover models\n$ pb discover entities\n$ pb discover features\n$ pb discover sources\n$ pb discover materials\n```\n\n**Optional parameters**\n\n| Parameter | Description |\n| --- | --- |\n| `-e` | Discovers specific entities with their name.<br><br>**Example**: `$ pb discover -e 'Name'` |\n| `-m` | Discovers a specific model.<br><br>**Example**: `$ pb discover -m 'MY_DATABASE.PROD_SCHEMA.CREATED_MODEL'` |\n| `-c` | Uses a site config other than the default one.<br><br>**Example**: `$ pb discover -c siteconfig.yaml` |\n| `-s` | Discovers entities in a specified schema. |\n| `-s \"*\"` | Discovers entities across all schemas (case-sensitive). |\n| `-u` | Discovers entities having the specified source URL’s.<br><br>**Example**: To discover all the entities coming from GitHub: `$ pb discover -u %github%` |\n| `-t` | Selects target (mentioned in `siteconfig.yaml`). |\n| `-p` | Uses project folder other than the one in current directory.<br><br>**Example**: `$ pb discover -p ThisFolder/ThatSubFolder/SomeOtherProject/` |\n| `-f` | Specifies a file path to dump the discovery output into a csv file.<br><br>**Example**: `$ pb discover -f path/to/csv_file.csv` |\n| `-k` | Restricts discovery of the specified model keys.<br><br>**Example**: `$ pb discover -k entity_key:mode_type:model_name` |\n| `--csv_file` | Specify this flag with a file path to dump the discovery output into a csv file. |\n\n### help\n\nProvides list information for any command.\n\n**Subcommand**\n\nGet usage information for a specific command, with subcommands, and optional parameters.\n\n### init\n\nCreates connection and initializes projects.\n\n**Subcommands**\n\nInputs values for a warehouse connection and then stores it in the `siteconfig.yaml` file.\n\nGenerates files in a folder named **HelloPbProject** with sample data. You can change it as per project information, models, etc.\n\n**Optional parameters**\n\n| Parameter | Description |\n| --- | --- |\n| `pb-project -o` | Creates a Profile Builder project with a different name by specifying it as an additional parameter.<br><br>**Example**: To create a Profile Builder project with the name **SomeOtherProject**: `$ pb init pb-project -o SomeOtherProject` |\n| `connection -c` | Creates `siteconfig.yaml` at a location other than `.pb` inside home directory.<br><br>**Example**: To create `myconfig.yaml` in the current folder: `$ pb init connection -c myconfig.yaml`. |\n\n### insert\n\nAllows you to store the test dataset in your (Snowflake) warehouse . It creates the tables `sample_rs_demo_identifies` and `sample_rs_demo_tracks` in your warehouse schema specified in the `test` connection.\n\n```\n# Select the first connection named test having target and output as dev, of type Snowflake.\n$ pb insert\n# By default it'll pick up connection named test. To use connection named red:\n$ pb insert -n red\n# To pick up connection named red, with target test .\n$ pb insert -n red -t test\n```\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> This command is supported only for Snowflake currently.\n\n### migrate\n\nMigrate your project to the latest schema.\n\n**Subcommands**\n\nBased on the current schema version of your project, it enlists all the steps needed to migrate it to the latest one.\n\nAutomatically migrate from one version to another.\n\nTo migrate your models:\n\n**Schema 44 onwards**\n\nNavigate to the folder where your project files are stored. Then execute one of the following:\n\n*   `pb migrate auto --inplace`: Replaces contents of existing folder with the migrated folder.\n*   `pb migrate auto -d <MigratedFolder>`: Keeps the original project intact and stores the migrated project in another folder.\n\n**Schema 43 -> 44:**\n\nUse `{{entity-name.Var(var-name)}}` to refer to an `entity-var` or an `input-var`.\n\nFor example, for entity\\_var `user_lifespan` in your HelloPbProject, change `select: last_seen - first_seen` to `select: '{{user.Var(\"last_seen\")}} - {{user.Var(\"first_seen\")}}'`.\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> Note that:\n> \n> *   You must use two curly brackets.\n> *   Anything contained within double curly brackets must be written in double quotes (`\" \"`). If you use single quotes within double quotes, then use the escape character (`\\`) that comes when using macros.\n\nFurther, navigate to the folder where your project files are stored. Then execute one of the following:\n\n*   `pb migrate auto --inplace`: Replaces contents of existing folder with the migrated folder.\n*   `pb migrate auto -d <MigratedFolder>`: Keeps the original project intact and stores the migrated project in another folder.\n\n**Linear dependency**\n\nSpecify this parameter when entity as vars migration is not done (till version 43). After the migration is done, it’s not necessary to mention this parameter and can be removed.\n\n```\n  compatibility_mode:\n    linear_dependency_of_vars: true\n```\n\n**Optional parameters**\n\n| Parameter | Description |\n| --- | --- |\n| `-p` | Uses a project file other than the one in current directory. |\n| `-c` | Uses a `siteconfig.yaml` file other than the one in your home directory. |\n| `-t` | Target name (defaults to the one specified in `siteconfig.yaml` file). |\n| `-v` | Version to which the project needs to be migrated (defaults to the latest version). |\n| `-d` | Destination folder to store the migrated project files.<br><br>**Example**: `pb migrate auto -d FolderName` |\n| `--force` | Ignores warnings (if any) and migrates the project. |\n| `--inplace` | Overwrites the source folder and stores migrated project files in place of original.<br><br>**Example**: `pb migrate auto --inplace` |\n| `-p` | Uses a project folder other than the one in current directory.<br><br>**Example**: `$ pb discover -p ThisFolder/ThatSubFolder/SomeOtherProject/` |\n| `-f` | Specifies a file path to dump the discovery output into a csv file.<br><br>**Example**: `$ pb discover -f path/to/csv_file.csv` |\n| `-k` | Restricts discovery of the specified model keys.<br><br>**Example**: `$ pb discover -k entity_key:mode_type:model_name` |\n\n### run\n\nCreates identity stitcher or feature table model in the Warehouse.\n\nIt generates the SQL files from models and executes them in the warehouse. Once executed, you can see the output table names, which are accessible from the warehouse.\n\n**Optional parameters**\n\nThe `run` command shares the same parameters as the [`compile`](#compile) command, in addition to the following ones:\n\n| Parameter | Description |\n| --- | --- |\n| `--force` | Does a force run even if the material already exists. |\n| `-- write_output_csv` | Writes all the generated tables to CSV files in the specified directory.<br><br>**Example**: `$ pb run -- WriteOutputHere.csv` |\n| `--model_args` | Customizes behavior of an individual model by passing configuration params to it.<br><br>The only argument type supported currently is `breakpoint` for feature table models.<br><br>The `breakpoint` parameter lets you generate and run SQL only till a specific feature/tablevar. You can specify it in the format `modelName:argType:argName` where argName is the name of feature/tablevar.<br><br>**Example**: `$ pb run --model_args domain_profile:breakpoint:salesforceEvents` |\n| `--model_refs` | Restricts the operation to a specified model. You can specify model references like `pb run --model_refs models/user_id_stitcher --seq_no latest` |\n| `--seq_no` | Sequence number for the run, for example, 0, 1, 2,…, latest/new. The default value is `new`. You can check run logs or use discover commands to know about existing sequence numbers. |\n| `--ignore_model_errors` | Allows the project to continue to run in case of an erroneous model. The execution will not stop due to one bad model. |\n| `--grep_var_dependencies` | Uses regex pattern matching over fields from vars to find references to other vars and set dependencies. By default, it is set to `true`. |\n| `--concurrency` | (_Experimental_) Lets you run the models concurrently in a warehouse (wherever possible) based on the dependency graph. In CLI, you can specify the concurrency level for running models in a project via `pb run --concurrency <int>` (default int value is 1). Currently, this is supported only for Snowflake warehouse. It is recommended to use this option judiciously as applying a large value may not be supported by your warehouse. The concurrency limit for Snowflake is 20. To increase the limit, see [Snowflake docs](https://community.snowflake.com/s/question/0D50Z00008VjQDkSAN/how-to-handle-thenumberofwaitersexceedsthe20statementslimit-error). |\n| `--begin_time` | Timestamp to be used as a start time in building model. |\n| `--end_time` | Timestamp to be used as an end time in building model. |\n| `--migrate_on_load` | Whether to automatically migrate the project and packages to the latest version. Defaults to false. |\n| `--migrated_folder_path` | Folder location of the migrated project. Defaults to sub-directory of the project folder. |\n| `--include_untimed` | Whether to include data without timestamps when running models. Defaults to true. |\n\n### show\n\nObtains a comprehensive overview of models, id\\_clusters, packages, and more in a project. Its capacity to provide detailed information makes it particularly useful when searching for specific details, like all the models in your project.\n\n**Subcommands**\n\n1.  `pb show models`\n\nThis command lets you view information about the models in your project. The output includes the following information about each model:\n\n*   **Warehouse name**: Name of the table/view to be created in the warehouse.\n*   **Model type**: Whether its an identity stitching, feature table, SQL model etc.\n*   **Output type**: Whether the output type is `ephemeral`, `table`, or `view`.\n*   **Run type**: Whether the model’s run type is `discrete` or `incremental`.\n*   **SQL type**: Whether the SQL type of the model is `single_sql` or `multi_sql`.\n\n2.  `pb show models --json`\n\nThis subcommand saves all model details in a JSON file.\n\n3.  `pb show dependencies`\n\nThis subcommand generates a graph file (`dependencies.png`) highlighting the dependencies of all models in your project.\n\n4.  `pb show dataflow`\n\nThis subcommand generates a graph file (`dataflow.png`) highlighting the data flow of all models in your project.\n\n5.  `pb show idstitcher-report --id_stitcher_model models/<ModelName> --migrate_on_load`\n\nThis subcommand creates a detailed report about the identity stitching model runs. To know the exact modelRef to be used, you can execute `pb show models`. By default, it picks up the last run, which can be changed using flag `-l`. The output consists of:\n\n*   **ModelRef**: The model reference name.\n*   **Seq No**: Sequence number of the run for which you are creating the report.\n*   **Material Name**: Output name as created in warehouse.\n*   **Creation Time**: Time when the material object was created.\n*   **Model Converged**: Indicates a successful run if `true`.\n*   **Pre Stitched IDs before run**: Count of all the IDs before stitching.\n*   **Post Stitched IDs after run**: Count of unique IDs after stitching.\n\nProfile Builder also generates a HTML report with relevant results and graphics including largest cluster, ID graph, etc. It is saved in `output` folder and the exact path is shown on screen when you execute the command.\n\n6.  `pb show user-lookup -v '<trait value>'`\n\nThis subcommand lists all the features associated with a user using any of the traits (flag `-v`) as ID types (email, user id, etc. that you are trying to discover).\n\n**Optional parameters**\n\n| Parameter | Description |\n| --- | --- |\n| `-h` | Displays help information for the command. |\n| `-p` | Specifies the project path to list the models. If not specified, it uses the project in the current directory. |\n| `-c` | File location of the `siteconfig.yaml` (defaults to the one in your home directory). |\n| `-t` | Target name (defaults to the target specified in `siteconfig.yaml` file). |\n| `--include_disabled` | Lets the disabled models be a part of the generated graph image (applicable to [`dataflow` and `dependencies`](#show)). |\n| `--seq_no` | Specifies a particular run for an ID stitcher model (applicable for [`idstitcher-report`](#show)). |\n\n### query\n\nExecutes SQL query on the warehouse and prints the output on screen (10 rows by default).\n\nFor example, if you want to print the output of a specific table/view named `user_id_stitcher`, run the following query:\n\n```\npb query \"select * from user_id_stitcher\"\n```\n\nTo reference a model with the name `user_default_id_stitcher` for a previous run with seq\\_no 26, you can execute:\n\n```\npb query 'select * from {{this.DeRef(\"path/to/user_default_id_stitcher\")}} limit 10' --seq_no=26\n```\n\n**Optional parameters**:\n\n| Parameter | Description |\n| --- | --- |\n| `-f` | Exports output to a CSV file. |\n| `-max_rows` | Maximum number of rows to be printed (default is 10). |\n| `-seq_no` | Sequence number for the run. |\n\n### validate\n\nValidates aspects of the project and configuration.\n\nIt allows you to run various tests on the project-related configurations and validate those. This includes but is not limited to validating the project configuration, privileges associated with the role specified in the site configuration of the project’s connection, etc.\n\n**Subcommands**\n\nRuns tests on the role specified in the site configuration file and validates if the role has privileges to access all the related objects in the warehouse. It throws an error if the role does not have required privileges to access the input tables or does not have the permissions to write the material output in the output schema.\n\n### version\n\nShows the Profile Builder’s current version along with its GitHash and native schema version.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Commands | RudderStack Docs",
    "description": "Learn about the Profiles commands and how to use them.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.13/resources/yaml-refresher/",
    "markdown": "# YAML Best Practices | RudderStack Docs\n\nQuick overview of YAML and its basics for use in Profiles.\n\n* * *\n\n*     4 minute read  \n    \n\nYAML is the preferred choice for writing Profile Builder files due to its simplicity and ease of use.\n\nThis guide explains the base concepts, syntax, and best practices for writing code in YAML.\n\n## What is YAML?\n\n[YAML](https://yaml.org/), short for **YAML Ain’t Markup Language** or **Yet Another Markup Language**, is a data serialization format often used in config files and exchange of data. A YAML file uses indentation, specific characters, and line breaks for representing various data structures.\n\n## Sample YAML file\n\nBelow is how a sample YAML document looks like. It contains key-value pairs where the keys are on the left, followed by a colon (`:`), and the associated values are on the right. The hierarchy and data structure is defined using indentation. The next section explains this in more detail.\n\n```\n# This is a comment\nperson:\n  name: Ruddy Buddy # Note the spacing used for indentation\n  age: 42\n  is_employed: true\n  address: # An object called address\n    street: Jefferson Davis Highway\n    city: Ruther Glen\n    state: Vermont\n    phone: 555-90-210\n  favorite_sports: # A list\n    - soccer\n    - baseball\n```\n\nThe above code has details of an object called `person` with properties like `name`, `age`, `gender`, `is_student`, `address` and `favorite sports`.\n\nHere’s how the same YAML file looks in the JSON format:\n\n```\n{\n  \"person\": {\n    \"name\": \"Ruddy Buddy\",\n    \"age\": 42,\n    \"is_employed\": true,\n    \"address\": {\n      \"street\": \"Jefferson Davis Highway\",\n      \"city\": \"Ruther Glen\",\n      \"state\": \"Vermont\",\n      \"phone\": \"555-90-210\"\n    },\n    \"favorite_sports\": [\n      \"soccer\",\n      \"baseball\"\n    ]\n  }\n}\n```\n\n## Indentation\n\nIn YAML, the indentation is done using spaces - to define the structure of data. Throughout the YAML file, the number of spacing should be consistent. Typically, we use two spaces for indentation. YAML is whitespace-sensitive, so do not mix spaces and tabs.\n\n```\n# Example of correct indentation\nperson:\n  name: Ruddy Buddy # We used 2 spaces\n  age: 42\n\n# Example of incorrect indentation\nperson:\n  name: Ruddy Buddy \n    age: 42 # We mixed spacing and tabs\n```\n\nAs shown above, YAML has single-line comments that start with hash (`#`) symbol, for providing additional explanation or context in the code. Comments are used to improve readability and they do not affect the code’s functionality.\n\n```\n# YAML comment\nperson:\n  name: Ruddy Buddy # Name of the person\n  age: 42 # Age of the person\n```\n\n## Data types in YAML\n\nYAML supports several data types:\n\n*   **Scalars**: Represent strings, numbers, and boolean values.\n*   **Sequences**: Represent lists and are denoted using a hyphen (`-`).\n*   **Mappings**: Key-value pairs used to define objects or dictionaries using colon (`:`).\n\n```\n# Example of data types in YAML\nperson:\n  name: Ruddy Buddy # Scalar (string)\n  age: 42 # Scalar (number)\n  is_employed: true # Scalar (boolean)\n  address: # Mapping (object)\n    street: Jefferson Davis Highway\n    city: Ruther Glen\n    state: Vermont\n    phone: 555-90-210\n  favorite_sports: # Sequence (list)\n    - soccer\n    - baseball\n```\n\n## Chomp modifiers\n\nYAML provides two chomp modifiers for handling line breaks in scalar values.\n\n*   `>`: Removes all newlines and replaces them with spaces.\n\n```\ndescription: >\n  Here is an example of long description\n  which has multiple lines. Later, it\n  will be converted into a single line.  \n```\n\n*   `|`: Preserves line breaks and spaces.\n\n```\ndescription: |\n  Here is another long description, however\n  it will preserve newlines and so the original\n  format shall be as-it-is.  \n```\n\n## Special characters\n\nYou can use escape symbols for special characters in YAML. For example, writing an apostrophe in description can cause the YAML parser to fail. In this case, you can use the escape character.\n\n## Best practices for writing YAML\n\nFollow these best practices for writing clean YAML code in your Profiles projects:\n\n*   Always keep consistent indentation (preferably spaces over tabs).\n*   Give meaningful names to your keys.\n*   Avoid excessive nesting.\n*   YAML is case sensitive, so be mindful of that.\n*   Add comments wherever required.\n*   Use blank lines to separate sections like ID stitcher, feature table, etc.\n*   If your strings contain special characters, then use escape symbols.\n*   Make sure you end the quotes in strings to avoid errors.\n*   Use chomp modifiers for multi-line SQL.\n\n## Conclusion\n\nThe above guidelines constitute some best practices to write effective [Builder](https://www.rudderstack.com/docs/profiles/get-started/profile-builder/) code in Profiles. You can also see the following references:\n\n*   [YAML for VS Code](https://marketplace.visualstudio.com/items?itemName=redhat.vscode-yaml): Extension for comprehensive YAML support in Visual Studio Code.\n*   [YAML Lint](https://www.yamllint.com/) for linting.\n\nFor more information or in any case of any issues, [contact](mailto:support@rudderstack.com) the RudderStack team.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "YAML Best Practices | RudderStack Docs",
    "description": "Quick overview of YAML and its basics for use in Profiles.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.13/resources/glossary/",
    "markdown": "# Glossary | RudderStack Docs\n\nFamiliarize yourself with the commonly used terms across RudderStack Profiles.\n\n* * *\n\n*     7 minute read  \n    \n\n## Cohorts\n\nA Cohort refers to a subset of instances of an entity meeting a specified set of characteristics, behaviors, or attributes. Using cohorts, you can define core customer segments and drive targeted marketing campaigns and deliver personalized experiences.\n\nFor example, you can define a cohort for users who purchased items in the last 90 days, users based in a specific geographical location, etc.\n\n## Custom Models (Python)\n\nOne can build custom Python models for ML by downloading pre-defined Python templates. The results are usually saved as attributes of related entities (for example, `churnProbability`).\n\nPython models and pymodels work differently. In case of Python, developers (both RudderStack and external) develop new model types in python using [profiles-rudderstack](https://pypi.org/project/profiles-rudderstack/) package. An example python package implementing a number of Python models is [profiles-pycorelib](https://pypi.org/project/profiles-pycorelib/).\n\n## Edge sources\n\nThe `edge_sources` field provides the input sources for an identity stitching model. You can specify it in the `models/profiles.yaml` file to list the input sources defined in the `inputs.yaml` file.\n\n## Entity\n\nEntity refers to a digital representation of a class of real world distinct objects for which you can create a profile. An entity can be a user, account, customer, product, or any other object that requires profiling.\n\n## `Entity var`/Entity features\n\nThese are various attributes related to an entity whose profile you are trying to create. For example, they can be `name`, `city`, `LastVisitTimestamp`, etc. for the `user` entity. Each attribute is called an `entity_var`, and it is derived by performing calculation or aggregation on a set of values. Together, all the attributes create a complete picture of the entity. By default, every `entity_var` gets stored as a feature, such as `days_active`, `last_seen`, etc.\n\n## Feature Views\n\nIf the features/traits of an entity are spread across multiple entity vars and ML models, you can use Feature Views to get them together into a single view. These models are usually defined in `pb_project.yaml` file by creating entries under `feature_views` key with corresponding entity.\n\n## Features\n\nFeatures are inputs for the machine learning model. In a general sense, features are pieces of user information we already know. For example, number of days they opened the app in the past week, items they left in the cart, etc.\n\n## Feature tables (legacy)\n\nFeature tables are the outputs based on events, user attributes, and other defined criteria across any data set in your data warehouse. You can define models that can create feature tables for users with ID stitching, ML notebooks and external sources, etc.\n\n## ID Stitcher\n\nData usually comes from different sources and these sources may assign different IDs. To track a user’s journey (or any other entity) uniquely across all these data sources, we need to stitch together all these IDs. ID stitching helps map different IDs of the same user (or any other entity) to a single canonical ID. It does this by doing connected component analysis over the Id-Id edge graph specified in its configuration.\n\n## ID Collator\n\nID Collator is similar to ID Stitcher. It is used when entity has only a single ID type associated (for example, session IDs). In these cases, connected component analysis is not required and we use a simpler model type called ID Collator. It consolidates all entity IDs from multiple input tables into a single collated list.\n\n## Inputs\n\nInputs refers to the input data sources used to create the material (output) tables in the warehouse. The inputs file (`models/inputs.yaml`) lists the tables/views you use as input sources, including the column name and SQL expression for retrieving the values.\n\nYou can use data from various input sources such as event stream (loaded from event data), ETL extract (loaded from Cloud Extract), and any existing tables in the warehouse (generated by external tools).\n\n## Input var\n\nInstead of a single value per entity ID, it represents a single value per row of an input model. Think of it as representing addition of an additional column to an input model. It can be used to define entity features. However, it is not itself an entity feature because it does not represent a single value per entity ID.\n\n## Label\n\nLabel is the output of the machine learning model and is the metric we want to predict. In our case, it is the unknown user trait we want to know in advance.\n\n## Machine learning model\n\nA machine learning model can be thought of as a function that takes in some input parameters and returns an output.\n\nUnlike regular programming, this function is not explicitly defined. Instead, a high level architecture is defined and several pieces are filled by looking at the data. This whole process of filling the gaps is driven by different optimisation algorithms as they try to learn complex patterns in the input data that explain the output.\n\n## Materialization\n\nMaterialization refers to the process of creating output tables/views in a warehouse by running models. You can define the following fields for materialization:\n\n*   `output_type`: Determines the type of output you want to create in your warehouse. Allowed values are:\n    \n    *   `table`: Output is built and stored in a table in the warehouse.\n    *   `view`: Output is built and stored as a view in the warehouse.\n    *   `ephemeral`: Output is created in the form of temporary data which serves as an intermediary stage for being consumed by another model.\n*   `run_type`: Determines the run type of models. Allowed values are:\n    \n    *   `discrete` (default): In this mode, the model runs in a full refresh mode, calculating its results from the input sources. A SQL model supports only the `discrete` run type.\n    *   `incremental`: In this mode, the model calculates its results from the previous run and only reads row inserts and updates from the input sources. It only updates or inserts data and does not delete anything making it efficient. However, only the identity stitching model supports this mode.\n\n## Material tables\n\nWhen you run the PB models, they produce materials - that is, tables/views in the database that contain the results of that model run. These output tables are known as material tables.\n\n## Predictions\n\nThe model’s output is called a prediction. A good model makes predictions that are close to the actual label. You generally need predictions where the labels are not available. In our case, most often the labels come a few days later.\n\n## Prediction horizon days\n\nThis refer to the number of days in advance when we make the prediction.\n\nFor example, statements like “A user is likely to sign-up in the next 30 days, 90 days, etc.” are often time-bound, that is, the predictions are meaningless without the time period.\n\n## Profile Builder (PB)\n\nProfile Builder (PB) is a command-line interface (CLI) tool that simplifies data transformation within your warehouse. It generates customer profiles by stitching data together from multiple sources. It takes existing tables or the output of other transformations as input to generate output tables or views based on your specifications.\n\n## PB project\n\nA PB project is a collection of interdependent warehouse transformations. These transformations are run over the warehouse to query the data for sample outputs, discover features in the warehouse, and more.\n\n## PB model\n\nAny transformation that can be applied to the warehouse data is called a PB model. RudderStack supports various types of models like ID stitching, feature tables, Python models, etc.\n\n## Schema versions\n\nEvery PB release supports a specific set of project schemas. A project schema determines the correct layout of a PB project, including the exact keys and their values in all project files.\n\n## SQL template models\n\nSometimes the standard model types provided by Profiles are insufficient to capture complex use cases. In such cases, RudderStack supports the use of SQL template models to explicitly templatize SQL.\n\nSQL template models can be used as an input to an entity-var/ input-var or as an edge-source in id-stitcher.\n\n## Timegrain\n\nUsing `time_grain` parameter for a model, you can restrict the context timestamp of that model to the specified time boundary. In other words, a feature v1 with `time_grain` value of a `day` will look at all the data up to UTC 00:00 hrs of any particular day only.\n\nIf you compute that feature at 3:00PM or 5:00PM, the result would be the same because its inputs change only at the very beginning of the day. Similarly, if `time_grain` for a model is set to a `week`, it needs to be run only once a week. Running it twice within the week won’t change its results.\n\n## Training\n\nTraining refers to the process of a machine learning model looking at the available data and trying to learn a function that explains the [labels](#label).\n\nOnce you train a model on historic users, you can use it to make predictions for new users. You need to keep retraining the model as you get new data so that the model continues to learn any emerging patterns in the new users.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Glossary | RudderStack Docs",
    "description": "Familiarize yourself with the commonly used terms across RudderStack Profiles.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.13/faq/",
    "markdown": "# Profiles FAQ | RudderStack Docs\n\nCommonly asked questions on RudderStack Profiles.\n\n* * *\n\n*     32 minute read  \n    \n\nThis guide contains solutions for some of the commonly asked questions on Profiles. For queries or issues not listed in this guide, contact [RudderStack Support](mailto:support@rudderstack.com).\n\n## Setup and installation\n\n**I have installed Python3, yet when I install and execute `pb` it doesn’t return anything on screen.**\n\nTry restarting your Terminal/Shell/PowerShell and try again.\n\nYou can also try to find the location of your Python executable. PB would be installed where the executables embedded in other Python packages are installed.\n\n**I am an existing user who updated to the new version and now I am unable to use the PB tool. On Windows, I get the error:** `'pb' is not recognized as an internal or external command, operable program or batch file.`\n\nExecute the following commands to do a fresh install:\n\n1.  `pip3 uninstall profiles-rudderstack-bin`\n2.  `pip3 uninstall profiles-rudderstack`\n3.  `pip3 install profiles-rudderstack --no-cache-dir`\n\n**I am unable to download, getting** `ERROR: Package 'profiles-rudderstack' requires a different Python: 3.7.10 not in '>=3.8, <=3.10'`\n\nUpdate your Python 3 to a version greater than or equal to 3.8 and less than or equal to 3.10.\n\n**I am unable to download profile builder by running `pip3 install profiles-rudderstack` even though I have Python installed.**\n\nFirstly, make sure that Python3 is correctly installed. You can also try to substitute `pip3` with `pip` and execute the install command.\n\nIf that doesn’t work, it’s high likely that Python3 is accessible from a local directory.\n\n1.  Navigate to that directory and try the install command again.\n2.  After installation, PB should be accessible from anywhere.\n3.  Validate that you’re able to access the path using `which pb`.\n4.  You may also execute `echo $PATH` to view current path settings.\n5.  In case it doesn’t show the path then you can find out where |ProductName| is installed using :substitution-code:`pip3 show profiles-rudderstack`. This command will display a list of the files associated with the application, including the location in which it was installed, navigate to that directory.\n6.  Navigate to `/bin` subdirectory and execute command `ls` to confirm that `pb` is present there.\n7.  To add the path of the location where PB is installed via pip3, execute: `export PATH=$PATH:<path_to_application>`. This will add the path to your system’s PATH variable, making it accessible from any directory. It is important to note that the path should be complete and not relative to the current working directory.\n\nIf you still face issues, then you can try to install it manually. [Contact us](mailto:support@rudderstack.com) for the executable file and download it on your machine. Follow the below steps afterwards:\n\n1.  Create `rudderstack` directory: `sudo mkdir /usr/local/rudderstack`.\n2.  Move the downloaded file to that directory: `sudo mv <name_of_downloaded_file> /usr/local/rudderstack/pb`.\n3.  Grant executable permission to the file: `chmod +x /usr/local/rudderstack/pb`.\n4.  Navigate to directory `/usr/local/rudderstack` from your file explorer. Ctrl+Click on pb and select **Open** to run it from Terminal.\n5.  Symlink to a filename pb in `/usr/local/bin` so that command can locate it from env PATH. Create file if it does not exist: `sudo touch /usr/local/bin/pb`. Then execute`sudo ln -sf /usr/local/rudderstack/pb /usr/local/bin/pb`.\n6.  Verify the installation by running `pb` in Terminal. In case you get error `command not found: pb` then check if `/usr/local/bin` is defined in PATH by executing command: `echo $PATH`. If not, then add `/usr/local/bin` to PATH.\n\n1.  If the Windows firewall prompts you after downloading, proceed with `Run Anyway`.\n2.  Rename the executable as `pb`.\n3.  Move the file to a safe directory such as `C:\\\\Program Files\\\\Rudderstack`, create the directory if not present.\n4.  Set the path of `pb.exe` file in environment variables.\n5.  Verify the installation by running `pb` in command prompt.\n\n**When I try to install Profile Builder tool using pip3 I get error message saying: `Requirement already satisfied`**\n\nTry the following steps:\n\n1.  Uninstall PB using `pip3 uninstall profiles-rudderstack`.\n2.  Install again using `pip3 install profiles-rudderstack`.\n\nNote that this won’t remove your existing data such as models and siteconfig files.\n\n**I have multiple models in my project. Can I run only a single model?**\n\nYes, you can. In your spec YAML file for the model you don’t want to run, set `materialization` to `disabled`:\n\n```\nmaterialization:\n    enable_status: disabled\n```\n\nA sample `profiles.yaml` file highlighted a disabled model:\n\n```\nmodels:\n- name: test_sql\n  model_type: sql_template\n  model_spec:\n    validity_time: 24h# 1 day\n    materialization:                \n      run_type: discrete\n      enable_status: disabled  // Disables running the model.\n    single_sql: |\n        {%- with input1 = this.DeRef(\"inputs/tbl_a\") -%}\n          select id1 as new_id1, {{input1}}.*\n            from {{input1}}\n        {%- endwith -%}        \n    occurred_at_col: insert_ts\n    ids:\n      - select: \"new_id1\"\n        type: test_id\n        entity: user\n```\n\n**I am facing this error while ugrading my Profiles project: `pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. profiles-pycorelib 0.2.2 requires profiles-rudderstack!=0.10.6,<=0.10.7,>=0.10.5, but you have profiles-rudderstack 0.11.0 which is incompatible.`**\n\nThis is because you must uninstall and then reinstall the `pycorelib` library while upgrading to a recent version.\n\n* * *\n\n## Warehouse permissions\n\n**I have two separate roles to read from input tables and write to output tables? How should I define the roles?**\n\nYou need to create an additional role as a union of those two roles. PB project needs to read the input tables and write the results back to the warehouse schema.\n\nFurthermore, each run is executed using a single role as specified in the `siteconfig.yaml` file. Hence, it is best in terms of security to create a new role which has read as well as write access for all the relevant inputs and the output schema.\n\n**How do I test if the role I am using has sufficient privileges to access the objects in the warehouse?**\n\nYou can use the `pb validate access` command to validate the access privileges on all the input/output objects.\n\n**While working with Profiles, how can I use the tables in my BigQuery warehouse that are partitioned on the time criteria?**\n\nTo refer to the partitioned tables in your Profiles project, you must include a filter based on the partitioned column. To do so, add `is_event_stream: true` and use the partitioned filter as `occurred_at_col: timestamp` while defining your `inputs.yaml` file:\n\n```\n- name: rsTracks\n  contract:\n    is_optional: false\n    is_event_stream: true\n    with_entity_ids:\n      - user\n    with_columns:\n      - name: timestamp\n      - name: user_id\n      - name: anonymous_id\n  app_defaults:\n    table: profiles_new.tracks\n    occurred_at_col: timestamp\n    ids:\n      - select: \"user_id\"\n        type: user_id\n        entity: user\n      - select: \"anonymous_id\"\n        type: anonymous_id\n        entity: user\n```\n\n* * *\n\n## Compile command\n\n**I am trying to execute the `compile` command by fetching a repo via GIT URL but getting this error: `making git new public keys: ssh: no key found`**\n\nYou need to add the OpenSSH private key to your `siteconfig.yaml` file. If you get the error `could not find expected` afterwards, try correcting the spacing in your `siteconfig.yaml` file.\n\n**While trying to segregate identity stitching and feature table in separate model files, I am getting this error: `mapping values are not allowed in this context`**\n\nThis is due to the spacing issue in `siteconfig.yaml` file. You may create a new project to compare the spacing. Also, make sure you haven’t missed any keys.\n\n**While using v0.13, I notice that two subfolders are created inside the output folder for compile and run, even if I execute only the `pb run command`. What exactly is the difference between them?**\n\nRudderStack generates two subfolders for easy debugging as compiling is a substep of running the project. If you encounter an error during the project run and are not able to get the corresponding SQL generated for this step, you can still rely on the SQL generated during the compile step to debug the error.\n\n* * *\n\n## Command progress & lifecycle\n\n**I executed a command and it is taking too long. Is there a way to kill a process on data warehouse?**\n\nIt could be due to the other queries running simultaneously on your warehouse. To clear them up, open the **Queries** tab in your warehouse and manually kill the long running processes.\n\n**Due to the huge data, I am experiencing long execution times. My screen is getting locked, thereby preventing the process from getting completed. What can I do?**\n\nYou can use the `screen` command on UNIX/MacOS to detach your screen and allow the process to run in the background. You can use your terminal for other tasks, thus avoiding screen lockouts and allowing the query to complete successfully.\n\nHere are some examples:\n\n*   To start a new screen session and execute a process in detached mode: `screen -L -dmS profiles_rn_1 pb run`. Here:\n    *   `-L` flag enables logging.\n    *   `-dmS` starts as a daemon process in detached mode.\n    *   `profiles_rn_1` is the process name.\n*   To list all the active screen sessions: `screen -ls`.\n*   To reattach to a detached screen session: `screen -r [PID or screen name]`.\n\n**The CLI was running earlier but it is unable to access the tables now. Does it delete the view and create again?**\n\nYes, every time you run the project, Profiles creates a new materials table and replaces the view.\n\nHence, you need to grant a select on future views/tables in the respective schema and not just the existing views/tables.\n\n**Does the CLI support downloading a git repo using siteconfig before executing** `pb run` **? Or do I have to manually clone the repo first?**\n\nYou can pass the Git URL as a parameter instead of project’s path, as shown:\n\n**When executing** `run` **command, I get a message:** `Please use sequence number ... to resume this project in future runs` **. Does it mean that a user can exit using Ctrl+C and later if they give this seq\\_no then it’ll continue from where it was cancelled earlier?**\n\nThe `pb run --seq_no <>` flag allows for the provision of a sequence number to run the project. This flag can either resume an existing project or use the same context to run it again.\n\nWith the introduction of time-grain models, multiple sequence numbers can be assigned and used for a single project run.\n\n**What flag should I set to force a run for the same input data (till a specified timestamp), even if a previous run exists?**\n\nYou can execute `pb run --force --model_refs models/my_id_stitcher,entity/user/user_var_1,entity/user/user_var_2,...`\n\n**Can the hash change even if schema version did not change?**\n\nYes, as the hash versions depends on project’s implementation while the schema versions are for the project’s YAML layout.\n\n**Is there a way to pick up from a point where my last pb run failed on a subsequent run? For large projects, I don’t want to have to rerun all of the features if something failed as some of these take several hours to run**\n\nYes, you can just execute the run command with the specific sequence number, for example, `pb run —seq_no 8`.\n\n**What is the intent of `pb discover models` and `pb discover materials` command?**\n\nYou can use `pb discover models` to list all the models from registry and `pb discover materials` to list all the materials from the registry.\n\n**I got this while running `pb show models`. What is “Maybe Enabled”?**\n\n[![](https://www.rudderstack.com/docs/images/profiles/show_models.webp)](https://www.rudderstack.com/docs/images/profiles/show_models.webp)\n\nIn the show models command, the enable status is computed without looking at tables in the warehouse. Imagine a model`M` that has an optional input column. So, `M` is enabled if and only if the optional input column is present. Hence, it may or may not be enabled, depending on whether the input column is present or not.\n\n**How can I handle my Profiles project in the development and production workspace in RudderStack?**\n\nProfiles support git branches in the RudderStack dashboard. Refer [Supported Git URLs](https://www.rudderstack.com/docs/archive/profiles/0.13/example/packages/#supported-git-urls) for more information.\n\nIn case you wish to run only one project in the CLI and run them differently in dev and prod, you can use **targets**:\n\n1.  Create a connection using `pb init connection` and give a connection name (say `test`). Then, give a default target name, say _prod_. Enter remaining details.\n2.  Create another connection using `pb init connection` and give the same connection name as before (`test`). Then, give a different target name, say `dev`. Enter remaining connection details for connecting to your warehouse.\n3.  When you execute a command via CLI, you need to pass `-t` flag. The first connection you’ve defined is the default one, hence, you don’t need to pass a flag explicitly. However, you can pass it for the other one. For example, `pb run -t dev`.\n\nTargets aren’t yet supported in the UI. So while you can run the same project on different instances (prod, dev) in the CLI; in the UI you have to make either a different project or a different branch/tag/subfolder.\n\n**I am getting an “operation timed out” error even though the `pb validate access` command worked fine.**\n\nRetry the command run after some time. It should resolve the issue.\n\n**I have defined a version constraint in my project and migrated it to the latest schema using `pb migrate auto` command. The project is migrated except the `python_requirements` key which has the same version constraints. How do I change that?**\n\nYou need to manually change the project version in CLI as the version constraints don’t change automatically.\n\n* * *\n\n## Identity stitching\n\n**There are many large size connected components in my warehouse. To increase the accuracy of stitched data, I want to increase the number of iterations. Is it possible?**\n\nThe default value of the largest diameter, that is, the longest path length in connected components, is 30.\n\nYou can increase it by defining a `max_iterations` key under `model_spec` of your ID stitcher model in `models/profiles.yaml`, and specifying its value as the max diameter of connected components.\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> Note that the algorithm can give incorrect results in case of large number of iterations.\n\n**Do I need to write different query each time for viewing the data of created tables?**\n\nNo, you can instead use a view name, which always points to the latest created material table. For example, if you’ve defined **user\\_stitching** in your `models/profiles.yaml` file, then execute `SELECT * FROM MY_WAREHOUSE.MY_SCHEMA.user_stitching`.\n\n**In my model, I have set the key `validity_time: 24h`. What happens when the validity of generated tables expire? Will re-running the identity stitching model generate the same hash until the validity expires?**\n\nFirstly, hash does not depend on the timestamp, it depends on the yaml in the underlying code. That’s why the material name is `material_name_HASH_SEQNO`. The sequence number (SEQNO) depends on timestamp.\n\nSecondly, a material generated for a specific timestamp (aside for the timeless timestamp) is not regenerated unless you do a `pb run --force`. The CLI checks if the material you are requesting already exists in the database, and if it does, returns that. The `validity_time` is an extension of that.\n\nFor a model with `validity_time: 24h` and inputs having the `timestamp` columns, if you request a material for latest time, but one was generated for that model 5 minutes ago, the CLI will return that one instead. Using the CLI to run a model always generates a model for a certain timestamp, it’s just if you don’t specify a timestamp then it uses the current timestamp.\n\nSo, for a model with validity\\_time (vt), and having the `timestamp` columns, if you request a material for t1, but one already exists for t0 where t1-vt <= t0 <= t1, the CLI will return that one instead.\n\nIf multiple materials exist that satisfy the requirement, then it returns the one with the timestamp closest to t1.\n\n**I want to use `customer_id` instead of `main_id` as the ID type. So I changed the name in `pb_project.yaml`, however now I am getting this error: `Error: validating project sample_attribution: listing models for child source models/: error listing models: error building model domain_profile_id_stitcher: main id type main_id not in project id types`.**\n\nIn addition to making changes in the file `pb_project.yaml` file, you also need to set `main_id_type: customer_id` in the `models/profiles.yaml` file.\n\n**I ran identity stitching model but not able to see the output tables under the list of tables in Snowflake. What might be wrong?**\n\nIn Snowflake, you can check the **Databases** > **Views** dropdown from the left sidebar. For example, if your model name is `domain_profile_id_stitcher`, you should be able to see the table with this name. In case it is still not visible, try changing the role using dropdown menu from the top right section.\n\n**I am using a view as an input source but getting an error that the view is not accessible, even though it exists in DB.**\n\nViews need to be refreshed from time-to-time. You can try recreating the view in your warehouse and also execute a `select *` on the same.\n\n**What might be the reason for following errors:**\n\n*   `processing no result iterator: pq: cannot change number of columns in view`. The output view name already exists in some other project. To fix this, try dropping the view or changing its name.\n    \n*   `creating Latest View of moldel 'model_name': processing no result iterator: pq: cannot change data type of view column \"valid_at\"` Drop the view `domain_profile` in your warehouse and execute the command again.\n    \n*   `processing no result iterator: pq: column \"rudder_id\" does not exist`. This occurs when you execute a PB project with a model name, having `main_id` in it, and then you run another project with the same model name but no `main_id`. To resolve this, try dropping the earlier materials using `cleanup materials` command.\n    \n\n**I have a source table in which `email` gets stored in the column for `user_id`, so the field has a mix of different ID types. I have to tie it to another table where email is a separate field. When doing so, I have two separate entries for email, as type `email` and `user_id`. What should I do?**\n\nYou can implement the following line in the inputs tables in question:\n\n```\n  - select: case when lower(user_id) like '%@%' THEN lower(user_id) else null end\n    type: email \n    entity: user\n    to_default_stitcher: true\n```\n\n**How do I validate the results of identity stitching model?**\n\nContact [RudderStack Support](mailto:support@rudderstack.com) if you need help in validating the clusters.\n\n**Which identifiers would you recommend that I include in the ID stitcher for an ecommerce Profiles project?**\n\nWe suggest including identifiers that are unique for every user and can be tracked across different platforms and devices. These identifiers might include but not limited to:\n\n*   Email ID\n*   Phone number\n*   Device ID\n*   Anonymous ID\n*   User names\n\nThese identifiers can be specified in the file `profiles.yaml` file in the identity stitching model.\n\nRemember, the goal of identity stitching is to create a unified user profile by correlating all of the different user identifiers into one canonical identifier, so that all the data related to a particular user or entity can be associated with that user or entity.\n\n**If I run `--force` with an ID Stitcher model and also pass a `--seq_no` for the most recent run, will it still recreate the full ID Graph? Also, is there a way to know if the model was run incrementally or not?**\n\nThis will re-run the ID stitcher and if it is incremental, it will look for the most recent run of the stitcher. After finding the existing run for that `seq_no`, it will use it as the base. This is because the base for an incremental run could be the current `seq_no`. If you do not want to do this, you can pass the `rebase_incremental` flag.\n\n**I am getting a bunch of NULL `VALID_AT` timestamps. Is it because the table where the data is being referenced from does not have a timestamp fields specified? Will this impact anything in the downstream?**\n\nYes, if there is no timestamp field in the input table (or it is NULL for the row from where the edge source was pulled), then `VALID_AT` column would have NULL value. This only affects the `VALID_AT` column in the final table and nothing in the ID stitching.\n\n**Which identifiers should I include in my `inputs.yaml` file?**\n\nInclude all the IDs that contribute to the ID stitcher model.\n\n**Should I re-run the stitching process once all `user_id`’s have been sorted out with market prefixes? I want to ensure that users are captured separately instead of being grouped under one `rudder_id`.**\n\nIt is recommended to use the `--rebase-incremental` flag and re-run the stitching process from scratch. While it may not be necessary in all cases, doing so ensures a fresh start and avoids any potential pooling of users under a single `rudder_id`. It’s important to note that if you make any changes to the YAML configuration, such as modifying the entity or model settings, the model’s hash will automatically update. However, some changes may not be captured automatically (for example, if you didn’t change YAML but simply edited column values in the input table), so manually rebasing is a good practice.\n\n**While running my ID stitcher model, I get the error “Could not find parent table for alias “”**\n\nThis is because RudderStack tries to access the cross-database objects (views/tables) for inputs, which is only supported on Redshift [RA3 node type clusters](https://docs.aws.amazon.com/redshift/latest/dg/cross-database_usage.html).\n\nTo resolve the issue, you can upgrade your cluster to RA3 node type or copy data from source objects to the database specified in the siteconfig file. **I want to use a SQL model for an exclusion filter which references tables that are not used in the ID stitching process. Do I still need to add those tables to the `inputs.yaml` file?**\n\nIt is not necessary to add the table references to the `inputs.yaml` file. However, it is advised to add it for the following reasons:\n\n*   You can rule out any access/permissions issues for the referenced tables.\n*   The `contract` field in `inputs.yaml` would help you handle errors if the required column doesn’t exist.\n\n* * *\n\n## Feature Table\n\n**How can I run a feature table without running its dependencies?**\n\nSuppose you want to re-run the user entity\\_var `days_active` and the `rsTracks` input\\_var `last_seen` for a previous run with `seq_no 18`.\n\nThen, execute the following command:\n\n```\npb run --force --model_refs entity/user/days_active,inputs/rsTracks/last_seen --seq_no 18\n```\n\n**I have imported a library project but it throws an error: `no matching model found for modelRef rsTracks in source inputs`.**\n\nYou can exclude the missing inputs of the library project by mapping them to nil in the `pb_project.yaml` file.\n\n**Can I run models which consider the input data within a specified time period?**\n\nYes, you can do so by using the `begin_time` and `end_time` parameters with the `run` command. For example, if you want to run the models for data from 2nd February, 2023, use:\n\n```\n$ pb run --begin_time 2023-01-02T12:00:00.0Z\n```\n\nIf you want to run the nmodels for data between 2 May 2022 and 30 April 2023, use:\n\n```\n$ pb run --begin_time 2022-05-01T12:00:00.0Z --end_time 2023-04-30T12:00:00.0Z\n```\n\nIf you want to run the models incrementally (run them from scratch ignoring any previous materials) irrespective of timestamp, use:\n\n```\n$ pb run --rebase_incremental\n```\n\n**Is it possible to run the feature table model independently, or does it require running alongside the ID stitcher model?**\n\nYou can provide a specific timestamp while running the project, instead of using the default latest time. PB recognizes if you have previously executed an identity stitching model for that time and reuses that table instead of generating it again.\n\nYou can execute a command similar to: `pb run --begin_time 2023-06-02T12:00:00.0Z --end_time 2023-06-03T12:00:00.0Z`. Note that:\n\n*   To reuse a specific identity stitching model, the timestamp value must match exactly to when it was run.\n*   If you have executed identity stitching model in the incremental mode and do not have an exact timestamp for reusing it, you can select any timestamp **greater** than a non-deleted run. This is because subsequent stitching takes less time.\n*   To perform another identity stitching using PB, pick a timestamp (for example, `1681542000`) and stick to it while running the feature table model. For example, the first time you execute `pb run --begin_time 2023-06-02T12:00:00.0Z --end_time 2023-06-03T12:00:00.0Z`, it will run the identity stitching model along with the feature models. However, in subsequent runs, it will reuse the identity stitching model and only run the feature table models.\n\n**While trying to add a feature table, I get an error at line 501, but I do not have these many lines in my YAML.**\n\nThe line number refers to the generated SQL file in the output folder. Check the console for the exact file name with the sequence number in the path.\n\n**While creating a feature table, I get this error:** `Material needs to be created but could not be: processing no result iterator: 001104 (42601): Uncaught exception of type 'STATEMENT ERROR': 'SYS _W. FIRSTNAME' in select clause is neither an aggregate nor in the group by clause.`\n\nThis error occurs when you use a window function `any_value` that requires a window frame clause. For example:\n\n```\n  - entity_var:\n      name: email\n      select: LAST_VALUE(email)\n      from: inputs/rsIdentifies\n      window:\n        order_by: \n        - timestamp desc\n```\n\n**Is it possible to create a feature out of an identifier? For example, I have a RS user\\_main\\_id with two of user\\_ids stitched to it. Only one of the user\\_ids has a purchase under it. Is it possible to show that user\\_id in the feature table for this particular user\\_main\\_id?**\n\nIf you know which input/warehouse table served as the source for that particular ID type, then you can create features from any input and also apply a `WHERE` clause within the entity\\_var.\n\nFor example, you can create an aggregate array of user\\_id’s from the purchase history table, where total\\_price > 0 (exclude refunds, for example). Or, if you have some LTV table with user\\_id’s, you could exclude LTV < 0.\n\n**Is it possible to reference an input var in another input var?**\n\nYes - input vars are similar to adding additional columns to the original table. You can use an input var `i1v1` in the definition of input var `i1v2` as long as both input vars are defined in the same input (or SQL model) `i1`.\n\n**I have not defined any input vars on I1. Why is the system still creating I1\\_var\\_table?**\n\nWhen you define an entity var using I1, an internal input var (for entity’s `main_id`) is created which creates `I1_var_table`. RudderStack team is evaluating whether internal input vars should create the var table or not.\n\n**I have an input model I1. Why is the system creating Material\\_I1\\_var\\_table\\_XXXXXX\\_N?**\n\nThis material table is created to keep the input vars defined on I1.\n\n**I am trying to run a single `entity_var` model. How should I reference it?**\n\nThe right way to reference an entity var is: `entity/<entity-name>/<entity-var-name>`.\n\n**I have two identical named fields in two `user` tables and I want my Profiles project to pick the most recently updated one (from either of the `user` tables). What is the best way to do it?**\n\nDefine different `entity_vars` (one for each input) and then pick the one with a non-null value and higher priority.\n\n**What does running material mean?**\n\nIt means that the output (material) table is being created in your warehouse. For example, an output table named `material_user_id_stitcher_3acd249d_21` would mean:\n\n*   `material`: Prefix for all the objects created by Profiles in your warehouse, such as ID stitcher and feature tables.\n*   `user_id_stitcher`: View created in your schema. It will always point to latest ID stitcher table. This name is the same as defined in the `models/profiles.yaml` file.\n*   `3acd249d`: Unique hash which remains the same for every model unless you make any changes to the model’s config, inputs or the config of model’s inputs.\n*   `21`: Sequence number for the run. It is a proxy for the context timestamp. Context timestamp is used to checkpoint input data. Any input row with `occured_at` timestamp value greater than the context timestamp cannot be used in the associated run.\n\n* * *\n\n## YAML\n\n**Are there any best practices I should follow when writing the PB project’s YAML files?**\n\n*   Use spaces instead of tabs.\n*   Always use proper casing. For example: id\\_stitching, and not id\\_Stitching.\n*   Make sure that the source table you are referring to, exists in data warehouse or data has been loaded into it.\n*   If you’re pasting table names from your Snowflake Console, remove the double quotes in the `inputs.yaml` file.\n*   Make sure your syntax is correct. You can compare with the sample files.\n*   Indentation is meaningful in YAML, make sure that the spaces have same level as given in sample files.\n\n**How do I debug my YAML file step-by-step?**\n\nYou can use the `--model_args` parameter of the `pb run` command to do so. It lets you run your YAML file till a specific feature/tablevar. For example:\n\n```\n$ pb run -p samples/attribution --model_args domain_profile:breakpoint:blacklistFlag\n```\n\nSee [run command](https://www.rudderstack.com/docs/archive/profiles/0.13/cli-user-guide/commands/#run) for more information.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> This is only applicable to versions prior to v0.9.0.\n\n**Can I use double quotes when referencing another entity\\_var in a macro?**\n\nYou can use an escape character. For example:\n\n```\n  - entity_var:\n      name: days_since_last_seen\n      select: \"{{macro_datediff('{{user.Var(\\\"max_timestamp_bw_tracks_pages\\\")}}')}}\"\n```\n\nAlso, if have a case statement, then you can add something like the following:\n\n`select: CASE WHEN {{user.Var(\"max_timestamp_tracks\")}}>={{user.Var(\"max_timestamp_pages\")}} THEN {{user.Var(\"max_timestamp_tracks\")}} ELSE {{user.Var(\"max_timestamp_pages\")}} END`\n\n**Is it possible to define default arguments in macros?**\n\nNo, RudderStack does not support default arguments in macros.\n\n* * *\n\n## ML/Python Models\n\n**Despite deleting WhtGitCache folder and adding keys to siteconfig, I get this error:** `Error: loading project: populating dependencies for project:base_features, model: churn_30_days_model: getting creator recipe while trying to get ProjectFolder: fetching git folder for git@github.com:rudderlabs/rudderstack-profiles-classifier.git: running git plain clone: repository not found`\n\nIf your token is valid, then replace `git@github.com:rudderlabs/rudderstack-profiles-classifier.git` with `https://github.com/rudderlabs/rudderstack-profiles-classifier.git` in the `profile-ml` file.\n\n**Why is my Profiles project taking so long to run?**\n\nThe first Profiles project run usually takes longer, especially if you are building predictive features.\n\n**I am debugging an error in ML models where I see a view with the model name, without material/hash prefix and suffix but it does not get refreshed even after all the entity vars are created and the material\\_<feature\\_table\\_model> table is also created. What might be the reason?**\n\nIt is because this view is now moved to `PostProjectRunCb`, meaning, it is created async after material Force run step.\n\n* * *\n\n## Activation API\n\n**While using Redis destination, I am facing an error: `These sample records were rejected by the destination`**?\n\nThis error is observed if you have enabled **Cluster mode** setting for Redis in the [RudderStack’s configuration settings](https://www.rudderstack.com/docs/destinations/streaming-destinations/redis/#connection-settings) but you are on the Redis free plan.\n\nTo overcome this, ensure that the Redis plan you are using allows clustering. Alternatively, you can turn off the **Cluster mode** setting.\n\n**Does the user-profiles API (old) and activation API (new) behave differently in updating a key that maps to two different primary keys? For example:**\n\n| Primary key | user\\_id | Feature\\_1 | Feature\\_2 |\n| --- | --- | --- | --- |\n| PK1 | U1  | F1  | null |\n| PK2 | U1  | null | F2  |\n\nUser profiles API\n\n```\n{\n  \"userId\": \"U1\",\n  \"profile\": {\n    \"feature_1\": \"F1\",\n    \"feature_2\": \"F2\"\n  }\n}\n```\n\nActivation API\n\n```\n{\n  \"entity\": \"entity_name\",\n  \"id\": {\n    \"type\": \"user_id\",\n    \"value\": \"U1\"\n  },\n  \"data\": {\n    \"model_name\": {\n      \"feature_1\": null,\n      \"feature_2\": F2\n    }\n  }\n}\n```\n\nIn user profiles API, RudderStack updates the value for a specific key (that is, feature\\_1 in this case). In activation API, RudderStack syncs the entire row as value for the `model_name` key.\n\n**Is it possible to use the Activation API without any Profiles project?**\n\nNo, the Activation API can only be used with a Profiles project and not on any of your non-Profiles output tables.\n\n## Profiles UI\n\n**I have included some features in the RudderStack dashboard while creating the Profiles project but when I click “download this project”, my project files does not include any feature. What might be the reason?**\n\nIf you have selected pre-defined features from any library project, they are referred to as `profiles-multieventstream-features` in the project by default.\n\nIf you have created any features using the custom feature functionality, they will be a part of your `models/resources.yaml` file. \n\n**While choosing pre-defined features in the RudderStack dashboard, I can preview code for only some of the features. What might be the reason?**\n\nYou can preview the code only for entity var based features. This functionality is not available for features built from ML and SQL models.\n\n**While creating a Profiles project by importing from Git, I dont see any warehouse options in the dropdown selector in the `Validate Profiles project` section. What might be the reason?**\n\nA Profiles project looks for the supported warehouse destinations configured for that workspace. Hence, make sure you have configured any of the following [warehouse destinations](https://www.rudderstack.com/docs/destinations/warehouse-destinations/) in your RudderStack dashboard:\n\n*   Snowflake\n*   Databricks\n*   Redshift\n*   BigQuery\n\n**Why am I not able to see the Concurrency option in the Settings tab of my Profiles project?**\n\nRudderStack supports the **Concurrency** option only for the Snowflake warehouse currently. You will not be able to see this option if you have configured your Profiles project using the Redshift, BigQuery, or Databricks warehouse.\n\n## Miscellaneous\n\n**Why am I getting _Authentication FAILED_ error on my data warehouse while executing the run/compile commands?**\n\nSome possible reasons for this error might be:\n\n*   Incorrect warehouse credentials.\n*   Insufficient user permissions to read and write data. You can ask your administrator to change your role or grant these privileges.\n\n**Why am I getting _Object does not exist or not authorized_ error on running this SQL query: `SELECT * FROM \"MY_WAREHOUSE\".\"MY_SCHEMA\".\"Material_domain_profile_c0635987_6\"`?**\n\nYou must remove double quotes from your warehouse and schema names before running the query, that is `SELECT * FROM MY_WAREHOUSE.MY_SCHEMA.Material_domain_profile_c0635987_6`.\n\n**Is there a way to obtain the timestamp of any material table?**\n\nYes, you can use the `GetTimeFilteringColSQL()` method to get the timestamp column of any material. It filters out rows based on the timestamp. It returns the `occurred_at_col` in case of an event\\_stream table or `valid_at` in case the material has that column. In absense of both, it returns an empty string. For example:\n\n```\n  SELECT * FROM {<from_material>}\n    WHERE\n      <from_material>.GetTimeFilteringColSQL() > <some_timestamp>;\n```\n\n**What is the difference between setting up Profiles in the RudderStack dashboard and Profile Builder CLI tool?**\n\nYou can run Profiles in the RudderStack dashboard or via [Profile Builder CLI](https://www.rudderstack.com/docs/archive/profiles/0.13/get-started/profile-builder/).\n\nThe main difference is that the RudderStack dashboard only generates outputs based on the pre-defined templates. However, you can augment those outputs by downloading the config file and updating it manually.\n\nOn the other hand, the CLI tool lets you achieve the end-to-end flow via creating a Profile Builder project.\n\n**Does the Profiles tool have logging enabled by default for security and compliance purposes?**\n\nLogging is enabled by default for nearly all the commands executed by CLI (`init`, `validate access`, `compile`, `run`, `cleanup`, etc.). Logs for all the output shown on screen are stored in the file `logfile.log` in the **logs** directory of your project folder. This includes logs for both successful and failed runs. RudderStack appends new entries at the end of the file once a command is executed.\n\nSome exceptions where the logs are not stored are:\n\n*   `query`: The logger file stores the printing output and does not store the actual database output. However, you can access the SQL queries logs in your warehouse.\n*   `help`: For any command.\n\n**In the warehouse, I see lots of material\\_user\\_id\\_stitcher\\_ tables generated in the rs\\_profiles schema. How do I identify the latest ID stitched table?**\n\nThe view `user_id_stitcher` will always point to the latest generated ID stitcher. You may check its definition to see the exact table name it is referring to.\n\n**How can I remove the material tables that are no longer needed?**\n\nTo clean up all the materials older than a specific duration, for example 10 days, execute the following command:\n\n```\npb cleanup materials -r 10\n```\n\nThe minimum value you can set here is `1`. So if you have run the ID stitcher today, then you can remove all the older materials using `pb cleanup materials -r 1`.\n\n**Which tables and views are important in Profiles schema that should not be deleted?**\n\n*   `material_registry`\n*   `material_registry_<number>`\n*   `pb_ct_version`\n*   `ptr_to_latest_seqno_cache`\n*   `wht_seq_no`\n*   `wht_seq_no_<number>`\n*   Views whose names match your models in the YAML files.\n*   Material tables from the latest run (you may use the `pb cleanup materials` command to delete materials older than a specific duration).\n\n**I executed the auto migrate command and now I see a bunch of nested** `original_project_folder`. **Are we migrating through each different version of the tool?**\n\nThis is a symlink to the original project. Click on it in the Finder (Mac) to open the original project folder.\n\n**I am getting a **`ssh: handshake failed`** error when referring to a public project hosted on GitHub. It throws error for https:// path and works fine for ssh: path. I have set up token in GitHub and added to siteconfig.yaml file but I still get this error.**\n\nYou need to follow a different format for `gitcreds:` in siteconfig. See [SiteConfiguration](https://www.rudderstack.com/docs/archive/profiles/0.13/cli-user-guide/structure/#site-configuration-file-configuration.md) for the format.\n\nAfter changing `siteconfig`, if you still get an error, then clear the `WhtGitCache` folder inside the directory having the `siteconfig` file.\n\n**If I add filters to** `id_types` **in the project file, then do all rows that include any of those values get filtered out of the analysis, or is it just the specific value of that id type that gets filered?**\n\nThe PB tool does not extract rows. Instead, it extracts pairs from rows.\n\nSo if you had a row with email, user\\_id, and anonymous\\_id and the anonymous\\_id is excluded, then the PB tool still extracts the email, user\\_id edge from the row.\n\n**In the material registry table, what does** `status: 2` **mean?**\n\n*   `status: 2` means that the material has successfully completed its run.\n*   `status: 1` means that the material did not complete its run.\n\n**I am using Windows and get the following error:** `Error: while trying to migrate project: applying migrations: symlink <path>: A required privilege is not held by the client`.\n\nYour user requires privileges to create a symlink. You may either grant extra privileges to the user or try with a user containing Admin privileges on PowerShell. In case that doesn’t help, try to install and use it via WSL (Widows subsystem for Linux).\n\n**Can I specify any git account like CommitCode while configuring a project in the web app?**\n\nProfiles UI supports repos hosted on GitHub, BitBucket and GitLab.\n\n**If I want to run multiple select models, how can I run something like: `pb run --model_refs \"models/ewc_user_id_graph_all, models/ewc_user_id_graph, models/ewc_user_id_graph_v2`**\n\nYou can do so by passing `--model_refs` multiple times per model:\n\n`pb run -p samples/test_feature_table --model_refs 'models/test_id__, user/all' --migrate_on_load` OR `pb run -p samples/test_feature_table --model_refs models/test_id__ --model_refs user/all --migrate_on_load`\n\n**How can I keep my Profiles projects up to date along with updating the Python package and migrating the schema version?**\n\nYou can check for the latest Profiles updates in the [changelog](https://www.rudderstack.com/docs/archive/profiles/0.13/changelog/).\n\nTo update the Python package and migrate the schema version, you can standardise on a single pip release across the org and use the schema version that is native to that binary. When you move to a different binary, migrate your projects to the schema version native to it.\n\nContact Profiles support team in our [Community Slack](https://rudderstack.com/join-rudderstack-slack-community) for specific questions.\n\n**I am facing this error on adding a custom ID `visitor_id` under the `id_types` field in the `pb_project.yaml` file:**\n\n`could not create project: failed to read project yaml Error: validating project sample_attribution: getting models for folder: user: error listing models: error building model user_default_id_stitcher: id type visitor_id not in project id types`\n\nWhile adding a custom ID type, you must extend the package to include its specification in the `pb_project.yaml` file as well. In this case, add the key `extends:` followed by name of the same/different id\\_type that you wish to extend, and corresponding filters with include/exclude values like below:\n\n```\nid_types:\n - name: visitor_id\n   extends: visitor_id\n  filters:\n   - type: exclude\n     value: \"someexcludedvalue\"\n```\n\n**Can I keep multiple projects in a Git Repo?**\n\nYes, you can create multiple folders in your project repo and keep different projects in each folder. While running the project, you can use any suitable URL to run a specific project:\n\n`https://github.com/<org-name>/<repo-name>/tree/<branch-name>/path/to/project` `https://github.com/<org-name>/<repo-name>/tag/<tag-name>/path/to/project` `https://github.com/<org-name>/<repo-name>/commit/<commit-hash>/path/to/project`\n\nSee [Supported Git URLs](https://www.rudderstack.com/docs/archive/profiles/0.13/example/packages/#supported-git-urls) for more information. **Can a models folder contain subfolders?**\n\nYes, you can manually add subfolders to the models folder and reference their path in the `pb_project.yaml` file:\n\n```\nmodel_folders:\n  - models/inputs\n  - models/inputs/web.yml\n```\n\nA sample folder structure is shown:\n\n```\n.\n├── models/\n│   ├── inputs/\n|   │   ├── web.yml\n|   │   ├── mobile.yml\n|   │   └── server.yml\n│   └── ...\n```\n\n**How is Activations different from Audiences?**\n\nActivations qualify as Audiences with a minor exception of having a Profiles project as a source instead of a Reverse ETL source (with schema, database, table etc).\n\n**I am running a Profiles project with the `timegrains` parameter and noticed that multiple subfolders having different `seq_no` are generated. Which `seq_no` should I use to resume an earlier run?**\n\nFor the CLI project, you can resume the project run using CLI commands(like `run`, `compile`, etc.) and passing the `--seq_no` displayed at the top of the terminal output. For the UI project, you cannot choose to stop/resume the project run.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Profiles FAQ | RudderStack Docs",
    "description": "Commonly asked questions on RudderStack Profiles.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.12/predictions/python-models/",
    "markdown": "# Python Models | RudderStack Docs\n\nProfiles model for generating predictive features.\n\n* * *\n\n*     2 minute read  \n    \n\nPredictive features are generated using a new type of Profiles models called `python_model`.\n\n## Using a Python model\n\nThere are two key steps involved in using a Python model - **train** and **predict**.\n\nTo use a Python model, you need to modify the `train` and `predict` blocks in your `profiles.yaml` file. The following snippet highlights these blocks:\n\n```\n# This is a sample file, for detailed reference see: https://rudderlabs.github.io/pywht/\nmodels:\n  - name: shopify_churn\n    model_type: python_model\n    model_spec:\n      occurred_at_col: insert_ts\n      entity_key: user\n      validity_time: 24h # 1 day\n      py_repo_url: https://github.com/rudderlabs/rudderstack-profiles-classifier.git\n      predict:\n        inputs:\n          - packages/feature_table/models/shopify_user_features\n        config:\n          outputs:\n            column_names:\n              percentile: &percentile_name percentile_churn_score_7_days\n              score: churn_score_7_days\n            feature_meta_data: &feature_meta_data\n              features:\n                - name: *percentile_name\n                  description: 'Percentile of churn score. Higher the percentile, higher the probability of churn'\n      train:\n        file_extension: .json\n        file_validity: 60m\n        inputs:\n          - packages/feature_table/models/shopify_user_features\n        config:\n          data:\n            label_column: is_churned_7_days\n            label_value: 1\n            prediction_horizon_days: 7\n            model_name: 'shopify_user_features'\n            \n      <<: *feature_meta_data\n```\n\nIn a Python model, the actual logic resides in a remote location defined by the key `py_repo_url`. This need not be modified for setting up a predictive feature.\n\nIn the `train` block, you can define the label columns by pointing to the `entity_var` defined in the feature table model. You also need to define the following:\n\n*   Expected label value for users who performed the event.\n*   Horizon days, that is, number of days in advance when the predictions need to be made.\n*   Feature table model name defined for your predictive features project. See [Set up a feature table model](https://www.rudderstack.com/docs/archive/profiles/0.12/predictions/#set-up-a-feature-table-model) for more information.\n*   Criteria for eligible users so the model need not be used to predict for all users. You can set this criteria by defining a SQL statement referring to the different `entity_vars`. For example:\n\n```\neligible_users: lower(country) = 'us' and amount_spent_overall > 0\n```\n\nThe above example ensures that the model is trained only on the paying users from the US. Also, the model makes predictions only on this set of users.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> Note that:\n> \n> *   The `eligible_users` key should be added as one more parameter in the data configuration.\n> *   To build a model based on all available users, you can leave the `eligible_users` parameter blank.\n\n## Optional: Run Python model locally\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> If you are using the RudderStack dashboard for running the models, you can skip this step. However, note that RudderStack runs the models locally a few time to get the correct setup.\n\nTo run python models locally, you need to set up a python environment with the required packages and add the python path to the [`siteconfig.yaml`](https://www.rudderstack.com/docs/archive/profiles/0.12/cli-user-guide/structure/#site-configuration-file-configuration.md) file.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Python Models | RudderStack Docs",
    "description": "Profiles model for generating predictive features.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.12/predictions/",
    "markdown": "# Predictions (Early Access) | RudderStack Docs\n\nUse Profiles’ predictive features to train machine learning models.\n\n* * *\n\n*     7 minute read  \n    \n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> Predictions is part of our [Early Access Program](https://www.rudderstack.com/docs/resources/early-access-program/), where we work with early users and customers to test new features and get feedback before making them generally available. These features are functional but can change as we improve them. We recommend connecting with our team before running them in production.\n> \n> [Contact us](https://www.rudderstack.com/contact/) to get access to this feature.\n\nPredictions extends Profiles’ standard [feature development](https://www.rudderstack.com/docs/archive/profiles/0.12/core-concepts/feature-development/) functionality and lets you easily create predictive features in your warehouse. You can predict features like:\n\n*   Is a customer likely to churn in the next 30 days?\n*   Will a user make a purchase in the next 7 days?\n*   Is a lead going to convert?\n*   How much is a user likely to spend in the next 90 days?\n\nFinally, you can add the predicted feature to user profiles in your warehouse automatically and deliver ML-based segments and audiences to your marketing, product, and customer success teams.\n\nThe following self-guided tour shows you how to build the predictive traits. You can follow the guide and build the project yourself, including sample data, in our [Predictions sample project](https://www.rudderstack.com/docs/archive/profiles/0.12/example/predictive-features-snowflake/).\n\n## Use cases\n\nThis section covers some common Predictions use cases.\n\n### Churn prediction\n\nPredicting churn is one of the crucial initiatives across businesses. Without a predicted churn score, your actions are reactive, whereas you can act proactively with a user trait like `is_likely_to_churn`. Once you have such features, you can activate them with the appropriate outreach programs to prevent user churn.\n\n### Customer LTV prediction\n\nPredictions helps you understand your customers’ purchasing behavior over time. You can predict how much amount a particular customer is likely to spend within the prediction time range.\n\n## Prerequisites\n\n*   You must be using a [Snowflake](https://www.rudderstack.com/docs/destinations/warehouse-destinations/snowflake/) or [Redshift](https://www.rudderstack.com/docs/destinations/warehouse-destinations/redshift/) warehouse.\n*   You must set up a standard Profiles project with a [feature table model](https://www.rudderstack.com/docs/archive/profiles/0.12/example/feature-table/).\n*   **Optional**: If you are using Snowflake, you might need to create a [Snowpark](https://www.snowflake.com/en/data-cloud/snowpark/)\\-optimized warehouse if your dataset is significantly large.\n\n## Project setup\n\nThis section highlights the project setup steps for the churn prediction and LTV models.\n\n### Churn prediction\n\nSetting up Predictions for predicting churn involves four easy-to-follow steps:\n\n1.  [Set up a feature table with labels](#1-set-up-a-feature_table_model)\n2.  [Configure training parameters](#2-training) to generate the predictive features.\n3.  [Configure prediction parameters](#3-prediction) to generate the predictive features.\n4.  [Schedule periodic Predictions](#4-scheduling) to generate the predictive features.\n\n### 1\\. Set up a `feature_table_model`\n\nFollow the [Feature table](https://www.rudderstack.com/docs/archive/profiles/0.12/example/feature-table/) guide to start with a basic Profiles project. The feature you want to predict should be a part of the feature table in the project.\n\nFor example, to predict 30-day inactive churn in advance, you should define it in the feature table so that the model knows how to compute this for historic users.\n\n```\nentity_var:\n  name: churn_30_days\n  select: case when days_since_last_seen >= 30 then 1 else 0 end\n```\n\n### 2\\. Training\n\nRudderStack simplifies your training configuration to a set of parameters. Start with a [`python_model`](https://www.rudderstack.com/docs/archive/profiles/0.12/predictions/python-models/) type and mention the following parameters:\n\n```\ntrain:\n    file_extension: .json\n    file_validity: 168h\n    inputs: &inputs\n      - packages/feature_table/models/shopify_user_features\n    config:\n      data: &model_data_input_configs\n        label_column: churn_30_days\n        label_value: 1\n        prediction_horizon_days: 30\n        output_profiles_ml_model: *model_name\n        eligible_users: ''\n        inputs: *inputs\n        entity_column: user_main_id\n        recall_to_precision_importance: 1.0\n      preprocessing: \n        ignore_features: [name, gender, device_type]\n```\n\n| Parameter | Description |\n| --- | --- |\n| `file_extension`  <br>Required | The file extension. This is a static value and does not need to be modified. |\n| `file_validity`  <br>Required | If the last trained model is older than this duration, then the model is trained again. |\n| `inputs`  <br>Required | Path to the base features project. |\n| `label_column`  <br>Required | Column for which we want the Predictions. |\n| `prediction_horizon_days`  <br>Required | Number of days in advance when the prediction should be made.<br><br>See [Prediction horizon days](https://www.rudderstack.com/docs/profiles/glossary/#prediction-horizon-days) for more information. |\n| `output_profiles_ml_model`  <br>Required | Name of the model. |\n| `eligible_users` | Definition of the feature that needs to be defined only for a segment of users.<br><br>For example, `country='US' and is_payer=true` |\n| `config.data.inputs` | Path to the referenced project. The `inputs` key above should have `&inputs` added to it. |\n| `entity_column` | If the value of`id_column_name` in the ID stitcher is changed, it should be referenced here too. This field is optional otherwise. |\n| `recall_to_precision_importance` | Also referred to as **beta** in f-beta score, this field is used in classification models to fine-tune the model threshold and give more weight to recall against precision.<br><br>**Note**: This is an optional parameter. If not specified, it defaults to `1.0`, giving equal weight to precision and recall. |\n| `ignore_features` | List of columns from the feature table which the model ignores for training. |\n\n#### 3\\. Prediction\n\nIn your `python_model`, mention the following parameters:\n\n```\npredict:\n    inputs:\n      - packages/feature_table/models/shopify_user_features\n    config:\n      data: *model_data_input_configs\n      outputs:\n        column_names:\n          percentile: &percentile_name percentile_churn_score_7_days\n          score: churn_score_7_days\n        feature_meta_data: &feature_meta_data\n          features:\n            - name: *percentile_name\n              description: 'Percentile of churn score. Higher the percentile, higher the probability of churn'\n```\n\n| Parameter | Description |\n| --- | --- |\n| `inputs`  <br>Required | Path to the base features project. |\n| `percentile`  <br>Required | Column in the output table having the percentile score. |\n| `score`  <br>Required | Column in the output table having the probabilistic score. |\n| `description`  <br>Required | Custom description to give for the feature. |\n\n#### 4\\. Scheduling\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> [Contact us](https://www.rudderstack.com/contact/) to enable this feature for your account.\n\n1.  Upload your project to a GitHub repository.\n2.  Create a Profiles project in the [RudderStack dashboard](https://app.rudderstack.com/). Use the GitHub repository to set up the project.\n3.  Schedule your project with the required cadence. Note that this schedule is for prediction.\n\nTrainings are scheduled as per your configuration of the `file_validity` parameter in the `training` section of your project.\n\n### LTV models\n\nWhile the default labels in the churn prediction model are Boolean, Profiles also lets you predict a continuous variable like revenue or LTV. The configuration is almost similar to churn prediction with some minor adjustments.\n\n#### Set up a `feature_table_model`\n\nThe steps are similar to the setup for [churn prediction](#1-set-up-a-feature_table_model).\n\n#### Training\n\nStart with a [`python_model`](https://github.com/rudderlabs/rudderstack-profiles-base-features-git-flow/blob/feature/prml-319-add-ltv-specific-predictive-feature-in-base-features/models/profiles-ml.yaml) type and specify the following parameters:\n\n```\ntrain:\n    file_extension: .json\n    file_validity: 168h\n    inputs: &inputs\n      - packages/feature_table/models/shopify_user_features\n    config:\n      data: &model_data_input_configs\n        label_column: amount_spent_past_7_days\n        task: regression\n        prediction_horizon_days: 7\n        output_profiles_ml_model: *model_name\n        eligible_users: ''\n        inputs: *inputs\n        entity_column: user_main_id\n      preprocessing: \n        ignore_features: [name, gender, device_type]\n```\n\n| Parameter | Description |\n| --- | --- |\n| `file_extension`  <br>Required | The file extension. This is a static value and does not need to be modified. |\n| `file_validity`  <br>Required | If the last trained model is older than this duration, then the model is trained again. |\n| `inputs`  <br>Required | Path to the base features project. |\n| `label_column`  <br>Required | Column for which we want the Predictions. |\n| `task` | Set to `regression`. Unless specified explicitly, Profiles sets it to `classification` by default. |\n| `prediction_horizon_days`  <br>Required | Number of days in advance when the prediction should be made.<br><br>See [Prediction horizon days](https://www.rudderstack.com/docs/profiles/glossary/#prediction-horizon-days) for more information. |\n| `output_profiles_ml_model`  <br>Required | Name of the model. |\n| `eligible_users` | Definition of the feature that needs to be defined only for a segment of users.<br><br>For example, `country='US' and is_payer=true` |\n| `config.data.inputs` | Path to the referenced project. The `inputs` key above should have `&inputs` added to it. |\n| `entity_column` | If the value of`id_column_name` in the ID stitcher is changed, it should be referenced here too. This field is optional otherwise. |\n| `ignore_features` | List of columns from the feature table which the model ignores for training. |\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> Note that:\n> \n> *   Unlike churn prediction, you should not specify the `label_value` and `recall_to_precision_importance` labels.\n> *   The LTV model introduces a new parameter called `task` which you must set to `regression`. Profiles assumes a classification model by default, unless explicitly specified otherwise.\n\n#### Prediction\n\nThe `python_model` for the LTV use case remains almost the same as churn prediction, except some minor changes:\n\n```\npredict:\n    inputs:\n      - packages/feature_table/models/shopify_user_features\n    config:\n      data: *model_data_configs\n      preprocessing: *model_prep_configs\n      outputs:\n        column_names:\n          percentile: &percentile_name percentile_predicted_amount_spent\n          score: predicted_amount_spent\n        feature_meta_data: &feature_meta_data\n          features:\n            - name: *percentile_name\n              description: 'Percentile of predicted future LTV. Higher the percentile, higher the expected LTV.'\n```\n\n| Parameter | Description |\n| --- | --- |\n| `inputs`  <br>Required | Path to the base features project. |\n| `percentile`  <br>Required | Column in the output table having the percentile score. |\n| `score`  <br>Required | Column in the output table having the probabilistic score. |\n| `description`  <br>Required | Custom description to give for the feature. |\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> The label column is missing in the above `predict` block as the `score` parameter captures the actual prediction value and a Boolean flag is not meaningful in regression use cases.\n\n#### Scheduling\n\nThe steps are similar to the setup for [churn prediction](#4-scheduling).\n\n## Results\n\nFinal output or the predicted features are pushed to your customer360 table. Use the **Explorer** tab to check the predicted value for any given user along with the historical values up to last 5 runs.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> To check the predicted value for a given user:\n> \n> 1.  In the **Preview** section, go to the **Predictive features** tab.\n> 2.  Check the user profile for which the predictive feature has a value.\n> 3.  Search the **USER\\_MAIN\\_ID** of the profile in **Profile viewer**.\n\n[![Predictions predictive feature value](https://www.rudderstack.com/docs/images/profiles/profilesml-predictivefeaturevalue.webp)](https://www.rudderstack.com/docs/images/profiles/profilesml-predictivefeaturevalue.webp)\n\nThe value of the predictive feature is a probability. You can consider it as `true` or `false` based on your threshold.\n\nAll your predictive features are listed separately in the **Overview** tab of your Profiles project. You can check the logs of each run in the `artifacts` directory (available in the **History** tab of your Profiles project).\n\n[![ProfilesML artifacts](https://www.rudderstack.com/docs/images/profiles/profilesml-artifacts.webp)](https://www.rudderstack.com/docs/images/profiles/profilesml-artifacts.webp)\n\nPredictions is a part of RudderStack’s [Early Access Program](https://www.rudderstack.com/docs/resources/early-access-program/). [Contact us](https://www.rudderstack.com/contact/) to get access to this feature.\n\n## FAQ\n\n#### **Is there a project to understand Predictions further?**\n\nYou can check the [Shopify churn model](https://github.com/rudderlabs/rudderstack-profiles-shopify-churn/) that builds a churn prediction score on top of the Shopify library project.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Predictions (Early Access) | RudderStack Docs",
    "description": "Use Profiles' predictive features to train machine learning models.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.12/activation-api/",
    "markdown": "# Activation API (Early Access) | RudderStack Docs\n\nExpose user profiles stored in your Redis instance over an API.\n\n* * *\n\n*     7 minute read  \n    \n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> The Activation API is part of our [Early Access Program](https://www.rudderstack.com/docs/resources/early-access-program/), where we work with users and customers to test new features and get feedback before making them generally available. These features are fully functional but can change as we improve them. We recommend connecting with our team before running them in production.\n> \n> [Contact us](https://www.rudderstack.com/contact/) if you would like access to this feature.\n\nWith RudderStack’s Activation API, you can fetch enriched user traits stored in your Redis instance and use them for near real-time personalization for your target audience.\n\n[![Activation API](https://www.rudderstack.com/docs/images/profiles/activation-api.webp)](https://www.rudderstack.com/docs/images/profiles/activation-api.webp)\n\n## Overview\n\nA brief summary of how the Activation API works:\n\n1.  Sync all your customer 360 data from your Profiles project to your Redis store.\n2.  The Activation API sits on top of this Redis instance and provides endpoints for retrieving and using the enriched user data for personalization.\n\n## How to use the Activation API\n\n1.  Use your Profiles project to create a 360-degree view of the features stored in your data warehouse.\n2.  In your Profiles project settings, scroll down to **Activation API** and turn on the **Enable sync to Redis** toggle.\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> Before you enable the Activation API toggle, make sure that:\n> \n> *   You have at least one successful Profiles run.\n> *   Your `pb_project.yaml` > `entities` defines a `feature_views` property.\n> \n> Also, note that the first time you toggle on this API, RudderStack requests for your Redis credentials of the Profiles Redis store.\n\n[![Enable Redis sync for using Activation API](https://www.rudderstack.com/docs/images/profiles/enable-redis-sync.webp)](https://www.rudderstack.com/docs/images/profiles/enable-redis-sync.webp)\n\n3.  [Generate a personal access token](#faq) with **Admin** role in your RudderStack dashboard. You will need this token for authenticating the Activation API.\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> Personal access token with an **Admin** role is only available to the **Org Admin** users. Refer [User management](https://www.rudderstack.com/docs/dashboard-guides/user-management/#org-admin) guide for more information.\n\n4.  Note the Redis destination ID. See [FAQ](#redis-destination-id) for more information on obtaining this ID.\n5.  [Use the Activation API endpoint](#get-user-profiles) to access your Redis instance and get user data.\n\n> ![success](https://www.rudderstack.com/docs/images/tick.svg)\n> \n> See [Use case](#use-case) for more information on how you can use this data.\n\nThis API uses [Bearer Authentication](https://swagger.io/docs/specification/authentication/bearer-authentication/) for authenticating all requests.\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> Set the [personal access token](#faq) as the bearer token for authentication.\n\n## Base URL\n\n```\nhttps://profiles.rudderstack.com/v1/\n```\n\n## Get user profiles\n\n#### Request body\n\nString\n\nRedis destination ID.\n\nObject\n\nID containing `type` and `value`\n\n```\n{\n  \"entity\": <entity_type>,  // User, project, account, etc.\n  \"destinationId\": <redis_destination_id> , // Redis destination ID\n  \"id\": {\n    \"type\": <id_type>,\n    \"value\": <id_value>\n  }\n}\n```\n\n#### Example request\n\n```\nPOST /v1/activation HTTP/1.1\nHost: profiles.rudderstack.com\nContent-Type: application/json\nAuthorization: Bearer <personal_access_token>\nContent-Length: 90\n\n{\n \"entity\": <entity_type>,\n \"destinationId\": <redis_destination_id>, // Redis destination ID\n \"id\": {\n   \"type\": <id_type>,\n   \"value\": <id_value>\n }\n}\n```\n\n```\ncurl --location 'https://profiles.rudderstack.com/v1/activation' \\\n--header 'Content-Type: application/json' \\\n--header 'Authorization: Bearer <personal_access_token>' \\\n--data '{\n \"entity\": <entity_type>,\n \"destinationId\": <redis_destination_id>, // Redis destination ID\n \"id\": {\n   \"type\": <id_type>,\n   \"value\": <id_value>\n }\n}'\n```\n\n```\nconst axios = require('axios');\nlet data = JSON.stringify({\n  \"destinationId\": <redis_destination_id>,\n  \"entity\": <entity_type>,\n  \"id\": {\n    \"type\": <id_type>,\n    \"value\": <id_value>\n  }\n});\n\nlet config = {\n  method: 'post',\n  maxBodyLength: Infinity,\n  url: 'https://profiles.rudderstack.com/v1/activation',\n  headers: {\n    'Content-Type': 'application/json',\n    'authorization': 'Bearer <personal_access_token>'\n  },\n  data: data\n};\n\naxios.request(config)\n  .then((response) => {\n    console.log(JSON.stringify(response.data));\n  })\n  .catch((error) => {\n    console.log(error);\n  });\n```\n\n#### Responses\n\n*   If the personal access token is absent or trying to access a destination to which it does not have access:\n\n```\nstatusCode: 401\nResponse: {\n  \"error\": \"Unauthorized request. Please check your access token\"\n}\n```\n\n*   If the destination is not Redis or the destination ID is absent/blank:\n\n```\nstatusCode: 404\nResponse: {\n  \"error\": \"Invalid Destination. Please verify you are passing the right destination ID\"\n}\n```\n\n*   If ID is present:\n\n```\nstatusCode: 200\nResponse:\n{\n  \"entity\": <entity_type>,\n  \"id\": {\n    \"type\": <id_type>,\n    \"value\": <id_value>\n  },\n  \"data\": {\n    <traits_from_Redis>\n  }\n}\n```\n\n*   If ID is not present in Redis:\n\n```\nstatusCode: 200\nResponse:\n{\n  \"entity\": <entity_type>,\n  \"id\": {\n    \"type\": <id_type>,\n    \"value\": <id_value>\n  },\n  \"data\": {}\n}\n```\n\n## Use case\n\nYou can use the Activation API for real-time personalization. Once you fetch the user traits from your Redis instance via the API, you can pull them into your client application to alter the application behavior in real-time based on user interactions.\n\nYou can respond immediately with triggered, user-focused messaging based on actions like page views or app clicks and provide a better customer experience.\n\n[![Real time personalization use case](https://www.rudderstack.com/docs/images/profiles/activation-api-use-case.webp)](https://www.rudderstack.com/docs/images/profiles/activation-api-use-case.webp)\n\n## Redis configuration\n\nThis section describes the settings required to set up a [Reverse ETL connection](https://www.rudderstack.com/docs/sources/reverse-etl/) from your warehouse (containing the project’s output table) to the [Redis destination](https://www.rudderstack.com/docs/destinations/streaming-destinations/redis/) in RudderStack.\n\n[![Redis connection settings](https://www.rudderstack.com/docs/images/profiles/redis-connection-settings.webp)](https://www.rudderstack.com/docs/images/profiles/redis-connection-settings.webp)\n\n*   **Address**: Enter the public endpoint of your Redis database. If you are using [Redis Cloud](https://app.redislabs.com/#/), you can find this endpoint by going to your Redis database and navigating to **Configuration** tab > **General**.\n\n[![Redis database public endpoint](https://www.rudderstack.com/docs/images/profiles/redis-public-endpoint.webp)](https://www.rudderstack.com/docs/images/profiles/redis-public-endpoint.webp)\n\n*   **Password**: Enter the database password. You can find the password in the **Security** section of the **Configuration** tab:\n\n[![Redis database password](https://www.rudderstack.com/docs/images/profiles/redis-database-password.webp)](https://www.rudderstack.com/docs/images/profiles/redis-database-password.webp)\n\n*   **Database**: Enter the database name.\n*   **Cluster Mode**: Disable this setting if you haven’t set up Redis in a cluster.\n\n## Data mapping\n\nRudderStack creates multiple Reverse ETL sources automatically based on your Profiles project. You will see separate sources for different `id_served` connected to the same Redis destination.\n\nThe following `pb_project.yaml` snippet shows the sources to be created:\n\n```\nentities:\n  - name: user\n    id_types:\n      - main_id\n      - user_id\n    feature_views:\n      using_ids:\n        - id: email\n          name: features_by_email # Optional. Takes default view name, if not specified.\n        - id: salesforce_id\n          name: salesforce_id_stitched_features\n      features: # Optional\n        - from: models/cart_feature_table\n          include:\n            - \"*\"\n```\n\n## FAQ\n\n#### How do I generate a personal access token to use the Activation API?\n\n1.  Log in to your [RudderStack dashboard](https://app.rudderstack.com/).\n2.  Go to **Settings** > **Your Profile** > **Account** tab and scroll down to **Personal access tokens**. Then, click **Generate new token**:\n\n[![New personal access token in RudderStack dashboard](https://www.rudderstack.com/docs/images/rudderstack-api/personal-access-token-1.webp)](https://www.rudderstack.com/docs/images/rudderstack-api/personal-access-token-1.webp)\n\n3.  Enter the **Token name**. Set **Role** to **Admin** and click **Generate**.\n\n[![Personal access token name and role](https://www.rudderstack.com/docs/images/rudderstack-api/personal-access-token-2.webp)](https://www.rudderstack.com/docs/images/rudderstack-api/personal-access-token-2.webp)\n\n4.  Use the personal access token to authenticate to the Activation API.\n\n[![Personal access token details](https://www.rudderstack.com/docs/images/rudderstack-api/personal-access-token-3.webp)](https://www.rudderstack.com/docs/images/rudderstack-api/personal-access-token-3.webp)\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> Save the generated token securely as it will not be visible again once you close this window.\n\n#### Where can I find the Redis destination ID?\n\n1.  Set up a [Redis destination](https://www.rudderstack.com/docs/destinations/streaming-destinations/redis/) in RudderStack.\n2.  Go to the **Settings** tab of your destination to see the **Destination ID**:\n\n[![Redis destination ID](https://www.rudderstack.com/docs/images/rudderstack-api/redis-destination-id.webp)](https://www.rudderstack.com/docs/images/rudderstack-api/redis-destination-id.webp)\n\n#### How can I make Profiles work with the Activation API?\n\nTo use the Activation API with your Profiles project, you need a successful run of your Profiles project that is not past the retention period.\n\nTo enable the Activation API for your Profiles project, turn on the **Enable sync to Redis** setting. A Profile run will then sync automatically.\n\n[![Toggle API in Settings](https://www.rudderstack.com/docs/images/rudderstack-api/activation-api-toggle-settings.png)](https://www.rudderstack.com/docs/images/rudderstack-api/activation-api-toggle-settings.png)\n\n#### Why am I getting an error trying to enable API in my instance for a custom project hosted on GitHub?\n\nFor GitHub projects, you need to explicitly add the IDs of the custom project that need to be served.\n\nIn your `pb_project.yaml` file, you can specify them as shown:\n\n```\nentities:\n  - name: user\n    id_types:\n      - main_id\n      - user_id\n    feature_views:\n      using_ids:\n        - id: email\n          name: features_by_email\n        - id: salesforce_id\n          name: salesforce_id_stitched_features\n      features:\n        - from: models/cart_feature_table\n          include:\n            - \"*\"\n```\n\n#### If I force a full resync, stop it, and then start a new sync, does RudderStack always perform a full sync the next time?\n\nIt depends on the state of the task when it was canceled.\n\n*   If the sync is cancelled while RudderStack is preparing a snapshot, then next run depends on the state of the previous successful run and any mapping changes.\n*   If it is cancelled after the sync data is prepared, the next run is incremental.\n\nGenerally if a sync is cancelled manually, it is recommended to trigger a full sync if the previous cancelled task was a full sync. If the previously cancelled sync was incremental, triggering an incremental sync is recommended.\n\n#### Does RudderStack perform a full sync if I add a new column?\n\nRudderStack does not change the sync mode if you make any column additions. It triggers a full sync only if you change/update the data mappings, for example, if the newly added column is sent to the destination via the [Visual Data Mapper](https://www.rudderstack.com/docs/sources/reverse-etl/visual-data-mapper/).\n\nFor Profiles activation syncs, RudderStack updates the mappings and automatically sends all columns from the customer 360 view by triggering a full sync.\n\n#### Suppose I’m running a full sync and the Profiles job is running in parallel and finishes eventually. What happens to the scheduled sync? Does it get queued?\n\nRudderStack first creates a temporary snapshot copy of any sync when it starts. So its syncing the created copy. Even if a Profiles job is running in parallel, the sync - if started - is not impacted by it.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Activation API (Early Access) | RudderStack Docs",
    "description": "Expose user profiles stored in your Redis instance over an API.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.12/example/id-stitcher/",
    "markdown": "# Identity Stitching | RudderStack Docs\n\nStep-by-step tutorial on how to stitch together different user identities.\n\n* * *\n\n*     7 minute read  \n    \n\nThis guide provides a detailed walkthrough on how to use a PB project and create output tables in a warehouse for a custom identity stitching model.\n\n## Prerequisites\n\n*   Familiarize yourself with:\n    \n    *   A basic Profile Builder project by following the [Profile Builder CLI](https://www.rudderstack.com/docs/archive/profiles/0.12/get-started/profile-builder/) steps.\n    *   [Structure](https://www.rudderstack.com/docs/archive/profiles/0.12/cli-user-guide/structure/) of a Profile Builder project and the parameters used in different files.\n\n## Output\n\nAfter [running the project](https://www.rudderstack.com/docs/archive/profiles/0.12/get-started/profile-builder/#7-generate-output-tables), you can view the generated material tables:\n\n1.  Log in to your Snowflake console.\n2.  Click **Worksheets** from the top navigation bar.\n3.  In the left sidebar, click **Database** and the corresponding **Schema** to view the list of all tables. You can hover over a table to see the full table name along with its creation date and time.\n4.  Write a SQL query like `select * from <table_name> limit 10;` and execute it to see the results:\n\n1.  Open [Postico2](https://eggerapps.at/postico2/). If required, create a new connection by entering the relevant details. Click **Test Connection** followed by **Connect**.\n2.  Click the **+** icon next to **Queries** in the left sidebar.\n3.  You can click **Database** and the corresponding schema to view the list of all tables/views.\n4.  Double click on the appropriate view name to paste the name on an empty worksheet.\n5.  You can prefix `SELECT *` from the view name pasted previously and suffix `LIMIT 10;` at the end.\n6.  Press Cmd+Enter keys, or click the **Run** button to execute the query.\n\n1.  Enter your Databricks workspace URL in the web browser and log in with your username and password.\n2.  Click the **Catalog** icon in left sidebar.\n3.  Choose the appropriate catalog from the list and click on it to view the contents.\n4.  You will see list of tables/views. Click on the appropriate table/view name to paste the name on the worksheet.\n5.  Then, you can prefix `SELECT * FROM` before the pasted view name and suffix `LIMIT 10;` at the end.\n6.  Select the query text. Press Cmd+Enter or click the **Run** button to execute the query.\n\n1.  Log in to your [Google Cloud Console](https://console.cloud.google.com/)\n2.  Search for Bigquery in the search bar.\n3.  Select Bigquery from **Product and Pages** to open the Bigquery **Explorer**.\n4.  Select the correct project from top left drop down menu.\n5.  In the left sidebar, click the project ID, then the corresponding dataset view list of all the tables and views.\n6.  Write a SQL query like `select * from <table_name> limit 10;` and execute it to see the results.\n\nA sample output containing the results in Snowflake:\n\n![Generated tables (Snowflake)](https://www.rudderstack.com/docs/images/profiles/snowflake-console.webp)\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> Profiles creates a default ID stitcher even if you do not define any specs for creating one. It takes `default ID stitcher` as the input and all the sources and ID types defined in the file `inputs.yaml`. When you define the specs, it creates a custom ID stitcher.\n\n## Sample project for Custom ID Stitcher\n\nThis sample project considers multiple user identifiers in different warehouse tables to ties them together to create a unified user profile. The following sections describe how to define your PB project files:\n\n### Project detail\n\nThe [`pb_project.yaml`](https://www.rudderstack.com/docs/archive/profiles/0.12/cli-user-guide/structure/#project-details) file defines the project details such as name, schema version, connection name and the entities which represent different identifiers.\n\nThere can be different ID types for an entity. You can include all such identifiers in the `id_types` field under `entities`. `main_id` specified under `id_types` is not an ID type but a placeholder for the column which serves as the primary identifier for that entity.\n\nIn case of `id_stitcher` model, the `main_id` for the entity is `rudder_id` (predefined ID type) by default. For other models, any other ID type can be the `main_id`, for example `session_id`. Hence, if you want to specify the ID type of a column as a primary identifier, you can specify `main_id`.\n\n```\n# Project name\nname: sample_id_stitching\n# Project's yaml schema version\nschema_version: 61\n# Warehouse connection\nconnection: test\n# Allow inputs without timestamps\ninclude_untimed: true\n# Folder containing models\nmodel_folders:\n  - models\n# Entities in this project and their ids.\nentities:\n  - name: user\n    id_stitcher: models/user_id_stitcher # modelRef of custom ID stitcher model\n    id_types:\n      - main_id # You need to add ``main_id`` to the list only if you have defined ``main_id_type: main_id`` in the id stitcher buildspec.\n      - user_id # one of the identifier from your data source.\n      - email\n# lib packages can be imported in project signifying that this project inherits its properties from there\npackages:\n  - name: corelib\n    url: \"https://github.com/rudderlabs/profiles-corelib/tag/schema_{{best_schema_version}}\"\n    # if required then you can extend the package definition such as for ID types.\n```\n\n### Input\n\nThe [input file](https://www.rudderstack.com/docs/archive/profiles/0.12/cli-user-guide/structure/#inputs) (`models/inputs.yaml`) file includes the input table references and corresponding SQL for the above-mentioned entities:\n\n```\ninputs:\n- name: rsIdentifies\n  contract: # constraints that a model adheres to\n    is_optional: false\n    is_event_stream: true\n    with_entity_ids:\n      - user\n    with_columns:\n      - name: timestamp\n      - name: user_id\n      - name: anonymous_id\n      - name: email\n  app_defaults:\n    table: rudder_events_production.web.identifies # one of the WH table RudderStack generates when processing identify or track events.\n    occurred_at_col: timestamp\n    ids:\n      - select: \"user_id\" # kind of identity sql to pick this column from above table.\n        type: user_id\n        entity: user # as defined in project file\n        to_default_stitcher: true\n      - select: \"anonymous_id\"\n        type: anonymous_id\n        entity: user\n        to_default_stitcher: true\n      - select: \"lower(email)\" # can use sql.\n        type: email\n        entity: user\n        to_default_stitcher: true\n- name: rsTracks\n  contract:\n    is_optional: false\n    is_event_stream: true\n    with_entity_ids:\n      - user\n    with_columns:\n      - name: timestamp\n      - name: user_id\n      - name: anonymous_id\n  app_defaults:\n    table: rudder_events_production.web.tracks # another table in WH maintained by RudderStack processing track events.\n    occurred_at_col: timestamp\n    ids:\n      - select: \"user_id\"\n        type: user_id\n        entity: user\n        to_default_stitcher: true\n      - select: \"anonymous_id\"\n        type: anonymous_id\n        entity: user\n        to_default_stitcher: true\n```\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> As seen in the above file, you can use SQL to achieve some complex scenario as well.\n\n### Model\n\nProfiles **Identity stitching** model maps and unifies all the specified identifiers (in `pb_project.yaml` file) across different platforms. It tracks the user journey uniquely across all the data sources and stitches them together to a `rudder_id`.\n\nA sample `profiles.yaml` file specifying an identity stitching model (`user_id_stitcher`) with relevant inputs:\n\n```\nmodels:\n  - name: user_id_stitcher\n    model_type: id_stitcher\n    model_spec:\n      validity_time: 24h\n      entity_key: user\n      materialization:\n        run_type: incremental\n      incremental_timedelta: 12h\n      main_id_type: main_id\n      edge_sources:\n        - from: inputs/rsIdentifies\n        - from: inputs/rsTracks\n```\n\n##### Model specification fields\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `validity_time` | Time | Specifies the validity of the model with respect to its timestamp. For example, a model run as part of a scheduled nightly job for 2009-10-23 00:00:00 UTC with `validity_time`: `24h` would still be considered potentially valid and usable for any run requests, which do not require precise timestamps between 2009-10-23 00:00:00 UTC and 2009-10-24 00:00:00 UTC. This specifies the validity of generated feature table. Once the validity is expired, scheduling takes care of generating new tables. For example: 24h for 24 hours, 30m for 30 minutes, 3d for 3 days |\n| `entity_key` | String | Specifies the relevant entity from your `input.yaml` file. For example, here it should be set to `user`. |\n| `materialization` | List | Adds the key `run_type`: `incremental` to run the project in incremental mode. This mode considers row inserts and updates from the `edge_sources` input. These are inferred by checking the timestamp column for the next run. One can provide buffer time to consider any lag in data in the warehouse for the next incremental run like if new rows are added during the time of its run. If you do not specify this key then it’ll default to `run_type`: `discrete`. |\n| `incremental_timedelta` | List | (Optional )If materialization key is set to `run_type`: `incremental`, then this field sets how far back data should be fetched prior to the previous material for a model (to handle data lag, for example). The default value is 4 days. |\n| `main_id_type` | ProjectRef | (Optional) ID type reserved for the output of the identity stitching model, often set to `main_id`. It must not be used in any of the inputs and must be listed as an id type for the entity being stitched. If you do not set it, it defaults to `rudder_id`. Do not add this key unless it’s explicitly required, like if you want your identity stitcher table’s `main_id` column to be called `main_id`. For more information, see below. |\n| `edge_sources` | List | Specifies inputs for the identity stitching model as mentioned in the `inputs.yaml` file. |\n\n## Use cases\n\nThis section describes some common identity stitching use cases:\n\n*   **Identifiers from multiple data sources**: You can consider multiple identifiers and tables by:\n    \n    *   Adding entities in `pb_project.yaml` representing identifiers.\n    *   Adding references to table and corresponding sql in `models/inputs.yaml`\n    *   Adding table reference names defined in `models/inputs.yaml` as `edge_sources` in your model definition.\n*   **Leverage Sql Support**: You can use SQL in your `models/inputs.yaml` to achieve different scenarios. For example, you want to tag all the internal users in your organization as one entity. You can use the email domain as the identifier by adding a SQL query to extract the email domain as the identifier value: `lower(split_part({{email_col}}, '@', 2))`\n    \n*   **Custom ID Stitcher**: You can define a custom ID stitcher by defining the required id stitching model in `models/profiles.yaml`.\n    \n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Identity Stitching | RudderStack Docs",
    "description": "Step-by-step tutorial on how to stitch together different user identities.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.12/example/id-collator/",
    "markdown": "# ID Collator | RudderStack Docs\n\nStep-by-step tutorial stitching different user identities together.\n\n* * *\n\n*     3 minute read  \n    \n\nID Stitching is one of the most important features of Profiles. Being able to perform ID stitching to determine the accounts belonging to the same customer/user is very important to get a 360-degree view of that user.\n\nHowever many a times, we may not require ID stitching for a particular entity, especially if there are no edges in the ID graph of an entity. To build a feature table on such an entity, you will still need to perform ID stitching. Although this approach is not wrong, it is computationally redundant.\n\nProfiles provides the ID Collator is to get all IDs of that particular entity from various input tables and create one collated list of IDs.\n\n## Sample project\n\nLet’s take a case where we have defined two entities in our project - one is `user` and the other is `session`.\n\nIf `user` entity has multiple IDs defined, there are basically edges which make the use of an ID stitcher logical. On the other hand, `session` may have only one ID, `ssn_id`, there won’t be any possibility of edges. In such a case, all we need is a complete list of `ssn_id`.\n\nHere is the corresponding inputs and entities definition.\n\n```\nentities:\n  - name: user\n    id_column_name: user_rud_id\n    id_types:\n      - user_id\n      - anonymous_id\n  - name: session\n    id_column_name: session_id\n    id_types:\n      - ssn_id\n```\n\nProject file:\n\n```\ninputs:\n  - name: user_accounts\n    table: tbl_user_accounts\n    occurred_at_col: insert_ts\n    ids:\n      - select: \"user_id\"\n        type: user_id\n        entity: user\n  - name: sign_in\n    table: tbl_sign_in\n    occurred_at_col: insert_ts\n    ids:\n      - select: \"user_id\"\n        type: user_id\n        entity: user\n      - select: \"ssn_id\"\n        type: ssn_id\n        entity: session\n      - select: \"anonymous_id\"\n        type: anonymous_id\n        entity: user\n  - name: sign_up\n    table: tbl_sign_up\n    occurred_at_col: insert_ts\n    ids:\n      - select: \"user_id\"\n        type: user_id\n        entity: user\n      - select: \"ssn_id\"\n        type: ssn_id\n        entity: session\n      - select: \"anonymous_id\"\n        type: anonymous_id\n        entity: user\n```\n\nHere, the `entity: session` has only one ID type. Creating an ID stitcher for such an entity is possible but unnecessary.\n\nUsing all the models having `ssn_id`, we can just make a union of all `ssn_id` and get all distinct values of it and obtain the final list of sessions.\n\nThe underlying SQL will look as follows:\n\n```\nSELECT ssn_id as session_id from sign_in\n        UNION\n    SELECT ssn_id as session_id from sign_up\n;\n```\n\n## YAML Changes\n\nThe YAML writer cannot define a custom ID collator the way they define a custom ID stitcher. If an entity has no edges, the PB project will automatically figure out if an ID collator is needed. To exclude certain inputs (having the required ID) from being used in the collation, we can just set `to_id_stitcher: false` in the input.\n\n```\nentities:\n  - name: session\n    id_column_name: session_id\n    id_types:\n      - ssn_id\n```\n\nThe `id_column_name` is a new field added in the entity definition which will be the name of the ID column and it applies to both ID stitcher and ID collator.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> In the ID collator, you won’t generate a UUID like in ID stitcher.\n\n## Comparing ID Collator and ID Stitcher\n\n| ID Stitcher | ID Collator |\n| --- | --- |\n| Uses edges to converge the ID graph. | Collates all distinct IDs as there is only one ID Type and no edges are present. |\n| Higher cost of computation. | Lower cost of computation. |\n| A UUID is generated and used as the unique identifier for the entity. | Collates the existing IDs only. |\n| The generated ID is always of the type: `rudder_id` | The ID column of the generated ID collator table/view will be of the ID type of the corresponding ID. |\n| User may override the default ID stitcher with custom one. | You cannot override the default ID collator, though you can define a custom ID stitcher to override default ID collator. |\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "ID Collator | RudderStack Docs",
    "description": "Step-by-step tutorial stitching different user identities together.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.12/example/feature-table/",
    "markdown": "# Feature Table | RudderStack Docs\n\nStep-by-step tutorial on creating a feature table model.\n\n* * *\n\n*     10 minute read  \n    \n\nOnce you have done [identity stitching](https://www.rudderstack.com/docs/archive/profiles/0.12/core-concepts/identity-stitching/) to unify the identity of your users across all the cross-platforms, you can evaluate and maintain the required features/traits for each identified user in a feature table.\n\nThis guide provides a detailed walkthrough on how to use a PB project and create output tables in a warehouse for a feature table model.\n\n## Prerequisites\n\nFamiliarize yourself with:\n\n*   A basic Profile Builder project by following the [Profile Builder CLI](https://www.rudderstack.com/docs/archive/profiles/0.12/get-started/profile-builder/) steps.\n*   [Structure](https://www.rudderstack.com/docs/archive/profiles/0.12/cli-user-guide/structure/) of a Profile Builder project and the parameters used in different files.\n*   [Identity Stitching](https://www.rudderstack.com/docs/archive/profiles/0.12/example/id-stitcher/) model as feature table reuses its output to extract the required features/traits.\n\n## Sample project\n\nThis sample project uses the output of an identity stitching model as an input to create a feature table. The following sections describe how to define your PB project files:\n\n### Project detail\n\nThe [`pb_project.yaml`](https://www.rudderstack.com/docs/archive/profiles/0.12/cli-user-guide/structure/#project-details) file defines the project details such as name, schema version, connection name and the entities which represent different identifiers.\n\nYou can define all the identifiers from different input sources you want to stitch together as a `user_main_id`:\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> You need to add `main_id` to the list only if you have defined `main_id_type: main_id` in the ID stitcher buildspec.\n\n```\n# Project name\nname: sample_id_stitching\n# Project's yaml schema version\nschema_version: 61\n# Warehouse connection\nconnection: test\n# Allow inputs without timestamps\ninclude_untimed: true\n# Folder containing models\nmodel_folders:\n  - models\n# Entities in this project and their ids.\nentities:\n  - name: user\n    id_types:\n      - main_id # You need to add ``main_id`` to the list only if you have defined ``main_id_type: main_id`` in the id stitcher buildspec.\n      - user_id # one of the identifier from your data source.\n      - email\n# lib packages can be imported in project signifying that this project inherits its properties from there\npackages:\n  - name: corelib\n    url: \"https://github.com/rudderlabs/profiles-corelib/tag/schema_{{best_schema_version}}\"\n    # if required then you can extend the package definition such as for ID types.\n```\n\n### Input\n\nThe [input file](https://www.rudderstack.com/docs/archive/profiles/0.12/cli-user-guide/structure/#inputs) file includes the input table references and corresponding SQL for the above-mentioned entities:\n\n```\ninputs:\n- name: rsIdentifies\n  contract: # constraints that a model adheres to\n    is_optional: false\n    is_event_stream: true\n    with_entity_ids:\n      - user\n    with_columns:\n      - name: timestamp\n      - name: user_id\n      - name: anonymous_id\n      - name: email\n  app_defaults:\n    table: rudder_events_production.web.identifies # one of the WH table RudderStack generates when processing identify or track events.\n    occurred_at_col: timestamp\n    ids:\n      - select: \"user_id\" # kind of identity sql to pick this column from above table.\n        type: user_id\n        entity: user # as defined in project file\n        to_default_stitcher: true\n      - select: \"anonymous_id\"\n        type: anonymous_id\n        entity: user\n        to_default_stitcher: true\n      - select: \"lower(email)\" # can use sql.\n        type: email\n        entity: user\n        to_default_stitcher: true\n- name: rsTracks\n  contract:\n    is_optional: false\n    is_event_stream: true\n    with_entity_ids:\n      - user\n    with_columns:\n      - name: timestamp\n      - name: user_id\n      - name: anonymous_id\n  app_defaults:\n    table: rudder_events_production.web.tracks # another table in WH maintained by RudderStack processing track events.\n    occurred_at_col: timestamp\n    ids:\n      - select: \"user_id\"\n        type: user_id\n        entity: user\n        to_default_stitcher: true\n      - select: \"anonymous_id\"\n        type: anonymous_id\n        entity: user\n        to_default_stitcher: true\n```\n\n### Model\n\nProfiles **Feature Table** model lets you define the specific features/traits you want to evaluate from the huge spread of scattered data in your warehouse tables.\n\nA sample `profiles.yaml` file specifying a feature table model (`user_profile`):\n\n```\nmodels:\n  - name: user_profile\n    model_type: feature_table_model\n    model_spec:\n      validity_time: 24h\n      entity_key: user\n      features:\n        - user_lifespan\n        - days_active\n        - min_num_c_rank_num_b_partition\nvar_groups:\n  - name: user_vars\n    entity_key: user\n    vars:\n      - entity_var:\n          name: first_seen\n          select: min(timestamp::date)\n          from: inputs/rsTracks\n          where: properties_country is not null and properties_country != ''\n      - entity_var:\n          name: last_seen\n          select: max(timestamp::date)\n          from: inputs/rsTracks\n      - entity_var:\n          name: user_lifespan\n          select: '{{user.Var(\"last_seen\")}} - {{user.Var(\"first_seen\")}}'\n          description: Life Time Value of a customer\n      - entity_var:\n          name: days_active\n          select: count(distinct timestamp::date)\n          from: inputs/rsTracks\n          description: No. of days a customer was active\n      - entity_var:\n          name: campaign_source\n          default: \"'organic'\"\n      - entity_var:\n          name: user_rank\n          default: -1\n      - entity_var:\n          name: campaign_source_first_touch\n          select: first_value(context_campaign_source)\n          window:\n            order_by:\n              - timestamp asc\n          from: inputs/rsIdentifies\n          where: context_campaign_source is not null and context_campaign_source != ''\n      - input_var:\n          name: num_c_rank_num_b_partition\n          select: rank()\n          from: inputs/tbl_c\n          default: -1\n          window:\n            partition_by:\n              - \"{{tbl_c}}.num_b\"\n            order_by:\n              - \"{{tbl_c}}.num_c asc\"\n          where: \"{{tbl_c}}.num_b >= 10\"\n      - entity_var:\n          name: min_num_c_rank_num_b_partition\n          select: min(num_c_rank_num_b_partition)\n          from: inputs/tbl_c\n      - entity_var:\n          name: first_bill\n          select: min({{tbl_billing.Var(\"payment\")}})\n          from: inputs/tbl_billing\n          column_data_type: '{{warehouse.DataType(\"float\")}}'\n```\n\n##### Model specification fields\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `validity_time` | Time | Specifies the validity of the model with respect to its timestamp. For example, a model run as part of a scheduled nightly job for 2009-10-23 00:00:00 UTC with `validity_time`: `24h` would still be considered potentially valid and usable for any run requests, which do not require precise timestamps between 2009-10-23 00:00:00 UTC and 2009-10-24 00:00:00 UTC. This specifies the validity of generated feature table. Once the validity is expired, scheduling takes care of generating new tables. For example: `24h` for 24 hours, `30m` for 30 minutes, `3d` for 3 days, and so on. |\n| `entity_key` | String | Specifies the relevant entity from your `input.yaml` file. |\n| `features` | String | Specifies the list of `name` in `entity_var`, that must act as a feature. |\n\n**`entity_var`**\n\nThe `entity_var` field provides inputs for the feature table model. This variable stores the data temporarily, however, you can choose to store its data permanently by specifying the `name` in it as a feature in the `features` key.\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `name` | String | Name of the `entity_var` to identify it uniquely. |\n| `select` | String | Column name/value you want to select from the table. This defines the actual value that will be stored in the variable. You can use simple SQL expressions or select an `entity_var` as `{{entityName.Var(\\\"entity_var\\\")}}`. It has to be an aggregate operation that ensures the output is a unique value for a given `main_id`. For example: min(timestamp), count(\\*), sum(amount) etc. This holds true even when a window function (optional) is used. For example:: first\\_value(), last\\_value() etc are valid while rank(), row\\_number(), etc. are not valid and give unpredictable results. |\n| `from` | List | Reference to the source table from where data is to be fetched. You can either refer to another model from the same YAML or some other table specified in input YAML. |\n| `where` | String | Any filters you want to apply on the input table before selecting a value. This must be SQL compatible and should consider the data type of the table. |\n| `default` | String | Default value in case no data matches the filter. When defining default values, make sure you enclose the string values in single quotes followed by double quotes to avoid SQL failure. However, you can use the non-string values without any quotes. |\n| `description` | String | Textual description of the `entity_var`. |\n| `window` | Object | Specifies the window function. Window functions in SQL usually have both `partition_by` and `order_by` properties. But for `entity_var`, `partition_by` is added with `main_id` as default; so, adding `partition_by` manually is not supported. If you need partitioning on other columns too, check out `input_var` where `partition_by` on arbitrary and multiple columns is supported. |\n| `column_data_type` | String | (Optional) Data type for the `entity_var`. Supported data types are: `integer`, `variant`, `float`, `varchar`, `text`, and `timestamp`. |\n\n**`input_var`**\n\nThe syntax of `input_var` is similar to `entity_var`, with the only difference that instead of each value being associated to a row of the feature table, it’s associated with a row of the specified input. While you can think of an `entity_var` as adding a helper column to the feature table, you can consider an `input_var` as adding a helper column to the input.\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `name` | String | Name to store the retrieved data. |\n| `select` | String | Data to be stored in the name. |\n| `from` | List | Reference to the source table from where data is to be fetched. |\n| `where` | String | (Optional) Applies conditions for fetching data. |\n| `default` | String | (Optional) Default value for any entity for which the calculated value would otherwise be NULL. |\n| `description` | String | (Optional) Textual description. |\n| `column_data_type` | String | (Optional) Data type for the `input_var`. Supported data types are: `integer`, `variant`, `float`, `varchar`, `text`, and `timestamp`. |\n| `window` | Object | (Optional) Specifies a window over which the value should be calculated. |\n\n**`window`**\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `partition_by` | String | (Optional) List of SQL expressions to use in partitioning the data. |\n| `order_by` | String | (Optional) List of SQL expressions to use in ordering the data. |\n\nIn window option, `main_id` is not added by default, it can be any arbitrary list of columns from the input table. So if a feature should be partitioned by `main_id`, you must add it in the `partition_by` key.\n\n### Output\n\nAfter [running the project](https://www.rudderstack.com/docs/archive/profiles/0.12/get-started/profile-builder/#7-generate-output-tables), you can view the generated material tables.\n\n1.  Log in to your Snowflake console.\n2.  Click **Worksheets** from the top navigation bar.\n3.  In the left sidebar, click **Database** and the corresponding **Schema** to view the list of all tables. You can hover over a table to see the full table name along with its creation date and time.\n4.  Write a SQL query like `select * from <table_name>` and execute it to see the results:\n\n1.  Open [Postico2](https://eggerapps.at/postico2/). If required, create a new connection by entering the relevant details. Click **Test Connection** followed by **Connect**.\n2.  Click the **+** icon next to **Queries** in the left sidebar.\n3.  You can click **Database** and the corresponding schema to view the list of all tables/views.\n4.  Double click on the appropriate view name to paste the name on an empty worksheet.\n5.  You can prefix `SELECT *` from the view name pasted previously and suffix `LIMIT 10;` at the end.\n6.  Press Cmd+Enter keys, or click the **Run** button to execute the query.\n\n1.  Enter your Databricks workspace URL in the web browser and log in with your username and password.\n2.  Click the **Catalog** icon in left sidebar.\n3.  Choose the appropriate catalog from the list and click on it to view contents.\n4.  You will see list of tables/views. Click the appropriate table/view name to paste the name on worksheet.\n5.  You can prefix `SELECT * FROM` before the pasted view name and suffix `LIMIT 10;` at the end.\n6.  Select the query text. Press Cmd+Enter, or click the **Run** button to execute the query.\n\n1.  Log in to your [Google Cloud Console](https://console.cloud.google.com/)\n2.  Search for Bigquery in the search bar.\n3.  Select Bigquery from **Product and Pages** to open the Bigquery **Explorer**.\n4.  Select the correct project from top left drop down menu.\n5.  In the left sidebar, click the project ID, then the corresponding dataset view list of all the tables and views.\n6.  Write a SQL query like `select * from <table_name> limit 10;` and execute it to see the results.\n\nA sample output containing the results in Snowflake:\n\n![Generated table (Snowflake)](https://www.rudderstack.com/docs/images/profiles/profiles-feature-table.webp)\n\n## Window functions\n\nA window function operates on a window (group) of related rows. It performs calculation on a subset of table rows that are connected to the current row in some way. The window function has the ability to access more than just the current row in the query result.\n\nThe window function returns one output row for each input row. The values returned are calculated by using values from the sets of rows in that window. A window is defined using a window specification, and is based on three main concepts:\n\n*   Window partitioning, which forms the groups of rows (`PARTITION BY` clause)\n*   Window ordering, which defines an order or sequence of rows within each partition (`ORDER BY` clause)\n*   Window frames, which are defined relative to each row to further restrict the set of rows (`ROWS` specification). It is also known as the frame clause.\n\n**Snowflake** does not enforces users to define the cumulative or sliding frames, and considers `ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING` as the default cumulative window frame. However, you can override this by defining the frame manually.\n\nOn the **Redshift** aggregate window function list given below, specify the `frame_clause` while using any function from the list:\n\n*   `AVG`\n*   `COUNT`\n*   `CUME_DIST`\n*   `DENSE_RANK`\n*   `FIRST_VALUE`\n*   `LAG`\n*   `LAST_VALUE`\n*   `LEAD`\n*   `LISTAGG`\n*   `MAX`\n*   `MEDIAN`\n*   `MIN`\n*   `NTH_VALUE`\n*   `PERCENTILE_CONT`\n*   `PERCENTILE_DISC`\n*   `RATIO_TO_REPORT`\n*   `STDDEV_POP`\n*   `STDDEV_SAMP` (synonym for `STDDEV`)\n*   `SUM`\n*   `VAR_POP`\n*   `VAR_SAMP` (synonym for `VARIANCE`)\n\nOn the Redshift ranking window functions given below, **do not** specify the `frame_clause` while using any function from the list:\n\n*   `DENSE_RANK`\n*   `NTILE`\n*   `PERCENT_RANK`\n*   `RANK`\n*   `ROW_NUMBER`\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> When using a window function, use `frame_clause` carefully. While It is not very critical for Snowflake, using it incorrectly in Redshift can lead to errors.\n\nExample of using `frame_clause`:\n\n```\n- entity_var:\n    name: first_num_b_order_num_b\n    select: first_value(tbl_c.num_b) # Specify frame clause as aggregate window function is used\n    from: inputs/tbl_c\n    default: -1\n    where: tbl_c.num_b >= 10\n    window:\n        order_by:\n        - tbl_c.num_b desc\n        frame_clause: ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING\n- entity_var:\n    name: first_num_b_order_num_b_rank\n    select: rank() # DO NOT specify frame clause as ranking window function is used\n    window:\n        order_by:\n        - first_num_b_order_num_b asc\n```\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> Note how `frame_clause` is specified in first `entity_var` and not in the second one.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Feature Table | RudderStack Docs",
    "description": "Step-by-step tutorial on creating a feature table model.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.12/example/entity-traits-360/",
    "markdown": "# Feature Views | RudderStack Docs\n\nStep-by-step tutorial on creating an feature view models.\n\n* * *\n\n*     9 minute read  \n    \n\nOnce you have done [identity stitching](https://www.rudderstack.com/docs/archive/profiles/0.12/core-concepts/identity-stitching/) to unify the identity of your users across all the cross-platforms, you can evaluate and maintain the required features/traits for each identified user in a feature views.\n\nThis guide provides a detailed walkthrough on how to use a PB project and create output tables in a warehouse for an feature views model.\n\n## Prerequisites\n\n*   A basic Profile Builder project by following the [Profile Builder CLI](https://www.rudderstack.com/docs/archive/profiles/0.12/get-started/profile-builder/) steps.\n*   [Structure](https://www.rudderstack.com/docs/archive/profiles/0.12/cli-user-guide/structure/) of a Profile Builder project and the parameters used in different files.\n*   [Identity Stitching](https://www.rudderstack.com/docs/archive/profiles/0.12/example/id-stitcher/) model as Feature Views reuses its output to extract the required features/traits.\n\n## Feature Views Model\n\nOnce traits are defined on an entity, we need the means to use them.\n\nA primary application to send them to the downstream destinations. The destination could either be the [Activation API](https://www.rudderstack.com/docs/archive/profiles/0.12/activation-api/) or via any of the [rETL destinations](https://www.rudderstack.com/docs/sources/reverse-etl/) that RudderStack supports. Each such destination requires data in the form of a table with an ID column and 1 or more feature columns. This is possible using Feature Views.\n\nFeature Views provides a way to access entity features based on any of the given ID types, including the entity main ID as the identifier column. It creates a view which will have all or a specified set of features on that entity from across the project.\n\nTo configure the creation of a specific set of feature views model, add `feature_views` section in the entity. You need to provide a list of ID types as `id_served`. Optionally, you can also give a name which specifies the name of the generated model. If you don’t specify a name, it will create the model with a default name. By default, it will add all available features on the entity into the view.\n\nIf you want finer control, you can also include or exclude any features from any models by defining a custom feature views model and add the reference to the `feature_views` section like `model_served: models/name_of_custom_traits_model`. See below on more examples of this.\n\n### Default feature views model\n\n`pb_project.yaml`:\n\n```\n...\nentities:\n  - name: user\n    id_types:\n      - main_id\n      - user_id\n    feature_views:\n      using_ids:\n        - id: email\n          name: features_by_email\n        - id: salesforce_id\n          name: salesforce_id_stitched_features\n      features:\n        - from: models/cart_feature_table\n          include:\n            - \"*\"\n```\n\n**Custom feature views model:**\n\nThis is an example of custom feature views model. Here we include or exclude features from models of choice.\n\n`pb_project.yaml`:\n\n```\n...\nentities:\n  - name: user\n    id_types:\n      - main_id\n      - user_id\n    feature_views:\n      using_ids:\n        - id: email\n          name: features_by_email\n        - id: salesforce_id\n          name: salesforce_id_stitched_features\n      features:\n        - from: models/cart_feature_table\n          include:\n            - \"*\"\n```\n\n`models/profiles.yaml`:\n\n```\nmodels:\n  - name: cart_feature_views\n    model_type: feature_views\n    model_spec:\n      validity_time: 24h # 1 day\n      entity_key: user\n      id_served: user_id\n      feature_list:\n        - from: packages/pkg/models/cart_table # a table created by package\n          include: [\"*\"] # will include all the traits\n        - from: models/user_var_table\n          include: [\"*\"]\n          exclude: [cart_quantity, purchase_status] # except two, all the other traits will be included\n        - from: models/sql_model\n          include: [lifetime_value] # will include only one trait\n```\n\n## Sample project\n\nThis sample project uses the output of an identity stitching model as an input to create a feature views. The following sections describe how to define your PB project files:\n\n### Project detail\n\nThe [`pb_project.yaml`](https://www.rudderstack.com/docs/archive/profiles/0.12/cli-user-guide/structure/#project-details) file defines the project details such as name, schema version, connection name and the entities which represent different identifiers.\n\nYou can define all the identifiers from different input sources you want to stitch together as a `user_main_id`:\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> You need to add `main_id` to the list only if you have defined `main_id_type: main_id` in the ID stitcher buildspec.\n\n```\n# Project name\nname: sample_id_stitching\n# Project's yaml schema version\nschema_version: 61\n# Warehouse connection\nconnection: test\n# Allow inputs without timestamps\ninclude_untimed: true\n# Folder containing models\nmodel_folders:\n  - models\n# Entities in this project and their ids.\nentities:\n  - name: user\n    id_types:\n      - main_id # You need to add ``main_id`` to the list only if you have defined ``main_id_type: main_id`` in the id stitcher buildspec.\n      - user_id # one of the identifier from your data source.\n      - email\n# lib packages can be imported in project signifying that this project inherits its properties from there\npackages:\n  - name: corelib\n    url: \"https://github.com/rudderlabs/profiles-corelib/tag/schema_{{best_schema_version}}\"\n    # if required then you can extend the package definition such as for ID types.\n```\n\n### Input\n\nThe [input file](https://www.rudderstack.com/docs/archive/profiles/0.12/cli-user-guide/structure/#inputs) file includes the input table references and corresponding SQL for the above-mentioned entities:\n\n```\ninputs:\n- name: rsIdentifies\n  contract: # constraints that a model adheres to\n    is_optional: false\n    is_event_stream: true\n    with_entity_ids:\n      - user\n    with_columns:\n      - name: timestamp\n      - name: user_id\n      - name: anonymous_id\n      - name: email\n  app_defaults:\n    table: rudder_events_production.web.identifies # one of the WH table RudderStack generates when processing identify or track events.\n    occurred_at_col: timestamp\n    ids:\n      - select: \"user_id\" # kind of identity sql to pick this column from above table.\n        type: user_id\n        entity: user # as defined in project file\n        to_default_stitcher: true\n      - select: \"anonymous_id\"\n        type: anonymous_id\n        entity: user\n        to_default_stitcher: true\n      - select: \"lower(email)\" # can use sql.\n        type: email\n        entity: user\n        to_default_stitcher: true\n- name: rsTracks\n  contract:\n    is_optional: false\n    is_event_stream: true\n    with_entity_ids:\n      - user\n    with_columns:\n      - name: timestamp\n      - name: user_id\n      - name: anonymous_id\n  app_defaults:\n    table: rudder_events_production.web.tracks # another table in WH maintained by RudderStack processing track events.\n    occurred_at_col: timestamp\n    ids:\n      - select: \"user_id\"\n        type: user_id\n        entity: user\n        to_default_stitcher: true\n      - select: \"anonymous_id\"\n        type: anonymous_id\n        entity: user\n        to_default_stitcher: true\n```\n\n### Model\n\nProfiles **feature views** model lets you define the specific features/traits you want to evaluate from the huge spread of scattered data in your warehouse tables.\n\nA sample `profiles.yaml` file specifying a feature views model (`user_profile`):\n\n```\nvar_groups:\n  - name: first_group\n    entity_key: user\n    vars:\n      - entity_var:\n          name: first_seen\n          select: min(timestamp::date)\n          from: inputs/rsTracks\n          where: properties_country is not null and properties_country != ''\n      - entity_var:\n          name: last_seen\n          select: max(timestamp::date)\n          from: inputs/rsTracks\n      - entity_var:\n          name: user_lifespan\n          select: '{{user.Var(\"last_seen\")}} - {{user.Var(\"first_seen\")}}'\n          description: Life Time Value of a customer\n      - entity_var:\n          name: days_active\n          select: count(distinct timestamp::date)\n          from: inputs/rsTracks\n          description: No. of days a customer was active\n      - entity_var:\n          name: campaign_source\n          default: \"'organic'\"\n      - entity_var:\n          name: user_rank\n          default: -1\n      - entity_var:\n          name: campaign_source_first_touch\n          select: first_value(context_campaign_source)\n          window:\n              order_by:\n                  - timestamp asc\n              partition_by:\n                  - main_id\n          from: inputs/rsIdentifies\n          where: context_campaign_source is not null and context_campaign_source != ''\n      - input_var:\n          name: num_c_rank_num_b_partition\n          select: rank()\n          from: inputs/tbl_c\n          default: -1\n          window:\n            partition_by:\n              - '{{tbl_c}}.num_b'\n            order_by:\n              - '{{tbl_c}}.num_c asc'\n          where: '{{tbl_c}}.num_b >= 10'\n      - entity_var:\n          name: min_num_c_rank_num_b_partition\n          select: min(num_c_rank_num_b_partition)\n          from: inputs/tbl_c\n```\n\n**`entity_var`**\n\nThe `entity_var` field provides inputs for the feature views model. This variable stores the data temporarily, however, you can choose to store its data permanently by specifying the `name` in it as a feature in the `features` key.\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `name` | String | Name of the `entity_var` to identify it uniquely. |\n| `select` | String | Column name/value you want to select from the table. This defines the actual value that will be stored in the variable. You can use simple SQL expressions or select an `entity_var` as `{{entityName.Var(\\\"entity_var\\\")}}`. It has to be an aggregate operation that ensures the output is a unique value for a given `main_id`. For example: min(timestamp), count(\\*), sum(amount) etc. This holds true even when a window function (optional) is used. For example:: first\\_value(), last\\_value() etc are valid while rank(), row\\_number(), etc. are not valid and give unpredictable results. |\n| `from` | List | Reference to the source table from where data is to be fetched. You can either refer to another model from the same YAML or some other table specified in input YAML. |\n| `where` | String | Any filters you want to apply on the input table before selecting a value. This must be SQL compatible and should consider the data type of the table. |\n| `default` | String | Default value in case no data matches the filter. When defining default values, make sure you enclose the string values in single quotes followed by double quotes to avoid SQL failure. However, you can use the non-string values without any quotes. |\n| `description` | String | Textual description of the `entity_var`. |\n| `window` | Object | Specifies the window function. Window functions in SQL usually have both `partition_by` and `order_by` properties. But for `entity_var`, `partition_by` is added with `main_id` as default; so, adding `partition_by` manually is not supported. If you need partitioning on other columns too, check out `input_var` where `partition_by` on arbitrary and multiple columns is supported. |\n\n**`input_var`**\n\nThe syntax of `input_var` is similar to `entity_var`, with the only difference that instead of each value being associated to a row of the feature views, it’s associated with a row of the specified input. While you can think of an `entity_var` as adding a helper column to the feature views, you can consider an `input_var` as adding a helper column to the input.\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `name` | String | Name to store the retrieved data. |\n| `select` | String | Data to be stored in the name. |\n| `from` | List | Reference to the source table from where data is to be fetched. |\n| `where` | String | (Optional) Applies conditions for fetching data. |\n| `default` | String | (Optional) Default value for any entity for which the calculated value would otherwise be NULL. |\n| `description` | String | (Optional) Textual description. |\n| `window` | Object | (Optional) Specifies a window over which the value should be calculated. |\n\n**`window`**\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `partition_by` | String | (Optional) List of SQL expressions to use in partitioning the data. |\n| `order_by` | String | (Optional) List of SQL expressions to use in ordering the data. |\n\nIn window option, `main_id` is not added by default, it can be any arbitrary list of columns from the input table. So if a feature should be partitioned by `main_id`, you must add it in the `partition_by` key.\n\n### Output\n\nAfter [running the project](https://www.rudderstack.com/docs/archive/profiles/0.12/get-started/profile-builder/#7-generate-output-tables), you can view the generated material tables.\n\n1.  Log in to your Snowflake console.\n2.  Click **Worksheets** from the top navigation bar.\n3.  In the left sidebar, click **Database** and the corresponding **Schema** to view the list of all tables. You can hover over a table to see the full table name along with its creation date and time.\n4.  Write a SQL query like `select * from <table_name>` and execute it to see the results:\n\n1.  Open [Postico2](https://eggerapps.at/postico2/). If required, create a new connection by entering the relevant details. Click **Test Connection** followed by **Connect**.\n2.  Click the **+** icon next to **Queries** in the left sidebar.\n3.  You can click **Database** and the corresponding schema to view the list of all tables/views.\n4.  Double click on the appropriate view name to paste the name on an empty worksheet.\n5.  You can prefix `SELECT *` from the view name pasted previously and suffix `LIMIT 10;` at the end.\n6.  Press Cmd+Enter keys, or click the **Run** button to execute the query.\n\n1.  Enter your Databricks workspace URL in the web browser and log in with your username and password.\n2.  Click the **Catalog** icon in left sidebar.\n3.  Choose the appropriate catalog from the list and click on it to view contents.\n4.  You will see list of tables/views. Click the appropriate table/view name to paste the name on worksheet.\n5.  You can prefix `SELECT * FROM` before the pasted view name and suffix `LIMIT 10;` at the end.\n6.  Select the query text. Press Cmd+Enter, or click the **Run** button to execute the query.\n\n1.  Log in to your [Google Cloud Console](https://console.cloud.google.com/)\n2.  Search for Bigquery in the search bar.\n3.  Select Bigquery from **Product and Pages** to open the Bigquery **Explorer**.\n4.  Select the correct project from top left drop down menu.\n5.  In the left sidebar, click the project ID, then the corresponding dataset view list of all the tables and views.\n6.  Write a SQL query like `select * from <table_name> limit 10;` and execute it to see the results.\n\nA sample output containing the results in Snowflake:\n\n![Generated table (Snowflake)](https://www.rudderstack.com/docs/images/profiles/profiles-feature-table.webp)\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Feature Views | RudderStack Docs",
    "description": "Step-by-step tutorial on creating an feature view models.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.12/example/sql-model/",
    "markdown": "# SQL Models | RudderStack Docs\n\nStep-by-step tutorial on how to create a SQL Template model.\n\n* * *\n\n*     5 minute read  \n    \n\nThis guide provides a detailed walkthrough on how to use a PB project and create SQL Template models using custom SQL queries.\n\n## Prerequisites\n\n*   Familiarize yourself with:\n    \n    *   A basic Profile Builder project by following the [Profile Builder CLI](https://www.rudderstack.com/docs/archive/profiles/0.12/get-started/profile-builder/) steps.\n    *   [Structure](https://www.rudderstack.com/docs/archive/profiles/0.12/cli-user-guide/structure/) of a Profile Builder project and the parameters used in different files.\n\n## Sample project\n\nThe following sections describe how to define your PB project files:\n\n### Project detail\n\nThe [`pb_project.yaml`](https://www.rudderstack.com/docs/archive/profiles/0.12/cli-user-guide/structure/#project-details) file defines the project details such as name, schema version, connection name and the entities which represent different identifiers.\n\nYou can define all the identifiers from different input sources you want to stitch together as a single ID (`main_id` in this example):\n\n```\nname: sample_test\nschema_version: 61\nconnection: test\nmodel_folders:\n  - models\nentities:\n  - name: user\n    id_stitcher: models/test_id__\n    id_types:\n      - test_id\n      - exclude_id\ninclude_untimed: true\nid_types:\n  - name: test_id\n    filters:\n      - type: include\n        regex: \"([0-9a-z])*\"\n      - type: exclude\n        value: \"\"\n  - name: exclude_id\n```\n\n### Input\n\nThe [input file](https://www.rudderstack.com/docs/archive/profiles/0.12/cli-user-guide/structure/#inputs) (`models/inputs.yaml`) file includes the input table references and corresponding SQL for the above-mentioned entities:\n\n```\ninputs:\n  - name: tbl_a\n    app_defaults:\n      table: Temp_tbl_a\n    occurred_at_col: insert_ts\n    ids:\n      - select: TRIM(COALESCE(NULL, id1))\n        type: test_id\n        entity: user\n        to_default_stitcher: true\n      - select: \"id2\"\n        type: test_id\n        entity: user\n        to_default_stitcher: true\n      - select: \"id3\"\n        type: exclude_id\n        entity: user\n        to_default_stitcher: true\n  - name: tbl_b\n    app_defaults:\n      view: Temp_view_b\n    occurred_at_col: timestamp\n    ids:\n      - select: \"id1\"\n        type: test_id\n        entity: user\n        to_default_stitcher: true\n      - select: \"id2\"\n        type: test_id\n        entity: user\n        to_default_stitcher: true\n      - select: \"id3\"\n        type: test_id\n        entity: user\n        to_default_stitcher: true\n  - name: tbl_c\n    app_defaults:\n      table: Temp_tbl_c\n    ids:\n      - select: \"id1\"\n        type: test_id\n        entity: user\n        to_default_stitcher: true\n      - select: \"id2\"\n        type: test_id\n        entity: user\n        to_default_stitcher: true\n```\n\n### Model\n\nProfiles **SQL model** lets you write custom SQL queries to achieve advanced use-cases to create desired output tables.\n\nA sample `profiles.yaml` file specifying a SQL model (`test_sql`):\n\n```\nmodels:\n- name: test_sql\n  model_type: sql_template\n  model_spec:\n    validity_time: 24h# 1 day\n    materialization:                 // optional\n      run_type: discrete             // optional [discrete, incremental]\n    single_sql: |\n        {%- with input1 = this.DeRef(\"inputs/tbl_a\") -%}\n          select id1 as new_id1, id2 as new_id2, {{input1}}.*\n            from {{input1}}\n        {%- endwith -%}        \n    occurred_at_col: insert_ts        // optional\n    ids:\n      - select: \"new_id1\"\n        type: test_id\n        entity: user\n        to_default_stitcher: true\n      - select: \"new_id2\"\n        type: test_id\n        entity: user\n        to_default_stitcher: true\n      - select: \"id3\"\n        type: test_id\n        entity: user\n        to_default_stitcher: true\n```\n\n##### Model specification fields\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `validity_time` | Time | Time Specifies the validity of the model with respect to its timestamp. For example, a model run as part of a scheduled nightly job for 2009-10-23 00:00:00 UTC with `validity_time`: `24h` would still be considered potentially valid and usable for any run requests, which do not require precise timestamps between 2009-10-23 00:00:00 UTC and 2009-10-24 00:00:00 UTC. This specifies the validity of generated feature table. Once the validity is expired, scheduling takes care of generating new tables. For example: 24h for 24 hours, 30m for 30 minutes, 3d for 3 days. |\n| `materialization` | List | Adds the key `run_type`: `incremental` to run the project in incremental mode. This mode considers row inserts and updates from the edge\\_sources input. These are inferred by checking the timestamp column for the next run. One can provide buffer time to consider any lag in data in the warehouse for the next incremental run like if new rows are added during the time of its run. If you do not specify this key then it’ll default to `run_type`: `discrete`. |\n| `single_sql` | List | Specifies the SQL template which must evaluate to a single SELECT SQL statement. After execution, it should produce a dataset which will materialize based on the provided materialization. |\n| `multi-sql` | List | Specifies the SQL template which can evaluate to multiple SQL statements. One of these SQL statements (typically the last one) must be a CREATE statement which shall be responsible for materializing the model into a table.<br><br>**Note**: You should set only one of `single_sql` or `multi_sql`. |\n| `occurred_at_col` | List | Name of the column which contains the timestamp value in the output of sql template. |\n| `ids` | List | Specifies the list of all IDs present in the source table along with their column names (or column SQL expressions). It is required in case you want to use SQL models as an input to the `input_var` or `entity_var` fields. |\n\n## SQL template\n\nYou can pass custom SQL queries to the `single_sql` or `multi_sql` fields, which is also known as a **SQL template**. It provides the flexibility to write custom SQL by refering to any of the input sources listed in the `inputs.yaml` or any model listed in `models/profiles.yaml`.\n\nThe SQL templates follow a set query syntax which serves the purpose of creating a model. Follow the below rules to write SQL templates:\n\n*   Write SQL templates in the [pongo2 template engine](https://pkg.go.dev/github.com/flosch/pongo2#readme-first-impression-of-a-template) syntax.\n*   Avoid circular referencing while referencing the models. For example, `sql_model_a` references `sql_model_b` and `sql_model_b` references `sql_model_a`.\n*   Use `timestamp` variable (refers to the start time of the current run) to filter new events.\n*   `this` refers to the current model’s material. You can use the following methods to access the material properties available for `this`:\n    *   `DeRef(\"path/to/model\")`: Use this syntax `{{ this.DeRef(\"path/to/model\") }}` to refer to any model and return a database object corresponding to that model. The database object, in return, gives the actual name of the table/view in the warehouse. Then, generate the output, for example:\n\n```\n{% with input_table = this.DeRef(\"inputs/tbl_a\") %}\n    SELECT\n        t.a AS new_a,\n        t.b AS new_b,\n        t.*\n    FROM {{input_table}} AS t\n{% endwith %}\n```\n\n*   `GetMaterialization()`: Returns a structure with two fields: `MaterializationSpec{OutputType, RunType}`.\n    *   `OutputType`: You must use `OutputType` with `ToSQL()` method:  \n        For example, `CREATE OR REPLACE {{this.GetMaterialization().OutputType.ToSQL()}} {{this.GetSelectTargetSQL()}} AS ...`\n    *   `RunType`: For example, `this.GetMaterialization().RunType`\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "SQL Models | RudderStack Docs",
    "description": "Step-by-step tutorial on how to create a SQL Template model.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.12/example/predictive-features-snowflake/",
    "markdown": "# Predictive features | RudderStack Docs\n\nCreate predictive features with RudderStack and Snowflake without using MLOps\n\n* * *\n\n*     2 minute read  \n    \n\nPredictive features like future LTV and churn propensity can be game changing for a business. If your marketing, customer success, and other teams want to use them, though, your company often faces a binary choice: use a one-size-fits-all solution within an existing SaaS platform (i.e., marketing automation tool), or build out ML and MLOps capabilities internally.\n\nBoth options have significant drawbacks. First, templated SaaS-based solutions can’t leverage all of your customer data and aren’t configurable, which results in low accuracy and impact. On the other hand, hiring data scientists and setting up MLOps is expensive and complex.\n\nModern data teams need an option in the middle: the ability to deploy model templates on all of their customer data, but without additional tooling, processes and headcount.\n\nWith RudderStack Predictions and Snowflake, you can create predictive features directly in your warehouse, without the need to set up MLOps processes and infrastructure. Predictions leverages the full power of Snowpark to run ML models within your existing data engineering workflow.\n\nIn this section, you will learn about two ways to build predictive features in RudderStack Predictions:\n\n1.  [Set up automated features in the RudderStack UI](https://www.rudderstack.com/docs/archive/profiles/0.12/example/predictive-features-snowflake/setup-automated-features/) - You can setup and run the jobs within the RudderStack UI. This process makes it easy for less technical users to implement basic predictive features.\n2.  [Code your own custom predictions](https://www.rudderstack.com/docs/archive/profiles/0.12/example/predictive-features-snowflake/custom-code/) - Predictions also supports a code-based approach that gives technical users full control to define custom predictive features that match their unique business logic.\n\nIt’s important to note that Predictions runs on top of RudderStack [Profiles](https://www.rudderstack.com/docs/profiles/overview/), a product that automates identity resolution and user feature development in Snowflake.\n\nPredictions leverages the Profiles identity graph to train and run ML models. Because Predictions is part of Profiles, project outputs include an identity graph, standard user featuers (i.e., `last_seen`) and predictive user features (i.e., `percentile_churn_score_30_days`). Both types of features are built using RudderStack data sources and standardized feature definitions.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Predictive features | RudderStack Docs",
    "description": "Create predictive features with RudderStack and Snowflake without using MLOps",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.11/developer-guides/permissions/",
    "markdown": "# Warehouse Permissions | RudderStack Docs\n\nGrant RudderStack the required permissions on your data warehouse.\n\n* * *\n\n*     5 minute read  \n    \n\nRudderStack supports **Snowflake**, **Redshift**, **Databricks**, and **BigQuery** for creating unified user profiles.\n\nTo read and write data to the warehouse, RudderStack requires specific warehouse permissions as explained in the following sections.\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> Keeping separate schemas for projects running via CLI and web is recommended. This way projects run from the CLI will never risk overwriting your production data.\n\n## Snowflake\n\nSnowflake uses a combination of DAC and RBAC models for [access control](https://docs.snowflake.com/en/user-guide/security-access-control-overview.html). However, RudderStack chooses an RBAC-based access control mechanism as multiple users can launch the [Profile Builder CLI](https://www.rudderstack.com/docs/archive/profiles/0.11/get-started/profile-builder/).\n\nAlso, it is not ideal to tie the result of an individual user run with that user. Hence, it is recommended to create a generic role (for example, `PROFILES_ROLE`) with the following privileges:\n\n*   Read access to all the inputs to the model (can be shared in case of multiple schemas/tables).\n*   Write access to the schemas and common tables as the PB project creates material (output) tables.\n\nIf you want to access any material created from the project run, the role (`PROFILES_ROLE`) must also have read access to all of those schemas.\n\nBelow are some sample commands which grant the required privileges to the role (`PROFILES_ROLE`) in a Snowflake warehouse:\n\n```\n-- Create role\nCREATE ROLE PROFILES_ROLE;\nSHOW ROLES; -- To validate\n```\n\n```\n-- Create user\nCREATE USER PROFILES_TEST_USER PASSWORD='<StrongPassword>' DEFAULT_ROLE='PROFILES_ROLE';\nSHOW USERS; -- To validate\n```\n\n```\n-- Grant role to user and database\nGRANT ROLE PROFILES_ROLE TO USER PROFILES_TEST_USER;\nGRANT USAGE ON DATABASE YOUR_RUDDERSTACK_DB TO ROLE PROFILES_ROLE;\n```\n\n```\n-- Create separate schema for Profiles and grant privileges to role\nCREATE SCHEMA YOUR_RUDDERSTACK_DB.RS_PROFILES;\nGRANT ALL PRIVILEGES ON SCHEMA YOUR_RUDDERSTACK_DB.RS_PROFILES TO ROLE PROFILES_ROLE;\nGRANT USAGE ON WAREHOUSE RUDDER_WAREHOUSE TO ROLE PROFILES_ROLE;\nGRANT USAGE ON SCHEMA YOUR_RUDDERSTACK_DB.EVENTSSCHEMA TO ROLE PROFILES_ROLE;\n```\n\nFor accessing input sources, you can individually grant select on tables/views, or give blanket grant to all in a schema.\n\n```\n-- Assuming we want read access to tables/views in schema EVENTSSCHEMA\nGRANT SELECT ON ALL TABLES IN SCHEMA YOUR_RUDDERSTACK_DB.RS_PROFILES TO PROFILES_ROLE;\nGRANT SELECT ON FUTURE TABLES IN SCHEMA YOUR_RUDDERSTACK_DB.RS_PROFILES TO PROFILES_ROLE;\nGRANT SELECT ON ALL VIEWS IN SCHEMA YOUR_RUDDERSTACK_DB.RS_PROFILES TO PROFILES_ROLE;\nGRANT SELECT ON FUTURE VIEWS IN SCHEMA YOUR_RUDDERSTACK_DB.RS_PROFILES TO PROFILES_ROLE;\n```\n\n```\n-- Assuming we want read access to tracks and identifies tables in schema EVENTSSCHEMA\nGRANT SELECT ON TABLE YOUR_RUDDERSTACK_DB.RS_PROFILES.TRACKS TO PROFILES_ROLE;\nGRANT SELECT ON TABLE YOUR_RUDDERSTACK_DB.RS_PROFILES.IDENTIFIES TO PROFILES_ROLE;\n```\n\n## Redshift\n\nWhen working with Redshift, the required privileges are different compared to Snowflake.\n\nSuppose the inputs/edge sources are in a single schema `website_eventstream` and the name of the newly created PB user is `rudderstack_admin`. In this case, the requirements are as follows:\n\n*   A separate schema `rs_profiles` (to store all the common and output tables).\n*   The `rudderstack_admin` user should have all the privileges on the above schema and the associated tables.\n*   The `rudderstack_admin` user should have `USAGE` privilege on schemas that have the edge sources and input tables (`website_eventstream`) and read (`SELECT`) privileges on specific tables as well. This privilege can extend to the migration schema and other schemas from where data from warehouses comes in.\n*   The `rudderstack_admin` user should have privileges to use `plpythonu` to create some UDFs.\n\nThe sample commands are as follows:\n\n```\nCREATE USER rudderstack_admin WITH PASSWORD '<strong_unique_password>';\nCREATE SCHEMA rs_profiles;\nGRANT ALL ON SCHEMA \"rs_profiles\" TO rudderstack_admin;\nGRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA \"rs_profiles\" TO rudderstack_admin;\nGRANT USAGE ON SCHEMA \"website_eventstream\" TO rudderstack_admin;\nGRANT USAGE ON LANGUAGE plpythonu TO rudderstack_admin;\n```\n\n```\nGRANT SELECT ON ALL TABLES IN SCHEMA \"website_eventstream\" TO rudderstack_admin;\n```\n\nTo give access to only specific input tables/views referred in your Profiles project, use the below command:\n\n```\nGRANT SELECT ON TABLE \"<YOUR_SCHEMA>\".\"<YOUR_TABLE>\" TO rudderstack_admin;\n```\n\n## Databricks\n\nThe steps are as follows:\n\n1.  Open the Databricks UI.\n2.  Create a new user.\n3.  Reuse an existing catalog or create a new one by clicking **Create Catalog**.\n4.  Grant `USE SCHEMA` privilege on the catalog.\n5.  Create a separate schema to write objects created by RudderStack Profiles.\n6.  Grant all privileges on this schema.\n7.  Grant privileges to access relevant schemas for the input tables. For example, if an input schema is in a schema named `website_eventstream`, then you can run the following commands to assign a blanket grant to all schemas or only specific tables/views referred in your Profiles project:\n\n```\nCREATE USER rudderstack_admin WITH PASSWORD <strong_unique_password>;\nGRANT USE SCHEMA ON CATALOG <catalog name> TO rudderstack_admin;\nCREATE SCHEMA RS_PROFILES;\nGRANT ALL PRIVILEGES ON SCHEMA RS_PROFILES TO rudderstack_admin;\nGRANT SELECT ON SCHEMA website_eventstream TO rudderstack_admin;\n```\n\n```\nGRANT SELECT ON ALL TABLES IN SCHEMA \"website_eventstream\" TO rudderstack_admin;\n```\n\nTo give access to only specific input tables/views referred in your Profiles project, use the below command:\n\n```\nGRANT SELECT ON TABLE public.input_table TO rudderstack_admin; \n```\n\n## BigQuery\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> For BigQuery, RudderStack recommends you to use a view instead of table for streaming data sets.\n\n1.  Open the [BigQuery UI](https://console.cloud.google.com/) (Google Cloud Console) and select your project (for example, `rudderstack`).\n2.  Click **Query Editor** from the left sidebar.\n3.  Execute the following command to create a schema for a dataset (for example, `prod_dataset`):\n\n```\nCREATE SCHEMA rudderstack.prod_dataset.rs_profiles;\n```\n\n4.  Grant read access to all your input source tables:\n\n```\nGRANT SELECT ON ALL TABLES IN SCHEMA rudderstack.prod_dataset.rs_profiles;\n```\n\n5.  Grant permission to your user (for example, `rudder_user`) for creating tables/views in the schema:\n\n```\nGRANT CREATE ON SCHEMA rudderstack.prod_dataset.rs_profiles TO rudder_user;\n```\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Warehouse Permissions | RudderStack Docs",
    "description": "Grant RudderStack the required permissions on your data warehouse.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.11/developer-guides/site-configuration/",
    "markdown": "# Site Configuration | RudderStack Docs\n\nPrivacy Overview\n\nThis site uses cookies to improve your experience while you navigate through the website. Out of these cookies, the cookies that are categorized as necessary are stored on your browser as they are as essential for the working of basic functionalities of the website. We also use third-party cookies that help us analyze and understand how you use this website. These cookies will be stored in your browser only with your consent. You also have the option to opt-out of these cookies. But opting out of some of these cookies may have an effect on your browsing experience.\n\nNecessary cookies are absolutely essential for the website to function properly. This category only includes cookies that ensures basic functionalities and security features of the website. These cookies do not store any personal information.",
    "title": "Site Configuration | RudderStack Docs",
    "description": "Know the detailed specifications mentioned in a site configuration file.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.12/example/predictive-features-snowflake/prerequisite/",
    "markdown": "# Prerequisites | RudderStack Docs\n\nPrerequisites to generate predictive features in Snowflake using RudderStack Predictions\n\n* * *\n\n*     8 minute read  \n    \n\nTo follow this guide, you will need access to both RudderStack and Snowflake. If you do not have access, follow these links to create a free [RudderStack account](https://app.rudderstack.com/signup?type=freetrial) and [Snowflake account](https://signup.snowflake.com/).\n\nOnce you set up your RudderStack account, [reach out to our support team](mailto:support@rudderstack.com?subject=I%20would%20like%20access%20to%20your%20Predictions%20feature) to request access to our Predictions feature.\n\n## Set up Snowflake for Event Stream data\n\nBecause Predictions is designed to run in a production environment, you need to perform some basic set up in Snowflake (and later, your RudderStack workspace) to simulate the pipelines you would run when collecting user event data.\n\n### Create a new role and user in Snowflake\n\nIn your Snowflake console, run the following commands to create the role `QUICKSTART`.\n\nVerify the role `QUICKSTART` was successfully created.\n\nCreate a new user QUICKSTART\\_USER with a password `<strong_unique_password>`.\n\n```\nCREATE USER QUICKSTART_USER PASSWORD = '<strong_unique_password>' DEFAULT_ROLE = 'QUICKSTART';\n```\n\nVerify the user `QUICKSTART_USER` was successfully created.\n\n### Create RudderStack schema and grant permissions to role\n\nCreate a dedicated schema `_RUDDERSTACK` in your database.\n\n**Replace `<YOUR_DATABASE>` in all queries with your actual database name.**\n\n```\nCREATE SCHEMA \"<YOUR_DATABASE>\".\"_RUDDERSTACK\";\n```\n\nGrant full access to the schema \\_RUDDERSTACK for the previously created role `QUICKSTART`.\n\n```\nGRANT ALL PRIVILEGES ON SCHEMA \"<YOUR_DATABASE>\".\"_RUDDERSTACK\" TO ROLE QUICKSTART;\n```\n\n### Grant permissions on the warehouse, database, schema, and table\n\nEnable the user `QUICKSTART_USER` to perform all operations allowed for the role `QUICKSTART` (via the privileges granted to it).\n\n```\nGRANT ROLE QUICKSTART TO USER QUICKSTART_USER;\n```\n\nRun the following commands to allow the role `QUICKSTART` to look up the objects within your warehouse, database, schema, and the specific table or view:\n\n```\nGRANT USAGE ON WAREHOUSE \"<YOUR_WAREHOUSE>\" TO ROLE QUICKSTART;\nGRANT USAGE ON DATABASE \"<YOUR_DATABASE>\" TO ROLE QUICKSTART;\nGRANT USAGE ON SCHEMA \"<YOUR_DATABASE>\".\"_RUDDERSTACK\" TO ROLE QUICKSTART;\nGRANT SELECT ON ALL TABLES IN SCHEMA \"<YOUR_DATABASE>\".\"_RUDDERSTACK\" TO ROLE  QUICKSTART;\nGRANT SELECT ON FUTURE TABLES IN SCHEMA \"<YOUR_DATABASE>\".\"_RUDDERSTACK\" TO ROLE QUICKSTART;\nGRANT SELECT ON ALL VIEWS IN SCHEMA \"<YOUR_DATABASE>\".\"_RUDDERSTACK\" TO ROLE QUICKSTART;\nGRANT SELECT ON FUTURE VIEWS IN SCHEMA \"<YOUR_DATABASE>\".\"_RUDDERSTACK\" TO ROLE QUICKSTART;\n```\n\n**Replace `<YOUR_DATABASE>` with the exact Snowflake database name.**\n\n## Import RudderStack event data from the Snowflake marketplace\n\nTo set up automated features, you will need the RudderStack event data in your Snowflake warehouse. If you already use RudderStack and have the following tables and fields (see below), skip to the [Profiles Schema and Permissions](#profiles-schema-and-permissions) section. For this guide, using the provided sample data is recommended.\n\n*   `TRACKS`\n*   `IDENTIFIES`\n    *   `user_id`\n    *   `anonymous_id`\n    *   `email`\n*   `PAGES`\n*   `ORDER_COMPLETED`\n\n**NOTE:** You must have all the three identity types in your `INDENTIFIES` table. If you are using your own data and don’t normally track email, you can send the following `identify` call to add the column:\n\n```\nrudderanalytics.identify('userId', {\n    email:'email@address.com',\n    name:'name'\n})\n```\n\n### Get sample data\n\nIf you are setting up RudderStack for the first time go to the [Snowflake Marketplace](https://app.snowflake.com/marketplace/listing/GZT0Z856CMJ/rudderstack-inc-rudderstack-event-data-for-quickstart) and add RudderStack Event Data for Quickstart to your Snowflake account for free. This will add a database with the needed tables to your Snowflake warehouse with no additional storage cost for you.\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/Snowflake-marketplace.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/Snowflake-marketplace.webp)\n\nAt the next screen, open **Options** and add role `QUICKSTART` to have access to this database.\n\n### Create schema for sample data\n\nThe database with the sample data is read-only so you will need to copy it to a new schema to be able to create a valid event stream pipeline (and run a Predictions job on the data).\n\nCreate a new schema in the database you already set up. Name the schema “EVENTS”.\n\n```\nCREATE SCHEMA \"<YOUR_DATABASE>\".\"EVENTS\";\n```\n\nGive permission to the `QUICKSTART` role to create new tables in the above schema.\n\n```\nGRANT ALL PRIVILEGES ON SCHEMA \"<YOUR_DATABASE>\".\"EVENTS\" FOR ROLE QUICKSTART;\n```\n\nCopy the sample data into the newly create schema.\n\n```\nCREATE TABLE \"<YOUR_DATABASE>\".\"EVENTS\".\"TRACKS\" AS SELECT * FROM \"SNOWFLAKE_QUICKSTART\".\"PUBLIC\".\"TRACKS\";\nCREATE TABLE \"<YOUR_DATABASE>\".\"EVENTS\".\"IDENTIFIES\" AS SELECT * FROM \"SNOWFLAKE_QUICKSTART\".\"PUBLIC\".\"IDENTIFIES\";\nCREATE TABLE \"<YOUR_DATABASE>\".\"EVENTS\".\"PAGES\" AS SELECT * FROM \"SNOWFLAKE_QUICKSTART\".\"PUBLIC\".\"PAGES\";\nCREATE TABLE \"<YOUR_DATABASE>\".\"EVENTS\".\"ORDER_COMPLETED\" AS SELECT * FROM \"SNOWFLAKE_QUICKSTART\".\"PUBLIC\".\"ORDER_COMPLETED\";\n```\n\nNow you are ready to create a pipeline connection in RudderStack.\n\n## Create JavaScript source\n\nRudderStack’s Profiles and Predictions products require a warehouse destination with an active sync from a source (a data pipeline). Therefore we will create a JavaScript source that can send a test event to Snowflake.\n\nAfter logging into RudderStack, navigate to the **Directory** from the sidebar on the left, then select the JavaScript source from the list of sources.\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/select-js-source.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/select-js-source.webp)\n\nEnter “QuickStart Test Site” for the source name and click `Continue`. You have successfully added a source!\n\nNote at the bottom of the JavaScript Source page is a `Write Key`. You will need this for sending a test event after connecting the Snowflake destination.\n\n## Create Snowflake destination\n\nNavigate to the **Overview** tab in the JavaScript source view and click on **Add Destination**.\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/add-destination.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/add-destination.webp)\n\nSelect the Snowflake destination from the list, then on the next page give it the name “Snowflake QuickStart” and click **Continue**.\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/select-snowflake.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/select-snowflake.webp)\n\nAdd in your Snowflake connection credentials:\n\n*   **Account**: Your account name.\n*   **Database**: Your database name that you used in the previous steps for `QUICKSTART`.\n*   **Warehouse**: Your warehouse that you granted usage to `QUICKSTART`.\n*   **User**: `QUICKSTART_USER`\n*   **Role**: `QUICKSTART`\n*   **Password**: Password for `QUICKSTART_USER`.\n*   **Namespace**: `EVENTS`\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/snowflake-config.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/snowflake-config.webp)\n\nAt the bottom under **Object Storage Configuration** toggle **Use RudderStack managed object storage** ON.\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/object-storage-toggle.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/object-storage-toggle.webp)\n\nLeave the defaults for all other settings and click **Continue**. RudderStack will verify credentials and that it has the needed permissions.\n\nYou have now created a pipeline connection in RudderStack!\n\n## Send test event\n\nYou can use a test site to send a `connection_setup` event. This will not effect the sample data tables. But first, get the following configuration data from RudderStack:\n\n*   RudderStack Data Plane URL\n*   JavaScript Source Write Key\n\n### Data Plane URL\n\nGo to the **Connections** page in the RudderStack app and copy the **Data Plane** URL from the top of the page.\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/data-plane-url.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/data-plane-url.webp)\n\n### Write key\n\nGo to your JavaScript source in RudderStack and in the **Setup** tab scroll down and copy the **Write key**.\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/write-key.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/write-key.webp)\n\n### Test event\n\nGo to RudderStack’s [test website](https://ryanmccrary.github.io/rudderstackdemo/) and copy your Data Plane URL and Write Key into the top fields and press **Submit**.\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/test-site-setup.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/test-site-setup.webp)\n\nEnter `connection_setup` into the `event_name` field next to **Send Custom Event** and then click on **Send Custom Event**.\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/test-site-event.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/test-site-event.webp)\n\nYou can check the event using RudderStack’s [**Live events**](https://www.rudderstack.com/docs/dashboard-guides/live-events/) view or check the **Syncs** tab in the Snowflake destination.\n\n**Note that the test event needs to be delivered to Snowflake to validate the pipeline.** If needed, you can run a manual sync by clicking **Sync now** in the **Syncs** tab of the Snowflake destination view in RudderStack.\n\n## Profiles schema and permissions\n\nRemember that Predictions automatically runs a Profiles job to create an identity graph. In this step, create a new schema where the identity graph and the related tables and views will be generated.\n\n```\nCREATE SCHEMA \"<YOUR_DATABASE>\".\"PROFILES\";\n```\n\nNow we need to grant permissions to the `QUICKSTART` role.\n\nProfiles will need the following permissions to run:\n\n*   Read access to all input tables to the model (already complete if you followed the previous setup steps)\n*   Write access to the schemas and common tables that the Profiles project creates.\n\nFor the write access run the following statements:\n\n```\nGRANT ALL PRIVILEGES ON SCHEMA PROFILES_QUICKSTART.PROFILES TO ROLE QUICKSTART;\nGRANT SELECT ON ALL TABLES IN SCHEMA PROFILES_QUICKSTART.PROFILES TO ROLE QUICKSTART;\nGRANT SELECT ON FUTURE TABLES IN SCHEMA PROFILES_QUICKSTART.PROFILES TO ROLE QUICKSTART;\nGRANT SELECT ON ALL VIEWS IN SCHEMA PROFILES_QUICKSTART.PROFILES TO ROLE QUICKSTART;\nGRANT SELECT ON FUTURE VIEWS IN SCHEMA PROFILES_QUICKSTART.PROFILES TO ROLE QUICKSTART;\n```\n\nYou are now ready to run Profiles and Predictions projects in the RudderStack UI!\n\n## Profiles CLI setup\n\nBefore you start building automated features, you need to perform some additional setup steps so that you can transition seamlessly from the UI-based workflow to the code-based workflow in the [code your own custom predictions](https://www.rudderstack.com/docs/archive/profiles/0.12/example/predictive-features-snowflake/custom-code/) section.\n\nTo build custom features with code, you will need Python3 and the RudderStack Profiles CLI tool (`PB`, for Profiles Builder) installed on your machine. If you do not have `PB` installed, follow the instructions below. This includes authentication for your Snowflake environment. **Use the warehouse, database, and schema setup in the previous steps.** This authentication will be used for accessing your Snowflake warehouse and running Snowpark. For more information about Profiles CLI tool, see [documentation](https://www.rudderstack.com/docs/profiles/get-started/profile-builder/).\n\n### Install Profile Builder tool\n\nOpen a console window and install the Profile Builder `PB` tool.\n\n```\npip3 install profiles-rudderstack\n```\n\nCheck the version to make sure it is at least `0.10.5`\n\n### Install ML dependency\n\nIn order to run ML models you will need to install the python package `profiles-multieventstream-features`. Run the following command to install it.\n\n```\npip install git+https://github.com/rudderlabs/profiles-pycorelib\n```\n\nEnsure you have the following python packages installed. These are required to use the `rudderstack-profiles-classifier` package to train classification models for predictive features.\n\n```\ncachetools>=4.2.2\nhyperopt>=0.2.7\njoblib>=1.2.0\nmatplotlib>=3.7.1\nseaborn>=0.12.0\nnumpy>=1.23.1\npandas>=1.4.3\nPyYAML>=6.0.1\nsnowflake_connector_python>=3.1.0\nsnowflake-snowpark-python[pandas]>=0.10.0\nscikit_learn>=1.1.1\nscikit_plot>=0.3.7\nshap>=0.41.0\nplatformdirs>=3.8.1\nxgboost>=1.5.0\nredshift-connector\n```\n\n### Create warehouse connection\n\nInitiate a warehouse connection:\n\nFollow the prompts and enter the details for your Snowflake warehouse/database/schema/user.\n\n```\nEnter Connection Name: quickstart\nEnter target:  (default:dev)  # Press enter, leaving it to default\nEnter account: <YOUR_ACCOUNT>\nEnter warehouse: <YOUR_WAREHOUSE>\nEnter dbname: <YOUR_DATABASE>\nEnter schema: PROFILES\nEnter user: QUICKSTART_USER\nEnter password: <password>\nEnter role: QUICKSTART\nAppend to /Users/<user_name>/.pb/siteconfig.yaml? [y/N]\ny\n```\n\n### Enable ML models\n\nFinally, enable ML models within `siteconfig.yaml`.\n\nOpen the file `/Users/<user_name>/.pb/siteconfig.yaml` in a text editor.\n\nAt the bottom of the file there is a `py_models` section. Update it to look like this:\n\n```\npy_models:\n    enabled: true\n    python_path: $(which python3)\n    credentials_presets: null\n    allowed_git_urls_regex: \"\"\n```\n\n## Snowpark\n\nPredictive features utilizes Snowpark within your Snowflake environment. It uses the same authentication as Snowflake and is able to run jobs within Snowflake.\n\nThis will run python code in a virtual warehouse in Snowflake and will incur compute costs. These costs vary depending on the type of model and the quantity of data used in training and prediction. For more general information on Snowflake compute costs, see [Understanding Compute Costs](https://docs.snowflake.com/en/user-guide/cost-understanding-compute).\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Prerequisites | RudderStack Docs",
    "description": "Prerequisites to generate predictive features in Snowflake using RudderStack Predictions",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.11/developer-guides/yaml-refresher/",
    "markdown": "# YAML 101 | RudderStack Docs\n\nA quick overview on YAML and its basics for use in Profile Builder.\n\n* * *\n\n*     4 minute read  \n    \n\nYAML is the preferred choice for writing Profile Builder files due to its simplicity and ease of use.\n\nThis guide explains the base concepts, syntax, and best practices for writing code in YAML.\n\n## What is YAML?\n\n[YAML](https://yaml.org/), short for **YAML Ain’t Markup Language** or **Yet Another Markup Language**, is a data serialization format often used in config files and exchange of data. A YAML file uses indentation, specific characters, and line breaks for representing various data structures.\n\n## Sample YAML file\n\nBelow is how a sample YAML document looks like. It contains key-value pairs where the keys are on the left, followed by a colon (`:`), and the associated values are on the right. The hierarchy and data structure is defined using indentation. The next section explains this in more detail.\n\n```\n# This is a comment\nperson:\n  name: Ruddy Buddy # Note the spacing used for indentation\n  age: 42\n  is_employed: true\n  address: # An object called address\n    street: Jefferson Davis Highway\n    city: Ruther Glen\n    state: Vermont\n    phone: 555-90-210\n  favorite_sports: # A list\n    - soccer\n    - baseball\n```\n\nThe above code has details of an object called `person` with properties like `name`, `age`, `gender`, `is_student`, `address` and `favorite sports`.\n\nHere’s how the same YAML file looks in the JSON format:\n\n```\n{\n  \"person\": {\n    \"name\": \"Ruddy Buddy\",\n    \"age\": 42,\n    \"is_employed\": true,\n    \"address\": {\n      \"street\": \"Jefferson Davis Highway\",\n      \"city\": \"Ruther Glen\",\n      \"state\": \"Vermont\",\n      \"phone\": \"555-90-210\"\n    },\n    \"favorite_sports\": [\n      \"soccer\",\n      \"baseball\"\n    ]\n  }\n}\n```\n\n## Indentation\n\nIn YAML, the indentation is done using spaces - to define the structure of data. Throughout the YAML file, the number of spacing should be consistent. Typically, we use two spaces for indentation. YAML is whitespace-sensitive, so do not mix spaces and tabs.\n\n```\n# Example of correct indentation\nperson:\n  name: Ruddy Buddy # We used 2 spaces\n  age: 42\n\n# Example of incorrect indentation\nperson:\n  name: Ruddy Buddy \n    age: 42 # We mixed spacing and tabs\n```\n\nAs shown above, YAML has single-line comments that start with hash (`#`) symbol, for providing additional explanation or context in the code. Comments are used to improve readability and they do not affect the code’s functionality.\n\n```\n# YAML comment\nperson:\n  name: Ruddy Buddy # Name of the person\n  age: 42 # Age of the person\n```\n\n## Data types in YAML\n\nYAML supports several data types:\n\n*   **Scalars**: Represent strings, numbers, and boolean values.\n*   **Sequences**: Represent lists and are denoted using a hyphen (`-`).\n*   **Mappings**: Key-value pairs used to define objects or dictionaries using colon (`:`).\n\n```\n# Example of data types in YAML\nperson:\n  name: Ruddy Buddy # Scalar (string)\n  age: 42 # Scalar (number)\n  is_employed: true # Scalar (boolean)\n  address: # Mapping (object)\n    street: Jefferson Davis Highway\n    city: Ruther Glen\n    state: Vermont\n    phone: 555-90-210\n  favorite_sports: # Sequence (list)\n    - soccer\n    - baseball\n```\n\n## Chomp modifiers\n\nYAML provides two chomp modifiers for handling line breaks in scalar values.\n\n*   `>`: Removes all newlines and replaces them with spaces.\n\n```\ndescription: >\n  Here is an example of long description\n  which has multiple lines. Later, it\n  will be converted into a single line.  \n```\n\n*   `|`: Preserves line breaks and spaces.\n\n```\ndescription: |\n  Here is another long description, however\n  it will preserve newlines and so the original\n  format shall be as-it-is.  \n```\n\n## Special characters\n\nYou can use escape symbols for special characters in YAML. For example, writing an apostrophe in description can cause the YAML parser to fail. In this case, you can use the escape character.\n\n## Best practices for writing YAML\n\nFollow these best practices for writing clean YAML code in your Profiles projects:\n\n*   Always keep consistent indentation (preferably spaces over tabs).\n*   Give meaningful names to your keys.\n*   Avoid excessive nesting.\n*   YAML is case sensitive, so be mindful of that.\n*   Add comments wherever required.\n*   Use blank lines to separate sections like ID stitcher, feature table, etc.\n*   If your strings contain special characters, then use escape symbols.\n*   Make sure you end the quotes in strings to avoid errors.\n*   Use chomp modifiers for multi-line SQL.\n\n## Conclusion\n\nThe above guidelines constitute some best practices to write effective [Builder](https://www.rudderstack.com/docs/profiles/get-started/profile-builder/) code in Profiles. You can also see the following references:\n\n*   [YAML for VS Code](https://marketplace.visualstudio.com/items?itemName=redhat.vscode-yaml): Extension for comprehensive YAML support in Visual Studio Code.\n*   [YAML Lint](https://www.yamllint.com/) for linting.\n\nFor more information or in any case of any issues, [contact](mailto:support@rudderstack.com) the RudderStack team.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "YAML 101 | RudderStack Docs",
    "description": "A quick overview on YAML and its basics for use in Profile Builder.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.11/developer-guides/commands/",
    "markdown": "# Commands | RudderStack Docs\n\nLearn more about the Profile Builder commands and how to use them.\n\n* * *\n\n*     11 minute read  \n    \n\nThe Profile Builder tool supports specific commands, making executing the usual operations easier. The basic syntax of executing a command is:\n\n```\n$ pb <command> <subcommand> [parameters]\n```\n\n## Supported commands\n\nYou can use the following Profile Builder commands:\n\n### cleanup\n\nDisplays and removes materials, older than the retention time period specified by the user (default value is 180 days).\n\n```\npb cleanup materials -r <number of days>\n```\n\n**Optional Parameter**\n\n| Parameter | Description |\n| --- | --- |\n| `-r` | Retention time in number of days.<br><br>**Example**: If you pass 1, then all the materials created prior to one day (24 hours) are listed. This is followed by prompts asking you for confirmation, after which you can view the material names and delete them. |\n\n### compile\n\nGenerates SQL queries from models.\n\nIt creates SQL queries from the `models/profiles.yaml` file, storing the generated results in the **Output** subfolder in the project’s folder. With each run, a new folder is created inside it. You can manually execute these SQL files on the warehouse.\n\n**Optional parameters**\n\n| Parameter | Description |\n| --- | --- |\n| `clean_output` | Empties the output folder(s) before executing the command. |\n| `-c` | Uses a site configuration file other than the one in `.pb` directory.<br><br>**Example**: `$ pb compile -c MyOtherConnection/siteconfig.yaml` |\n| `-t` | Defines target name (mentioned in `siteconfig.yaml`) or timestamp in building the model.<br><br>**Example**: If your `siteconfig.yaml` has two targets, `dev` and `test`, and you want to use the `test` instance: `$ pb compile -t test` |\n| `--timestamp` | *   Runs the model now (default).  <br>    **Example**: `$ pb compile --timestamp now`.<br>  <br>*   Utilizes all the data in source tables regardless of timestamp.  <br>    **Example**:`$ pb compile --timestamp timeless`.<br>  <br>*   Utilizes data until a user-defined timestamp (epoch).  <br>    **Example**: `$ pb compile --timestamp 1659794654` |\n| `-p` | *   Uses a project file (`pb_project.yaml`) other than the one in current directory.  <br>    **Example**: `$ pb compile -p MyOtherProject`.<br>  <br>*   Fetches project from a URL such as GitHub.  <br>    **Example**:`$ pb compile -p git@github.com:<orgname>/<repo>`. You can also fetch a specific tag, like `$ pb compile -p git@github.com:<orgname>/<repo>/tag/<tag_version>/<folderpath>` |\n| `--rebase_incremental` | Rebases any incremental models (build afresh from their inputs) instead of starting from a previous run. You can do this every once in a while to address the stale data or migration/cleanup of an input table. |\n\n### discover\n\nDiscovers elements in the warehouse, such as models, entities, features and sources.\n\nIt allows you to discover all the registered elements in the warehouse.\n\n**Subcommands**\n\nDiscover all the `models`, `entities`, `features`, `sources`, and `materials` in the warehouse.\n\n```\n$ pb discover models\n$ pb discover entities\n$ pb discover features\n$ pb discover sources\n$ pb discover materials\n```\n\n**Optional parameters**\n\n| Parameter | Description |\n| --- | --- |\n| `-e` | Discovers specific entities with their name.<br><br>**Example**: `$ pb discover -e 'Name'` |\n| `-m` | Discovers a specific model.<br><br>**Example**: `$ pb discover -m 'MY_DATABASE.PROD_SCHEMA.CREATED_MODEL'` |\n| `-c` | Uses a site config other than the default one.<br><br>**Example**: `$ pb discover -c siteconfig.yaml` |\n| `-s` | Discovers entities in a specified schema. |\n| `-s \"*\"` | Discovers entities across all schemas (case-sensitive). |\n| `-u` | Discovers entities having the specified source URL’s.<br><br>**Example**: To discover all the entities coming from GitHub: `$ pb discover -u %github%` |\n| `-t` | Selects target (mentioned in `siteconfig.yaml`). |\n| `-p` | Uses project folder other than the one in current directory.<br><br>**Example**: `$ pb discover -p ThisFolder/ThatSubFolder/SomeOtherProject/` |\n| `-f` | Specifies a file path to dump the discovery output into a csv file.<br><br>**Example**: `$ pb discover -f path/to/csv_file.csv` |\n| `-k` | Restricts discovery of the specified model keys.<br><br>**Example**: `$ pb discover -k entity_key:mode_type:model_name` |\n\n### help\n\nProvides list information for any command.\n\n**Subcommand**\n\nGet usage information for a specific command, with subcommands, and optional parameters.\n\n### init\n\nCreates connection and initializes projects.\n\n**Subcommands**\n\nInputs values for a warehouse connection and then stores it in the `siteconfig.yaml` file.\n\nGenerates files in a folder named **HelloPbProject** with sample data. You can change it as per project information, models, etc.\n\n**Optional parameters**\n\n| Parameter | Description |\n| --- | --- |\n| `pb-project -o` | Creates a Profile Builder project with a different name by specifying it as an additional parameter.<br><br>**Example**: To create a Profile Builder project with the name **SomeOtherProject**: `$ pb init pb-project -o SomeOtherProject` |\n| `connection -c` | Creates `siteconfig.yaml` at a location other than `.pb` inside home directory.<br><br>**Example**: To create `myconfig.yaml` in the current folder: `$ pb init connection -c myconfig.yaml`. |\n\n### insert\n\nAllows you to store the test dataset in your (Snowflake) warehouse . It creates the tables `sample_rs_demo_identifies` and `sample_rs_demo_tracks` in your warehouse schema specified in the `test` connection.\n\n```\n# Select the first connection named test having target and output as dev, of type Snowflake.\n$ pb insert\n# By default it'll pick up connection named test. To use connection named red:\n$ pb insert -n red\n# To pick up connection named red, with target test .\n$ pb insert -n red -t test\n```\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> This command is supported only for Snowflake currently.\n\n### migrate\n\nMigrate your project to the latest schema.\n\n**Subcommands**\n\nBased on the current schema version of your project, it enlists all the steps needed to migrate it to the latest one.\n\nAutomatically migrate from one version to another.\n\nTo migrate your models:\n\n**Schema 44 onwards**\n\nNavigate to the folder where your project files are stored. Then execute one of the following:\n\n*   `pb migrate auto --inplace`: Replaces contents of existing folder with the migrated folder.\n*   `pb migrate auto -d <MigratedFolder>`: Keeps the original project intact and stores the migrated project in another folder.\n\n**Schema 43 -> 44:**\n\nUse `{{entity-name.Var(var-name)}}` to refer to an `entity-var` or an `input-var`.\n\nFor example, for entity\\_var `user_lifespan` in your HelloPbProject, change `select: last_seen - first_seen` to `select: '{{user.Var(\"last_seen\")}} - {{user.Var(\"first_seen\")}}'`.\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> Note that:\n> \n> *   You must use two curly brackets.\n> *   Anything contained within double curly brackets must be written in double quotes (`\" \"`). If you use single quotes within double quotes, then use the escape character (`\\`) that comes when using macros.\n\nFurther, navigate to the folder where your project files are stored. Then execute one of the following:\n\n*   `pb migrate auto --inplace`: Replaces contents of existing folder with the migrated folder.\n*   `pb migrate auto -d <MigratedFolder>`: Keeps the original project intact and stores the migrated project in another folder.\n\n**Linear dependency**\n\nSpecify this parameter when entity as vars migration is not done (till version 43). After the migration is done, it’s not necessary to mention this parameter and can be removed.\n\n```\n  compatibility_mode:\n    linear_dependency_of_vars: true\n```\n\n**Optional parameters**\n\n| Parameter | Description |\n| --- | --- |\n| `-p` | Uses a project file other than the one in current directory. |\n| `-c` | Uses a `siteconfig.yaml` file other than the one in your home directory. |\n| `-t` | Target name (defaults to the one specified in `siteconfig.yaml` file). |\n| `-v` | Version to which the project needs to be migrated (defaults to the latest version). |\n| `-d` | Destination folder to store the migrated project files.<br><br>**Example**: `pb migrate auto -d FolderName` |\n| `--force` | Ignores warnings (if any) and migrates the project. |\n| `--inplace` | Overwrites the source folder and stores migrated project files in place of original.<br><br>**Example**: `pb migrate auto --inplace` |\n| `-p` | Uses a project folder other than the one in current directory.<br><br>**Example**: `$ pb discover -p ThisFolder/ThatSubFolder/SomeOtherProject/` |\n| `-f` | Specifies a file path to dump the discovery output into a csv file.<br><br>**Example**: `$ pb discover -f path/to/csv_file.csv` |\n| `-k` | Restricts discovery of the specified model keys.<br><br>**Example**: `$ pb discover -k entity_key:mode_type:model_name` |\n\n### run\n\nCreates identity stitcher or feature table model in the Warehouse.\n\nIt generates the SQL files from models and executes them in the warehouse. Once executed, you can see the output table names, which are accessible from the warehouse.\n\n**Optional parameters**\n\nThe `run` command shares the same parameters as the [`compile`](#compile) command, in addition to the following ones:\n\n| Parameter | Description |\n| --- | --- |\n| `--force` | Does a force run even if the material already exists. |\n| `-- write_output_csv` | Writes all the generated tables to CSV files in the specified directory.<br><br>**Example**: `$ pb run -- WriteOutputHere.csv` |\n| `--model_args` | Customizes behavior of an individual model by passing configuration params to it.<br><br>The only argument type supported currently is `breakpoint` for feature table models.<br><br>The `breakpoint` parameter lets you generate and run SQL only till a specific feature/tablevar. You can specify it in the format `modelName:argType:argName` where argName is the name of feature/tablevar.<br><br>**Example**: `$ pb run --model_args domain_profile:breakpoint:salesforceEvents` |\n| `--model_refs` | Restricts the operation to a specified model. You can specify model references like `pb run --model_refs models/user_id_stitcher --seq_no latest` |\n| `--seq_no` | Sequence number for the run, for example, 0, 1, 2,…, latest/new. The default value is `new`. You can check run logs or use discover commands to know about existing sequence numbers. |\n| `--ignore_model_errors` | Allows the project to continue to run in case of an erroneous model. The execution will not stop due to one bad model. |\n| `--grep_var_dependencies` | Uses regex pattern matching over fields from vars to find references to other vars and set dependencies. By default, it is set to `true`. |\n| `--concurrency` | Lets you run the models concurrently in a warehouse (wherever possible) based on the dependency graph. In CLI, you can specify the concurrency level for running models in a project via `pb run --concurrency <int>` (default int value is 1). Currently, this is supported only for Snowflake warehouse. It is recommended to use this option judiciously as applying a large value can impact your system resources. |\n\n### show\n\nObtains a comprehensive overview of models, id\\_clusters, packages, and more in a project. Its capacity to provide detailed information makes it particularly useful when searching for specific details, like all the models in your project.\n\n**Subcommands**\n\n1.  `pb show models`\n\nThis command lets you view information about the models in your project. The output includes the following information about each model:\n\n*   **Warehouse name**: Name of the table/view to be created in the warehouse.\n*   **Model type**: Whether its an identity stitching, feature table, SQL model etc.\n*   **Output type**: Whether the output type is `ephemeral`, `table`, or `view`.\n*   **Run type**: Whether the model’s run type is `discrete` or `incremental`.\n*   **SQL type**: Whether the SQL type of the model is `single_sql` or `multi_sql`.\n\n2.  `pb show dependencies`\n\nThis subcommand generates a graph file (`dependencies.png`) highlighting the dependencies of all models in your project.\n\n3.  `pb show dataflow`\n\nThis subcommand generates a graph file (`dataflow.png`) highlighting the data flow of all models in your project.\n\n4.  `pb show models --json`\n\nThis subcommand displays the models in a JSON format.\n\n5.  `pb show idstitcher-report --id_stitcher_model models/<ModelName> --migrate_on_load`\n\nThis subcommand creates a detailed report about the identity stitching model runs. To know the exact modelRef to be used, you can execute `pb show models`. By default, it picks up the last run, which can be changed using flag `-l`. The output consists of:\n\n*   **ModelRef**: The model reference name.\n*   **Seq No**: Sequence number of the run for which you are creating the report.\n*   **Material Name**: Output name as created in warehouse.\n*   **Creation Time**: Time when the material object was created.\n*   **Model Converged**: Indicates a successful run if `true`.\n*   **Pre Stitched IDs before run**: Count of all the IDs before stitching.\n*   **Post Stitched IDs after run**: Count of unique IDs after stitching.\n\nProfile Builder also generates a HTML report with relevant results and graphics including largest cluster, ID graph, etc. It is saved in `output` folder and the exact path is shown on screen when you execute the command.\n\n6.  `pb show user-lookup -v '<trait value>'`\n\nThis subcommand lists all the features associated with a user using any of the traits (flag `-v`) as ID types (email, user id, etc. that you are trying to discover).\n\n**Optional parameters**\n\n| Parameter | Description |\n| --- | --- |\n| `-h` | Displays help information for the command. |\n| `-p` | Specifies the project path to list the models. If not specified, it uses the project in the current directory. |\n| `-c` | File location of the `siteconfig.yaml` (defaults to the one in your home directory). |\n| `-t` | Target name (defaults to the target specified in `siteconfig.yaml` file). |\n| `--include_disabled` | Lets the disabled models be a part of the generated graph image (applicable to [`dataflow` and `dependencies`](#show)). |\n| `--seq_no` | Specifies a particular run for an ID stitcher model (applicable for [`idstitcher-report`](#show)). |\n\n### query\n\nExecutes SQL query on the warehouse and prints the output on screen (10 rows by default).\n\nFor example, if you want to print the output of a specific table/view named `user_id_stitcher`, run the following query:\n\n```\npb query \"select * from user_id_stitcher\"\n```\n\nTo reference a model with the name `user_default_id_stitcher` for a previous run with seq\\_no 26, you can execute:\n\n```\npb query 'select * from {{this.DeRef(\"path/to/user_default_id_stitcher\")}} limit 10' --seq_no=26\n```\n\n**Optional parameters**:\n\n| Parameter | Description |\n| --- | --- |\n| `-f` | Exports output to a CSV file. |\n| `-max_rows` | Maximum number of rows to be printed (default is 10). |\n| `-seq_no` | Sequence number for the run. |\n\n### validate\n\nValidates aspects of the project and configuration.\n\nIt allows you to run various tests on the project-related configurations and validate those. This includes but is not limited to validating the project configuration, privileges associated with the role specified in the site configuration of the project’s connection, etc.\n\n**Subcommands**\n\nRuns tests on the role specified in the site configuration file and validates if the role has privileges to access all the related objects in the warehouse. It throws an error if the role does not have required privileges to access the input tables or does not have the permissions to write the material output in the output schema.\n\n### version\n\nShows the Profile Builder’s current version along with its GitHash.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Commands | RudderStack Docs",
    "description": "Learn more about the Profile Builder commands and how to use them.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.11/example/",
    "markdown": "# Examples | RudderStack Docs\n\nPrivacy Overview\n\nThis site uses cookies to improve your experience while you navigate through the website. Out of these cookies, the cookies that are categorized as necessary are stored on your browser as they are as essential for the working of basic functionalities of the website. We also use third-party cookies that help us analyze and understand how you use this website. These cookies will be stored in your browser only with your consent. You also have the option to opt-out of these cookies. But opting out of some of these cookies may have an effect on your browsing experience.\n\nNecessary cookies are absolutely essential for the website to function properly. This category only includes cookies that ensures basic functionalities and security features of the website. These cookies do not store any personal information.",
    "title": "Examples | RudderStack Docs",
    "description": "Detailed tutorials on executing different models in warehouse to generate material (output) tables.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.12/example/predictive-features-snowflake/setup-automated-features/",
    "markdown": "# Setup Automated Features | RudderStack Docs\n\nSet up RudderStack to build automated features in the RudderStack UI\n\n* * *\n\n*     6 minute read  \n    \n\nSetting up automated features in the RudderStack UI is a straight-forward process. Predictive features are configured within a Profiles project and automatically added to the feature table output when the project is run.\n\n## Project setup\n\nFollow the steps below to set up a project and build predictive features:\n\n### Log into RudderStack\n\nYou can log-in [here](https://app.rudderstack.com/login).\n\n### Navigate to Profiles screen\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/Navigation.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/Navigation.webp)\n\n### Enter a name and description\n\nEnter a unique name and description for the Profiles Project where you want to build the predictive features.\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/Profiles-Name.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/Profiles-Name.webp)\n\n### Select sources\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/Sources.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/Sources.webp)\n\nSelect your Snowflake warehouse. If you have not configured the Snowflake warehouse, set up an event stream connection to Snowflake in RudderStack ([see details here](https://www.rudderstack.com/docs/destinations/warehouse-destinations/snowflake/)) and refer to the setup steps above.\n\nOnce you select the warehouse, you will be able to choose from RudderStack event sources that are connected to Snowflake. In this example, the JavaScript source created above is used to write to the same schema as the sample data. Profiles will use the `PAGES`, `TRACKS`, `IDENTIFIES`, and `ORDER_COMPLETED` tables from that schema to build automated and predictive features.\n\n## Map ID fields\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/Map-ID.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/Map-ID.webp)\n\nMap the fields from the source table(s) to the correct type of ID. The standard ID types are:\n\n*   `user_id`\n*   `anonymous_id`\n*   `email`\n\n**Note that for RudderStack event sources, standard ID column names will be mapped for you automatically**. If you have included additional identifiers in your payloads, you can map those custom column names to standard identifiers by clicking **Add mapping** at the bottom of the table.\n\n### Map `Order_Completed` table\n\nClick on **Add mapping** and map the `USER_ID` and `ANONYMOUS_ID` columns to standard identifiers to include the `ORDER_COMPLETED` table as a source for the identity graph and user features.\n\n| Source | Event | Property | ID Type |\n| --- | --- | --- | --- |\n| QuickStart Test Site | ORDER\\_COMPLETED | USER\\_ID | user\\_id |\n| QuickStart Test Site | ORDER\\_COMPLETED | ANONYMOUS\\_ID | anonymous\\_id |\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/map-id-orders.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/map-id-orders.webp)\n\n## Create default features in the UI\n\nThere are two types of automated features you can define in the UI:\n\n*   Default features\n*   Custom features\n\nThis guide focuses on the default features that are automatically generated.\n\n### Set up default features\n\nDefault features are features commonly used in Profiles projects. RudderStack provides a template library for these features to make them easy to add to your project. Templated features give you access to over 40 different standard and predictive features, which are generated in Snowflake automatically.\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/feature-categories.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/feature-categories.webp)\n\nDefault features are divided into 4 categories:\n\n*   **Attribution** - campaign, source, and churn features\n*   **Demographics** - user trait features\n*   **Engagement** - user activity features\n*   **Predictive ML Features** - predictive features\n\nYou can open the drop down menu for each category and select as many as you would like for your project.\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/feature-attribution.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/feature-attribution.webp)\n\nFor this guide, select:\n\n*   Attribution\n    *   `first_source_name`\n    *   `is_churned_30_days`\n    *   `is_churned_90_days`\n*   Demographics\n    *   `first_name`\n    *   `last_name`\n    *   `state`\n*   Engagement\n    *   `first_date_seen`\n    *   `last_date_seen`\n    *   `total_sessions_90_days`\n    *   `total_sessions_last_week`\n*   Predictive ML Features\n    *   `percentile_churn_score_30_days`\n\nIt is important to remember that RudderStack runs all of the feature-generation code transparently in Snowflake. For any of the default features, other than Predictive ML Features, you can click on **Preview Code** and get a yaml code snippet defining that feature (the yaml definition is used to generate SQL). This is helpful for technical users who want a deeper understanding of feature logic (and a running start for coding their own features).\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/churn-code-snippet.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/churn-code-snippet.webp)\n\n#### Churn definition\n\nRudderStack Predictions automatically generates a binary churn value for every user based on inactivity over a 7, 30, or 90-day period.\n\nFor example, to calculate the `is_churned_7_days` value, RudderStack looks for any activity timestamp for a particular user in the `TRACKS` and `PAGES` tables over the previous 7 day period. Practically, this means that RudderStack executes a ‘max timestamp’ query against those tables to see if users have viewed a page or performed other tracked actions (like clicks, form submits, add to carts, etc.) and then calculates the difference from today. If the query returns 7 or more, that means they haven’t performed any activity over the last 7 days and their `is_churned_7_days` trait is set to `1`.\n\n#### How Predictions models percentile churn scores\n\nUsing the standard definition (no activity over a defined period), RudderStack Predictions automatically runs a python-based churn model in Snowpark that predicts whether users will become inactive (churn) over the next 7, 30, or 90-day period. This model is trained on existing user data, using the Profiles identity graph, so it is recommended that you have a minimum of 5,000-10,000 unique users to achieve accurate output for business use cases.\n\n**How Predictions automates ML with Snowpark**\n\nPredictions streamlines integration with Snowpark by using the authentication from your existing Snowflake integration in RudderStack.\n\nIn order to run models in Snowpark, there is one additional set of permissions required. To run Predictions jobs, you must have permission to create stages within your schema. For more information see the **CREATE STAGE** [documentation](https://docs.snowflake.com/en/sql-reference/sql/create-stage#access-control-requirements).\n\nOnce permissions are granted, you will be able to run jobs that produce predictive features. **If you have followed the steps in [Prerequisite](https://www.rudderstack.com/docs/archive/profiles/0.12/example/predictive-features-snowflake/prerequisite/) guide, that permission has already been granted.**\n\n## Create custom features in the UI\n\nIf a needed feature is not in the template library, you can define a custom feature in the UI. Custom features can be standard or predictive features.\n\n### Add Custom Features\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/custom-feature.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/custom-feature.webp)\n\nClick on **Add a custom feature** at the top of the page and build an `average_order` feature with the following values:\n\n| Field | Value |\n| --- | --- |\n| **Name** | average\\_order |\n| **Description** | Average Order Size including shipping, taxes, and discounts |\n| **Function Type** | AGGREGATE |\n| **Function** | AVG |\n| **Event** | EVENTS.ORDER\\_COMPLETED |\n| **Property or Trait** | TOTAL |\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/custom-feature-define.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/custom-feature-define.webp)\n\nOnce complete click **Save**. The custom feature will be added to the top of the page.\n\n## Set Schedule\n\nThere are three options to set a schedule for how often the feature generation job runs:\n\n*   Basic\n*   Cron\n*   Manual\n\n### Basic\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/schedule-basic.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/schedule-basic.webp)\n\nSchedule on a predetermined interval.\n\nThe frequency can be every:\n\n*   30 minutes\n*   1 hour\n*   3 hours\n*   6 hours\n*   12 hours\n*   24 hours\n\nThen select a starting time for the initial sync.\n\n### Cron\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/schedule-cron.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/schedule-cron.webp)\n\nSchedule using cron expressions for more specific scheduling (i.e. Daily on Tuesdays and Thursdays).\n\nIf you are not familiar with cron expressions, you can use the builder in the UI.\n\n### Manual\n\nOnly runs when manually triggered within the UI. For this guide, select **Manual**.\n\n## Save, review, and create project\n\n### Save project\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/save-project.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/save-project.webp)\n\nFill in the **Schema** field with `PROFILES` (to match what we created earlier). This is where the feature table will be written to in Snowflake.\n\n### Review and create project\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/review-create.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/review-create.webp)\n\nFinally, review all the settings and when ready, click `Create user 360`.\n\n## Review created features\n\nOnce the initial project run is initiated, it may take up to 25-30 minutes to complete. Once the job is done, you are able to explore the data in RudderStack’s UI, including model fit charts for predictive features and individual user records with all features.\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/ui-predictive.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/ui-predictive.webp)\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/ui-predictive-graphs.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/ui-predictive-graphs.webp)\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/ui-explorer.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/ui-explorer.webp)\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Setup Automated Features | RudderStack Docs",
    "description": "Set up RudderStack to build automated features in the RudderStack UI",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.11/example/id-stitcher/",
    "markdown": "# Identity Stitching | RudderStack Docs\n\nStep-by-step tutorial on how to stitch together different user identities.\n\n* * *\n\n*     7 minute read  \n    \n\nThis guide provides a detailed walkthrough on how to use a PB project and create output tables in a warehouse for a custom identity stitching model.\n\n## Prerequisites\n\n*   Familiarize yourself with:\n    \n    *   A basic Profile Builder project by following the [Profile Builder CLI](https://www.rudderstack.com/docs/archive/profiles/0.11/get-started/profile-builder/) steps.\n    *   [Structure](https://www.rudderstack.com/docs/archive/profiles/0.11/example/id-stitcher/) of a Profile Builder project and the parameters used in different files.\n\n## Output\n\nAfter [running the project](https://www.rudderstack.com/docs/archive/profiles/0.11/get-started/profile-builder/#7-generate-output-tables), you can view the generated material tables:\n\n1.  Log in to your Snowflake console.\n2.  Click **Worksheets** from the top navigation bar.\n3.  In the left sidebar, click **Database** and the corresponding **Schema** to view the list of all tables. You can hover over a table to see the full table name along with its creation date and time.\n4.  Write a SQL query like `select * from <table_name> limit 10;` and execute it to see the results:\n\n1.  Open [Postico2](https://eggerapps.at/postico2/). If required, create a new connection by entering the relevant details. Click **Test Connection** followed by **Connect**.\n2.  Click the **+** icon next to **Queries** in the left sidebar.\n3.  You can click **Database** and the corresponding schema to view the list of all tables/views.\n4.  Double click on the appropriate view name to paste the name on an empty worksheet.\n5.  You can prefix `SELECT *` from the view name pasted previously and suffix `LIMIT 10;` at the end.\n6.  Press Cmd+Enter keys, or click the **Run** button to execute the query.\n\n1.  Enter your Databricks workspace URL in the web browser and log in with your username and password.\n2.  Click the **Catalog** icon in left sidebar.\n3.  Choose the appropriate catalog from the list and click on it to view the contents.\n4.  You will see list of tables/views. Click on the appropriate table/view name to paste the name on the worksheet.\n5.  Then, you can prefix `SELECT * FROM` before the pasted view name and suffix `LIMIT 10;` at the end.\n6.  Select the query text. Press Cmd+Enter or click the **Run** button to execute the query.\n\n1.  Log in to your [Google Cloud Console](https://console.cloud.google.com/)\n2.  Search for Bigquery in the search bar.\n3.  Select Bigquery from **Product and Pages** to open the Bigquery **Explorer**.\n4.  Select the correct project from top left drop down menu.\n5.  In the left sidebar, click the project ID, then the corresponding dataset view list of all the tables and views.\n6.  Write a SQL query like `select * from <table_name> limit 10;` and execute it to see the results.\n\nA sample output containing the results in Snowflake:\n\n![Generated tables (Snowflake)](https://www.rudderstack.com/docs/images/profiles/snowflake-console.webp)\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> Profiles creates a default ID stitcher even if you do not define any specs for creating one. It takes `default ID stitcher` as the input and all the sources and ID types defined in the file `inputs.yaml`. When you define the specs, it creates a custom ID stitcher.\n\n## Sample project for Custom ID Stitcher\n\nThis sample project considers multiple user identifiers in different warehouse tables to ties them together to create a unified user profile. The following sections describe how to define your PB project files:\n\n### Project detail\n\nThe [`pb_project.yaml`](https://www.rudderstack.com/docs/archive/profiles/0.11/developer-guides/structure/#project-details) file defines the project details such as name, schema version, connection name and the entities which represent different identifiers.\n\nYou can define all the identifiers from different input sources you want to stitch together as a `rudder_id` (`main_id` in this example):\n\n```\n# Project name\nname: sample_id_stitching\n# Project's yaml schema version\nschema_version: 54\n# Warehouse connection\nconnection: test\n# Allow inputs without timestamps\ninclude_untimed: true\n# Folder containing models\nmodel_folders:\n  - models\n# Entities in this project and their ids.\nentities:\n  - name: user\n    id_stitcher: models/user_id_stitcher # modelRef of custom ID stitcher model\n    id_types:\n      - main_id # You need to add ``main_id`` to the list only if you have defined ``main_id_type: main_id`` in the id stitcher buildspec.\n      - user_id # one of the identifier from your data source.\n      - email\n# lib packages can be imported in project signifying that this project inherits its properties from there\npackages:\n  - name: corelib\n    url: \"https://github.com/rudderlabs/profiles-corelib/tag/schema_{{best_schema_version}}\"\n    # if required then you can extend the package definition such as for ID types.\n```\n\n### Input\n\nThe [input file](https://www.rudderstack.com/docs/archive/profiles/0.11/developer-guides/structure/#inputs) (`models/inputs.yaml`) file includes the input table references and corresponding SQL for the above-mentioned entities:\n\n```\ninputs:\n- name: rsIdentifies\n  contract: # constraints that a model adheres to\n    is_optional: false\n    is_event_stream: true\n    with_entity_ids:\n      - user\n    with_columns:\n      - name: timestamp\n      - name: user_id\n      - name: anonymous_id\n      - name: email\n  app_defaults:\n    table: rudder_events_production.web.identifies # one of the WH table RudderStack generates when processing identify or track events.\n    occurred_at_col: timestamp\n    ids:\n      - select: \"user_id\" # kind of identity sql to pick this column from above table.\n        type: user_id\n        entity: user # as defined in project file\n        to_default_stitcher: true\n      - select: \"anonymous_id\"\n        type: anonymous_id\n        entity: user\n        to_default_stitcher: true\n      - select: \"lower(email)\" # can use sql.\n        type: email\n        entity: user\n        to_default_stitcher: true\n- name: rsTracks\n  contract:\n    is_optional: false\n    is_event_stream: true\n    with_entity_ids:\n      - user\n    with_columns:\n      - name: timestamp\n      - name: user_id\n      - name: anonymous_id\n  app_defaults:\n    table: rudder_events_production.web.tracks # another table in WH maintained by RudderStack processing track events.\n    occurred_at_col: timestamp\n    ids:\n      - select: \"user_id\"\n        type: user_id\n        entity: user\n        to_default_stitcher: true\n      - select: \"anonymous_id\"\n        type: anonymous_id\n        entity: user\n        to_default_stitcher: true\n```\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> As seen in the above file, you can use SQL to achieve some complex scenario as well.\n\n### Model\n\nProfiles **Identity stitching** model maps and unifies all the specified identifiers (in `pb_project.yaml` file) across different platforms. It tracks the user journey uniquely across all the data sources and stitches them together to a `rudder_id`.\n\nA sample `profiles.yaml` file specifying an identity stitching model (`user_id_stitcher`) with relevant inputs:\n\n```\nmodels:\n  - name: user_id_stitcher\n    model_type: id_stitcher\n    model_spec:\n      validity_time: 24h\n      entity_key: user\n      materialization:\n        run_type: incremental\n      incremental_timedelta: 12h\n      main_id_type: main_id\n      edge_sources:\n        - from: inputs/rsIdentifies\n        - from: inputs/rsTracks\n```\n\n##### Model specification fields\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `validity_time` | Time | Specifies the validity of the model with respect to its timestamp. For example, a model run as part of a scheduled nightly job for 2009-10-23 00:00:00 UTC with `validity_time`: `24h` would still be considered potentially valid and usable for any run requests, which do not require precise timestamps between 2009-10-23 00:00:00 UTC and 2009-10-24 00:00:00 UTC. This specifies the validity of generated feature table. Once the validity is expired, scheduling takes care of generating new tables. For example: 24h for 24 hours, 30m for 30 minutes, 3d for 3 days |\n| `entity_key` | String | Specifies the relevant entity from your `input.yaml` file. For example, here it should be set to `user`. |\n| `materialization` | List | Adds the key `run_type`: `incremental` to run the project in incremental mode. This mode considers row inserts and updates from the `edge_sources` input. These are inferred by checking the timestamp column for the next run. One can provide buffer time to consider any lag in data in the warehouse for the next incremental run like if new rows are added during the time of its run. If you do not specify this key then it’ll default to `run_type`: `discrete`. |\n| `incremental_timedelta` | List | (Optional )If materialization key is set to `run_type`: `incremental`, then this field sets how far back data should be fetched prior to the previous material for a model (to handle data lag, for example). The default value is 4 days. |\n| `main_id_type` | ProjectRef | (Optional) ID type reserved for the output of the identity stitching model, often set to `main_id`. It must not be used in any of the inputs and must be listed as an id type for the entity being stitched. If you do not set it, it defaults to `rudder_id`. Do not add this key unless it’s explicitly required, like if you want your identity stitcher table’s `main_id` column to be called `main_id`. |\n| `edge_sources` | List | Specifies inputs for the identity stitching model as mentioned in the `inputs.yaml` file. |\n\n## Use cases\n\nThis section describes some common identity stitching use cases:\n\n*   **Identifiers from multiple data sources**: You can consider multiple identifiers and tables by:\n    \n    *   Adding entities in `pb_project.yaml` representing identifiers.\n    *   Adding references to table and corresponding sql in `models/inputs.yaml`\n    *   Adding table reference names defined in `models/inputs.yaml` as `edge_sources` in your model definition.\n*   **Leverage Sql Support**: You can use SQL in your `models/inputs.yaml` to achieve different scenarios. For example, you want to tag all the internal users in your organization as one entity. You can use the email domain as the identifier by adding a SQL query to extract the email domain as the identifier value: `lower(split_part({{email_col}}, '@', 2))`\n    \n*   **Custom ID Stitcher**: You can define a custom ID stitcher by defining the required id stitching model in `models/profiles.yaml`.\n    \n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Identity Stitching | RudderStack Docs",
    "description": "Step-by-step tutorial on how to stitch together different user identities.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.11/example/feature-table/",
    "markdown": "# Feature Table | RudderStack Docs\n\nStep-by-step tutorial on creating a feature table model.\n\n* * *\n\n*     10 minute read  \n    \n\nOnce you have done [identity stitching](https://www.rudderstack.com/docs/archive/profiles/0.11/identity-stitching/) to unify the identity of your users across all the cross-platforms, you can evaluate and maintain the required features/traits for each identified user in a feature table.\n\nThis guide provides a detailed walkthrough on how to use a PB project and create output tables in a warehouse for a feature table model.\n\n## Prerequisites\n\nFamiliarize yourself with:\n\n*   A basic Profile Builder project by following the [Profile Builder CLI](https://www.rudderstack.com/docs/archive/profiles/0.11/get-started/profile-builder/) steps.\n*   [Structure](https://www.rudderstack.com/docs/archive/profiles/0.11/developer-guides/structure/) of a Profile Builder project and the parameters used in different files.\n*   [Identity Stitching](https://www.rudderstack.com/docs/archive/profiles/0.11/example/id-stitcher/) model as feature table reuses its output to extract the required features/traits.\n\n## Sample project\n\nThis sample project uses the output of an identity stitching model as an input to create a feature table. The following sections describe how to define your PB project files:\n\n### Project detail\n\nThe [`pb_project.yaml`](https://www.rudderstack.com/docs/archive/profiles/0.11/developer-guides/structure/#project-details) file defines the project details such as name, schema version, connection name and the entities which represent different identifiers.\n\nYou can define all the identifiers from different input sources you want to stitch together as a `user_main_id`:\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> You need to add `main_id` to the list only if you have defined `main_id_type: main_id` in the ID stitcher buildspec.\n\n```\n# Project name\nname: sample_id_stitching\n# Project's yaml schema version\nschema_version: 54\n# Warehouse connection\nconnection: test\n# Allow inputs without timestamps\ninclude_untimed: true\n# Folder containing models\nmodel_folders:\n  - models\n# Entities in this project and their ids.\nentities:\n  - name: user\n    id_types:\n      - main_id # You need to add ``main_id`` to the list only if you have defined ``main_id_type: main_id`` in the id stitcher buildspec.\n      - user_id # one of the identifier from your data source.\n      - email\n# lib packages can be imported in project signifying that this project inherits its properties from there\npackages:\n  - name: corelib\n    url: \"https://github.com/rudderlabs/profiles-corelib/tag/schema_{{best_schema_version}}\"\n    # if required then you can extend the package definition such as for ID types.\n```\n\n### Input\n\nThe [input file](https://www.rudderstack.com/docs/archive/profiles/0.11/developer-guides/structure/#inputs) file includes the input table references and corresponding SQL for the above-mentioned entities:\n\n```\ninputs:\n- name: rsIdentifies\n  contract: # constraints that a model adheres to\n    is_optional: false\n    is_event_stream: true\n    with_entity_ids:\n      - user\n    with_columns:\n      - name: timestamp\n      - name: user_id\n      - name: anonymous_id\n      - name: email\n  app_defaults:\n    table: rudder_events_production.web.identifies # one of the WH table RudderStack generates when processing identify or track events.\n    occurred_at_col: timestamp\n    ids:\n      - select: \"user_id\" # kind of identity sql to pick this column from above table.\n        type: user_id\n        entity: user # as defined in project file\n        to_default_stitcher: true\n      - select: \"anonymous_id\"\n        type: anonymous_id\n        entity: user\n        to_default_stitcher: true\n      - select: \"lower(email)\" # can use sql.\n        type: email\n        entity: user\n        to_default_stitcher: true\n- name: rsTracks\n  contract:\n    is_optional: false\n    is_event_stream: true\n    with_entity_ids:\n      - user\n    with_columns:\n      - name: timestamp\n      - name: user_id\n      - name: anonymous_id\n  app_defaults:\n    table: rudder_events_production.web.tracks # another table in WH maintained by RudderStack processing track events.\n    occurred_at_col: timestamp\n    ids:\n      - select: \"user_id\"\n        type: user_id\n        entity: user\n        to_default_stitcher: true\n      - select: \"anonymous_id\"\n        type: anonymous_id\n        entity: user\n        to_default_stitcher: true\n```\n\n### Model\n\nProfiles **Feature Table** model lets you define the specific features/traits you want to evaluate from the huge spread of scattered data in your warehouse tables.\n\nA sample `profiles.yaml` file specifying a feature table model (`user_profile`):\n\n```\nmodels:\n  - name: user_profile\n    model_type: feature_table_model\n    model_spec:\n      validity_time: 24h\n      entity_key: user\n      features:\n        - user_lifespan\n        - days_active\n        - min_num_c_rank_num_b_partition\nvar_groups:\n  - name: user_vars\n    entity_key: user\n    vars:\n      - entity_var:\n          name: first_seen\n          select: min(timestamp::date)\n          from: inputs/rsTracks\n          where: properties_country is not null and properties_country != ''\n      - entity_var:\n          name: last_seen\n          select: max(timestamp::date)\n          from: inputs/rsTracks\n      - entity_var:\n          name: user_lifespan\n          select: '{{user.Var(\"last_seen\")}} - {{user.Var(\"first_seen\")}}'\n          description: Life Time Value of a customer\n      - entity_var:\n          name: days_active\n          select: count(distinct timestamp::date)\n          from: inputs/rsTracks\n          description: No. of days a customer was active\n      - entity_var:\n          name: campaign_source\n          default: \"'organic'\"\n      - entity_var:\n          name: user_rank\n          default: -1\n      - entity_var:\n          name: campaign_source_first_touch\n          select: first_value(context_campaign_source)\n          window:\n            order_by:\n              - timestamp asc\n          from: inputs/rsIdentifies\n          where: context_campaign_source is not null and context_campaign_source != ''\n      - input_var:\n          name: num_c_rank_num_b_partition\n          select: rank()\n          from: inputs/tbl_c\n          default: -1\n          window:\n            partition_by:\n              - \"{{tbl_c}}.num_b\"\n            order_by:\n              - \"{{tbl_c}}.num_c asc\"\n          where: \"{{tbl_c}}.num_b >= 10\"\n      - entity_var:\n          name: min_num_c_rank_num_b_partition\n          select: min(num_c_rank_num_b_partition)\n          from: inputs/tbl_c\n      - entity_var:\n          name: first_bill\n          select: min({{tbl_billing.Var(\"payment\")}})\n          from: inputs/tbl_billing\n          column_data_type: '{{warehouse.DataType(\"float\")}}'\n```\n\n##### Model specification fields\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `validity_time` | Time | Specifies the validity of the model with respect to its timestamp. For example, a model run as part of a scheduled nightly job for 2009-10-23 00:00:00 UTC with `validity_time`: `24h` would still be considered potentially valid and usable for any run requests, which do not require precise timestamps between 2009-10-23 00:00:00 UTC and 2009-10-24 00:00:00 UTC. This specifies the validity of generated feature table. Once the validity is expired, scheduling takes care of generating new tables. For example: `24h` for 24 hours, `30m` for 30 minutes, `3d` for 3 days, and so on. |\n| `entity_key` | String | Specifies the relevant entity from your `input.yaml` file. |\n| `features` | String | Specifies the list of `name` in `entity_var`, that must act as a feature. |\n\n**`entity_var`**\n\nThe `entity_var` field provides inputs for the feature table model. This variable stores the data temporarily, however, you can choose to store its data permanently by specifying the `name` in it as a feature in the `features` key.\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `name` | String | Name of the `entity_var` to identify it uniquely. |\n| `select` | String | Column name/value you want to select from the table. This defines the actual value that will be stored in the variable. You can use simple SQL expressions or select an `entity_var` as `{{entityName.Var(\\\"entity_var\\\")}}`. It has to be an aggregate operation that ensures the output is a unique value for a given `main_id`. For example: min(timestamp), count(\\*), sum(amount) etc. This holds true even when a window function (optional) is used. For example:: first\\_value(), last\\_value() etc are valid while rank(), row\\_number(), etc. are not valid and give unpredictable results. |\n| `from` | List | Reference to the source table from where data is to be fetched. You can either refer to another model from the same YAML or some other table specified in input YAML. |\n| `where` | String | Any filters you want to apply on the input table before selecting a value. This must be SQL compatible and should consider the data type of the table. |\n| `default` | String | Default value in case no data matches the filter. When defining default values, make sure you enclose the string values in single quotes followed by double quotes to avoid SQL failure. However, you can use the non-string values without any quotes. |\n| `description` | String | Textual description of the `entity_var`. |\n| `window` | Object | Specifies the window function. Window functions in SQL usually have both `partition_by` and `order_by` properties. But for `entity_var`, `partition_by` is added with `main_id` as default; so, adding `partition_by` manually is not supported. If you need partitioning on other columns too, check out `input_var` where `partition_by` on arbitrary and multiple columns is supported. |\n| `column_data_type` | String | (Optional) Data type for the `entity_var`. Supported data types are: `integer`, `variant`, `float`, `varchar`, `text`, and `timestamp`. |\n\n**`input_var`**\n\nThe syntax of `input_var` is similar to `entity_var`, with the only difference that instead of each value being associated to a row of the feature table, it’s associated with a row of the specified input. While you can think of an `entity_var` as adding a helper column to the feature table, you can consider an `input_var` as adding a helper column to the input.\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `name` | String | Name to store the retrieved data. |\n| `select` | String | Data to be stored in the name. |\n| `from` | List | Reference to the source table from where data is to be fetched. |\n| `where` | String | (Optional) Applies conditions for fetching data. |\n| `default` | String | (Optional) Default value for any entity for which the calculated value would otherwise be NULL. |\n| `description` | String | (Optional) Textual description. |\n| `column_data_type` | String | (Optional) Data type for the `input_var`. Supported data types are: `integer`, `variant`, `float`, `varchar`, `text`, and `timestamp`. |\n| `window` | Object | (Optional) Specifies a window over which the value should be calculated. |\n\n**`window`**\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `partition_by` | String | (Optional) List of SQL expressions to use in partitioning the data. |\n| `order_by` | String | (Optional) List of SQL expressions to use in ordering the data. |\n\nIn window option, `main_id` is not added by default, it can be any arbitrary list of columns from the input table. So if a feature should be partitioned by `main_id`, you must add it in the `partition_by` key.\n\n### Output\n\nAfter [running the project](https://www.rudderstack.com/docs/archive/profiles/0.11/get-started/profile-builder/#7-generate-output-tables), you can view the generated material tables.\n\n1.  Log in to your Snowflake console.\n2.  Click **Worksheets** from the top navigation bar.\n3.  In the left sidebar, click **Database** and the corresponding **Schema** to view the list of all tables. You can hover over a table to see the full table name along with its creation date and time.\n4.  Write a SQL query like `select * from <table_name>` and execute it to see the results:\n\n1.  Open [Postico2](https://eggerapps.at/postico2/). If required, create a new connection by entering the relevant details. Click **Test Connection** followed by **Connect**.\n2.  Click the **+** icon next to **Queries** in the left sidebar.\n3.  You can click **Database** and the corresponding schema to view the list of all tables/views.\n4.  Double click on the appropriate view name to paste the name on an empty worksheet.\n5.  You can prefix `SELECT *` from the view name pasted previously and suffix `LIMIT 10;` at the end.\n6.  Press Cmd+Enter keys, or click the **Run** button to execute the query.\n\n1.  Enter your Databricks workspace URL in the web browser and log in with your username and password.\n2.  Click the **Catalog** icon in left sidebar.\n3.  Choose the appropriate catalog from the list and click on it to view contents.\n4.  You will see list of tables/views. Click the appropriate table/view name to paste the name on worksheet.\n5.  You can prefix `SELECT * FROM` before the pasted view name and suffix `LIMIT 10;` at the end.\n6.  Select the query text. Press Cmd+Enter, or click the **Run** button to execute the query.\n\n1.  Log in to your [Google Cloud Console](https://console.cloud.google.com/)\n2.  Search for Bigquery in the search bar.\n3.  Select Bigquery from **Product and Pages** to open the Bigquery **Explorer**.\n4.  Select the correct project from top left drop down menu.\n5.  In the left sidebar, click the project ID, then the corresponding dataset view list of all the tables and views.\n6.  Write a SQL query like `select * from <table_name> limit 10;` and execute it to see the results.\n\nA sample output containing the results in Snowflake:\n\n![Generated table (Snowflake)](https://www.rudderstack.com/docs/images/profiles/profiles-feature-table.webp)\n\n## Window functions\n\nA window function operates on a window (group) of related rows. It performs calculation on a subset of table rows that are connected to the current row in some way. The window function has the ability to access more than just the current row in the query result.\n\nThe window function returns one output row for each input row. The values returned are calculated by using values from the sets of rows in that window. A window is defined using a window specification, and is based on three main concepts:\n\n*   Window partitioning, which forms the groups of rows (`PARTITION BY` clause)\n*   Window ordering, which defines an order or sequence of rows within each partition (`ORDER BY` clause)\n*   Window frames, which are defined relative to each row to further restrict the set of rows (`ROWS` specification). It is also known as the frame clause.\n\n**Snowflake** does not enforces users to define the cumulative or sliding frames, and considers `ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING` as the default cumulative window frame. However, you can override this by defining the frame manually.\n\nOn the **Redshift** aggregate window function list given below, specify the `frame_clause` while using any function from the list:\n\n*   `AVG`\n*   `COUNT`\n*   `CUME_DIST`\n*   `DENSE_RANK`\n*   `FIRST_VALUE`\n*   `LAG`\n*   `LAST_VALUE`\n*   `LEAD`\n*   `LISTAGG`\n*   `MAX`\n*   `MEDIAN`\n*   `MIN`\n*   `NTH_VALUE`\n*   `PERCENTILE_CONT`\n*   `PERCENTILE_DISC`\n*   `RATIO_TO_REPORT`\n*   `STDDEV_POP`\n*   `STDDEV_SAMP` (synonym for `STDDEV`)\n*   `SUM`\n*   `VAR_POP`\n*   `VAR_SAMP` (synonym for `VARIANCE`)\n\nOn the Redshift ranking window functions given below, **do not** specify the `frame_clause` while using any function from the list:\n\n*   `DENSE_RANK`\n*   `NTILE`\n*   `PERCENT_RANK`\n*   `RANK`\n*   `ROW_NUMBER`\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> When using a window function, use `frame_clause` carefully. While It is not very critical for Snowflake, using it incorrectly in Redshift can lead to errors.\n\nExample of using `frame_clause`:\n\n```\n- entity_var:\n    name: first_num_b_order_num_b\n    select: first_value(tbl_c.num_b) # Specify frame clause as aggregate window function is used\n    from: inputs/tbl_c\n    default: -1\n    where: tbl_c.num_b >= 10\n    window:\n        order_by:\n        - tbl_c.num_b desc\n        frame_clause: ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING\n- entity_var:\n    name: first_num_b_order_num_b_rank\n    select: rank() # DO NOT specify frame clause as ranking window function is used\n    window:\n        order_by:\n        - first_num_b_order_num_b asc\n```\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> Note how `frame_clause` is specified in first `entity_var` and not in the second one.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Feature Table | RudderStack Docs",
    "description": "Step-by-step tutorial on creating a feature table model.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.11/example/id-collator/",
    "markdown": "# ID Collator | RudderStack Docs\n\nStep-by-step tutorial stitching different user identities together.\n\n* * *\n\n*     3 minute read  \n    \n\nID Stitching is one of the most important features of Profiles. Being able to perform ID stitching to determine the accounts belonging to the same customer/user is very important to get a 360-degree view of that user.\n\nHowever many a times, we may not require ID stitching for a particular entity, especially if there are no edges in the ID graph of an entity. To build a feature table on such an entity, you will still need to perform ID stitching. Although this approach is not wrong, it is computationally redundant.\n\nProfiles provides the ID Collator is to get all IDs of that particular entity from various input tables and create one collated list of IDs.\n\n## Sample project\n\nLet’s take a case where we have defined two entities in our project - one is `user` and the other is `session`.\n\nIf `user` entity has multiple IDs defined, there are basically edges which make the use of an ID stitcher logical. On the other hand, `session` may have only one ID, `ssn_id`, there won’t be any possibility of edges. In such a case, all we need is a complete list of `ssn_id`.\n\nHere is the corresponding inputs and entities definition.\n\n```\nentities:\n  - name: user\n    id_column_name: user_rud_id\n    id_types:\n      - user_id\n      - anonymous_id\n  - name: session\n    id_column_name: session_id\n    id_types:\n      - ssn_id\n```\n\nProject file:\n\n```\ninputs:\n  - name: user_accounts\n    table: tbl_user_accounts\n    occurred_at_col: insert_ts\n    ids:\n      - select: \"user_id\"\n        type: user_id\n        entity: user\n  - name: sign_in\n    table: tbl_sign_in\n    occurred_at_col: insert_ts\n    ids:\n      - select: \"user_id\"\n        type: user_id\n        entity: user\n      - select: \"ssn_id\"\n        type: ssn_id\n        entity: session\n      - select: \"anonymous_id\"\n        type: anonymous_id\n        entity: user\n  - name: sign_up\n    table: tbl_sign_up\n    occurred_at_col: insert_ts\n    ids:\n      - select: \"user_id\"\n        type: user_id\n        entity: user\n      - select: \"ssn_id\"\n        type: ssn_id\n        entity: session\n      - select: \"anonymous_id\"\n        type: anonymous_id\n        entity: user\n```\n\nHere, the `entity: session` has only one ID type. Creating an ID stitcher for such an entity is possible but unnecessary.\n\nUsing all the models having `ssn_id`, we can just make a union of all `ssn_id` and get all distinct values of it and obtain the final list of sessions.\n\nThe underlying SQL will look as follows:\n\n```\nSELECT ssn_id as session_id from sign_in\n        UNION\n    SELECT ssn_id as session_id from sign_up\n;\n```\n\n## YAML Changes\n\nThe YAML writer cannot define a custom ID collator the way they define a custom ID stitcher. If an entity has no edges, the PB project will automatically figure out if an ID collator is needed. To exclude certain inputs (having the required ID) from being used in the collation, we can just set `to_id_stitcher: false` in the input.\n\n```\nentities:\n  - name: session\n    id_column_name: session_id\n    id_types:\n      - ssn_id\n```\n\nThe `id_column_name` is a new field added in the entity definition which will be the name of the ID column and it applies to both ID stitcher and ID collator.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> In the ID collator, you won’t generate a UUID like in ID stitcher.\n\n## Comparing ID Collator and ID Stitcher\n\n| ID Stitcher | ID Collator |\n| --- | --- |\n| Uses edges to converge the ID graph. | Collates all distinct IDs as there is only one ID Type and no edges are present. |\n| Higher cost of computation. | Lower cost of computation. |\n| A UUID is generated and used as the unique identifier for the entity. | Collates the existing IDs only. |\n| The generated ID is always of the type: `rudder_id` | The ID column of the generated ID collator table/view will be of the ID type of the corresponding ID. |\n| User may override the default ID stitcher with custom one. | You cannot override the default ID collator, though you can define a custom ID stitcher to override default ID collator. |\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "ID Collator | RudderStack Docs",
    "description": "Step-by-step tutorial stitching different user identities together.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.12/example/packages/",
    "markdown": "# Additional Concepts | RudderStack Docs\n\nAdditional concepts related to Profiles like packages, best practices, partial feature tables, etc.\n\n* * *\n\n*     13 minute read  \n    \n\nThis guide explains some of the advanced concepts related to Profiles.\n\n## Packages\n\nProfiles gives you the flexibility to utilize models from existing library projects while defining your own models and inputs within the PB project. This approach allows for a seamless integration of library of pre-existing features, which are readily available and can be applied directly to data streamed into your warehouse.\n\nIn the absence of any explicitly defined models, the PB project is capable of compiling and running models from the library package given that inputs are present in the warehouse as assumed in the lib package.\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> Packages currently work only on Snowflake.\n\nThe following list of packages are currently available in Profiles. You can [contact the RudderStack team](mailto:support@rudderstack.com) to access these:\n\n*   [profiles-corelib](https://github.com/rudderlabs/profiles-corelib)\n*   [profiles-base-features](https://github.com/rudderlabs/rudderstack-profiles-base-features)\n*   profiles-shopify-features\n*   profiles-ecommerce-features\n*   profiles-stripe-features\n*   profiles-multieventstream-features\n\nGenerally, there will be some deviations in terms of the database name and schema name of input models - however, you can easily handle this by remapping inputs.\n\nA sample `pb_project.yaml` file may look as follows:\n\n```\nname: app_project\nschema_version: 54\nprofile: test\npackages:\n  - name: test_ft\n    gitUrl: \"https://github.com/rudderlabs/librs360-shopify-features/tree/main\"\n```\n\nIn this case, the PB project imports a single package. It does not require a separate `models` folder or entities as the input and output models will be sourced from the imported packages.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> Note that:\n> \n> *   If non-mandatory inputs required by the model are not present in the warehouse, you can still run the model.\n> *   If there is any deviation in the table/view name for input models, that is, if the inputs assumed in library package are present under some other name, make sure to do the remapping.\n> *   If some of the assumed inputs are not present at all, they should be remapped to `nil`. This way you can create and run imported packages with minimal set of inputs present.\n\nFor example, to import a library package with the name of `shopify_features`:\n\n```\npackages: \n  - name: shopify_features\n    url: https://github.com/rudderlabs/librs360-shopify-features/tree/main\n    inputsMap: \n      rsCartCreate: inputs/rsWarehouseCartCreate\n      rsCartUpdate: inputs/rsCartUpdate\n      rsIdentifies: inputs/rsIdentifies\n      rsOrderCancelled: inputs/rsOrderCancelled\n      rsOrderCreated: inputs/rsOrderCreated\n      rsPages: nil\n      rsTracks: nil\n```\n\nIn `models`/`inputs.yaml`, these inputs need to be defined with table names present in the warehouse.\n\n```\ninputs:\n  - name: rsWarehouseCartCreate\n    table: YOUR_DB.YOUR_SCHEMA.CART_CREATE_TABLE_NAME_IN_YOUR_WH\n    occurred_at_col: timestamp\n    ids:\n      - select: \"anonymous_id\"\n        type: anonymous_id\n        entity: user\n    source_metadata:\n      role: shopify\n      category: webhook\n  - name: rsIdentifies\n    table: YOUR_DB.YOUR_SCHEMA.IDENTIFIES\n    occurred_at_col: timestamp\n    ids:\n      - select: \"user_id\"\n        type: user_id\n        entity: user\n      - select: \"anonymous_id\"\n        type: anonymous_id\n        entity: user\n      - select: \"lower(email)\"\n        type: email\n        entity: user\n    source_metadata:\n      role: shopify\n      category: webhook\n```\n\nNote that the name of the table/view is changed to the appropriate name in your warehouse. If tables are present with the same name (including database name and schema name) then no remapping is required.\n\n### Modify ID types\n\n#### Extend existing package\n\nYou can add custom ID types to the default list or modify an existing one by extending the package to include your specifications.\n\nFor the corresponding `id_type`, add the key `extends:` followed by name of the same/different `id_type` that you wish to extend and the `filters` with `include`/`exclude` values.\n\n```\n---pb_project.yaml---\npackages:\n  - name: corelib\n    url: \"https://github.com/rudderlabs/rudderstack-profiles-corelib/tag/schema_{{best_schema_version}}\"\nid_types:\n  - name: user_id\n    extends: user_id\n    filters:\n      - type: exclude\n        value: 123456\nid_types:\n  - name: customer_id\n    extends: user_id\n    filters:\n      - type: include\n        regex: sample\n```\n\n*   **id\\_types**: Enlists the type of data to be used for creating ID stitcher / EntityVar / InputVar. For example, anonymous IDs that do not include the value `undefined` or email addresses in proper format.\n    *   **extends**: Name of the ID type that you wish to extend.\n    *   **name**: The type of data that will be fetched, like email, user ID, etc. It is different from whatever is present in the table column, like int or varchar.\n    *   **filters**: Filter(s) that the type should go through before being included. Filters are processed in order. Current filters enable one to include and exclude specific values or regular expressions.\n\n#### Custom list of ID types\n\nTo have custom list of ID types other than the provisions in the default package, you can remove and add your list as follows:\n\n```\nentities:\n  - name: user\n    id_types:\n      - user_id\n      - anonymous_id\n      - email\n\nid_types:\n  - name: user_id\n  - name: anonymous_id\n    filters:\n      - type: exclude\n        value: \"\"\n      - type: exclude\n        value: \"unknown\"\n      - type: exclude\n        value: \"NaN\"\n  - name: email\n    filters:\n    - type: include\n      regex: \"[A-Za-z0-9+_.-]+@(.+)\"\n```\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> Make sure that the ID types are also defined in the entity definition.\n\n## Model Contracts\n\nWith model contracts, you can declare constraints that the model adheres to. A model having a dependency on another model would also need to declare a contract specifying what columns and entities the input model must have. For contract validation, these columns should be present in the referenced model.\n\nFor an input of a project like a library project, the model contract is used to enforce constraints on tables/views that get wired to it downstream.\n\n```\n# inputs.yaml\n  - name: rsIdentifies\n    contract:\n      is_optional: false\n      is_event_stream: true\n      with_entity_ids:\n        - user\n      with_columns:\n        - name: timestamp\n        - name: user_id\n        - name: anonymous_id\n        - name: email\n```\n\nIn SQL model, the contract would contain all the columns from IDs and features. Each internal model also publishes the contract it promises to adhere to. Suppose `rsSessionTable` has an input `shopify_session_features`. Model contracts enable `rsSessionTable` to specify the constraints that `shopify_session_features` must adhere to.\n\n```\nmodels:\n- name: rsSessionTable\n  model_type: sql_template\n  model_spec:\n    ... # model specifications\n    single_sql: |\n      {% set contract = BuildContract('{\"with_columns\":[{\"name\":\"user_id\"}, {\"name\":\"anonymous_id\"}]}') %}\n      {% with SessionFeature = this.DeRef(\"models/shopify_session_features\",contract)%}\n          select user_id as id1, anonymous_id as id2 from {{SessionFeature}}\n  \tcontract:\n    with_entity_ids:\n      - user\n    with_columns:\n      - name: user_id\n        type: string\n        is_optional: true # false being default\n      - name: anonymous_id\n        type: string\n        is_optional: true # false being default\n```\n\nHere, `rsSessionTable` declares that its input `shopify_session_features` must have columns `user_id` and `anonymous_id`. This helps in improving data quality and error handling. Internally, this requested contract is validated against `shopify_session_features`’s actual contract. For validation to pass, `input_shopify_session_features_contract` must be a subset of `shopify_session_features`’s published contract.\n\nThis enables more comprehensive static and dynamic validations of our projects.\n\n## Partial feature tables\n\nPartial feature tables are created when only a few input sources are available.\n\nFor example, lets say that you import a library package and some of the input models assumed in the package are not present in your warehouse.\n\nWhen you remap some of these input models to nil, those inputs and the features directly or indirectly dependent upon those inputs are disabled. In such cases, a partial feature table is created from the rest of the available inputs. Similarly, ID stitcher also runs even if few of the edge sources are not present in the warehouse or remapped to nil.\n\n## Pre and post hooks\n\nA pre hook enables you to execute an SQL before running a model, for example, if you want to change DB access, create a DB object, etc. Likewise, a post hook enables you to execute an SQL after running a model. The SQL can also be templatized. Here’s an example code snippet:\n\n```\nmodels:\n  - name: test_id_stitcher\n    model_type: id_stitcher\n    hooks:\n      pre_run: \"CREATE OR REPLACE VIEW {{warehouse.ObjRef('V1')}} AS (SELECT * from {{warehouse.ObjRef('Temp_tbl_a')}});\"\n      post_run: 'CREATE OR REPLACE VIEW {{warehouse.ObjRef(\"V2\")}} AS (SELECT * from {{warehouse.ObjRef(\"Temp_tbl_a\")}});'\n    model_spec:\n      - # rest of model specs go here\n```\n\n## Use Amazon S3 bucket as input\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> This is an experimental feature.\n\nIf you store data in your Amazon S3 bucket in a CSV file format, you can use it as an input for the Profiles models. The S3 URI path must be specified in the `app_defaults.s3`:\n\n```\nname: s3_table\ncontract:\n  is_optional: false\n  is_event_stream: true\n  with_entity_ids:\n    - user\n  with_columns:\n    - name: insert_ts\n      datatype: timestamp\n    - name: num_a\n      datatype: integer\napp_defaults:\n  s3: \"s3://bucket-name/prefix/example.csv\"\n  occurred_at_col: insert_ts\n  ids:\n    - select: \"id1\"\n      type: test_id\n      entity: user\n    - select: \"id2\"\n      type: test_id\n      entity: user\n```\n\nEnsure that the CSV file follows the standard format with the first row as the header containing column names, for example:\n\n```\nID1,ID2,ID3,INSERT_TS,NUM_A\na,b,ex,2000-01-01T00:00:01Z,1\nD,e,ex,2000-01-01T00:00:01Z,3\nb,c,ex,2000-01-01T00:00:01Z,2\nNULL,d,ex,2000-01-01T00:00:01Z,4\n```\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> Note that:\n> \n> *   To escape comma (`,`) from any cell of the CSV file, enclose that cell with double quotes `\" \"` .\n> *   Double quotes (`\" \"`) enclosing a cell are ignored.\n\nFollow the below steps to grant PB the required permissions to access the file in S3 Bucket:\n\n### Private S3 bucket\n\nAdd `region`, [`access key id`](#generate-access-key-id-and-secret-access-key), [`secret access key`](#generate-access-key-id-and-secret-access-key), and [`session token`](#generate-session-token) in your `siteconfig` file so that PB can access the private bucket. By default, the region is set to `us-east-1` unless specified otherwise.\n\n```\naws_credential:\n    region: us-east-1\n    access_key: **********\n    secret_access_key: **********\n    session_token: **********\n```\n\n#### Generate `access key id` and `secret access key`\n\n1.  Open the AWS IAM console in your AWS account.\n2.  Click **Policies**.\n3.  Click **Create policy**.\n4.  In the Policy editor section, click the JSON option.\n5.  Replace the existing JSON policy with the following policy and replace the <bucket\\_name> with your actual bucket name:\n\n```\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:GetObjectVersion\"\n            ],\n            \"Resource\": \"arn:aws:s3:::<bucket_name>/*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:ListBucket\",\n                \"s3:GetBucketLocation\"\n            ],\n            \"Resource\": \"arn:aws:s3:::<bucket_name>\",\n            \"Condition\": {\n                \"StringLike\": {\n                    \"s3:prefix\": [\n                        \"*\"\n                    ]\n                }\n            }\n        }\n    ]\n}\n```\n\n6.  Click **Review policy**.\n7.  Enter the policy name. Then, click **Create policy** to create the policy.\n\nFurther, create an IAM user by following the below steps:\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> An IAM user requires the following permissions on an S3 bucket and folder to access files in the folder (and sub-folders):\n> \n> *   s3:GetBucketLocation\n> *   s3:GetObject\n> *   s3:GetObjectVersion\n> *   s3:ListBucket\n\n1.  In AWS IAM console, Click **Users**.\n2.  Click **Create user**.\n3.  Enter a name for the user.\n4.  Select Programmatic access as the access type, then click **Next: Permissions**.\n5.  Click **Attach existing policies directly**, and select the policy you created earlier. Then click **Next**.\n6.  Review the user details, then click **Create user**.\n7.  Copy the access key ID and secret access key values.\n\n#### Generate `session token`\n\n1.  Use the AWS CLI to create a named profile with the AWS credentials that you copied in the previous step.\n2.  To get the session token, run the following command:\n\n```\n $ aws sts get-session-token --profile <named-profile>\n```\n\nSee [Snowflake](https://docs.snowflake.com/en/user-guide/data-load-s3-config-aws-iam-user), [Redshift](https://docs.aws.amazon.com/redshift/latest/dg/copy-usage_notes-access-permissions.html), and [Databricks](https://docs.databricks.com/en/ingestion/copy-into/generate-temporary-credentials.html) for more information.\n\n### Public S3 Bucket\n\nYou **must** have the following permissions on the S3 bucket and folder to access files in the folder (and sub-folders):\n\n*   s3:GetBucketLocation\n*   s3:GetObject\n*   s3:GetObjectVersion\n*   s3:ListBucket\n\nYou can use the following policy in your bucket to grant the above permissions:\n\n1.  Go to the **Permissions** tab of your S3 bucket.\n2.  Edit bucket policy in **Permissions** tab and add the following policy. Replace the <bucket\\_name> with your actual bucket name:\n\n```\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": \"*\",\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:GetObjectVersion\"\n            ],\n            \"Resource\": \"arn:aws:s3:::<bucket_name>/*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": \"*\",\n            \"Action\": [\n                \"s3:ListBucket\",\n                \"s3:GetBucketLocation\"\n            ],\n            \"Resource\": \"arn:aws:s3:::<bucket_name>\"\n        }\n    ]\n}\n```\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> In Redshift, you additionally need to set an IAM role as **default** for your cluster, unless access keys are provided. It is necessary because more than one IAM role can be associated with the cluster, and Redshift needs explicit permission granted through an IAM role to access the S3 bucket (Public or Private).\n> \n> Follow [Redshift Documentation](https://docs.aws.amazon.com/redshift/latest/mgmt/default-iam-role.html#set-default-iam) for setting an IAM role as default.\n\n## Use CSV file as input\n\nAn input file (`models/inputs.yaml`) contains details of input sources such as tables, views, or CSV files along with column name and SQL expression for retrieving values.\n\nYou can read data from a CSV file by using `csv: <path_to_filename>` under `app_defaults` in the input specs. CSV data is loaded internally as a single SQL select query, making it useful for seeding tests.\n\nA sample code is as shown:\n\n```\n    app_defaults:\n      csv: \"../common.xtra/Temp_tbl_a.csv\"\n      # remaining syntax is same for all input sources\n```\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> RudderStack does not support CSV files with more than a few hundred rows.\n\n## Filter data\n\nYou can filter out any data by using the `filters` field in your projects file:\n\nFor example, if you want to exclude all the blacklisted email addresses, you can create an input model (for example, `csv_email_blacklist`) with CSV file as a source, that contains all such email addresses:\n\n```\nid_types:\n  - name: email\n    filters:\n      - type: exclude\n        sql:\n          select: email\n          from: inputs/csv_email_blacklist\n```\n\nAnother example, if you want to exclude all the user\\_ids, you can create an SQL model (for example, `sql_exclusion_model`) that contains a specific logic to enlist all such IDs:\n\n```\nid_types:\n  - name: user_id\n    filters:\n      - type: exclude\n        sql:\n          select: user_id\n          from: inputs/models/sql_exclusion_model\n```\n\nIt is recommended to use git-tags instead of the latest commit on main branch of your library projects. You can use a specific tag, for example: `https://github.com/org-name/lib-name/tag/schema_<n>`. If you want Profile Builder to figure out the best schema version for every run, you can use the placeholder {{best\\_schema\\_version}}, for example, `https://github.com/org-name/lib-name/tag/schema_{{best_schema_version}}`. The selection of compatible git tags is done by PB, that is it will figure out the best compatible version for the lib package.\n\nA sample project file:\n\n```\npackages:\n  - name: shopify_features\n    url: https://github.com/org-name/lib-names/tag/schema_{{best_schema_version}}\n    inputsMap:\n      rsCartUpdate: inputs/rsCartUpdate\n      rsIdentifies: inputs/rsIdentifies\n```\n\nUsing this will make Profiles use the best compatible version of the library project in case of any schema updates.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> You don’t have to replace the placeholder `{{best_schema_version}}`. For instance, if `https://github.com/org-name/lib-names/tags/` has a tag for schema\\_44, then `https://github.com/org-name/lib-names/tag/schema_44` will be automatically used. In any case, if you replace the placeholder with actual tag name, the project will work without any issues.\n\n## Use private Git repos via CLI\n\nFollow these steps:\n\n1.  [Generate the SSH Key](https://git-scm.com/book/en/v2/Git-on-the-Server-Generating-Your-SSH-Public-Key).\n2.  Associate the SSH Key to your Git Project. Check the section on [Profile Builder CLI](https://www.rudderstack.com/docs/archive/profiles/0.12/get-started/quickstart-ui/).\n3.  Add private keys as credentials in the `siteconfig.yaml` file:\n\n```\ngitcreds:\n  - reporegex: git@<provider-host>:<org-name>/*\n    key: |\n    -----BEGIN OPENSSH PRIVATE KEY-----\n    b3BlbnNzaC1rZXktdjEAAAAABG5vbmUAAAAEb..........\n    -----END OPENSSH PRIVATE KEY-----    \n```\n\n## Supported Git URLs\n\nProfiles supports Git URLs for packages and scheduling via UI. You can host the repos at:\n\n*   GitHub\n*   GitLab\n*   BitBucket\n\n[Contact the RudderStack team](mailto:support@rudderstack.com) if your preferred host isn’t included.\n\nFor private repos, RudderStack only supports SSH Git URLs. You need to add credentials to the `siteconfig.yaml` and public ssh key manually to the platforms. See [Use private Git repos via CLI](#use-private-git-repos-via-cli).\n\nThe URL scheme doesn’t depend on individual Git provider host. You can use the below-mentioned Git URLs:\n\n**1\\. URL for the default branch of a repository**\n\n*   **Syntax:**\n    \n    `https://<provider-host>/<org-name>/<repo-name>/path/to/project`\n    \n*   **Example:**\n    \n    `https://github.com/rudderlabs/librs360-shopify-features/shopify-features` `https://gitlab.com/rudderlabs/librs360-shopify-features/shopify-features` `https://bitbucket.org/rudderlabs/librs360-shopify-features/shopify-features`\n    \n\n**For private repos, RudderStack support SSH URLs:**\n\n*   **Syntax:**\n    \n    `git@<provider-host>:<org-name>/<repo-name>/path/to/project`\n    \n*   **Example:**\n    \n    `git@github.com:rudderlabs/librs360-shopify-features/shopify-features` `git@gitlab.com:rudderlabs/librs360-shopify-features/shopify-features` `git@gbitbucket.org:rudderlabs/librs360-shopify-features/shopify-features`\n    \n\n**2\\. URL for a specific branch of a repository**\n\n*   **Syntax:**\n    \n    `https://<provider-host>/<org-name>/<repo-name>/tree/<branch-name>/path/to/project`\n    \n*   **Example:**\n    \n    `https://github.com/rudderlabs/librs360-shopify-features/tree/main/shopify-features` `https://gitlab.com/rudderlabs/librs360-shopify-features/tree/main/shopify-features` `https://bitbucket.org/rudderlabs/librs360-shopify-features/tree/main/shopify-features`\n    \n\n**3\\. URL for a specific tag within the repository**\n\n*   **Syntax:**\n    \n    `https://<provider-host>/<org-name>/<repo-name>/tag/<tag-name>/path/to/project`\n    \n*   **Example:**\n    \n    `https://github.com/rudderlabs/librs360-shopify-features/tag/wht_test/shopify-features` `https://gitlab.com/rudderlabs/librs360-shopify-features/tag/wht_test/shopify-features` `https://bitbucket.org/rudderlabs/librs360-shopify-features/tag/wht_test/shopify-features`\n    \n\n**4\\. URL for a specific commit within the repository**\n\n*   **Syntax:**\n    \n    `https://<provider-host>/<org-name>/<repo-name>/commit/<commit-hash>/path/to/project`\n    \n*   **Example:**\n    \n    `https://github.com/rudderlabs/librs360-shopify-features/commit/b8d49/shopify-features` `https://gitlab.com/rudderlabs/librs360-shopify-features/commit/b8d49/shopify-features` `https://bitbucket.org/rudderlabs/librs360-shopify-features/commit/b8d49/shopify-features`\n    \n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> RudderStack supports the git SSH URL with following pattern only in the dashboard:\n> \n> *   `git@<provider-host>:<org-name>/<repo-name>/tree/<branch-name>`\n> *   `git@<provider-host>:<org-name>/<repo-name>`\n> *   `git@<provider-host>:<org-name>/<repo-name>/tree/main/path/to/project`\n> \n> RudderStack supports any subfolder in git project without .git extension.\n\n## View model dependencies\n\nYou can create a DAG to see all the model dependencies, that is, how a model is dependent on other models by using any one of the following commands:\n\n`pb show dataflow`  \nOR  \n`pb show dependencies`\n\nFurther, you can use the `pb show models` command to view information about the models in your project. See [show](https://www.rudderstack.com/docs/archive/profiles/0.12/cli-user-guide/commands/#show) command for more information.\n\n## Multi-version support\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> This feature is supported for the Profiles versions 0.10.8, 0.11.5, and 0.12.0 onwards.\n\nYou can constraint your Profiles project to run only on specific version(s) by specifying it in the `pb_project.yaml` file, under `python_requirements` key. For example, use the below snippet to run your project on v0.10.8:\n\n```\npython_requirements:\n  - profiles-rudderstack==0.10.8\n```\n\nUse the below snippet to stay on any minor version between 0.12.0 and 0.13.0. If a new minor version is released, your project will be auto-migrated to that version:\n\n```\npython_requirements:\n  - profiles-rudderstack>=0.12.0,<0.13.0\n```\n\nIf you do not specify any version in `pb_project.yaml`, the latest Profiles version is used by default. The version constraints follow the same syntax as those of [Python dependency specifiers](https://packaging.python.org/en/latest/specifications/dependency-specifiers/).\n\nMake sure that the version of Profiles project is the same in your environment and the `pb_project.yaml` file otherwise, RudderStack will throw an error.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Additional Concepts | RudderStack Docs",
    "description": "Additional concepts related to Profiles like packages, best practices, partial feature tables, etc.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.12/example/predictive-features-snowflake/custom-code/",
    "markdown": "# Custom Predictive Features | RudderStack Docs\n\nCode your own custom predictive features\n\n* * *\n\n*     8 minute read  \n    \n\nWhile automated features are incredibly useful for quickly deploying activity-based churn scores, data teams inevitably want to go deeper and define custom predictions that match their unique business logic and KPIs.\n\nBasic customization is possible in the UI as we covered above, but Predictions also supports a code-based workflow that gives technical users full control and complete customizability, as well as the ability to integrate the process into their existing development workflow.\n\nFor example, if you are an eCommerce company, it can be helpful to predict whether or not a user will make a purchase over a certain dollar amount, over the next `n` days.\n\nRudderStack makes it easy to migrate from the UI-based workflow to the code-based workflow to build these more complex use cases.\n\n## Download project files\n\nOn the Profiles screen, find your project and click the **Download this Project** button on the top right side. This will download all the files for that Profiles project in a compressed (zip) file including the modeling files.\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/download-project.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/download-project.webp)\n\nInside the Profiles folder you will find `pb_project.yaml` and a `models` folder with `resources.yaml`.\n\n```\nRudderStack QuickStart\n├── pb_project.yaml\n├── models\n│   ├── resources.yaml\n```\n\n### pb\\_project.yaml\n\n`pb_project.yaml` is the main configuration file for the Profiles project. The top section defines the `name`, `schema_version`, `connection`, and `model_folder` (where the files that define the details of the Profiles project can be found).\n\nUpdate the following values:\n\n*   `name` to `Profile-Quickstart` to match the name in the UI.\n*   `connection` to `QUICKSTART` to match the database connection we made in the Prerequisites section.\n\n```\nname: Profile-QuickStart\nschema_version: 49 # Or most recent version\nconnection: QUICKSTART\nmodel_folders:\n    - models\n```\n\nBelow there is an `entities` section that defines the entities and the kinds of ID’s make up that entity. An entity is a business concept or unit that will be used to build the identity graph and features. Projects can contain multiple entities like user, household, and organization.\n\nFor this project, there is one entity called `user` with 5 different types of IDs. An ID type maps which ID fields can be joined together. For example, if you have two tables with `user_id` columns called `id` and `userid`, by giving each the type `user_id` Profiles knows to join those tables on those columns.\n\nThe ID fields are already mapped to these types in the UI. Nothing needs to be updated in this section.\n\n```\nentities:\n  - name: user\n    id_types:\n      - main_id\n      - user_id\n    feature_views:\n      using_ids:\n        - id: email\n          name: features_by_email\n        - id: salesforce_id\n          name: salesforce_id_stitched_features\n      features:\n        - from: models/cart_feature_table\n          include:\n            - \"*\"\n```\n\nFinally there is a `packages` section. This section allows you to import a Profiles project from GitHub and use the feature definitions from that project in this one. The imported project provides the definitions for the standard features selected in the UI. Nothing needs to be updated in this section.\n\n```\npackages:\n    - name: base_features\n      url: https://github.com/rudderlabs/rudderstack-profiles-multieventstream-features\n      inputsMap: # These are the tables automatically mapping in the UI (TRACKS, PAGES, IDENTIFIES)\n        rsIdentifies_1: inputs/rsIdentifies_1\n        rsIdentifies_2: nil\n        rsIdentifies_3: nil\n        rsPages_1: inputs/rsPages_1\n        rsPages_2: nil\n        rsPages_3: nil\n        rsTracks_1: inputs/rsTracks_1\n        rsTracks_2: nil\n        rsTracks_3: nil\n      overrides: # By default all features are imported from the project, therefore the features we did not select need to be disabled\n        - requested_enable_status: disabled\n          models:\n            - entity/user/active_days_in_past_365_days\n            - entity/user/active_days_in_past_7_days\n            - entity/user/avg_session_length_in_sec_365_days\n            - entity/user/avg_session_length_in_sec_last_week\n            - entity/user/avg_session_length_in_sec_overall\n            - entity/user/campaign_sources\n            - entity/user/campaigns_list\n            - models/churn_7_days_model\n            - models/churn_90_days_model\n            - entity/user/country\n            - entity/user/currency\n            - entity/user/days_since_account_creation\n            - entity/user/days_since_last_seen\n            - entity/user/first_campaign_name\n            - entity/user/is_churned_7_days\n            - entity/user/last_campaign_name\n            - entity/user/last_source_name\n            - entity/user/max_timestamp_bw_tracks_pages\n            - entity/user/mediums_list\n            - entity/user/sources_list\n            - entity/user/total_sessions_365_days\n            - entity/user/total_sessions_till_date\n```\n\n### resource.yaml\n\n`resources.yaml` contains two main sections: `inputs` and `var_groups`.\n\nThe `inputs` section defines what ID’s are in each table and their mapping. Currently these are all the tables and mappings that were defined in the UI. These tables are used for creating an identity graph and all features related to it.\n\nIf you want to add another table in the future, the table and ID mappings would be added here. Below is an example of the `ORDER_COMPLETED` table we manually mapped in the UI. It consists of the following fields:\n\n| Field | Description |\n| --- | --- |\n| name | alias for the table; the primary reference in the rest of the yaml files |\n| table | `<SCHEMA>.<TABLE_NAME>` |\n| select | column with ID |\n| type | kind of ID |\n| entity | what entity the ID should be mapped to |\n| to\\_default\\_stitcher | `true` unless you decide to use a different ID stitcher |\n| remapping | leave as `null` |\n\n```\n- name: rs_EVENTS_ORDER_COMPLETED\n  app_defaults:\n    table: EVENTS.ORDER_COMPLETED\n    ids:\n        - select: USER_ID\n          type: user_id\n          entity: user\n          to_default_stitcher: true\n        - select: ANONYMOUS_ID\n          type: anonymous_id\n          entity: user\n          to_default_stitcher: true\n  remapping: null\n```\n\nThe `var_groups` section is where custom features are defined, both custom features created in the UI and those added via code in this file. Custom features are organized into groups by entity (in our case only `user`). The entity is like the `group by` variable in a SQL query.\n\nBelow that custom features are defined in the `vars` subsection. Here is the `average_order` feature we created in the UI.\n\n```\n- entity_var:\n    is_feature: true\n    name: average_order\n    description: Average Order Size including shipping, taxes, and discounts\n    select: AVG(TOTAL)\n    from: inputs/rs_EVENTS_ORDER_COMPLETED\n```\n\nA name and description are required for the custom feature and then it is defined using declarative SQL syntax. This allows you to define the custom feature the same way you would if creating a new table with SQL.\n\n## Create a custom predictive feature\n\nJust like in the UI workflow, you must already have defined the feature you want to predict. Therefore we are going to add a new custom feature for large purchases in the last 90 days. **NOTE: Currently predictive features can only be binary (i.e. 1/0)**\n\nA large order is defined here as any order with a `TOTAL` of > $100.\n\nAt the bottom of the `resources.yaml`, add the name and definition for `large_purchase_last_90`.\n\n```\n- entity_var:\n  name: large_purchase_last_90\n  description: Customer that made a purchase of >$100 in the last 90 days.\n  select: CASE WHEN MAX(TOTAL) > 100 THEN 1 ELSE 0 END\n  from: inputs/re_EVENTS_ORDER_COMPLETED\n  where: DATEDIFF(days, TIMESTAMP, CURRENT_DATE) <= 90\n```\n\nYou can use SQL functions and keywords in the definition. FOr example, a `CASE` statement in the SELECT statement and add a `where` statement and use the `DATEDIFF` function. You can also use the alias for the `ORDER_COMPLETED` table in the `from` statement.\n\nFor more details on Profiles and project file structure, you can review the Profiles [documentation](https://www.rudderstack.com/docs/profiles/overview/).\n\n## Organize the project in two files (**OPTIONAL**)\n\nProfiles does not need specific yaml files in the `models` folder in order to run. That allows you to organize your code as you feel is best. You can keep it all in one file or can split it over multiple files.\n\nYou can split the `resources.yaml` file into `inputs.yaml` and `profiles.yaml` by creating the two yaml files. Then copy everything from the `inputs` section into `inputs.yaml` and `var_groups` into `profiles.yaml`.\n\nOnce done, you can delete the `resources.yaml`.\n\n## Add a custom predictive feature\n\nThis section explains how to create 2 new custom predictive features from `large_purchase_last_90` called `likelihood_large_purchase_90` (raw score) and `percentile_large_purchase_90`(percentile score).\n\n#### Add Python ML requirement\n\nIn order to add custom predictive features, add the `profiles-pycorelib` package to the project requirements. At the bottom of `pb_project.yaml` add the following code to `pb_project.yaml`.\n\n```\npython_requirements:\n  - profiles-pycorelib==0.2.1\n```\n\n#### Create ml\\_models.yaml\n\nNow, create a new file and name it `ml_models.yaml`. This file is where you can define 2 new custom predictive features and how to train the ML model. The code for these new predictive features is discussed below.\n\nThis file is organized by the predictive model created for predictive features, not the individual features. The top level consists of:\n\n| Field/Section | Description |\n| --- | --- |\n| `name` | Name of the model (not feature) |\n| `model_type` | `python_model` |\n| `model_spec` | All of the model specifications |\n\n* * *\n\n`model_spec` section:\n\n| Section | Description |\n| --- | --- |\n| `train` | Training configuration |\n| `predict` | Scoring configuration |\n\n```\nmodels:\n    - name: &model_name large_purchase_90_model\n      model_type: python_model\n      model_spec:\n        occurred_at_col: insert_ts\n        entity_key: user\n        validity_time: 24h\n        py_repo_url: git@github.com:rudderlabs/rudderstack-profiles-classifier.git # Model training and scoring repo\n\n        train:\n          file_extension: .json\n          file_validity: 2160h # 90 days; when the model will be retrained\n          inputs: &inputs\n            - packages/base_features/models/rudder_user_base_features # location of the base features created in the UI\n            - packages/large_purchase_last_90 # custom feature created in var_groups\n            - models/average_order # custom feature we created in the UI\n          config:\n            data: &model_data_config\n              package_name: feature_table\n              label_column: large_purchase_last_90 # target feature\n              label_value: 1 # target feature value predicting\n              prediction_horizon_days: 90 # how far into the future\n              features_profiles_model:  'rudder_user_base_features' #taken from inputs\n              output_profiles_ml_model: *model_name\n              eligible_users: 'large_purchase_last_90 is not null' # limit training data to those with non-null values\n              inputs: *inputs\n            preprocessing: &model_prep_config\n              ignore_features: [first_name, last_name, state] # features we do not used in a model\n\n        predict:\n          inputs: *inputs # copy from train\n          config:\n            data: *model_data_config # copy from train\n            preprocessing: *model_prep_config # copy from train\n            outputs:\n              column_names:\n                percentile: &percentile percentile_large_purchase_90 # name of percentile feature\n                score: &raw_score likelihood_large_purchase_90 # name of raw likelihood feature\n              feature_meta_data: &feature_meta_data\n                features:\n                  - name: *percentile\n                    description: 'Percentile of likelihood score. Higher the score the more likely to make a larger purchase'\n                  - name: *raw_score\n                    description: 'Raw likelihood score. Higher the score the more likely to make a larger purchase'\n\n        <<: *feature_meta_data\n```\n\n## Compile and run\n\nSave all files. Now compile the project, this will make sure all SQL and python files are able to be created.\n\nFinally, run the project. This will generate the same files as `compile` and then execute them in Snowflake. The first run can take at least 30 minutes because of training ML models.\n\n## Final table\n\nThe final predictive features can be found in your Snowflake environment together in the same table. The table will provide you with the unified user ID, created by RudderStack, when the features are valid as of (i.e. when the model was last run to create these features), and model ID, and your predictive features.\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/final-table.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/final-table.webp)\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Custom Predictive Features | RudderStack Docs",
    "description": "Code your own custom predictive features",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.11/example/entity-traits-360/",
    "markdown": "# Entity Traits 360 | RudderStack Docs\n\nStep-by-step tutorial on creating an entity traits 360 model.\n\n* * *\n\n*     9 minute read  \n    \n\nOnce you have done [identity stitching](https://www.rudderstack.com/docs/archive/profiles/0.11/identity-stitching/) to unify the identity of your users across all the cross-platforms, you can evaluate and maintain the required features/traits for each identified user in a entity traits 360.\n\nThis guide provides a detailed walkthrough on how to use a PB project and create output tables in a warehouse for an entity traits 360 model.\n\n## Prerequisites\n\n*   A basic Profile Builder project by following the [Profile Builder CLI](https://www.rudderstack.com/docs/archive/profiles/0.11/get-started/profile-builder/) steps.\n*   [Structure](https://www.rudderstack.com/docs/archive/profiles/0.11/developer-guides/structure/) of a Profile Builder project and the parameters used in different files.\n*   [Identity Stitching](https://www.rudderstack.com/docs/archive/profiles/0.11/example/id-stitcher/) model as Entity Traits 360 reuses its output to extract the required features/traits.\n\n## Entity Traits 360 Models\n\nOnce traits are defined on an entity, we need the means to use them.\n\nA primary application to send them to the downstream destinations. The destination could either be the [Activation API](https://www.rudderstack.com/docs/archive/profiles/0.11/activation-api/) or via any of the [rETL destinations](https://www.rudderstack.com/docs/sources/reverse-etl/) that RudderStack supports. Each such destination requires data in the form of a table with an ID column and 1 or more feature columns. This is possible using Entity Traits 360.\n\nEntity Traits 360 provides a way to access entity features based on any of the given ID types, including the entity main ID as the identifier column. It creates a view which will have all or a specified set of features on that entity from across the project.\n\nTo configure the creation of a specific set of traits 360 models, add `serve_traits` section in the entity. You need to provide a list of ID types as `id_served`. Optionally, you can also give a name which specifies the name of the generated model. If you don’t specify a name, it will create the model with a default name. By default, it will add all available features on the entity into the view.\n\nIf you want finer control, you can also include or exclude any features from any models by defining a custom entity traits 360 model and add the reference to the `serve_traits` section like `model_served: models/name_of_custom_traits_360_model`. See below on more examples of this.\n\n### Default entity traits 360 model\n\n`pb_project.yaml`:\n\n```\n...\nentities:\n  - name: user\n    id_types:\n      - user_id\n    serve_traits:\n      - id_served: user_id\n        name: user_id_stitched_features\n        # This will add an entity-traits-360 model with user_id as the identifier with model name user_id_stitched_features.\n        # It will contain all the available features.\n```\n\n**Custom entity traits 360 model:**\n\nThis is an example of custom entity traits 360 model. Here we include or exclude features from models of choice.\n\n`pb_project.yaml`:\n\n```\n...\nentities:\n  - name: user\n    id_types:\n      - user_id\n    serve_traits:\n      - id_served: user_id\n        model_served: models/cart_entity_traits_360\n```\n\n`models/profiles.yaml`:\n\n```\nmodels:\n  - name: cart_entity_traits_360\n    model_type: entity_traits_360\n    model_spec:\n      validity_time: 24h # 1 day\n      entity_key: user\n      id_served: user_id\n      feature_list:\n        - from: packages/pkg/models/cart_table # a table created by package\n          include: [\"*\"] # will include all the traits\n        - from: models/user_var_table\n          include: [\"*\"]\n          exclude: [cart_quantity, purchase_status] # except two, all the other traits will be included\n        - from: models/sql_model\n          include: [lifetime_value] # will include only one trait\n```\n\n## Sample project\n\nThis sample project uses the output of an identity stitching model as an input to create a entity traits 360. The following sections describe how to define your PB project files:\n\n### Project detail\n\nThe [`pb_project.yaml`](https://www.rudderstack.com/docs/archive/profiles/0.11/developer-guides/structure/#project-details) file defines the project details such as name, schema version, connection name and the entities which represent different identifiers.\n\nYou can define all the identifiers from different input sources you want to stitch together as a `user_main_id`:\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> You need to add `main_id` to the list only if you have defined `main_id_type: main_id` in the ID stitcher buildspec.\n\n```\n# Project name\nname: sample_id_stitching\n# Project's yaml schema version\nschema_version: 54\n# Warehouse connection\nconnection: test\n# Allow inputs without timestamps\ninclude_untimed: true\n# Folder containing models\nmodel_folders:\n  - models\n# Entities in this project and their ids.\nentities:\n  - name: user\n    id_types:\n      - main_id # You need to add ``main_id`` to the list only if you have defined ``main_id_type: main_id`` in the id stitcher buildspec.\n      - user_id # one of the identifier from your data source.\n      - email\n# lib packages can be imported in project signifying that this project inherits its properties from there\npackages:\n  - name: corelib\n    url: \"https://github.com/rudderlabs/profiles-corelib/tag/schema_{{best_schema_version}}\"\n    # if required then you can extend the package definition such as for ID types.\n```\n\n### Input\n\nThe [input file](https://www.rudderstack.com/docs/archive/profiles/0.11/developer-guides/structure/#inputs) file includes the input table references and corresponding SQL for the above-mentioned entities:\n\n```\ninputs:\n- name: rsIdentifies\n  contract: # constraints that a model adheres to\n    is_optional: false\n    is_event_stream: true\n    with_entity_ids:\n      - user\n    with_columns:\n      - name: timestamp\n      - name: user_id\n      - name: anonymous_id\n      - name: email\n  app_defaults:\n    table: rudder_events_production.web.identifies # one of the WH table RudderStack generates when processing identify or track events.\n    occurred_at_col: timestamp\n    ids:\n      - select: \"user_id\" # kind of identity sql to pick this column from above table.\n        type: user_id\n        entity: user # as defined in project file\n        to_default_stitcher: true\n      - select: \"anonymous_id\"\n        type: anonymous_id\n        entity: user\n        to_default_stitcher: true\n      - select: \"lower(email)\" # can use sql.\n        type: email\n        entity: user\n        to_default_stitcher: true\n- name: rsTracks\n  contract:\n    is_optional: false\n    is_event_stream: true\n    with_entity_ids:\n      - user\n    with_columns:\n      - name: timestamp\n      - name: user_id\n      - name: anonymous_id\n  app_defaults:\n    table: rudder_events_production.web.tracks # another table in WH maintained by RudderStack processing track events.\n    occurred_at_col: timestamp\n    ids:\n      - select: \"user_id\"\n        type: user_id\n        entity: user\n        to_default_stitcher: true\n      - select: \"anonymous_id\"\n        type: anonymous_id\n        entity: user\n        to_default_stitcher: true\n```\n\n### Model\n\nProfiles **entity traits 360** model lets you define the specific features/traits you want to evaluate from the huge spread of scattered data in your warehouse tables.\n\nA sample `profiles.yaml` file specifying a entity traits 360 model (`user_profile`):\n\n```\nvar_groups:\n  - name: first_group\n    entity_key: user\n    vars:\n      - entity_var:\n          name: first_seen\n          select: min(timestamp::date)\n          from: inputs/rsTracks\n          where: properties_country is not null and properties_country != ''\n      - entity_var:\n          name: last_seen\n          select: max(timestamp::date)\n          from: inputs/rsTracks\n      - entity_var:\n          name: user_lifespan\n          select: '{{user.Var(\"last_seen\")}} - {{user.Var(\"first_seen\")}}'\n          description: Life Time Value of a customer\n      - entity_var:\n          name: days_active\n          select: count(distinct timestamp::date)\n          from: inputs/rsTracks\n          description: No. of days a customer was active\n      - entity_var:\n          name: campaign_source\n          default: \"'organic'\"\n      - entity_var:\n          name: user_rank\n          default: -1\n      - entity_var:\n          name: campaign_source_first_touch\n          select: first_value(context_campaign_source)\n          window:\n              order_by:\n                  - timestamp asc\n              partition_by:\n                  - main_id\n          from: inputs/rsIdentifies\n          where: context_campaign_source is not null and context_campaign_source != ''\n      - input_var:\n          name: num_c_rank_num_b_partition\n          select: rank()\n          from: inputs/tbl_c\n          default: -1\n          window:\n            partition_by:\n              - '{{tbl_c}}.num_b'\n            order_by:\n              - '{{tbl_c}}.num_c asc'\n          where: '{{tbl_c}}.num_b >= 10'\n      - entity_var:\n          name: min_num_c_rank_num_b_partition\n          select: min(num_c_rank_num_b_partition)\n          from: inputs/tbl_c\n```\n\n**`entity_var`**\n\nThe `entity_var` field provides inputs for the entity traits 360 model. This variable stores the data temporarily, however, you can choose to store its data permanently by specifying the `name` in it as a feature in the `features` key.\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `name` | String | Name of the `entity_var` to identify it uniquely. |\n| `select` | String | Column name/value you want to select from the table. This defines the actual value that will be stored in the variable. You can use simple SQL expressions or select an `entity_var` as `{{entityName.Var(\\\"entity_var\\\")}}`. It has to be an aggregate operation that ensures the output is a unique value for a given `main_id`. For example: min(timestamp), count(\\*), sum(amount) etc. This holds true even when a window function (optional) is used. For example:: first\\_value(), last\\_value() etc are valid while rank(), row\\_number(), etc. are not valid and give unpredictable results. |\n| `from` | List | Reference to the source table from where data is to be fetched. You can either refer to another model from the same YAML or some other table specified in input YAML. |\n| `where` | String | Any filters you want to apply on the input table before selecting a value. This must be SQL compatible and should consider the data type of the table. |\n| `default` | String | Default value in case no data matches the filter. When defining default values, make sure you enclose the string values in single quotes followed by double quotes to avoid SQL failure. However, you can use the non-string values without any quotes. |\n| `description` | String | Textual description of the `entity_var`. |\n| `window` | Object | Specifies the window function. Window functions in SQL usually have both `partition_by` and `order_by` properties. But for `entity_var`, `partition_by` is added with `main_id` as default; so, adding `partition_by` manually is not supported. If you need partitioning on other columns too, check out `input_var` where `partition_by` on arbitrary and multiple columns is supported. |\n\n**`input_var`**\n\nThe syntax of `input_var` is similar to `entity_var`, with the only difference that instead of each value being associated to a row of the entity traits 360, it’s associated with a row of the specified input. While you can think of an `entity_var` as adding a helper column to the entity traits 360, you can consider an `input_var` as adding a helper column to the input.\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `name` | String | Name to store the retrieved data. |\n| `select` | String | Data to be stored in the name. |\n| `from` | List | Reference to the source table from where data is to be fetched. |\n| `where` | String | (Optional) Applies conditions for fetching data. |\n| `default` | String | (Optional) Default value for any entity for which the calculated value would otherwise be NULL. |\n| `description` | String | (Optional) Textual description. |\n| `window` | Object | (Optional) Specifies a window over which the value should be calculated. |\n\n**`window`**\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `partition_by` | String | (Optional) List of SQL expressions to use in partitioning the data. |\n| `order_by` | String | (Optional) List of SQL expressions to use in ordering the data. |\n\nIn window option, `main_id` is not added by default, it can be any arbitrary list of columns from the input table. So if a feature should be partitioned by `main_id`, you must add it in the `partition_by` key.\n\n### Output\n\nAfter [running the project](https://www.rudderstack.com/docs/archive/profiles/0.11/get-started/profile-builder/#7-generate-output-tables), you can view the generated material tables.\n\n1.  Log in to your Snowflake console.\n2.  Click **Worksheets** from the top navigation bar.\n3.  In the left sidebar, click **Database** and the corresponding **Schema** to view the list of all tables. You can hover over a table to see the full table name along with its creation date and time.\n4.  Write a SQL query like `select * from <table_name>` and execute it to see the results:\n\n1.  Open [Postico2](https://eggerapps.at/postico2/). If required, create a new connection by entering the relevant details. Click **Test Connection** followed by **Connect**.\n2.  Click the **+** icon next to **Queries** in the left sidebar.\n3.  You can click **Database** and the corresponding schema to view the list of all tables/views.\n4.  Double click on the appropriate view name to paste the name on an empty worksheet.\n5.  You can prefix `SELECT *` from the view name pasted previously and suffix `LIMIT 10;` at the end.\n6.  Press Cmd+Enter keys, or click the **Run** button to execute the query.\n\n1.  Enter your Databricks workspace URL in the web browser and log in with your username and password.\n2.  Click the **Catalog** icon in left sidebar.\n3.  Choose the appropriate catalog from the list and click on it to view contents.\n4.  You will see list of tables/views. Click the appropriate table/view name to paste the name on worksheet.\n5.  You can prefix `SELECT * FROM` before the pasted view name and suffix `LIMIT 10;` at the end.\n6.  Select the query text. Press Cmd+Enter, or click the **Run** button to execute the query.\n\n1.  Log in to your [Google Cloud Console](https://console.cloud.google.com/)\n2.  Search for Bigquery in the search bar.\n3.  Select Bigquery from **Product and Pages** to open the Bigquery **Explorer**.\n4.  Select the correct project from top left drop down menu.\n5.  In the left sidebar, click the project ID, then the corresponding dataset view list of all the tables and views.\n6.  Write a SQL query like `select * from <table_name> limit 10;` and execute it to see the results.\n\nA sample output containing the results in Snowflake:\n\n![Generated table (Snowflake)](https://www.rudderstack.com/docs/images/profiles/profiles-feature-table.webp)\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Entity Traits 360 | RudderStack Docs",
    "description": "Step-by-step tutorial on creating an entity traits 360 model.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.11/developer-guides/structure/",
    "markdown": "# Project Structure | RudderStack Docs\n\nKnow the detailed PB project structure, configuration files and their parameters.\n\n* * *\n\n*     7 minute read  \n    \n\nOnce you complete the [Profile Builder CLI](https://www.rudderstack.com/docs/archive/profiles/0.11/get-started/profile-builder/) steps, you will be able to see the Profile Builder project on your machine.\n\nThis guide explains the configuration files structure along with their fields and description:\n\n[![Project structure](https://www.rudderstack.com/docs/images/profiles/project-structure.webp)](https://www.rudderstack.com/docs/images/profiles/project-structure.webp)\n\n## `pb_project.yaml`\n\nThe `pb_project.yaml` file contains the project details like the name, schema version, warehouse connection, [entityEntity refers to a digital representation of a class of real world distinct objects for which you can create a profile.](https://www.rudderstack.com/docs/resources/glossary/#entity) names along with ID types, etc.\n\nA sample `pb_project.yaml` file with entity type as `user`:\n\n```\nname: sample_attribution\nschema_version: 54\nconnection: test\ninclude_untimed: true\nmodel_folders:\n  - models\nentities:\n  - name: user\n    id_types:\n      - main_id\n      - user_id\n      - anonymous_id\n      - email\npackages:\n  - name: corelib\n    url: \"https://github.com/rudderlabs/rudderstack-profiles-corelib/tag/schema_{{best_schema_version}}\"\n\n# Profiles can also use certain model types defined in Python.\n# Examples include ML models. Those dependencies are specified here.\npython_requirements:\n  - profiles-pycorelib==0.1.0\n```\n\nThe following table explains the fields used in the above file:\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `name` | String | Name of the project. |\n| `schema_version` | Integer | Project’s YAML version. Each new schema version comes with improvements and added functionalities. |\n| `connection` | String | Connection name from [`siteconfig.yaml`](https://www.rudderstack.com/docs/archive/profiles/0.11/developer-guides/site-configuration/) used for connecting to the warehouse. |\n| `include_untimed` | Boolean | Determines if inputs having no timestamps should be allowed. If true, data without timestamps is included when running the models. |\n| `model_folders` | String | Names of folders where model files are stored. |\n| [`entities`](#entities) | List | Lists all the entities used in the project for which you can define models. Each entry for an entity here is a JSON object specifying entity’s name and attributes. |\n| `packages` | List | List of packages with their name and URL. Optionally, you can also extend ID types filters for including or excluding certain values from this list. |\n\n##### `entities`\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `name` | String | Name of the entity used in the project. |\n| [`id_types`](#id_types) | List | List of all identifier types associated with the current entity. |\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> The identifiers listed in `id_types` may have a many-to-one relationship with an entity but each ID must belong to a single entity.\n> \n> For example, a `user` entity might have `id_types` as the `salesforce_id`, `anonymous_id`, `email`, and `session_id` (a user may have many session IDs over time). However, it should not include something like `ip_address`, as a single IP can be used by different users at different times and it is not considered as a user identifier.\n\n##### `packages`\n\nYou can import library packages in a project signifying where the project inherits its properties from.\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `name` | String | Specify a name. |\n| `url` | String | HTTPS URL of the lib package, with a tag for the best schema version. |\n\n## `inputs.yaml`\n\nThe `inputs.yaml` file lists all the input sources (tables/views) which should be used to obtain values for [models](#models) and eventually create output tables.\n\nIt also specifies the table/view along with column name and SQL expression for retrieving values. The input specification may also include metadata, and the constraints on those columns.\n\nA sample `inputs.yaml` file:\n\n```\ninputs:\n  - name: salesforceTasks\n    contract:\n      is_optional: false\n      is_event_stream: true\n      with_entity_ids:\n        - user\n      with_columns:\n        - name: activitydate\n        - name: whoid\n    app_defaults:\n      table: salesforce.task\n      # For BigQuery, it is recommended to use view (view: _views_<view_name>) instead of table for event streaming data sets.\n      occurred_at_col: activitydate\n      ids:\n        # column name or sql expression\n        - select: \"whoid\" \n          type: salesforce_id\n          entity: user\n          to_default_stitcher: true\n  - name: salesforceContact\n    contract:\n      is_optional: false\n      is_event_stream: true\n      with_entity_ids:\n        - user\n      with_columns:\n        - name: createddate\n        - name: id\n        - name: email\n    app_defaults:\n      table: salesforce.contact \n      # For BigQuery, it is recommended to use view (view: _views_<view_name>) instead of table for event streaming data sets.\n      occurred_at_col: createddate\n      ids:\n        - select: \"id\"\n          type: salesforce_id\n          entity: user\n          to_default_stitcher: true\n        - select: \"case when lower(email) like any ('%gmail%', '%yahoo%') then lower(email)  else split_part(lower(email),'@',2) end\"\n          type: email\n          entity: user\n          to_default_stitcher: true\n  - name: websitePageVisits\n    contract:\n      is_optional: false\n      is_event_stream: true\n      with_entity_ids:\n        - user\n      with_columns:\n        - name: timestamp\n        - name: anonymous_id\n        - name: context_traits_email\n        - name: user_id\n    app_defaults:\n      table: autotrack.pages\n      # For BigQuery, it is recommended to use view (view: _views_<view_name>) instead of table for event streaming data sets.\n      occurred_at_col: timestamp\n      ids:\n        - select: \"anonymous_id\"\n          type: rudder_anon_id\n          entity: user\n          to_default_stitcher: true\n        # below sql expression check the email type, if it is gmail and yahoo return email otherwise spilt email return domain of email.  \n        - select: \"case when lower(coalesce(context_traits_email, user_id)) like any ('%gmail%', '%yahoo%') then lower(coalesce(context_traits_email, user_id))  \\\n              else split_part(lower(coalesce(context_traits_email, user_id)),'@',2) end\"\n          type: email\n          entity: user\n          to_default_stitcher: true\n```\n\nFor BigQuery, RudderStack recommends you to use a view instead of table for streaming data sets.\n\nThe following table explains the fields used in the above file:\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `name` | String | Name of the input model. |\n| `contract` | Dictionary | A model contract provides essential information about the model like the necessary columns and entity IDs that it should contain. This is crucial for other models that depend on it, as it helps find errors early and closer to the point of their origin. |\n| `app_defaults` | Dictionary | Values that input defaults to when you run the project directly. For library projects, you can remap the inputs and override the app defaults while importing the library projects. |\n\n##### `contract`\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `is_optional` | Boolean | Whether the model’s existence in the warehouse is mandatory. |\n| `is_event_stream` | Boolean | Whether the table/view is a series/stream of events. A model that has a `timestamp` column is an event stream model. |\n| `with_entity_ids` | List | List of all entities with which the model is related. A model M1 is considered related to model M2 if there is an ID of model M2 in M1’s output columns. |\n| `with_columns` | List | List of all ID columns that this contract is applicable for. |\n\n##### `app_defaults`\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `table`/`view` | String | Name of the warehouse table/view containing the data. You can prefix the table/view with an external schema or database in the same warehouse, if applicable. Note that you can specify either a table or view but not both. |\n| `occurred_at_col` | String | Name of the column in table/view containing the timestamp. |\n| [`ids`](#ids) | List | Specifies the list of all IDs present in the source table along with their column names (or column SQL expressions).<br><br>**Note**: Some input columns may contain IDs of associated entities. By their presence, such ID columns associate the row with the entity of the ID. The ID Stitcher may use these declarations to automatically discover ID-to-ID edges. |\n\n##### `ids`\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `select` | String | Specifies the column name to be used as the identifier. You can also specify a SQL expression if some transformation is required.<br><br>**Note**: You can also refer table from another Database/Schema in the same data warehouse. For example, `table: <database_name>.<schema_name>.<table_name>`. |\n| `type` | String | Type of identifier. All the ID types of a project are declared in [`pb_project.yaml`](#project-details). You can specify additional filters on the column expression.<br><br>**Note**: Each ID type is linked only with a single entity. |\n| `entity` | String | Entity name defined in the [`pb_project.yaml`](#project-details) file to which the ID belongs. |\n| `to_default_stitcher` | Boolean | Set this **optional** field to `false` for the ID to be excluded from the default ID stitcher. |\n\n## `profiles.yaml`\n\nThe `profiles.yaml` file lists entity\\_vars / input\\_vars used to create the output tables under `var_groups:`.\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `name` | String | A unique name for the var\\_group. |\n| `entity_key` | String | The entity to which the var\\_group belongs to. |\n| `vars` | Object | This section is used to specify variables, with the help of `entity_var` and `input_var`. Aggregation on stitched ID type is done by default and is implicit. |\n\nOptionally, you can create models using the above vars. The following fields are common for all the model types:\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `name` | String | Name of the model. Note that a table with the same name is created in the data warehouse. For example, if you define the name as `user_table`, the output table will be named something like `Material_user_table_<rest-of-generated-hash>_<timestamp-number>`. |\n| `model_type` | String | Defines the type of model. Possible values are: `id_stitcher`, `feature_table_model`, and `sql_template`. |\n| `model_spec` | Object | Creates a detailed configuration specification for the target model. Different schema is applicable for different model types as explained in each section below. |\n\nRudderStack supports the following model types:\n\n*   [Entity Traits 360 / Feature Table](https://www.rudderstack.com/docs/archive/profiles/0.11/example/feature-table/)\n*   [SQL Template](https://www.rudderstack.com/docs/archive/profiles/0.11/example/sql-model/)\n*   [ID Stitcher](https://www.rudderstack.com/docs/archive/profiles/0.11/example/id-stitcher/)\n*   [ID Collator](https://www.rudderstack.com/docs/archive/profiles/0.11/example/id-collator/)\n*   [Python Models](https://www.rudderstack.com/docs/archive/profiles/0.11/predictions/python-models/)\n*   [Packages](https://www.rudderstack.com/docs/archive/profiles/0.11/example/packages/)\n\n## `README.md`\n\nThe `README.md` file provides a quick overview on how to use PB along with SQL queries for data analysis.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Project Structure | RudderStack Docs",
    "description": "Know the detailed PB project structure, configuration files and their parameters.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.11/example/sql-model/",
    "markdown": "# SQL Models | RudderStack Docs\n\nStep-by-step tutorial on how to create a SQL Template model.\n\n* * *\n\n*     5 minute read  \n    \n\nThis guide provides a detailed walkthrough on how to use a PB project and create SQL Template models using custom SQL queries.\n\n## Prerequisites\n\n*   Familiarize yourself with:\n    \n    *   A basic Profile Builder project by following the [Profile Builder CLI](https://www.rudderstack.com/docs/archive/profiles/0.11/get-started/profile-builder/) steps.\n    *   [Structure](https://www.rudderstack.com/docs/archive/profiles/0.11/developer-guides/structure/) of a Profile Builder project and the parameters used in different files.\n\n## Sample project\n\nThe following sections describe how to define your PB project files:\n\n### Project detail\n\nThe [`pb_project.yaml`](https://www.rudderstack.com/docs/archive/profiles/0.11/developer-guides/structure/#project-details) file defines the project details such as name, schema version, connection name and the entities which represent different identifiers.\n\nYou can define all the identifiers from different input sources you want to stitch together as a single ID (`main_id` in this example):\n\n```\nname: sample_test\nschema_version: 54\nconnection: test\nmodel_folders:\n  - models\nentities:\n  - name: user\n    id_stitcher: models/test_id__\n    id_types:\n      - test_id\n      - exclude_id\ninclude_untimed: true\nid_types:\n  - name: test_id\n    filters:\n      - type: include\n        regex: \"([0-9a-z])*\"\n      - type: exclude\n        value: \"\"\n  - name: exclude_id\n```\n\n### Input\n\nThe [input file](https://www.rudderstack.com/docs/archive/profiles/0.11/developer-guides/structure/#inputs) (`models/inputs.yaml`) file includes the input table references and corresponding SQL for the above-mentioned entities:\n\n```\ninputs:\n  - name: tbl_a\n    app_defaults:\n      table: Temp_tbl_a\n    occurred_at_col: insert_ts\n    ids:\n      - select: TRIM(COALESCE(NULL, id1))\n        type: test_id\n        entity: user\n        to_default_stitcher: true\n      - select: \"id2\"\n        type: test_id\n        entity: user\n        to_default_stitcher: true\n      - select: \"id3\"\n        type: exclude_id\n        entity: user\n        to_default_stitcher: true\n  - name: tbl_b\n    app_defaults:\n      view: Temp_view_b\n    occurred_at_col: timestamp\n    ids:\n      - select: \"id1\"\n        type: test_id\n        entity: user\n        to_default_stitcher: true\n      - select: \"id2\"\n        type: test_id\n        entity: user\n        to_default_stitcher: true\n      - select: \"id3\"\n        type: test_id\n        entity: user\n        to_default_stitcher: true\n  - name: tbl_c\n    app_defaults:\n      table: Temp_tbl_c\n    ids:\n      - select: \"id1\"\n        type: test_id\n        entity: user\n        to_default_stitcher: true\n      - select: \"id2\"\n        type: test_id\n        entity: user\n        to_default_stitcher: true\n```\n\n### Model\n\nProfiles **SQL model** lets you write custom SQL queries to achieve advanced use-cases to create desired output tables.\n\nA sample `profiles.yaml` file specifying a SQL model (`test_sql`):\n\n```\nmodels:\n- name: test_sql\n  model_type: sql_template\n  model_spec:\n    validity_time: 24h# 1 day\n    materialization:                 // optional\n      run_type: discrete             // optional [discrete, incremental]\n    single_sql: |\n        {%- with input1 = this.DeRef(\"inputs/tbl_a\") -%}\n          select id1 as new_id1, id2 as new_id2, {{input1}}.*\n            from {{input1}}\n        {%- endwith -%}        \n    occurred_at_col: insert_ts        // optional\n    ids:\n      - select: \"new_id1\"\n        type: test_id\n        entity: user\n        to_default_stitcher: true\n      - select: \"new_id2\"\n        type: test_id\n        entity: user\n        to_default_stitcher: true\n      - select: \"id3\"\n        type: test_id\n        entity: user\n        to_default_stitcher: true\n```\n\n##### Model specification fields\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `validity_time` | Time | Time Specifies the validity of the model with respect to its timestamp. For example, a model run as part of a scheduled nightly job for 2009-10-23 00:00:00 UTC with `validity_time`: `24h` would still be considered potentially valid and usable for any run requests, which do not require precise timestamps between 2009-10-23 00:00:00 UTC and 2009-10-24 00:00:00 UTC. This specifies the validity of generated feature table. Once the validity is expired, scheduling takes care of generating new tables. For example: 24h for 24 hours, 30m for 30 minutes, 3d for 3 days. |\n| `materialization` | List | Adds the key `run_type`: `incremental` to run the project in incremental mode. This mode considers row inserts and updates from the edge\\_sources input. These are inferred by checking the timestamp column for the next run. One can provide buffer time to consider any lag in data in the warehouse for the next incremental run like if new rows are added during the time of its run. If you do not specify this key then it’ll default to `run_type`: `discrete`. |\n| `single_sql` | List | Specifies the SQL template which must evaluate to a single SELECT SQL statement. After execution, it should produce a dataset which will materialize based on the provided materialization. |\n| `multi-sql` | List | Specifies the SQL template which can evaluate to multiple SQL statements. One of these SQL statements (typically the last one) must be a CREATE statement which shall be responsible for materializing the model into a table.<br><br>**Note**: You should set only one of `single_sql` or `multi_sql`. |\n| `occurred_at_col` | List | Name of the column which contains the timestamp value in the output of sql template. |\n| `ids` | List | Specifies the list of all IDs present in the source table along with their column names (or column SQL expressions). It is required in case you want to use SQL models as an input to the `input_var` or `entity_var` fields. |\n\n## SQL template\n\nYou can pass custom SQL queries to the `single_sql` or `multi_sql` fields, which is also known as a **SQL template**. It provides the flexibility to write custom SQL by refering to any of the input sources listed in the `inputs.yaml` or any model listed in `models/profiles.yaml`.\n\nThe SQL templates follow a set query syntax which serves the purpose of creating a model. Follow the below rules to write SQL templates:\n\n*   Write SQL templates in the [pongo2 template engine](https://pkg.go.dev/github.com/flosch/pongo2#readme-first-impression-of-a-template) syntax.\n    \n*   Avoid circular referencing while referencing the models. For example, `sql_model_a` references `sql_model_b` and `sql_model_b` references `sql_model_a`.\n    \n*   Use `timestamp` variable (refers to the start time of the current run) to filter new events.\n    \n*   `this` refers to the current model’s material. You can use the following methods to access the material properties available for `this`:\n    \n    *   `DeRef(\"path/to/model\")`: Use this syntax `{{ this.DeRef(\"path/to/model\") }}` to refer to any model and return a database object corresponding to that model. The database object, in return, gives the actual name of the table/view in the warehouse. Then, generate the output, for example:\n    \n    ```\n    {% with input_table = this.DeRef(\"inputs/tbl_a\") %}\n        select a as new_a, b as new_b, {{input_table}}.*\n          from {{input_table}}\n    {% endwith %}\n    ```\n    \n    *   `GetMaterialization()`: Returns a structure with two fields: `MaterializationSpec{OutputType, RunType}`.\n        *   `OutputType`: You must use `OutputType` with `ToSQL()` method:  \n            For example, `CREATE OR REPLACE {{this.GetMaterialization().OutputType.ToSQL()}} {{this.GetSelectTargetSQL()}} AS ...`\n        *   `RunType`: For example, `this.GetMaterialization().RunType`\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "SQL Models | RudderStack Docs",
    "description": "Step-by-step tutorial on how to create a SQL Template model.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.12/faq/",
    "markdown": "# Profiles FAQ | RudderStack Docs\n\nCommonly asked questions on RudderStack Profiles.\n\n* * *\n\n*     28 minute read  \n    \n\nThis guide contains solutions for some of the commonly asked questions on Profiles. For queries or issues not listed in this guide, contact [RudderStack Support](mailto:support@rudderstack.com).\n\n## Setup and installation\n\n**I have installed Python3, yet when I install and execute `pb` it doesn’t return anything on screen.**\n\nTry restarting your Terminal/Shell/PowerShell and try again.\n\nYou can also try to find the location of your Python executable. PB would be installed where the executables embedded in other Python packages are installed.\n\n**I am an existing user who updated to the new version and now I am unable to use the PB tool. On Windows, I get the error:** `'pb' is not recognized as an internal or external command, operable program or batch file.`\n\nExecute the following commands to do a fresh install:\n\n1.  `pip3 uninstall profiles-rudderstack-bin`\n2.  `pip3 uninstall profiles-rudderstack`\n3.  `pip3 install profiles-rudderstack --no-cache-dir`\n\n**I am unable to download, getting** `ERROR: Package 'profiles-rudderstack' requires a different Python: 3.7.10 not in '>=3.8, <=3.10'`\n\nUpdate your Python 3 to a version greater than or equal to 3.8 and less than or equal to 3.10.\n\n**I am unable to download profile builder by running `pip3 install profiles-rudderstack` even though I have Python installed.**\n\nFirstly, make sure that Python3 is correctly installed. You can also try to substitute `pip3` with `pip` and execute the install command.\n\nIf that doesn’t work, it’s high likely that Python3 is accessible from a local directory.\n\n1.  Navigate to that directory and try the install command again.\n2.  After installation, PB should be accessible from anywhere.\n3.  Validate that you’re able to access the path using `which pb`.\n4.  You may also execute `echo $PATH` to view current path settings.\n5.  In case it doesn’t show the path then you can find out where |ProductName| is installed using :substitution-code:`pip3 show profiles-rudderstack`. This command will display a list of the files associated with the application, including the location in which it was installed, navigate to that directory.\n6.  Navigate to `/bin` subdirectory and execute command `ls` to confirm that `pb` is present there.\n7.  To add the path of the location where PB is installed via pip3, execute: `export PATH=$PATH:<path_to_application>`. This will add the path to your system’s PATH variable, making it accessible from any directory. It is important to note that the path should be complete and not relative to the current working directory.\n\nIf you still face issues, then you can try to install it manually. [Contact us](mailto:support@rudderstack.com) for the executable file and download it on your machine. Follow the below steps afterwards:\n\n1.  Create `rudderstack` directory: `sudo mkdir /usr/local/rudderstack`.\n2.  Move the downloaded file to that directory: `sudo mv <name_of_downloaded_file> /usr/local/rudderstack/pb`.\n3.  Grant executable permission to the file: `chmod +x /usr/local/rudderstack/pb`.\n4.  Navigate to directory `/usr/local/rudderstack` from your file explorer. Ctrl+Click on pb and select **Open** to run it from Terminal.\n5.  Symlink to a filename pb in `/usr/local/bin` so that command can locate it from env PATH. Create file if it does not exist: `sudo touch /usr/local/bin/pb`. Then execute`sudo ln -sf /usr/local/rudderstack/pb /usr/local/bin/pb`.\n6.  Verify the installation by running `pb` in Terminal. In case you get error `command not found: pb` then check if `/usr/local/bin` is defined in PATH by executing command: `echo $PATH`. If not, then add `/usr/local/bin` to PATH.\n\n1.  If the Windows firewall prompts you after downloading, proceed with `Run Anyway`.\n2.  Rename the executable as `pb`.\n3.  Move the file to a safe directory such as `C:\\\\Program Files\\\\Rudderstack`, create the directory if not present.\n4.  Set the path of `pb.exe` file in environment variables.\n5.  Verify the installation by running `pb` in command prompt.\n\n**When I try to install Profile Builder tool using pip3 I get error message saying: `Requirement already satisfied`**\n\nTry the following steps:\n\n1.  Uninstall PB using `pip3 uninstall profiles-rudderstack`.\n2.  Install again using `pip3 install profiles-rudderstack`.\n\nNote that this won’t remove your existing data such as models and siteconfig files.\n\n**I have multiple models in my project. Can I run only a single model?**\n\nYes, you can. In your spec YAML file for the model you don’t want to run, set `materialization` to `disabled`:\n\n```\nmaterialization:\n    enable_status: disabled\n```\n\nA sample `profiles.yaml` file highlighted a disabled model:\n\n```\nmodels:\n- name: test_sql\n  model_type: sql_template\n  model_spec:\n    validity_time: 24h# 1 day\n    materialization:                \n      run_type: discrete\n      enable_status: disabled  // Disables running the model.\n    single_sql: |\n        {%- with input1 = this.DeRef(\"inputs/tbl_a\") -%}\n          select id1 as new_id1, {{input1}}.*\n            from {{input1}}\n        {%- endwith -%}        \n    occurred_at_col: insert_ts\n    ids:\n      - select: \"new_id1\"\n        type: test_id\n        entity: user\n```\n\n**I am facing this error while ugrading my Profiles project: `pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. profiles-pycorelib 0.2.2 requires profiles-rudderstack!=0.10.6,<=0.10.7,>=0.10.5, but you have profiles-rudderstack 0.11.0 which is incompatible.`**\n\nThis is because you must uninstall and then reinstall the `pycorelib` library while upgrading to a recent version.\n\n* * *\n\n## Warehouse permissions\n\n**I have two separate roles to read from input tables and write to output tables? How should I define the roles?**\n\nYou need to create an additional role as a union of those two roles. PB project needs to read the input tables and write the results back to the warehouse schema.\n\nFurthermore, each run is executed using a single role as specified in the `siteconfig.yaml` file. Hence, it is best in terms of security to create a new role which has read as well as write access for all the relevant inputs and the output schema.\n\n**How do I test if the role I am using has sufficient privileges to access the objects in the warehouse?**\n\nYou can use the `pb validate access` command to validate the access privileges on all the input/output objects.\n\n* * *\n\n## Compile command\n\n**I am trying to execute the `compile` command by fetching a repo via GIT URL but getting this error: `making git new public keys: ssh: no key found`**\n\nYou need to add the OpenSSH private key to your `siteconfig.yaml` file. If you get the error `could not find expected` afterwards, try correcting the spacing in your `siteconfig.yaml` file.\n\n**While trying to segregate identity stitching and feature table in separate model files, I am getting this error: `mapping values are not allowed in this context`**\n\nThis is due to the spacing issue in `siteconfig.yaml` file. You may create a new project to compare the spacing. Also, make sure you haven’t missed any keys.\n\n* * *\n\n## Command progress & lifecycle\n\n**I executed a command and it is taking too long. Is there a way to kill a process on data warehouse?**\n\nIt could be due to the other queries running simultaneously on your warehouse. To clear them up, open the **Queries** tab in your warehouse and manually kill the long running processes.\n\n**Due to the huge data, I am experiencing long execution times. My screen is getting locked, thereby preventing the process from getting completed. What can I do?**\n\nYou can use the `screen` command on UNIX/MacOS to detach your screen and allow the process to run in the background. You can use your terminal for other tasks, thus avoiding screen lockouts and allowing the query to complete successfully.\n\nHere are some examples:\n\n*   To start a new screen session and execute a process in detached mode: `screen -L -dmS profiles_rn_1 pb run`. Here:\n    *   `-L` flag enables logging.\n    *   `-dmS` starts as a daemon process in detached mode.\n    *   `profiles_rn_1` is the process name.\n*   To list all the active screen sessions: `screen -ls`.\n*   To reattach to a detached screen session: `screen -r [PID or screen name]`.\n\n**The CLI was running earlier but it is unable to access the tables now. Does it delete the view and create again?**\n\nYes, every time you run the project, Profiles creates a new materials table and replaces the view.\n\nHence, you need to grant a select on future views/tables in the respective schema and not just the existing views/tables.\n\n**Does the CLI support downloading a git repo using siteconfig before executing** `pb run` **? Or do I have to manually clone the repo first?**\n\nYou can pass the Git URL as a parameter instead of project’s path, as shown:\n\n**When executing** `run` **command, I get a message:** `Please use sequence number ... to resume this project in future runs` **. Does it mean that a user can exit using Ctrl+C and later if they give this seq\\_no then it’ll continue from where it was cancelled earlier?**\n\nThe `pb run --seq_no <>` flag allows for the provision of a sequence number to run the project. This flag can either resume an existing project or use the same context to run it again.\n\nWith the introduction of time-grain models, multiple sequence numbers can be assigned and used for a single project run.\n\n**What flag should I set to force a run for same end time, even if a previous run exists?**\n\nYou can execute `pb run --force --model_refs models/my_id_stitcher,entity/user/user_var_1,entity/user/user_var_2,...`\n\n**Can the hash change even if schema version did not change?**\n\nYes, as the hash versions depends on project’s implementation while the schema versions are for the project’s YAML layout.\n\n**Is there a way to pick up from a point where my last pb run failed on a subsequent run? For large projects, I don’t want to have to rerun all of the features if something failed as some of these take several hours to run**\n\nYes, you can just execute the run command with the specific sequence number, for example, `pb run —seq_no 8`.\n\n**What is the intent of `pb discover models` and `pb discover materials` command?**\n\nYou can use `pb discover models` to list all the models from registry and `pb discover materials` to list all the materials from the registry.\n\n**I got this while running `pb show models`. What is “Maybe Enabled”?**\n\n[![](https://www.rudderstack.com/docs/images/profiles/show-models.webp)](https://www.rudderstack.com/docs/images/profiles/show-models.webp)\n\nIn the show models command, the enable status is computed without looking at tables in the warehouse. Imagine a model`M` that has an optional input column. So, `M` is enabled if and only if the optional input column is present. Hence, it may or may not be enabled, depending on whether the input column is present or not.\n\n**How can I handle my Profiles project in the development and production workspace in RudderStack?**\n\nProfiles support git branches in the RudderStack dashboard. Refer [Supported Git URLs](https://www.rudderstack.com/docs/archive/profiles/0.12/example/packages/#supported-git-urls) for more information.\n\nIn case you wish to run only one project in the CLI and run them differently in dev and prod, you can use **targets**:\n\n1.  Create a connection using `pb init connection` and give a connection name (say `test`). Then, give a default target name, say _prod_. Enter remaining details.\n2.  Create another connection using `pb init connection` and give the same connection name as before (`test`). Then, give a different target name, say `dev`. Enter remaining connection details for connecting to your warehouse.\n3.  When you execute a command via CLI, you need to pass `-t` flag. The first connection you’ve defined is the default one, hence, you don’t need to pass a flag explicitly. However, you can pass it for the other one. For example, `pb run -t dev`.\n\nTargets aren’t yet supported in the UI. So while you can run the same project on different instances (prod, dev) in the CLI; in the UI you have to make either a different project or a different branch/tag/subfolder.\n\n* * *\n\n## Identity stitching\n\n**There are many large size connected components in my warehouse. To increase the accuracy of stitched data, I want to increase the number of iterations. Is it possible?**\n\nThe default value of the largest diameter, that is, the longest path length in connected components, is 30.\n\nYou can increase it by defining a `max_iterations` key under `model_spec` of your ID stitcher model in `models/profiles.yaml`, and specifying its value as the max diameter of connected components.\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> Note that the algorithm can give incorrect results in case of large number of iterations.\n\n**Do I need to write different query each time for viewing the data of created tables?**\n\nNo, you can instead use a view name, which always points to the latest created material table. For example, if you’ve defined **user\\_stitching** in your `models/profiles.yaml` file, then execute `SELECT * FROM MY_WAREHOUSE.MY_SCHEMA.user_stitching`.\n\n**In my model, I have set the key `validity_time: 24h`. What happens when the validity of generated tables expire? Will re-running the identity stitching model generate the same hash until the validity expires?**\n\nFirstly, hash does not depend on the timestamp, it depends on the yaml in the underlying code. That’s why the material name is `material_name_HASH_SEQNO`. The sequence number (SEQNO) depends on timestamp.\n\nSecondly, a material generated for a specific timestamp (aside for the timeless timestamp) is not regenerated unless you do a `pb run --force`. The CLI checks if the material you are requesting already exists in the database, and if it does, returns that. The `validity_time` is an extension of that.\n\nFor a model with `validity_time: 24h` and inputs having the `timestamp` columns, if you request a material for latest time, but one was generated for that model 5 minutes ago, the CLI will return that one instead. Using the CLI to run a model always generates a model for a certain timestamp, it’s just if you don’t specify a timestamp then it uses the current timestamp.\n\nSo, for a model with validity\\_time (vt), and having the `timestamp` columns, if you request a material for t1, but one already exists for t0 where t1-vt <= t0 <= t1, the CLI will return that one instead.\n\nIf multiple materials exist that satisfy the requirement, then it returns the one with the timestamp closest to t1.\n\n**I want to use `customer_id` instead of `main_id` as the ID type. So I changed the name in `pb_project.yaml`, however now I am getting this error: `Error: validating project sample_attribution: listing models for child source models/: error listing models: error building model domain_profile_id_stitcher: main id type main_id not in project id types`.**\n\nIn addition to making changes in the file `pb_project.yaml` file, you also need to set `main_id_type: customer_id` in the `models/profiles.yaml` file.\n\n**I ran identity stitching model but not able to see the output tables under the list of tables in Snowflake. What might be wrong?**\n\nIn Snowflake, you can check the **Databases** > **Views** dropdown from the left sidebar. For example, if your model name is `domain_profile_id_stitcher`, you should be able to see the table with this name. In case it is still not visible, try changing the role using dropdown menu from the top right section.\n\n**I am using a view as an input source but getting an error that the view is not accessible, even though it exists in DB.**\n\nViews need to be refreshed from time-to-time. You can try recreating the view in your warehouse and also execute a `select *` on the same.\n\n**What might be the reason for following errors:**\n\n*   `processing no result iterator: pq: cannot change number of columns in view`. The output view name already exists in some other project. To fix this, try dropping the view or changing its name.\n    \n*   `creating Latest View of moldel 'model_name': processing no result iterator: pq: cannot change data type of view column \"valid_at\"` Drop the view `domain_profile` in your warehouse and execute the command again.\n    \n*   `processing no result iterator: pq: column \"rudder_id\" does not exist`. This occurs when you execute a PB project with a model name, having `main_id` in it, and then you run another project with the same model name but no `main_id`. To resolve this, try dropping the earlier materials using `cleanup materials` command.\n    \n\n**I have a source table in which `email` gets stored in the column for `user_id`, so the field has a mix of different ID types. I have to tie it to another table where email is a separate field. When doing so, I have two separate entries for email, as type `email` and `user_id`. What should I do?**\n\nYou can implement the following line in the inputs tables in question:\n\n```\n  - select: case when lower(user_id) like '%@%' THEN lower(user_id) else null end\n    type: email \n    entity: user\n    to_default_stitcher: true\n```\n\n**How do I validate the results of identity stitching model?**\n\nContact [RudderStack Support](mailto:support@rudderstack.com) if you need help in validating the clusters.\n\n**Which identifiers would you recommend that I include in the ID stitcher for an ecommerce Profiles project?**\n\nWe suggest including identifiers that are unique for every user and can be tracked across different platforms and devices. These identifiers might include but not limited to:\n\n*   Email ID\n*   Phone number\n*   Device ID\n*   Anonymous ID\n*   User names\n\nThese identifiers can be specified in the file `profiles.yaml` file in the identity stitching model.\n\nRemember, the goal of identity stitching is to create a unified user profile by correlating all of the different user identifiers into one canonical identifier, so that all the data related to a particular user or entity can be associated with that user or entity.\n\n**If I run `--force` with an ID Stitcher model and also pass a `--seq_no` for the most recent run, will it still recreate the full ID Graph? Also, is there a way to know if the model was run incrementally or not?**\n\nThis will re-run the ID stitcher and if it is incremental, it will look for the most recent run of the stitcher. After finding the existing run for that `seq_no`, it will use it as the base. This is because the base for an incremental run could be the current `seq_no`. If you do not want to do this, you can pass the `rebase_incremental` flag.\n\n**I am getting a bunch of NULL `VALID_AT` timestamps. Is it because the table where the data is being referenced from does not have a timestamp fields specified? Will this impact anything in the downstream?**\n\nYes, if there is no timestamp field in the input table (or it is NULL for the row from where the edge source was pulled), then `VALID_AT` column would have NULL value. This only affects the `VALID_AT` column in the final table and nothing in the ID stitching.\n\n**Which identifiers should I include in my inputs.yaml file?**\n\nInclude all the IDs that contribute to the ID stitcher model.\n\n* * *\n\n## Feature Table\n\n**How can I run a feature table without running its dependencies?**\n\nSuppose you want to re-run the user entity\\_var `days_active` and the `rsTracks` input\\_var `last_seen` for a previous run with `seq_no 18`.\n\nThen, execute the following command:\n\n```\npb run --force --model_refs entity/user/days_active,inputs/rsTracks/last_seen --seq_no 18\n```\n\n**I have imported a library project but it throws an error: `no matching model found for modelRef rsTracks in source inputs`.**\n\nYou can exclude the missing inputs of the library project by mapping them to nil in the `pb_project.yaml` file.\n\n**Is it possible to run the feature table model independently, or does it require running alongside the ID stitcher model?**\n\nYou can provide a specific timestamp while running the project, instead of using the default latest time. PB recognizes if you have previously executed an identity stitching model for that time and reuses that table instead of generating it again.\n\nYou can execute a command similar to: `pb run --begin_time 2023-06-02T12:00:00.0Z --end_time 2023-06-03T12:00:00.0Z`. Note that:\n\n*   To reuse a specific identity stitching model, the timestamp value must match exactly to when it was run.\n*   If you have executed identity stitching model in the incremental mode and do not have an exact timestamp for reusing it, you can select any timestamp **greater** than a non-deleted run. This is because subsequent stitching takes less time.\n*   To perform another identity stitching using PB, pick a timestamp (for example, `1681542000`) and stick to it while running the feature table model. For example, the first time you execute `pb run --begin_time 2023-06-02T12:00:00.0Z --end_time 2023-06-03T12:00:00.0Z`, it will run the identity stitching model along with the feature models. However, in subsequent runs, it will reuse the identity stitching model and only run the feature table models.\n\n**While trying to add a feature table, I get an error at line 501, but I do not have these many lines in my YAML.**\n\nThe line number refers to the generated SQL file in the output folder. Check the console for the exact file name with the sequence number in the path.\n\n**While creating a feature table, I get this error:** `Material needs to be created but could not be: processing no result iterator: 001104 (42601): Uncaught exception of type 'STATEMENT ERROR': 'SYS _W. FIRSTNAME' in select clause is neither an aggregate nor in the group by clause.`\n\nThis error occurs when you use a window function `any_value` that requires a window frame clause. For example:\n\n```\n  - entity_var:\n      name: email\n      select: LAST_VALUE(email)\n      from: inputs/rsIdentifies\n      window:\n        order_by: \n        - timestamp desc\n```\n\n**Is it possible to create a feature out of an identifier? For example, I have a RS user\\_main\\_id with two of user\\_ids stitched to it. Only one of the user\\_ids has a purchase under it. Is it possible to show that user\\_id in the feature table for this particular user\\_main\\_id?**\n\nIf you know which input/warehouse table served as the source for that particular ID type, then you can create features from any input and also apply a `WHERE` clause within the entity\\_var.\n\nFor example, you can create an aggregate array of user\\_id’s from the purchase history table, where total\\_price > 0 (exclude refunds, for example). Or, if you have some LTV table with user\\_id’s, you could exclude LTV < 0.\n\n**Is it possible to reference an input var in another input var?**\n\nYes - input vars are similar to adding additional columns to the original table. You can use an input var `i1v1` in the definition of input var `i1v2` as long as both input vars are defined in the same input (or SQL model) `i1`.\n\n**I have not defined any input vars on I1. Why is the system still creating I1\\_var\\_table?**\n\nWhen you define an entity var using I1, an internal input var (for entity’s `main_id`) is created which creates `I1_var_table`. RudderStack team is evaluating whether internal input vars should create the var table or not.\n\n**I have an input model I1. Why is the system creating Material\\_I1\\_var\\_table\\_XXXXXX\\_N?**\n\nThis material table is created to keep the input vars defined on I1.\n\n**I am trying to run a single `entity_var` model. How should I reference it?**\n\nThe right way to reference an entity var is: `entity/<entity-name>/<entity-var-name>`.\n\n**I have two identical named fields in two `user` tables and I want my Profiles project to pick the most recently updated one (from either of the `user` tables). What is the best way to do it?**\n\nDefine different `entity_vars` (one for each input) and then pick the one with a non-null value and higher priority.\n\n**What does running material mean?**\n\nIt means that the output (material) table is being created in your warehouse. For example, an output table named `material_user_id_stitcher_3acd249d_21` would mean:\n\n*   `material`: Prefix for all the objects created by Profiles in your warehouse, such as ID stitcher and feature tables.\n*   `user_id_stitcher`: View created in your schema. It will always point to latest ID stitcher table. This name is the same as defined in the `models/profiles.yaml` file.\n*   `3acd249d`: Unique hash which remains the same for every model unless you make any changes to the model’s config, inputs or the config of model’s inputs.\n*   `21`: Sequence number for the run. It is a proxy for the context timestamp. Context timestamp is used to checkpoint input data. Any input row with `occured_at` timestamp value greater than the context timestamp cannot be used in the associated run.\n\n* * *\n\n## YAML\n\n**Are there any best practices I should follow when writing the PB project’s YAML files?**\n\n*   Use spaces instead of tabs.\n*   Always use proper casing. For example: id\\_stitching, and not id\\_Stitching.\n*   Make sure that the source table you are referring to, exists in data warehouse or data has been loaded into it.\n*   If you’re pasting table names from your Snowflake Console, remove the double quotes in the `inputs.yaml` file.\n*   Make sure your syntax is correct. You can compare with the sample files.\n*   Indentation is meaningful in YAML, make sure that the spaces have same level as given in sample files.\n\n**How do I debug my YAML file step-by-step?**\n\nYou can use the `--model_args` parameter of the `pb run` command to do so. It lets you run your YAML file till a specific feature/tablevar. For example:\n\n```\n$ pb run -p samples/attribution --model_args domain_profile:breakpoint:blacklistFlag\n```\n\nSee [run command](https://www.rudderstack.com/docs/archive/profiles/0.12/cli-user-guide/commands/#run) for more information.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> This is only applicable to versions prior to v0.9.0.\n\n**Can I use double quotes when referencing another entity\\_var in a macro?**\n\nYou can use an escape character. For example:\n\n```\n  - entity_var:\n      name: days_since_last_seen\n      select: \"{{macro_datediff('{{user.Var(\\\"max_timestamp_bw_tracks_pages\\\")}}')}}\"\n```\n\nAlso, if have a case statement, then you can add something like the following:\n\n`select: CASE WHEN {{user.Var(\"max_timestamp_tracks\")}}>={{user.Var(\"max_timestamp_pages\")}} THEN {{user.Var(\"max_timestamp_tracks\")}} ELSE {{user.Var(\"max_timestamp_pages\")}} END`\n\n**Is it possible to define default arguments in macros?**\n\nNo, RudderStack does not support default arguments in macros.\n\n* * *\n\n## ML/Python Models\n\n**Despite deleting WhtGitCache folder and adding keys to siteconfig, I get this error:** `Error: loading project: populating dependencies for project:base_features, model: churn_30_days_model: getting creator recipe while trying to get ProjectFolder: fetching git folder for git@github.com:rudderlabs/rudderstack-profiles-classifier.git: running git plain clone: repository not found`\n\nIf your token is valid, then replace `git@github.com:rudderlabs/rudderstack-profiles-classifier.git` with `https://github.com/rudderlabs/rudderstack-profiles-classifier.git` in the `profile-ml` file.\n\n**Why is my Profiles project taking so long to run?**\n\nThe first Profiles project run usually takes longer, especially if you are building predictive features.\n\n**I am debugging an error in ML models where I see a view with the model name, without material/hash prefix and suffix but it does not get refreshed even after all the entity vars are created and the material\\_<feature\\_table\\_model> table is also created. What might be the reason?**\n\nIt is because this view is now moved to `PostProjectRunCb`, meaning, it is created async after material Force run step.\n\n* * *\n\n## Activation API\n\n**While using Redis destination, I am facing an error: `These sample records were rejected by the destination`**?\n\nThis error is observed if you have enabled **Cluster mode** setting for Redis in the [RudderStack’s configuration settings](https://www.rudderstack.com/docs/destinations/streaming-destinations/redis/#connection-settings) but you are on the Redis free plan.\n\nTo overcome this, ensure that the Redis plan you are using allows clustering. Alternatively, you can turn off the **Cluster mode** setting.\n\n**Does the user-profiles API (old) and activation API (new) behave differently in updating a key that maps to two different primary keys? For example:**\n\n| Primary key | user\\_id | Feature\\_1 | Feature\\_2 |\n| --- | --- | --- | --- |\n| PK1 | U1  | F1  | null |\n| PK2 | U1  | null | F2  |\n\nUser profiles API\n\n```\n{\n  \"userId\": \"U1\",\n  \"profile\": {\n    \"feature_1\": \"F1\",\n    \"feature_2\": \"F2\"\n  }\n}\n```\n\nActivation API\n\n```\n{\n  \"entity\": \"entity_name\",\n  \"id\": {\n    \"type\": \"user_id\",\n    \"value\": \"U1\"\n  },\n  \"data\": {\n    \"model_name\": {\n      \"feature_1\": null,\n      \"feature_2\": F2\n    }\n  }\n}\n```\n\nIn user profiles API, RudderStack updates the value for a specific key (that is, feature\\_1 in this case). In activation API, RudderStack syncs the entire row as value for the `model_name` key.\n\n**Is it possible to use the Activation API without any Profiles project?**\n\nNo, the Activation API can only be used with a Profiles project and not on any of your non-Profiles output tables.\n\n## Profiles UI\n\n**I have included some features in the RudderStack dashboard while creating the Profiles project but when I click “download this project”, my project files does not include any feature. What might be the reason?**\n\nIf you have selected pre-defined features from any library project, they are referred to as `profiles-multieventstream-features` in the project by default.\n\nIf you have created any features using the custom feature functionality, they will be a part of your `pb_profiles.yaml` file. \n\n**While choosing pre-defined features in the RudderStack dashboard, I can preview code for only some of the features. What might be the reason?**\n\nYou can preview the code only for entity var based features. This functionality is not available for features built from ML and SQL models.\n\n**While creating a Profiles project by importing from Git, I dont see any warehouse options in the dropdown selector in the `Validate Profiles project` section. What might be the reason?**\n\nA Profiles project looks for the supported warehouse destinations configured for that workspace. Hence, make sure you have configured any of the following [warehouse destinations](https://www.rudderstack.com/docs/destinations/warehouse-destinations/) in your RudderStack dashboard:\n\n*   Snowflake\n*   Databricks\n*   Redshift\n*   BigQuery\n\n**Why am I not able to see the Concurrency option in the Settings tab of my Profiles project?**\n\nRudderStack supports the **Concurrency** option only for the Snowflake warehouse currently. You will not be able to see this option if you have configured your Profiles project using the Redshift, BigQuery, or Databricks warehouse.\n\n## Miscellaneous\n\n**Why am I getting _Authentication FAILED_ error on my data warehouse while executing the run/compile commands?**\n\nSome possible reasons for this error might be:\n\n*   Incorrect warehouse credentials.\n*   Insufficient user permissions to read and write data. You can ask your administrator to change your role or grant these privileges.\n\n**Why am I getting _Object does not exist or not authorized_ error on running this SQL query: `SELECT * FROM \"MY_WAREHOUSE\".\"MY_SCHEMA\".\"Material_domain_profile_c0635987_6\"`?**\n\nYou must remove double quotes from your warehouse and schema names before running the query, that is `SELECT * FROM MY_WAREHOUSE.MY_SCHEMA.Material_domain_profile_c0635987_6`.\n\n**Is there a way to obtain the timestamp of any material table?**\n\nYes, you can use the `GetTimeFilteringColSQL()` method to get the timestamp column of any material. It filters out rows based on the timestamp. It returns the `occurred_at_col` in case of an event\\_stream table or `valid_at` in case the material has that column. In absense of both, it returns an empty string. For example:\n\n```\n  SELECT * FROM {<from_material>}\n    WHERE\n      <from_material>.GetTimeFilteringColSQL() > <some_timestamp>;\n```\n\n**What is the difference between setting up Profiles in the RudderStack dashboard and Profile Builder CLI tool?**\n\nYou can run Profiles in the RudderStack dashboard or via [Profile Builder CLI](https://www.rudderstack.com/docs/archive/profiles/0.12/get-started/profile-builder/).\n\nThe main difference is that the RudderStack dashboard only generates outputs based on the pre-defined templates. However, you can augment those outputs by downloading the config file and updating it manually.\n\nOn the other hand, the CLI tool lets you achieve the end-to-end flow via creating a Profile Builder project.\n\n**Does the Profiles tool have logging enabled by default for security and compliance purposes?**\n\nLogging is enabled by default for nearly all the commands executed by CLI (`init`, `validate access`, `compile`, `run`, `cleanup`, etc.). Logs for all the output shown on screen are stored in the file `logfile.log` in the **logs** directory of your project folder. This includes logs for both successful and failed runs. RudderStack appends new entries at the end of the file once a command is executed.\n\nSome exceptions where the logs are not stored are:\n\n*   `query`: The logger file stores the printing output and does not store the actual database output. However, you can access the SQL queries logs in your warehouse.\n*   `help`: For any command.\n\n**In the warehouse, I see lots of material\\_user\\_id\\_stitcher\\_ tables generated in the rs\\_profiles schema. How do I identify the latest ID stitched table?**\n\nThe view `user_id_stitcher` will always point to the latest generated ID stitcher. You may check its definition to see the exact table name it is referring to.\n\n**How can I remove the material tables that are no longer needed?**\n\nTo clean up all the materials older than a specific duration, for example 10 days, execute the following command:\n\n```\npb cleanup materials -r 10\n```\n\nThe minimum value you can set here is `1`. So if you have run the ID stitcher today, then you can remove all the older materials using `pb cleanup materials -r 1`.\n\n**Which tables and views are important in Profiles schema that should not be deleted?**\n\n*   `material_registry`\n*   `material_registry_<number>`\n*   `pb_ct_version`\n*   `ptr_to_latest_seqno_cache`\n*   `wht_seq_no`\n*   `wht_seq_no_<number>`\n*   Views whose names match your models in the YAML files.\n*   Material tables from the latest run (you may use the `pb cleanup materials` command to delete materials older than a specific duration).\n\n**I executed the auto migrate command and now I see a bunch of nested** `original_project_folder`. **Are we migrating through each different version of the tool?**\n\nThis is a symlink to the original project. Click on it in the Finder (Mac) to open the original project folder.\n\n\\*\\*I am getting a \\*\\* `ssh: handshake failed` **error when referring to a public project hosted on GitHub. It throws error for https:// path and works fine for ssh: path. I have set up token in GitHub and added to siteconfig.yaml file but I still get this error.**\n\nYou need to follow a different format for `gitcreds:` in siteconfig. See [SiteConfiguration](https://www.rudderstack.com/docs/archive/profiles/0.12/cli-user-guide/structure/#site-configuration-file-configuration.md) for the format.\n\nAfter changing `siteconfig`, if you still get an error, then clear the `WhtGitCache` folder inside the directory having the `siteconfig` file.\n\n**If I add filters to** `id_types` **in the project file, then do all rows that include any of those values get filtered out of the analysis, or is it just the specific value of that id type that gets filered?**\n\nThe PB tool does not extract rows. Instead, it extracts pairs from rows.\n\nSo if you had a row with email, user\\_id, and anonymous\\_id and the anonymous\\_id is excluded, then the PB tool still extracts the email, user\\_id edge from the row.\n\n**In the material registry table, what does** `status: 2` **mean?**\n\n*   `status: 2` means that the material has successfully completed its run.\n*   `status: 1` means that the material did not complete its run.\n\n**I am using Windows and get the following error:** `Error: while trying to migrate project: applying migrations: symlink <path>: A required privilege is not held by the client`.\n\nYour user requires privileges to create a symlink. You may either grant extra privileges to the user or try with a user containing Admin privileges on PowerShell. In case that doesn’t help, try to install and use it via WSL (Widows subsystem for Linux).\n\n**Can I specify any git account like CommitCode while configuring a project in the web app?**\n\nProfiles UI supports repos hosted on GitHub, BitBucket and GitLab.\n\n**If I want to run multiple select models, how can I run something like: `pb run --model_refs \"models/ewc_user_id_graph_all, models/ewc_user_id_graph, models/ewc_user_id_graph_v2`**\n\nYou can do so by passing `--model_refs` multiple times per model:\n\n`pb run -p samples/test_feature_table --model_refs 'models/test_id__, user/all' --migrate_on_load` OR `pb run -p samples/test_feature_table --model_refs models/test_id__ --model_refs user/all --migrate_on_load`\n\n**How can I keep my Profiles projects up to date along with updating the Python package and migrating the schema version?**\n\nYou can check for the latest Profiles updates in the [changelog](https://www.rudderstack.com/docs/profiles/changelog/).\n\nTo update the Python package and migrate the schema version, you can standardise on a single pip release across the org and use the schema version that is native to that binary. When you move to a different binary, migrate your projects to the schema version native to it.\n\nContact Profiles support team in our [Community Slack](https://rudderstack.com/join-rudderstack-slack-community) for specific questions.\n\n**I am facing this error on adding a custom ID `visitor_id` under the `id_types` field in the `pb_project.yaml` file:**\n\n`could not create project: failed to read project yaml Error: validating project sample_attribution: getting models for folder: user: error listing models: error building model user_default_id_stitcher: id type visitor_id not in project id types`\n\nWhile adding a custom ID type, you must extend the package to include its specification in the `pb_project.yaml` file as well. In this case, add the key `extends:` followed by name of the same/different id\\_type that you wish to extend, and corresponding filters with include/exclude values like below:\n\n```\nid_types:\n - name: visitor_id\n   extends: visitor_id\n  filters:\n   - type: exclude\n     value: \"someexcludedvalue\"\n```\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Profiles FAQ | RudderStack Docs",
    "description": "Commonly asked questions on RudderStack Profiles.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.12/resources/yaml-refresher/",
    "markdown": "# YAML Best Practices | RudderStack Docs\n\nQuick overview of YAML and its basics for use in Profiles.\n\n* * *\n\n*     4 minute read  \n    \n\nYAML is the preferred choice for writing Profile Builder files due to its simplicity and ease of use.\n\nThis guide explains the base concepts, syntax, and best practices for writing code in YAML.\n\n## What is YAML?\n\n[YAML](https://yaml.org/), short for **YAML Ain’t Markup Language** or **Yet Another Markup Language**, is a data serialization format often used in config files and exchange of data. A YAML file uses indentation, specific characters, and line breaks for representing various data structures.\n\n## Sample YAML file\n\nBelow is how a sample YAML document looks like. It contains key-value pairs where the keys are on the left, followed by a colon (`:`), and the associated values are on the right. The hierarchy and data structure is defined using indentation. The next section explains this in more detail.\n\n```\n# This is a comment\nperson:\n  name: Ruddy Buddy # Note the spacing used for indentation\n  age: 42\n  is_employed: true\n  address: # An object called address\n    street: Jefferson Davis Highway\n    city: Ruther Glen\n    state: Vermont\n    phone: 555-90-210\n  favorite_sports: # A list\n    - soccer\n    - baseball\n```\n\nThe above code has details of an object called `person` with properties like `name`, `age`, `gender`, `is_student`, `address` and `favorite sports`.\n\nHere’s how the same YAML file looks in the JSON format:\n\n```\n{\n  \"person\": {\n    \"name\": \"Ruddy Buddy\",\n    \"age\": 42,\n    \"is_employed\": true,\n    \"address\": {\n      \"street\": \"Jefferson Davis Highway\",\n      \"city\": \"Ruther Glen\",\n      \"state\": \"Vermont\",\n      \"phone\": \"555-90-210\"\n    },\n    \"favorite_sports\": [\n      \"soccer\",\n      \"baseball\"\n    ]\n  }\n}\n```\n\n## Indentation\n\nIn YAML, the indentation is done using spaces - to define the structure of data. Throughout the YAML file, the number of spacing should be consistent. Typically, we use two spaces for indentation. YAML is whitespace-sensitive, so do not mix spaces and tabs.\n\n```\n# Example of correct indentation\nperson:\n  name: Ruddy Buddy # We used 2 spaces\n  age: 42\n\n# Example of incorrect indentation\nperson:\n  name: Ruddy Buddy \n    age: 42 # We mixed spacing and tabs\n```\n\nAs shown above, YAML has single-line comments that start with hash (`#`) symbol, for providing additional explanation or context in the code. Comments are used to improve readability and they do not affect the code’s functionality.\n\n```\n# YAML comment\nperson:\n  name: Ruddy Buddy # Name of the person\n  age: 42 # Age of the person\n```\n\n## Data types in YAML\n\nYAML supports several data types:\n\n*   **Scalars**: Represent strings, numbers, and boolean values.\n*   **Sequences**: Represent lists and are denoted using a hyphen (`-`).\n*   **Mappings**: Key-value pairs used to define objects or dictionaries using colon (`:`).\n\n```\n# Example of data types in YAML\nperson:\n  name: Ruddy Buddy # Scalar (string)\n  age: 42 # Scalar (number)\n  is_employed: true # Scalar (boolean)\n  address: # Mapping (object)\n    street: Jefferson Davis Highway\n    city: Ruther Glen\n    state: Vermont\n    phone: 555-90-210\n  favorite_sports: # Sequence (list)\n    - soccer\n    - baseball\n```\n\n## Chomp modifiers\n\nYAML provides two chomp modifiers for handling line breaks in scalar values.\n\n*   `>`: Removes all newlines and replaces them with spaces.\n\n```\ndescription: >\n  Here is an example of long description\n  which has multiple lines. Later, it\n  will be converted into a single line.  \n```\n\n*   `|`: Preserves line breaks and spaces.\n\n```\ndescription: |\n  Here is another long description, however\n  it will preserve newlines and so the original\n  format shall be as-it-is.  \n```\n\n## Special characters\n\nYou can use escape symbols for special characters in YAML. For example, writing an apostrophe in description can cause the YAML parser to fail. In this case, you can use the escape character.\n\n## Best practices for writing YAML\n\nFollow these best practices for writing clean YAML code in your Profiles projects:\n\n*   Always keep consistent indentation (preferably spaces over tabs).\n*   Give meaningful names to your keys.\n*   Avoid excessive nesting.\n*   YAML is case sensitive, so be mindful of that.\n*   Add comments wherever required.\n*   Use blank lines to separate sections like ID stitcher, feature table, etc.\n*   If your strings contain special characters, then use escape symbols.\n*   Make sure you end the quotes in strings to avoid errors.\n*   Use chomp modifiers for multi-line SQL.\n\n## Conclusion\n\nThe above guidelines constitute some best practices to write effective [Builder](https://www.rudderstack.com/docs/profiles/get-started/profile-builder/) code in Profiles. You can also see the following references:\n\n*   [YAML for VS Code](https://marketplace.visualstudio.com/items?itemName=redhat.vscode-yaml): Extension for comprehensive YAML support in Visual Studio Code.\n*   [YAML Lint](https://www.yamllint.com/) for linting.\n\nFor more information or in any case of any issues, [contact](mailto:support@rudderstack.com) the RudderStack team.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "YAML Best Practices | RudderStack Docs",
    "description": "Quick overview of YAML and its basics for use in Profiles.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.12/resources/glossary/",
    "markdown": "# Glossary | RudderStack Docs\n\nFamiliarize yourself with the commonly used terms across RudderStack Profiles.\n\n* * *\n\n*     6 minute read  \n    \n\n## Custom Models (Python)\n\nOne can build custom Python models for ML by downloading pre-defined Python templates. The results are usually saved as attributes of related entities (for example, `churnProbability`).\n\nPython models and pymodels work differently. In case of Python, developers (both RudderStack and external) develop new model types in python using [profiles-rudderstack](https://pypi.org/project/profiles-rudderstack/) package. An example python package implementing a number of Python models is [profiles-pycorelib](https://pypi.org/project/profiles-pycorelib/).\n\n## Edge sources\n\nThe `edge_sources` field provides the input sources for an identity stitching model. You can specify it in the `models/profiles.yaml` file to list the input sources defined in the `inputs.yaml` file.\n\n## Entity\n\nEntity refers to an object for which you can create a profile. An entity can be a user, customer, product, or any other object that requires profiling.\n\n## Entity var\n\nThese are various attributes related to the entity whose profile you are trying to create (`name`, `city`, `LastVisitTimestamp`, etc). Together, all the attributes can create a complete picture of the entity. Attribute is a single value derived by performing calculation or aggregation on a set of values. By default, every entity var gets stored as a feature, such as `days_active`, `last_seen`, etc.\n\n## Feature Views\n\nIf the features/traits of an entity are spread across multiple entity vars and ML models, you can use Feature Views to get them together into a single view. These models are usually defined in `pb_project.yaml` file by creating entries under `feature_views` key with corresponding entity.\n\n## Features\n\nFeatures are inputs for the machine learning model. In a general sense, features are pieces of user information we already know. For example, number of days they opened the app in the past week, items they left in the cart, etc.\n\n## Feature tables\n\nFeature tables are the outputs based on events, user attributes, and other defined criteria across any data set in your data warehouse. You can define models that can create feature tables reflecting a complete 360 data for users with ID stitching, ML notebooks and external sources, etc.\n\n## ID Stitcher\n\nData usually comes from different sources and these sources may assign different IDs. To track a user’s journey (or any other entity) uniquely across all these data sources, we need to stitch together all these IDs. ID stitching helps map different IDs of the same user (or any other entity) to a single canonical ID. It does this by doing connected component analysis over the Id-Id edge graph specified in its configuration.\n\n## ID Collator\n\nID Collator is similar to ID Stitcher. It is used when entity has only a single ID type associated (for example, session IDs). In these cases, connected component analysis is not required and we use a simpler model type called ID Collator. It consolidates all entity IDs from multiple input tables into a single collated list.\n\n## Inputs\n\nInputs refers to the input data sources used to create the material (output) tables in the warehouse. The inputs file (`models/inputs.yaml`) lists the tables/views you use as input sources, including the column name and SQL expression for retrieving the values.\n\nYou can use data from various input sources such as event stream (loaded from event data), ETL extract (loaded from Cloud Extract), and any existing tables in the warehouse (generated by external tools).\n\n## Input var\n\nInstead of a single value per entity ID, it represents a single value per row of an input model. Think of it as representing addition of an additional column to an input model. It can be used to define entity features. However, it is not itself an entity feature because it does not represent a single value per entity ID.\n\n## Label\n\nLabel is the output of the machine learning model and is the metric we want to predict. In our case, it is the unknown user trait we want to know in advance.\n\n## Machine learning model\n\nA machine learning model can be thought of as a function that takes in some input parameters and returns an output.\n\nUnlike regular programming, this function is not explicitly defined. Instead, a high level architecture is defined and several pieces are filled by looking at the data. This whole process of filling the gaps is driven by different optimisation algorithms as they try to learn complex patterns in the input data that explain the output.\n\n## Materialization\n\nMaterialization refers to the process of creating output tables/views in a warehouse using PB models. You can define the `run_type` to create the output table:\n\n*   `run_type`: Possible values are:\n    *   `discrete`: Calculates the model result from it’s inputs whenever run (default mode). A SQL model supports only `discrete` run type.\n    *   `incremental`: The model reads updates from input sources and results from the previous run. It only updates or inserts data and does not delete anything. The `incremental` mode is more efficient. However, only identity stitching model supports it.\n\n## Material tables\n\nWhen you run the PB models, they produce materials - that is, tables/views in the database that contain the results of that model run. These output tables are known as material tables.\n\n## Predictions\n\nThe model’s output is called a prediction. A good model makes predictions that are close to the actual label. You generally need predictions where the labels are not available. In our case, most often the labels come a few days later.\n\n## Prediction horizon days\n\nThis refer to the number of days in advance when we make the prediction.\n\nFor example, statements like “A user is likely to sign-up in the next 30 days, 90 days, etc.” are often time-bound, that is, the predictions are meaningless without the time period.\n\n## Profile Builder (PB)\n\nProfile Builder (PB) is a command-line interface (CLI) tool that simplifies data transformation within your warehouse. It generates customer profiles by stitching data together from multiple sources. It takes existing tables or the output of other transformations as input to generate output tables or views based on your specifications.\n\n## PB project\n\nA PB project is a collection of interdependent warehouse transformations. These transformations are run over the warehouse to query the data for sample outputs, discover features in the warehouse, and more.\n\n## PB model\n\nAny transformation that can be applied to the warehouse data is called a PB model. RudderStack supports various types of models like ID stitching, feature tables, Python models, etc.\n\n## Schema versions\n\nEvery PB release supports a specific set of project schemas. A project schema determines the correct layout of a PB project, including the exact keys and their values in all project files.\n\n## SQL template models\n\nSometimes the standard model types provided by Profiles are insufficient to capture complex use cases. In such cases, RudderStack supports the use of SQL template models to explicitly templatize SQL.\n\nSQL template models can be used as an input to an entity-var/ input-var or as an edge-source in id-stitcher.\n\n## Training\n\nTraining refers to the process of a machine learning model looking at the available data and trying to learn a function that explains the [labels](#label).\n\nOnce you train a model on historic users, you can use it to make predictions for new users. You need to keep retraining the model as you get new data so that the model continues to learn any emerging patterns in the new users.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Glossary | RudderStack Docs",
    "description": "Familiarize yourself with the commonly used terms across RudderStack Profiles.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.10/",
    "markdown": "# Profiles 0.10 | RudderStack Docs\n\nPrivacy Overview\n\nThis site uses cookies to improve your experience while you navigate through the website. Out of these cookies, the cookies that are categorized as necessary are stored on your browser as they are as essential for the working of basic functionalities of the website. We also use third-party cookies that help us analyze and understand how you use this website. These cookies will be stored in your browser only with your consent. You also have the option to opt-out of these cookies. But opting out of some of these cookies may have an effect on your browsing experience.\n\nNecessary cookies are absolutely essential for the website to function properly. This category only includes cookies that ensures basic functionalities and security features of the website. These cookies do not store any personal information.",
    "title": "Profiles 0.10 | RudderStack Docs",
    "description": "Documentation for Profiles v0.10",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.11/faq/",
    "markdown": "# Profiles FAQ | RudderStack Docs\n\nCommonly asked questions on RudderStack Profiles.\n\n* * *\n\n*     26 minute read  \n    \n\nThis guide contains solutions for some of the commonly asked questions on Profiles. For queries or issues not listed in this guide, contact [RudderStack Support](mailto:support@rudderstack.com).\n\n## Setup and installation\n\n**I have installed Python3, yet when I install and execute `pb` it doesn’t return anything on screen.**\n\nTry restarting your Terminal/Shell/PowerShell and try again.\n\nYou can also try to find the location of your Python executable. PB would be installed where the executables embedded in other Python packages are installed.\n\n**I am an existing user who updated to the new version and now I am unable to use the PB tool. On Windows, I get the error:** `'pb' is not recognized as an internal or external command, operable program or batch file.`\n\nExecute the following commands to do a fresh install:\n\n1.  `pip3 uninstall profiles-rudderstack-bin`\n2.  `pip3 uninstall profiles-rudderstack`\n3.  `pip3 install profiles-rudderstack --no-cache-dir`\n\n**I am unable to download, getting** `ERROR: Package 'profiles-rudderstack' requires a different Python: 3.7.10 not in '>=3.8, <=3.10'`\n\nUpdate your Python 3 to a version greater than or equal to 3.8 and less than or equal to 3.10.\n\n**I am unable to download profile builder by running `pip3 install profiles-rudderstack` even though I have Python installed.**\n\nFirstly, make sure that Python3 is correctly installed. You can also try to substitute `pip3` with `pip` and execute the install command.\n\nIf that doesn’t work, it’s high likely that Python3 is accessible from a local directory.\n\n1.  Navigate to that directory and try the install command again.\n2.  After installation, PB should be accessible from anywhere.\n3.  Validate that you’re able to access the path using `which pb`.\n4.  You may also execute `echo $PATH` to view current path settings.\n5.  In case it doesn’t show the path then you can find out where |ProductName| is installed using :substitution-code:`pip3 show profiles-rudderstack`. This command will display a list of the files associated with the application, including the location in which it was installed, navigate to that directory.\n6.  Navigate to `/bin` subdirectory and execute command `ls` to confirm that `pb` is present there.\n7.  To add the path of the location where PB is installed via pip3, execute: `export PATH=$PATH:<path_to_application>`. This will add the path to your system’s PATH variable, making it accessible from any directory. It is important to note that the path should be complete and not relative to the current working directory.\n\nIf you still face issues, then you can try to install it manually. [Contact us](mailto:support@rudderstack.com) for the executable file and download it on your machine. Follow the below steps afterwards:\n\n1.  Create `rudderstack` directory: `sudo mkdir /usr/local/rudderstack`.\n2.  Move the downloaded file to that directory: `sudo mv <name_of_downloaded_file> /usr/local/rudderstack/pb`.\n3.  Grant executable permission to the file: `chmod +x /usr/local/rudderstack/pb`.\n4.  Navigate to directory `/usr/local/rudderstack` from your file explorer. Ctrl+Click on pb and select **Open** to run it from Terminal.\n5.  Symlink to a filename pb in `/usr/local/bin` so that command can locate it from env PATH. Create file if it does not exist: `sudo touch /usr/local/bin/pb`. Then execute`sudo ln -sf /usr/local/rudderstack/pb /usr/local/bin/pb`.\n6.  Verify the installation by running `pb` in Terminal. In case you get error `command not found: pb` then check if `/usr/local/bin` is defined in PATH by executing command: `echo $PATH`. If not, then add `/usr/local/bin` to PATH.\n\n1.  If the Windows firewall prompts you after downloading, proceed with `Run Anyway`.\n2.  Rename the executable as `pb`.\n3.  Move the file to a safe directory such as `C:\\\\Program Files\\\\Rudderstack`, create the directory if not present.\n4.  Set the path of `pb.exe` file in environment variables.\n5.  Verify the installation by running `pb` in command prompt.\n\n**When I try to install Profile Builder tool using pip3 I get error message saying: `Requirement already satisfied`**\n\nTry the following steps:\n\n1.  Uninstall PB using `pip3 uninstall profiles-rudderstack`.\n2.  Install again using `pip3 install profiles-rudderstack`.\n\nNote that this won’t remove your existing data such as models and siteconfig files.\n\n**I have multiple models in my project. Can I run only a single model?**\n\nYes, you can. In your spec YAML file for the model you don’t want to run, set `materialization` to `disabled`:\n\n```\nmaterialization:\n    enable_status: disabled\n```\n\nA sample `profiles.yaml` file highlighted a disabled model:\n\n```\nmodels:\n- name: test_sql\n  model_type: sql_template\n  model_spec:\n    validity_time: 24h# 1 day\n    materialization:                \n      run_type: discrete\n      enable_status: disabled  // Disables running the model.\n    single_sql: |\n        {%- with input1 = this.DeRef(\"inputs/tbl_a\") -%}\n          select id1 as new_id1, {{input1}}.*\n            from {{input1}}\n        {%- endwith -%}        \n    occurred_at_col: insert_ts\n    ids:\n      - select: \"new_id1\"\n        type: test_id\n        entity: user\n```\n\n**I am facing this error while ugrading my Profiles project: `pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. profiles-pycorelib 0.2.2 requires profiles-rudderstack!=0.10.6,<=0.10.7,>=0.10.5, but you have profiles-rudderstack 0.11.0 which is incompatible.`**\n\nThis is because you must uninstall and then reinstall the `pycorelib` library while upgrading to a recent version.\n\n* * *\n\n## Warehouse permissions\n\n**I have two separate roles to read from input tables and write to output tables? How should I define the roles?**\n\nYou need to create an additional role as a union of those two roles. PB project needs to read the input tables and write the results back to the warehouse schema.\n\nFurthermore, each run is executed using a single role as specified in the `siteconfig.yaml` file. Hence, it is best in terms of security to create a new role which has read as well as write access for all the relevant inputs and the output schema.\n\n**How do I test if the role I am using has sufficient privileges to access the objects in the warehouse?**\n\nYou can use the `pb validate access` command to validate the access privileges on all the input/output objects.\n\n* * *\n\n## Compile command\n\n**I am trying to execute the `compile` command by fetching a repo via GIT URL but getting this error: `making git new public keys: ssh: no key found`**\n\nYou need to add the OpenSSH private key to your `siteconfig.yaml` file. If you get the error `could not find expected` afterwards, try correcting the spacing in your `siteconfig.yaml` file.\n\n**While trying to segregate identity stitching and feature table in separate model files, I am getting this error: `mapping values are not allowed in this context`**\n\nThis is due to the spacing issue in `siteconfig.yaml` file. You may create a new project to compare the spacing. Also, make sure you haven’t missed any keys.\n\n* * *\n\n## Command progress & lifecycle\n\n**I executed a command and it is taking too long. Is there a way to kill a process on data warehouse?**\n\nIt could be due to the other queries running simultaneously on your warehouse. To clear them up, open the **Queries** tab in your warehouse and manually kill the long running processes.\n\n**Due to the huge data, I am experiencing long execution times. My screen is getting locked, thereby preventing the process from getting completed. What can I do?**\n\nYou can use the `screen` command on UNIX/MacOS to detach your screen and allow the process to run in the background. You can use your terminal for other tasks, thus avoiding screen lockouts and allowing the query to complete successfully.\n\nHere are some examples:\n\n*   To start a new screen session and execute a process in detached mode: `screen -L -dmS profiles_rn_1 pb run`. Here:\n    *   `-L` flag enables logging.\n    *   `-dmS` starts as a daemon process in detached mode.\n    *   `profiles_rn_1` is the process name.\n*   To list all the active screen sessions: `screen -ls`.\n*   To reattach to a detached screen session: `screen -r [PID or screen name]`.\n\n**The CLI was running earlier but it is unable to access the tables now. Does it delete the view and create again?**\n\nYes, every time you run the project, Profiles creates a new materials table and replaces the view.\n\nHence, you need to grant a select on future views/tables in the respective schema and not just the existing views/tables.\n\n**Does the CLI support downloading a git repo using siteconfig before executing** `pb run` **? Or do I have to manually clone the repo first?**\n\nYou can pass the Git URL as a parameter instead of project’s path, as shown:\n\n**When executing** `run` **command, I get a message:** `Please use sequence number ... to resume this project in future runs` **. Does it mean that a user can exit using Ctrl+C and later if they give this seq\\_no then it’ll continue from where it was cancelled earlier?**\n\nThe `pb run --seq_no <>` flag allows for the provision of a sequence number to run the project. This flag can either resume an existing project or use the same context to run it again.\n\nWith the introduction of time-grain models, multiple sequence numbers can be assigned and used for a single project run.\n\n**What flag should I set to force a run for same end time, even if a previous run exists?**\n\nYou can execute `pb run --force --model_refs models/my_id_stitcher,entity/user/user_var_1,entity/user/user_var_2,...`\n\n**Can the hash change even if schema version did not change?**\n\nYes, as the hash versions depends on project’s implementation while the schema versions are for the project’s YAML layout.\n\n**Is there a way to pick up from a point where my last pb run failed on a subsequent run? For large projects, I don’t want to have to rerun all of the features if something failed as some of these take several hours to run**\n\nYes, you can just execute the run command with the specific sequence number, for example, `pb run —seq_no 8`.\n\n* * *\n\n## Identity stitching\n\n**There are many large size connected components in my warehouse. To increase the accuracy of stitched data, I want to increase the number of iterations. Is it possible?**\n\nThe default value of the largest diameter, that is, the longest path length in connected components, is 30.\n\nYou can increase it by defining a `max_iterations` key under `model_spec` of your ID stitcher model in `models/profiles.yaml`, and specifying its value as the max diameter of connected components.\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> Note that the algorithm can give incorrect results in case of large number of iterations.\n\n**Do I need to write different query each time for viewing the data of created tables?**\n\nNo, you can instead use a view name, which always points to the latest created material table. For example, if you’ve defined **user\\_stitching** in your `models/profiles.yaml` file, then execute `SELECT * FROM MY_WAREHOUSE.MY_SCHEMA.user_stitching`.\n\n**In my model, I have set the key `validity_time: 24h`. What happens when the validity of generated tables expire? Will re-running the identity stitching model generate the same hash until the validity expires?**\n\nFirstly, hash does not depend on the timestamp, it depends on the yaml in the underlying code. That’s why the material name is `material_name_HASH_SEQNO`. The sequence number (SEQNO) depends on timestamp.\n\nSecondly, a material generated for a specific timestamp (aside for the timeless timestamp) is not regenerated unless you do a `pb run --force`. The CLI checks if the material you are requesting already exists in the database, and if it does, returns that. The `validity_time` is an extension of that.\n\nFor a model with `validity_time: 24h` and inputs having the `timestamp` columns, if you request a material for latest time, but one was generated for that model 5 minutes ago, the CLI will return that one instead. Using the CLI to run a model always generates a model for a certain timestamp, it’s just if you don’t specify a timestamp then it uses the current timestamp.\n\nSo, for a model with validity\\_time (vt), and having the `timestamp` columns, if you request a material for t1, but one already exists for t0 where t1-vt <= t0 <= t1, the CLI will return that one instead.\n\nIf multiple materials exist that satisfy the requirement, then it returns the one with the timestamp closest to t1.\n\n**I want to use `customer_id` instead of `main_id` as the ID type. So I changed the name in `pb_project.yaml`, however now I am getting this error: `Error: validating project sample_attribution: listing models for child source models/: error listing models: error building model domain_profile_id_stitcher: main id type main_id not in project id types`.**\n\nIn addition to making changes in the file `pb_project.yaml` file, you also need to set `main_id_type: customer_id` in the `models/profiles.yaml` file.\n\n**I ran identity stitching model but not able to see the output tables under the list of tables in Snowflake. What might be wrong?**\n\nIn Snowflake, you can check the **Databases** > **Views** dropdown from the left sidebar. For example, if your model name is `domain_profile_id_stitcher`, you should be able to see the table with this name. In case it is still not visible, try changing the role using dropdown menu from the top right section.\n\n**I am using a view as an input source but getting an error that the view is not accessible, even though it exists in DB.**\n\nViews need to be refreshed from time-to-time. You can try recreating the view in your warehouse and also execute a `select *` on the same.\n\n**What might be the reason for following errors:**\n\n*   `processing no result iterator: pq: cannot change number of columns in view`. The output view name already exists in some other project. To fix this, try dropping the view or changing its name.\n    \n*   `creating Latest View of moldel 'model_name': processing no result iterator: pq: cannot change data type of view column \"valid_at\"` Drop the view `domain_profile` in your warehouse and execute the command again.\n    \n*   `processing no result iterator: pq: column \"rudder_id\" does not exist`. This occurs when you execute a PB project with a model name, having `main_id` in it, and then you run another project with the same model name but no `main_id`. To resolve this, try dropping the earlier materials using `cleanup materials` command.\n    \n\n**I have a source table in which `email` gets stored in the column for `user_id`, so the field has a mix of different ID types. I have to tie it to another table where email is a separate field. When doing so, I have two separate entries for email, as type `email` and `user_id`. What should I do?**\n\nYou can implement the following line in the inputs tables in question:\n\n```\n  - select: case when lower(user_id) like '%@%' THEN lower(user_id) else null end\n    type: email \n    entity: user\n    to_default_stitcher: true\n```\n\n**How do I validate the results of identity stitching model?**\n\nContact [RudderStack Support](mailto:support@rudderstack.com) if you need help in validating the clusters.\n\n**Which identifiers would you recommend that I include in the ID stitcher for an ecommerce Profiles project?**\n\nWe suggest including identifiers that are unique for every user and can be tracked across different platforms and devices. These identifiers might include but not limited to:\n\n*   Email ID\n*   Phone number\n*   Device ID\n*   Anonymous ID\n*   User names\n\nThese identifiers can be specified in the file `profiles.yaml` file in the identity stitching model.\n\nRemember, the goal of identity stitching is to create a unified user profile by correlating all of the different user identifiers into one canonical identifier, so that all the data related to a particular user or entity can be associated with that user or entity.\n\n**If I run `--force` with an ID Stitcher model and also pass a `--seq_no` for the most recent run, will it still recreate the full ID Graph? Also, is there a way to know if the model was run incrementally or not?**\n\nThis will re-run the ID stitcher and if it is incremental, it will look for the most recent run of the stitcher. After finding the existing run for that `seq_no`, it will use it as the base. This is because the base for an incremental run could be the current `seq_no`. If you do not want to do this, you can pass the `rebase_incremental` flag.\n\n**I am getting a bunch of NULL `VALID_AT` timestamps. Is it because the table where the data is being referenced from does not have a timestamp fields specified? Will this impact anything in the downstream?**\n\nYes, if there is no timestamp field in the input table (or it is NULL for the row from where the edge source was pulled), then `VALID_AT` column would have NULL value. This only affects the `VALID_AT` column in the final table and nothing in the ID stitching.\n\n**Which identifiers should I include in my inputs.yaml file?**\n\nInclude all the IDs that contribute to the ID stitcher model.\n\n* * *\n\n## Feature Table\n\n**How can I run a feature table without running its dependencies?**\n\nSuppose you want to re-run the user entity\\_var `days_active` and the `rsTracks` input\\_var `last_seen` for a previous run with `seq_no 18`.\n\nThen, execute the following command:\n\n```\npb run --force --model_refs entity/user/days_active,inputs/rsTracks/last_seen --seq_no 18\n```\n\n**I have imported a library project but it throws an error: `no matching model found for modelRef rsTracks in source inputs`.**\n\nYou can exclude the missing inputs of the library project by mapping them to nil in the `pb_project.yaml` file.\n\n**Is it possible to run the feature table model independently, or does it require running alongside the ID stitcher model?**\n\nYou can provide a specific timestamp while running the project, instead of using the default latest time. PB recognizes if you have previously executed an identity stitching model for that time and reuses that table instead of generating it again.\n\nYou can execute a command similar to: `pb run --begin_time 2023-06-02T12:00:00.0Z --end_time 2023-06-03T12:00:00.0Z`. Note that:\n\n*   To reuse a specific identity stitching model, the timestamp value must match exactly to when it was run.\n*   If you have executed identity stitching model in the incremental mode and do not have an exact timestamp for reusing it, you can select any timestamp **greater** than a non-deleted run. This is because subsequent stitching takes less time.\n*   To perform another identity stitching using PB, pick a timestamp (for example, `1681542000`) and stick to it while running the feature table model. For example, the first time you execute `pb run --begin_time 2023-06-02T12:00:00.0Z --end_time 2023-06-03T12:00:00.0Z`, it will run the identity stitching model along with the feature models. However, in subsequent runs, it will reuse the identity stitching model and only run the feature table models.\n\n**While trying to add a feature table, I get an error at line 501, but I do not have these many lines in my YAML.**\n\nThe line number refers to the generated SQL file in the output folder. Check the console for the exact file name with the sequence number in the path.\n\n**While creating a feature table, I get this error:** `Material needs to be created but could not be: processing no result iterator: 001104 (42601): Uncaught exception of type 'STATEMENT ERROR': 'SYS _W. FIRSTNAME' in select clause is neither an aggregate nor in the group by clause.`\n\nThis error occurs when you use a window function `any_value` that requires a window frame clause. For example:\n\n```\n  - entity_var:\n      name: email\n      select: LAST_VALUE(email)\n      from: inputs/rsIdentifies\n      window:\n        order_by: \n        - timestamp desc\n```\n\n**Is it possible to create a feature out of an identifier? For example, I have a RS user\\_main\\_id with two of user\\_ids stitched to it. Only one of the user\\_ids has a purchase under it. Is it possible to show that user\\_id in the feature table for this particular user\\_main\\_id?**\n\nIf you know which input/warehouse table served as the source for that particular ID type, then you can create features from any input and also apply a `WHERE` clause within the entity\\_var.\n\nFor example, you can create an aggregate array of user\\_id’s from the purchase history table, where total\\_price > 0 (exclude refunds, for example). Or, if you have some LTV table with user\\_id’s, you could exclude LTV < 0.\n\n**Is it possible to reference an input var in another input var?**\n\nYes - input vars are similar to adding additional columns to the original table. You can use an input var `i1v1` in the definition of input var `i1v2` as long as both input vars are defined in the same input (or SQL model) `i1`.\n\n**I have not defined any input vars on I1. Why is the system still creating I1\\_var\\_table?**\n\nWhen you define an entity var using I1, an internal input var (for entity’s `main_id`) is created which creates `I1_var_table`. RudderStack team is evaluating whether internal input vars should create the var table or not.\n\n**I have an input model I1. Why is the system creating Material\\_I1\\_var\\_table\\_XXXXXX\\_N?**\n\nThis material table is created to keep the input vars defined on I1.\n\n**I am trying to run a single `entity_var` model. How should I reference it?**\n\nThe right way to reference an entity var is: `entity/<entity-name>/<entity-var-name>`.\n\n**I have two identical named fields in two `user` tables and I want my Profiles project to pick the most recently updated one (from either of the `user` tables). What is the best way to do it?**\n\nDefine different `entity_vars` (one for each input) and then pick the one with a non-null value and higher priority.\n\n**What does running material mean?**\n\nIt means that the output (material) table is being created in your warehouse. For example, an output table named `material_user_id_stitcher_3acd249d_21` would mean:\n\n*   `material`: Prefix for all the objects created by Profiles in your warehouse, such as ID stitcher and feature tables.\n*   `user_id_stitcher`: View created in your schema. It will always point to latest ID stitcher table. This name is the same as defined in the `models/profiles.yaml` file.\n*   `3acd249d`: Unique hash which remains the same for every model unless you make any changes to the model’s config, inputs or the config of model’s inputs.\n*   `21`: Sequence number for the run. It is a proxy for the context timestamp. Context timestamp is used to checkpoint input data. Any input row with `occured_at` timestamp value greater than the context timestamp cannot be used in the associated run.\n\n* * *\n\n## YAML\n\n**Are there any best practices I should follow when writing the PB project’s YAML files?**\n\n*   Use spaces instead of tabs.\n*   Always use proper casing. For example: id\\_stitching, and not id\\_Stitching.\n*   Make sure that the source table you are referring to, exists in data warehouse or data has been loaded into it.\n*   If you’re pasting table names from your Snowflake Console, remove the double quotes in the `inputs.yaml` file.\n*   Make sure your syntax is correct. You can compare with the sample files.\n*   Indentation is meaningful in YAML, make sure that the spaces have same level as given in sample files.\n\n**How do I debug my YAML file step-by-step?**\n\nYou can use the `--model_args` parameter of the `pb run` command to do so. It lets you run your YAML file till a specific feature/tablevar. For example:\n\n```\n$ pb run -p samples/attribution --model_args domain_profile:breakpoint:blacklistFlag\n```\n\nSee [run command](https://www.rudderstack.com/docs/archive/profiles/0.11/developer-guides/commands/#run) for more information.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> This is only applicable to versions prior to v0.9.0.\n\n**Can I use double quotes when referencing another entity\\_var in a macro?**\n\nYou can use an escape character. For example:\n\n```\n  - entity_var:\n      name: days_since_last_seen\n      select: \"{{macro_datediff('{{user.Var(\\\"max_timestamp_bw_tracks_pages\\\")}}')}}\"\n```\n\nAlso, if have a case statement, then you can add something like the following:\n\n`select: CASE WHEN {{user.Var(\"max_timestamp_tracks\")}}>={{user.Var(\"max_timestamp_pages\")}} THEN {{user.Var(\"max_timestamp_tracks\")}} ELSE {{user.Var(\"max_timestamp_pages\")}} END`\n\n**Is it possible to define default arguments in macros?**\n\nNo, RudderStack does not support default arguments in macros.\n\n* * *\n\n## ML/Python Models\n\n**Despite deleting WhtGitCache folder and adding keys to siteconfig, I get this error:** `Error: loading project: populating dependencies for project:base_features, model: churn_30_days_model: getting creator recipe while trying to get ProjectFolder: fetching git folder for git@github.com:rudderlabs/rudderstack-profiles-classifier.git: running git plain clone: repository not found`\n\nIf your token is valid, then replace `git@github.com:rudderlabs/rudderstack-profiles-classifier.git` with `https://github.com/rudderlabs/rudderstack-profiles-classifier.git` in the `profile-ml` file.\n\n**Why is my Profiles project taking so long to run?**\n\nThe first Profiles project run usually takes longer, especially if you are building predictive features.\n\n**I am debugging an error in ML models where I see a view with the model name, without material/hash prefix and suffix but it does not get refreshed even after all the entity vars are created and the material\\_<feature\\_table\\_model> table is also created. What might be the reason?**\n\nIt is because this view is now moved to `PostProjectRunCb`, meaning, it is created async after material Force run step.\n\n* * *\n\n## Activation API\n\n**While using Redis destination, I am facing an error: `These sample records were rejected by the destination`**?\n\nThis error is observed if you have enabled **Cluster mode** setting for Redis in the [RudderStack’s configuration settings](https://www.rudderstack.com/docs/destinations/streaming-destinations/redis/#connection-settings) but you are on the Redis free plan.\n\nTo overcome this, ensure that the Redis plan you are using allows clustering. Alternatively, you can turn off the **Cluster mode** setting.\n\n**Does the user-profiles API (old) and activation API (new) behave differently in updating a key that maps to two different primary keys? For example:**\n\n| Primary key | user\\_id | Feature\\_1 | Feature\\_2 |\n| --- | --- | --- | --- |\n| PK1 | U1  | F1  | null |\n| PK2 | U1  | null | F2  |\n\nUser profiles API\n\n```\n{\n  \"userId\": \"U1\",\n  \"profile\": {\n    \"feature_1\": \"F1\",\n    \"feature_2\": \"F2\"\n  }\n}\n```\n\nActivation API\n\n```\n{\n  \"entity\": \"entity_name\",\n  \"id\": {\n    \"type\": \"user_id\",\n    \"value\": \"U1\"\n  },\n  \"data\": {\n    \"model_name\": {\n      \"feature_1\": null,\n      \"feature_2\": F2\n    }\n  }\n}\n```\n\nIn user profiles API, RudderStack updates the value for a specific key (that is, feature\\_1 in this case). In activation API, RudderStack syncs the entire row as value for the `model_name` key.\n\n**Is it possible to use the Activation API without any Profiles project?**\n\nNo, the Activation API can only be used with a Profiles project and not on any of your non-Profiles output tables.\n\n## Profiles UI\n\n**I have included some features in the RudderStack dashboard while creating the Profiles project but when I click “download this project”, my project files does not include any feature. What might be the reason?**\n\nIf you have selected pre-defined features from any library project, they are referred to as `profiles-multieventstream-features` in the project by default.\n\nIf you have created any features using the custom feature functionality, they will be a part of your `pb_profiles.yaml` file. \n\n**While choosing pre-defined features in the RudderStack dashboard, I can preview code for only some of the features. What might be the reason?**\n\nYou can preview the code only for entity var based features. This functionality is not available for features built from ML and SQL models.\n\n**While creating a Profiles project by importing from Git, I dont see any warehouse options in the dropdown selector in the `Validate Profiles project` section. What might be the reason?**\n\nA Profiles project looks for the supported warehouse destinations configured for that workspace. Hence, make sure you have configured any of the following [warehouse destinations](https://www.rudderstack.com/docs/destinations/warehouse-destinations/) in your RudderStack dashboard:\n\n*   Snowflake\n*   Databricks\n*   Redshift\n*   BigQuery\n\n**Why am I not able to see the Concurrency option in the Settings tab of my Profiles project?**\n\nRudderStack supports the **Concurrency** option only for the Snowflake warehouse currently. You will not be able to see this option if you have configured your Profiles project using the Redshift, BigQuery, or Databricks warehouse.\n\n## Miscellaneous\n\n**Why am I getting _Authentication FAILED_ error on my data warehouse while executing the run/compile commands?**\n\nSome possible reasons for this error might be:\n\n*   Incorrect warehouse credentials.\n*   Insufficient user permissions to read and write data. You can ask your administrator to change your role or grant these privileges.\n\n**Why am I getting _Object does not exist or not authorized_ error on running this SQL query: `SELECT * FROM \"MY_WAREHOUSE\".\"MY_SCHEMA\".\"Material_domain_profile_c0635987_6\"`?**\n\nYou must remove double quotes from your warehouse and schema names before running the query, that is `SELECT * FROM MY_WAREHOUSE.MY_SCHEMA.Material_domain_profile_c0635987_6`.\n\n**Is there a way to obtain the timestamp of any material table?**\n\nYes, you can use the `GetTimeFilteringColSQL()` method to get the timestamp column of any material. It filters out rows based on the timestamp. It returns the `occurred_at_col` in case of an event\\_stream table or `valid_at` in case the material has that column. In absense of both, it returns an empty string. For example:\n\n```\n  SELECT * FROM {<from_material>}\n    WHERE\n      <from_material>.GetTimeFilteringColSQL() > <some_timestamp>;\n```\n\n**What is the difference between setting up Profiles in the RudderStack dashboard and Profile Builder CLI tool?**\n\nYou can run Profiles in the RudderStack dashboard or via [Profile Builder CLI](https://www.rudderstack.com/docs/archive/profiles/0.11/get-started/profile-builder/).\n\nThe main difference is that the RudderStack dashboard only generates outputs based on the pre-defined templates. However, you can augment those outputs by downloading the config file and updating it manually.\n\nOn the other hand, the CLI tool lets you achieve the end-to-end flow via creating a Profile Builder project.\n\n**Does the Profiles tool have logging enabled by default for security and compliance purposes?**\n\nLogging is enabled by default for nearly all the commands executed by CLI (`init`, `validate access`, `compile`, `run`, `cleanup`, etc.). Logs for all the output shown on screen are stored in the file `logfile.log` in the **logs** directory of your project folder. This includes logs for both successful and failed runs. RudderStack appends new entries at the end of the file once a command is executed.\n\nSome exceptions where the logs are not stored are:\n\n*   `query`: The logger file stores the printing output and does not store the actual database output. However, you can access the SQL queries logs in your warehouse.\n*   `help`: For any command.\n\n**In the warehouse, I see lots of material\\_user\\_id\\_stitcher\\_ tables generated in the rs\\_profiles schema. How do I identify the latest ID stitched table?**\n\nThe view `user_id_stitcher` will always point to the latest generated ID stitcher. You may check its definition to see the exact table name it is referring to.\n\n**How can I remove the material tables that are no longer needed?**\n\nTo clean up all the materials older than a specific duration, for example 10 days, execute the following command:\n\n```\npb cleanup materials -r 10\n```\n\nThe minimum value you can set here is `1`. So if you have run the ID stitcher today, then you can remove all the older materials using `pb cleanup materials -r 1`.\n\n**Which tables and views are important in Profiles schema that should not be deleted?**\n\n*   `material_registry`\n*   `material_registry_<number>`\n*   `pb_ct_version`\n*   `ptr_to_latest_seqno_cache`\n*   `wht_seq_no`\n*   `wht_seq_no_<number>`\n*   Views whose names match your models in the YAML files.\n*   Material tables from the latest run (you may use the `pb cleanup materials` command to delete materials older than a specific duration).\n\n**I executed the auto migrate command and now I see a bunch of nested** `original_project_folder`. **Are we migrating through each different version of the tool?**\n\nThis is a symlink to the original project. Click on it in the Finder (Mac) to open the original project folder.\n\n\\*\\*I am getting a \\*\\* `ssh: handshake failed` **error when referring to a public project hosted on GitHub. It throws error for https:// path and works fine for ssh: path. I have set up token in GitHub and added to siteconfig.yaml file but I still get this error.**\n\nYou need to follow a different format for `gitcreds:` in siteconfig. See [SiteConfiguration](https://www.rudderstack.com/docs/archive/profiles/0.11/developer-guides/site-configuration/) for the format.\n\nAfter changing `siteconfig`, if you still get an error, then clear the `WhtGitCache` folder inside the directory having the `siteconfig` file.\n\n**If I add filters to** `id_types` **in the project file, then do all rows that include any of those values get filtered out of the analysis, or is it just the specific value of that id type that gets filered?**\n\nThe PB tool does not extract rows. Instead, it extracts pairs from rows.\n\nSo if you had a row with email, user\\_id, and anonymous\\_id and the anonymous\\_id is excluded, then the PB tool still extracts the email, user\\_id edge from the row.\n\n**In the material registry table, what does** `status: 2` **mean?**\n\n*   `status: 2` means that the material has successfully completed its run.\n*   `status: 1` means that the material did not complete its run.\n\n**I am using Windows and get the following error:** `Error: while trying to migrate project: applying migrations: symlink <path>: A required privilege is not held by the client`.\n\nYour user requires privileges to create a symlink. You may either grant extra privileges to the user or try with a user containing Admin privileges on PowerShell. In case that doesn’t help, try to install and use it via WSL (Widows subsystem for Linux).\n\n**Can I specify any git account like CommitCode while configuring a project in the web app?**\n\nProfiles UI supports repos hosted on GitHub, BitBucket and GitLab.\n\n**If I want to run multiple select models, how can I run something like: `pb run --model_refs \"models/ewc_user_id_graph_all, models/ewc_user_id_graph, models/ewc_user_id_graph_v2`**\n\nYou can do so by passing `--model_refs` multiple times per model:\n\n`pb run -p samples/test_feature_table --model_refs 'models/test_id__, user/all' --migrate_on_load` OR `pb run -p samples/test_feature_table --model_refs models/test_id__ --model_refs user/all --migrate_on_load`\n\n**There was a performance drop in an early preview 0.11 binary. Is that fixed?**\n\nRudderStack has made changes to ensure that there won’t be any performance drop in 0.11, compared to 0.10.7. The system works slightly differently though as these internal input vars were not there before.\n\n**How can I keep my Profiles projects up to date along with updating the Python package and migrating the schema version?**\n\nYou can check for the latest Profiles updates in the [changelog](https://www.rudderstack.com/docs/profiles/changelog/).\n\nTo update the Python package and migrate the schema version, you can standardise on a single pip release across the org and use the schema version that is native to that binary. When you move to a different binary, migrate your projects to the schema version native to it.\n\nContact Profiles support team in our [Community Slack](https://rudderstack.com/join-rudderstack-slack-community) for specific questions.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Profiles FAQ | RudderStack Docs",
    "description": "Commonly asked questions on RudderStack Profiles.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.11/",
    "markdown": "# Profiles 0.11 | RudderStack Docs\n\nPrivacy Overview\n\nThis site uses cookies to improve your experience while you navigate through the website. Out of these cookies, the cookies that are categorized as necessary are stored on your browser as they are as essential for the working of basic functionalities of the website. We also use third-party cookies that help us analyze and understand how you use this website. These cookies will be stored in your browser only with your consent. You also have the option to opt-out of these cookies. But opting out of some of these cookies may have an effect on your browsing experience.\n\nNecessary cookies are absolutely essential for the website to function properly. This category only includes cookies that ensures basic functionalities and security features of the website. These cookies do not store any personal information.",
    "title": "Profiles 0.11 | RudderStack Docs",
    "description": "Documentation for Profiles v0.11",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.10/overview/",
    "markdown": "# Profiles Overview | RudderStack Docs\n\nLearn how to create unified customer records in your warehouse using RudderStack Profiles.\n\nAvailable Plans\n\n*   enterprise\n\n* * *\n\n*     6 minute read  \n    \n\nModern data teams rely on their warehouse as a single source of truth for customer data. RudderStack’s **Profiles** feature unifies every user touchpoint and trait into comprehensive customer profiles, establishing the data warehouse as the core of the customer data platform.\n\n> ![success](https://www.rudderstack.com/docs/images/tick.svg)\n> \n> With Profiles, data teams can efficiently resolve identities and create user features to produce a comprehensive customer 360 table.\n\n## Highlights\n\nSee the following guides to learn more about Profiles features and their usage:\n\n| Guide | Description |\n| --- | --- |\n| [Get started](https://www.rudderstack.com/docs/archive/profiles/0.10/get-started/) | Create your first Profiles project using the [RudderStack dashboard](https://www.rudderstack.com/docs/archive/profiles/0.10/get-started/quickstart-ui/) or [Profile Builder (PB) CLI tool](https://www.rudderstack.com/docs/archive/profiles/0.10/get-started/profile-builder/). |\n| [Identity stitching](https://www.rudderstack.com/docs/archive/profiles/0.10/identity-stitching/) | Stitch different identifiers across multiple channels to create a comprehensive user profile. |\n| [Feature development](https://www.rudderstack.com/docs/archive/profiles/0.10/feature-development/) | Enhance the unified profiles with additional data points and features. |\n| [Warehouse permissions](https://www.rudderstack.com/docs/archive/profiles/0.10/developer-guides/permissions/) | Grant RudderStack the required permissions on your data warehouse. |\n| [Project structure](https://www.rudderstack.com/docs/archive/profiles/0.10/developer-guides/structure/) | Know the detailed project structure of a Profiles project. |\n| [Examples](https://www.rudderstack.com/docs/archive/profiles/0.10/example/) | Create sample Profiles projects using different model types. |\n| [Glossary](https://www.rudderstack.com/docs/archive/profiles/0.10/glossary/) | Commonly used Profiles terminology. |\n\n## How Profiles works\n\nRudderStack helps you build a complete CDP on top of your data warehouse in three stages - **Collect**, **Unify**, and **Activate**.\n\nThe following sections highlight RudderStack’s comprehensive solution at every stage to create a complete customer profile.\n\n[![Profiles Overview](https://www.rudderstack.com/docs/images/profiles/profiles-overview.png)](https://www.rudderstack.com/docs/images/profiles/profiles-overview.png)\n\n### Collect\n\nFirst, RudderStack collects and stores all the source data in your warehouse. This includes:\n\n*   Event data (for example, user interaction data from web and mobile apps)\n*   Data from cloud sources (for example, CRM platforms like Salesforce, support tools like Zendesk, etc.)\n\n#### Known data\n\nRudderStack collects all the information from:\n\n1.  First-party data: Data collected from the enterprise’s own mobile application, websites, POS systems, etc.\n2.  Third-party apps like SalesForce CRM, Zendesk Support, ecommerce payments via Stripe, etc.\n\nThere is a known ID for all of these by which RudderStack collects the data like email, user ID, etc.\n\n#### Unknown Data\n\nThis includes unknown user attributes like `anonymousId` captured from the RudderStack SDKs on the web/mobile apps. It is helpful in tracking user activities in cases where they are not logged in.\n\nThe difference between known and unknown data is that in the former, we have information about the user. First-party data can be known data if a user is logged in. For example, the data from cloud sources will always be known. However, data from the event stream sources can be known or unknown depending on whether a user had logged in.\n\nAs the data is collected, you can apply relevant transformations to it for compliance/security purposes like data governance, privacy, etc.\n\nThe below image highlights a snapshot of the `identifies` and `tracks` [tables](https://www.rudderstack.com/docs/destinations/warehouse-destinations/warehouse-schema/#schema), that RudderStack leverages for unifying the data.\n\n[![Identifies and Tracks tables for stitching](https://www.rudderstack.com/docs/images/profiles/identifies-tracks-stitch.png)](https://www.rudderstack.com/docs/images/profiles/identifies-tracks-stitch.png)\n\n### Unify\n\nThis is the stage where Profiles comes into play. Primarily, two things are done here.\n\n#### ID Stitching\n\nProfiles stitches together all the known and unknown IDs into a single table. The IDs are linked using an autogenerated ID known as `rudderId`, which is akin to a golden record. Imagine a 1-to-many relationship, in which one `rudderId` has multiple values for other IDs like user ID, anonymous ID, email, etc.\n\nWith a `rudderId`, you can easily identify that a customer - who shopped on your website 6 months ago, anonymously browsed from mobile 4 months ago, raised a complaint with the support team 2 months ago - is actually the same customer. RudderStack represents them as different nodes/edges of the ID stitching graph.\n\n[![Identity stitching](https://www.rudderstack.com/docs/images/profiles/identity-stitching.png)](https://www.rudderstack.com/docs/images/profiles/identity-stitching.png)\n\n#### Entity Traits 360\n\nIf the features/traits of an entity are spread across multiple entity vars and ML models, you can use Entity Traits 360 to get them together into a single view. These models are usually defined in the `pb_project.yaml` file by creating entries under `serve_traits` key with corresponding entity.\n\n[![Features generation](https://www.rudderstack.com/docs/images/profiles/features-generation.png)](https://www.rudderstack.com/docs/images/profiles/features-generation.png)\n\n#### Feature Table\n\nThe entity vars specified in the project are unified into a view. A **Feature Table** is a unified customer profile containing useful information for each customer. Once all the known and unknown identities are stitched together, you can trace back activities for all such identifiers and aggregate them under the common `rudderId`. This is helpful in calculating features across all such interactions.\n\nSome common use cases are computing the customer’s total LTV (lifetime value), purchase history, number of days a customer was active, etc.\n\n### Activate\n\nActivation is a two-step process. In the first step, the user creates the target audience using RudderStack’s [Audiences](https://www.rudderstack.com/docs/data-pipelines/reverse-etl/features/audiences/) feature. They would then use Reverse ETL to route this audience information (also persisted in the same cloud warehouse) to downstream marketing tools like Braze, Mailchimp, etc.\n\n[![Audience Builder / Cohorts](https://www.rudderstack.com/docs/images/profiles/audience-builder-cohorts.png)](https://www.rudderstack.com/docs/images/profiles/audience-builder-cohorts.png)\n\nAdditionally - you can use the feature tables as inputs to ML models for use cases like churn prediction, lifetime value (LTV) prediction, etc. Again, you can route the output of such models to marketing platforms via Reverse ETL.\n\nAs you keep collecting data, Profiles continues to unify and send it for activation.\n\n[![Profiles lifecycle](https://www.rudderstack.com/docs/images/profiles/profiles-life-cycle.png)](https://www.rudderstack.com/docs/images/profiles/profiles-life-cycle.png)\n\nIn this way, you can make informed decisions, run personalized marketing campaigns, and enhance the overall customer experience across multiple platforms.\n\n## Why use Profiles?\n\nData teams often face challenges when building a complete view of their customer. Maintaining large, complex SQL models or working around the limitations of rigid SaaS platforms is time-consuming and expensive at scale.\n\nProfiles simplifies this process of unifying customer data by automating the manual data engineering and modeling required to build an identity graph, layer new data sources into customer profiles, and compute user features that leverage data from various disparate sources.\n\nUsing Profiles, data teams can quickly build and easily maintain a comprehensive customer 360 table and make it available to downstream teams and tools.\n\n### Move faster with an end-to-end platform\n\nProfiles integrates directly with RudderStack’s other pipelines, which enables it to automatically solve otherwise complex data challenges throughout the entire data lifecycle.\n\nCustomer data ingested through RudderStack’s [Event Stream](https://www.rudderstack.com/docs/sources/event-streams/) and [Cloud Extract](https://www.rudderstack.com/docs/sources/extract/) ETL pipelines have known schemas and unique identifiers. Profiles can produce a baseline identity graph, and user features out of the box. Data teams can then augment the graph and features using any other data in their warehouse.\n\nRudderStack’s [Reverse ETL](https://www.rudderstack.com/docs/sources/reverse-etl/) pipeline makes it easy to send data from the customer 360 table directly to the downstream tools used by marketing, customer success, product, and other teams.\n\n### Enrich user profiles with features\n\nWith Profiles, you can enhance user profiles with additional data points and features. When new data sources are added, discovered, or calculated, data teams can add them to their Profiles configuration without having to clean data and update complex models and dependencies.\n\nThe features/traits can include demographic information, preferences, purchase history, browsing behavior, or other static or computed data points.\n\n### Unlock deeper insights\n\nProfiles extends its capabilities to support features derived from complex concepts such as funnels, organizational metrics, and machine learning models. You can understand your customer’s journey through your sales funnel or locate each user across the histogram of customer metric values by simply defining a trait.\n\n### Deliver personalization and recommendations\n\nUsing Profiles, you can ship projects like personalization significantly faster by focusing entirely on activating key user features instead of cleaning and modeling data to build them.\n\n### Predict user conversions and churn\n\nYou can leverage [Profiles ML](https://www.rudderstack.com/docs/archive/profiles/0.10/predictions/) to build predictive features that help you predict in advance whether a lead is likely to convert, or a customer is likely to churn or make a purchase.\n\n## Who can leverage Profiles?\n\nProfiles is built for data engineers, data scientists, and technical marketers. You can define identity stitching, and user features as configuration files without requiring deep SQL, Python, or technical knowledge.\n\nYou can define the associated properties or attributes that provide detailed information for each new feature. For example, if the feature is `purchase_history`, its properties can include the date of purchase, product category, or order value.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Profiles Overview | RudderStack Docs",
    "description": "Learn how to create unified customer records in your warehouse using RudderStack Profiles.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.11/overview/",
    "markdown": "# Profiles Overview | RudderStack Docs\n\nLearn how to create unified customer records in your warehouse using RudderStack Profiles.\n\nAvailable Plans\n\n*   enterprise\n\n* * *\n\n*     6 minute read  \n    \n\nModern data teams rely on their warehouse as a single source of truth for customer data. RudderStack’s **Profiles** feature unifies every user touchpoint and trait into comprehensive customer profiles, establishing the data warehouse as the core of the customer data platform.\n\n> ![success](https://www.rudderstack.com/docs/images/tick.svg)\n> \n> With Profiles, data teams can efficiently resolve identities and create user features to produce a comprehensive customer 360 table.\n\n## Highlights\n\nSee the following guides to learn more about Profiles features and their usage:\n\n| Guide | Description |\n| --- | --- |\n| [Get started](https://www.rudderstack.com/docs/archive/profiles/0.11/get-started/) | Create your first Profiles project using the [RudderStack dashboard](https://www.rudderstack.com/docs/archive/profiles/0.11/get-started/quickstart-ui/) or [Profile Builder (PB) CLI tool](https://www.rudderstack.com/docs/archive/profiles/0.11/get-started/profile-builder/). |\n| [Identity stitching](https://www.rudderstack.com/docs/archive/profiles/0.11/identity-stitching/) | Stitch different identifiers across multiple channels to create a comprehensive user profile. |\n| [Feature development](https://www.rudderstack.com/docs/archive/profiles/0.11/feature-development/) | Enhance the unified profiles with additional data points and features. |\n| [Warehouse permissions](https://www.rudderstack.com/docs/archive/profiles/0.11/developer-guides/permissions/) | Grant RudderStack the required permissions on your data warehouse. |\n| [Project structure](https://www.rudderstack.com/docs/archive/profiles/0.11/developer-guides/structure/) | Know the detailed project structure of a Profiles project. |\n| [Examples](https://www.rudderstack.com/docs/archive/profiles/0.11/example/) | Create sample Profiles projects using different model types. |\n| [Glossary](https://www.rudderstack.com/docs/archive/profiles/0.11/glossary/) | Commonly used Profiles terminology. |\n\n## How Profiles works\n\nRudderStack helps you build a complete CDP on top of your data warehouse in three stages - **Collect**, **Unify**, and **Activate**.\n\nThe following sections highlight RudderStack’s comprehensive solution at every stage to create a complete customer profile.\n\n[![Profiles Overview](https://www.rudderstack.com/docs/images/profiles/profiles-overview.png)](https://www.rudderstack.com/docs/images/profiles/profiles-overview.png)\n\n### Collect\n\nFirst, RudderStack collects and stores all the source data in your warehouse. This includes:\n\n*   Event data (for example, user interaction data from web and mobile apps)\n*   Data from cloud sources (for example, CRM platforms like Salesforce, support tools like Zendesk, etc.)\n\n#### Known data\n\nRudderStack collects all the information from:\n\n1.  First-party data: Data collected from the enterprise’s own mobile application, websites, POS systems, etc.\n2.  Third-party apps like SalesForce CRM, Zendesk Support, ecommerce payments via Stripe, etc.\n\nThere is a known ID for all of these by which RudderStack collects the data like email, user ID, etc.\n\n#### Unknown Data\n\nThis includes unknown user attributes like `anonymousId` captured from the RudderStack SDKs on the web/mobile apps. It is helpful in tracking user activities in cases where they are not logged in.\n\nThe difference between known and unknown data is that in the former, we have information about the user. First-party data can be known data if a user is logged in. For example, the data from cloud sources will always be known. However, data from the event stream sources can be known or unknown depending on whether a user had logged in.\n\nAs the data is collected, you can apply relevant transformations to it for compliance/security purposes like data governance, privacy, etc.\n\nThe below image highlights a snapshot of the `identifies` and `tracks` [tables](https://www.rudderstack.com/docs/destinations/warehouse-destinations/warehouse-schema/#schema), that RudderStack leverages for unifying the data.\n\n[![Identifies and Tracks tables for stitching](https://www.rudderstack.com/docs/images/profiles/identifies-tracks-stitch.png)](https://www.rudderstack.com/docs/images/profiles/identifies-tracks-stitch.png)\n\n### Unify\n\nThis is the stage where Profiles comes into play. Primarily, two things are done here.\n\n#### ID Stitching\n\nProfiles stitches together all the known and unknown IDs into a single table. The IDs are linked using an autogenerated ID known as `rudderId`, which is akin to a golden record. Imagine a 1-to-many relationship, in which one `rudderId` has multiple values for other IDs like user ID, anonymous ID, email, etc.\n\nWith a `rudderId`, you can easily identify that a customer - who shopped on your website 6 months ago, anonymously browsed from mobile 4 months ago, raised a complaint with the support team 2 months ago - is actually the same customer. RudderStack represents them as different nodes/edges of the ID stitching graph.\n\n[![Identity stitching](https://www.rudderstack.com/docs/images/profiles/identity-stitching.png)](https://www.rudderstack.com/docs/images/profiles/identity-stitching.png)\n\n#### Entity Traits 360\n\nIf the features/traits of an entity are spread across multiple entity vars and ML models, you can use Entity Traits 360 to get them together into a single view. These models are usually defined in the `pb_project.yaml` file by creating entries under `serve_traits` key with corresponding entity.\n\n[![Features generation](https://www.rudderstack.com/docs/images/profiles/features-generation.png)](https://www.rudderstack.com/docs/images/profiles/features-generation.png)\n\n#### Feature Table\n\nThe entity vars specified in the project are unified into a view. A **Feature Table** is a unified customer profile containing useful information for each customer. Once all the known and unknown identities are stitched together, you can trace back activities for all such identifiers and aggregate them under the common `rudderId`. This is helpful in calculating features across all such interactions.\n\nSome common use cases are computing the customer’s total LTV (lifetime value), purchase history, number of days a customer was active, etc.\n\n### Activate\n\nActivation is a two-step process. In the first step, the user creates the target audience using RudderStack’s [Audiences](https://www.rudderstack.com/docs/data-pipelines/reverse-etl/features/audiences/) feature. They would then use Reverse ETL to route this audience information (also persisted in the same cloud warehouse) to downstream marketing tools like Braze, Mailchimp, etc.\n\n[![Audience Builder / Cohorts](https://www.rudderstack.com/docs/images/profiles/audience-builder-cohorts.png)](https://www.rudderstack.com/docs/images/profiles/audience-builder-cohorts.png)\n\nAdditionally - you can use the feature tables as inputs to ML models for use cases like churn prediction, lifetime value (LTV) prediction, etc. Again, you can route the output of such models to marketing platforms via Reverse ETL.\n\nAs you keep collecting data, Profiles continues to unify and send it for activation.\n\n[![Profiles lifecycle](https://www.rudderstack.com/docs/images/profiles/profiles-life-cycle.png)](https://www.rudderstack.com/docs/images/profiles/profiles-life-cycle.png)\n\nIn this way, you can make informed decisions, run personalized marketing campaigns, and enhance the overall customer experience across multiple platforms.\n\n## Why use Profiles?\n\nData teams often face challenges when building a complete view of their customer. Maintaining large, complex SQL models or working around the limitations of rigid SaaS platforms is time-consuming and expensive at scale.\n\nProfiles simplifies this process of unifying customer data by automating the manual data engineering and modeling required to build an identity graph, layer new data sources into customer profiles, and compute user features that leverage data from various disparate sources.\n\nUsing Profiles, data teams can quickly build and easily maintain a comprehensive customer 360 table and make it available to downstream teams and tools.\n\n### Move faster with an end-to-end platform\n\nProfiles integrates directly with RudderStack’s other pipelines, which enables it to automatically solve otherwise complex data challenges throughout the entire data lifecycle.\n\nCustomer data ingested through RudderStack’s [Event Stream](https://www.rudderstack.com/docs/sources/event-streams/) and [Cloud Extract](https://www.rudderstack.com/docs/sources/extract/) ETL pipelines have known schemas and unique identifiers. Profiles can produce a baseline identity graph, and user features out of the box. Data teams can then augment the graph and features using any other data in their warehouse.\n\nRudderStack’s [Reverse ETL](https://www.rudderstack.com/docs/sources/reverse-etl/) pipeline makes it easy to send data from the customer 360 table directly to the downstream tools used by marketing, customer success, product, and other teams.\n\n### Enrich user profiles with features\n\nWith Profiles, you can enhance user profiles with additional data points and features. When new data sources are added, discovered, or calculated, data teams can add them to their Profiles configuration without having to clean data and update complex models and dependencies.\n\nThe features/traits can include demographic information, preferences, purchase history, browsing behavior, or other static or computed data points.\n\n### Unlock deeper insights\n\nProfiles extends its capabilities to support features derived from complex concepts such as funnels, organizational metrics, and machine learning models. You can understand your customer’s journey through your sales funnel or locate each user across the histogram of customer metric values by simply defining a trait.\n\n### Deliver personalization and recommendations\n\nUsing Profiles, you can ship projects like personalization significantly faster by focusing entirely on activating key user features instead of cleaning and modeling data to build them.\n\n### Predict user conversions and churn\n\nYou can leverage [Predictions](https://www.rudderstack.com/docs/archive/profiles/0.11/predictions/) to build predictive features that help you predict in advance whether a lead is likely to convert, or a customer is likely to churn or make a purchase.\n\n## Who can leverage Profiles?\n\nProfiles is built for data engineers, data scientists, and technical marketers. You can define identity stitching, and user features as configuration files without requiring deep SQL, Python, or technical knowledge.\n\nYou can define the associated properties or attributes that provide detailed information for each new feature. For example, if the feature is `purchase_history`, its properties can include the date of purchase, product category, or order value.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Profiles Overview | RudderStack Docs",
    "description": "Learn how to create unified customer records in your warehouse using RudderStack Profiles.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.10/get-started/",
    "markdown": "# Get Started | RudderStack Docs\n\nPrivacy Overview\n\nThis site uses cookies to improve your experience while you navigate through the website. Out of these cookies, the cookies that are categorized as necessary are stored on your browser as they are as essential for the working of basic functionalities of the website. We also use third-party cookies that help us analyze and understand how you use this website. These cookies will be stored in your browser only with your consent. You also have the option to opt-out of these cookies. But opting out of some of these cookies may have an effect on your browsing experience.\n\nNecessary cookies are absolutely essential for the website to function properly. This category only includes cookies that ensures basic functionalities and security features of the website. These cookies do not store any personal information.",
    "title": "Get Started | RudderStack Docs",
    "description": "Create a Profiles project in RudderStack.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.11/get-started/",
    "markdown": "# Get Started | RudderStack Docs\n\nPrivacy Overview\n\nThis site uses cookies to improve your experience while you navigate through the website. Out of these cookies, the cookies that are categorized as necessary are stored on your browser as they are as essential for the working of basic functionalities of the website. We also use third-party cookies that help us analyze and understand how you use this website. These cookies will be stored in your browser only with your consent. You also have the option to opt-out of these cookies. But opting out of some of these cookies may have an effect on your browsing experience.\n\nNecessary cookies are absolutely essential for the website to function properly. This category only includes cookies that ensures basic functionalities and security features of the website. These cookies do not store any personal information.",
    "title": "Get Started | RudderStack Docs",
    "description": "Create a Profiles project in RudderStack.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.10/get-started/profile-builder/",
    "markdown": "# Profile Builder CLI | RudderStack Docs\n\nCreate a Profiles project using the Profile Builder (PB) tool.\n\n* * *\n\n*     5 minute read  \n    \n\n**Profile Builder (PB)** is a command-line interface (CLI) tool that simplifies data transformation within your warehouse. It generates customer profiles by stitching data together from multiple sources.\n\nThis guide lists the detailed steps to install and use the Profile Builder (PB) tool to create, configure, and run a new project.\n\n> ![success](https://www.rudderstack.com/docs/images/tick.svg)\n> \n> You can also use this tool to enhance the project created using predefined Profiles templates in the [RudderStack dashboard](https://www.rudderstack.com/docs/archive/profiles/0.10/get-started/quickstart-ui/).\n\n## Prerequisites\n\nYou must have [Python 3](https://www.python.org/downloads/) installed on your machine.\n\n## Steps\n\nTo set up a project using the PB tool, follow these steps:\n\n### 1: Install PB\n\nInstall the Profile Builder tool by running the following command:\n\n```\npip3 install profiles-rudderstack\n```\n\nIf you have already installed PB, use the following command to update its version:\n\n```\npip3 install profiles-rudderstack - U\n```\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> RudderStack recommends using a Python virtual environment to maintain an isolated and clean environment.\n> \n> ```\n> pipx install profiles-rudderstack\n> ```\n\n### 2: Create warehouse connection\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> RudderStack supports **Snowflake**, **Redshift**, and **Databricks** warehouses for Profiles. You must grant certain [warehouse permissions](https://www.rudderstack.com/docs/archive/profiles/0.10/developer-guides/permissions/) to let RudderStack read from schema having the source tables (for example, `tracks` and `identifies` tables generated via Event Stream sources), and write data in a new schema created for Profiles.\n\nCreate a warehouse connection to allow PB to access your data:\n\nThen, follow the prompts to enter details about your warehouse connection.\n\nA sample connection for a Snowflake account is as follows:\n\n```\nEnter Connection Name: test\nEnter target:  (default:dev):  # Press enter, leaving it to default\nEnter account: ina13147.us-east-1\nEnter warehouse: rudder_warehouse\nEnter dbname: your_rudderstack_db \nEnter schema: rs_profiles # A segregated schema for storing tables/views created by Profiles\nEnter user: profiles_test_user\nEnter password: <password>\nEnter role: profiles_role\nAppend to /Users/<user_name>/.pb/siteconfig.yaml? [y/N]\ny\n```\n\nA sample connection for a Redshift account is as follows:\n\n```\nEnter Connection Name: test\nEnter target:  (default:dev):  # Press enter, leaving it to default\nEnter host: warehouseabc.us-west-1.redshift.amazonaws.com\nEnter port: 5439\nEnter dbname: your_rudderstack_db \nEnter schema: rs_profiles # A segregated schema for storing tables/views created by Profiles\nEnter user: profiles_test_user\nEnter password: <password>\nAppend to /Users/<user_name>/.pb/siteconfig.yaml? [y/N]\ny\n```\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> To know more about host, log in to your AWS Console and go to **Clusters**.\n\nA sample connection for a Databricks account is as follows:\n\n```\nEnter Connection Name: test\nEnter target:  (default:dev):  # Press enter, leaving it to default\nEnter host: a1.8.azuredatabricks.net # The hostname or URL of your Databricks cluster\nEnter port: 443 # The port number used for establishing the connection. Usually it is 443 for https connections.\nEnter http_endpoint: /sql/1.0/warehouses/919uasdn92h # The path or specific endpoint you wish to connect to.\nEnter access_token: <password> # The access token created for authenticating the instance.\nEnter user: profiles_test_user # Username of your Databricks account.\nEnter schema: rs_profiles # A segregated schema for storing tables/views created by Profiles\nEnter catalog: your_rudderstack_db # The database or catalog having data that you’ll be accessing.\nAppend to /Users/<user_name>/.pb/siteconfig.yaml? [y/N]\ny\n```\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> RudderStack currently supports Databricks on Azure. To get the Databricks connection details:\n> \n> 1.  Log in to your Azure’s Databricks Web UI.\n> 2.  Click on **SQL Warehouses** on the left.\n> 3.  Select the warehouse to connect to.\n> 4.  Select the **Connection Details** tab.\n\nThis creates a [site configuration file](https://www.rudderstack.com/docs/archive/profiles/0.10/developer-guides/site-configuration/) inside your home directory: `~/.pb/siteconfig.yaml`. If you don’t see the file, enable the **View hidden files** option.\n\n### 3: Create project\n\nRun the following command to create a sample project:\n\n```\npb init pb-project -o MyProfilesProject\n```\n\nThe above command creates a new project in the **MyProfilesProject** folder with the following structure:\n\n[![Project structure](https://www.rudderstack.com/docs/images/profiles/project-structure.webp)](https://www.rudderstack.com/docs/images/profiles/project-structure.webp)\n\nSee [Project structure](https://www.rudderstack.com/docs/archive/profiles/0.10/developer-guides/structure/) for more information on the PB project files.\n\nNavigate to the `pb_project.yaml` file and set the value of `connection:` to the connection name as defined in the previous step.\n\n### 4: Change input sources\n\n*   Navigate to your project and open the `models/inputs.yaml` file. Here, you will see a list of tables/views along with their respective ID types.\n*   Replace the placeholder table names with the actual table names in the `table` field.\n\nSee [Project structure](https://www.rudderstack.com/docs/archive/profiles/0.10/developer-guides/structure/#inputs) for more information on setting these values.\n\n### 5: Validate project\n\nNavigate to your project and validate your warehouse connection and input sources:\n\nIf there are no errors, proceed to the next step. In case of errors, check if your warehouse schemas and tables have the [required permissions](https://www.rudderstack.com/docs/archive/profiles/0.10/developer-guides/permissions/).\n\n### 6: Generate SQL files\n\nCompile the project:\n\nThis generates SQL files in the `output/` folder that you can run directly on the warehouse.\n\n### 7: Generate output tables\n\nRun the project and generate [material tables](https://www.rudderstack.com/docs/archive/profiles/0.10/glossary/#material-tables):\n\nThis command generates and runs the SQL files in the warehouse, creating the material tables.\n\n### 8: View generated tables\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> The view `user_default_id_stitcher` will always point to the latest generated ID stitcher and `user_profile` to the feature table.\n\nExecute below query to view the generated tables in the warehouse:\n\n```\nselect * from <table_name> limit 10;\n```\n\nHere’s what the columns imply:\n\n![ID Stitcher Table](https://www.rudderstack.com/docs/images/profiles/idstitcher-table.webp)\n\n*   **user\\_main\\_id**: Rudder ID generated by Profile Builder. Think of a 1-to-many relationship, with one Rudder ID connected to different IDs belonging to same user such as User ID, Anonymous ID, Email, Phone number, etc.\n*   **other\\_id**: ID in input source tables that is stitched to a Rudder ID.\n*   **other\\_id\\_type**: Type of the other ID to be stitched (User ID, Anonymous ID, Email, etc).\n*   **valid\\_at**: Date at which the corresponding ID value occurred in the source tables. For example, the date at which a customer was first browsing anonymously, or when they logged into the CRM with their email ID, etc.\n\n![Feature Table](https://www.rudderstack.com/docs/images/profiles/feature-table.webp)\n\n*   **user\\_main\\_id**: Rudder ID generated by Profile Builder.\n*   **valid\\_at**: Date when the feature table entry was created for this record.\n*   **first\\_seen, last\\_seen, country, first\\_name, etc.** - All features for which values are computed.\n\n## Migrate your existing project\n\nTo migrate an existing PB project to the [schema version](https://www.rudderstack.com/docs/archive/profiles/0.10/glossary/#schema-versions) supported by your PB binary, navigate to your project’s folder. Then, run the following command to replace the contents of the existing folder with the new one:\n\n```\npb migrate auto --inplace\n```\n\nA confirmation message appears on screen indicating that the migration is complete. A sample message for a user migrating their project from version 25 to 44:\n\n```\n2023-10-17T17:48:33.104+0530\tINFO\tmigrate/migrate.go:161\t\nProject migrated from version 25 to version 44\n```\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Profile Builder CLI | RudderStack Docs",
    "description": "Create a Profiles project using the Profile Builder (PB) tool.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.11/example/packages/",
    "markdown": "# Additional Concepts | RudderStack Docs\n\nAdditional concepts related to Profiles like packages, best practices, partial feature tables, etc.\n\n* * *\n\n*     12 minute read  \n    \n\nThis guide explains some of the advanced concepts related to Profiles.\n\n## Packages\n\nProfiles gives you the flexibility to utilize models from existing library projects while defining your own models and inputs within the PB project. This approach allows for a seamless integration of library of pre-existing features, which are readily available and can be applied directly to data streamed into your warehouse.\n\nIn the absence of any explicitly defined models, the PB project is capable of compiling and running models from the library package given that inputs are present in the warehouse as assumed in the lib package.\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> Packages currently work only on Snowflake.\n\nThe following list of packages are currently available in Profiles. You can [contact the RudderStack team](mailto:support@rudderstack.com) to access these:\n\n*   [profiles-corelib](https://github.com/rudderlabs/profiles-corelib)\n*   [profiles-base-features](https://github.com/rudderlabs/rudderstack-profiles-base-features)\n*   profiles-shopify-features\n*   profiles-ecommerce-features\n*   profiles-stripe-features\n*   profiles-multieventstream-features\n\nGenerally, there will be some deviations in terms of the database name and schema name of input models - however, you can easily handle this by remapping inputs.\n\nA sample `pb_project.yaml` file may look as follows:\n\n```\nname: app_project\nschema_version: 54\nprofile: test\npackages:\n  - name: test_ft\n    gitUrl: \"https://github.com/rudderlabs/librs360-shopify-features/tree/main\"\n```\n\nIn this case, the PB project imports a single package. It does not require a separate `models` folder or entities as the input and output models will be sourced from the imported packages.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> Note that:\n> \n> *   If non-mandatory inputs required by the model are not present in the warehouse, you can still run the model.\n> *   If there is any deviation in the table/view name for input models, that is, if the inputs assumed in library package are present under some other name, make sure to do the remapping.\n> *   If some of the assumed inputs are not present at all, they should be remapped to `nil`. This way you can create and run imported packages with minimal set of inputs present.\n\nFor example, to import a library package with the name of `shopify_features`:\n\n```\npackages: \n  - name: shopify_features\n    url: https://github.com/rudderlabs/librs360-shopify-features/tree/main\n    inputsMap: \n      rsCartCreate: inputs/rsWarehouseCartCreate\n      rsCartUpdate: inputs/rsCartUpdate\n      rsIdentifies: inputs/rsIdentifies\n      rsOrderCancelled: inputs/rsOrderCancelled\n      rsOrderCreated: inputs/rsOrderCreated\n      rsPages: nil\n      rsTracks: nil\n```\n\nIn `models`/`inputs.yaml`, these inputs need to be defined with table names present in the warehouse.\n\n```\ninputs:\n  - name: rsWarehouseCartCreate\n    table: YOUR_DB.YOUR_SCHEMA.CART_CREATE_TABLE_NAME_IN_YOUR_WH\n    occurred_at_col: timestamp\n    ids:\n      - select: \"anonymous_id\"\n        type: anonymous_id\n        entity: user\n    source_metadata:\n      role: shopify\n      category: webhook\n  - name: rsIdentifies\n    table: YOUR_DB.YOUR_SCHEMA.IDENTIFIES\n    occurred_at_col: timestamp\n    ids:\n      - select: \"user_id\"\n        type: user_id\n        entity: user\n      - select: \"anonymous_id\"\n        type: anonymous_id\n        entity: user\n      - select: \"lower(email)\"\n        type: email\n        entity: user\n    source_metadata:\n      role: shopify\n      category: webhook\n```\n\nNote that the name of the table/view is changed to the appropriate name in your warehouse. If tables are present with the same name (including database name and schema name) then no remapping is required.\n\n### Modify ID types\n\n#### Extend existing package\n\nYou can add custom ID types to the default list or modify an existing one by extending the package to include your specifications.\n\nFor the corresponding `id_type`, add the key `extends:` followed by name of the same/different `id_type` that you wish to extend and the `filters` with `include`/`exclude` values.\n\n```\n---pb_project.yaml---\npackages:\n  - name: corelib\n    url: \"https://github.com/rudderlabs/rudderstack-profiles-corelib/tag/schema_{{best_schema_version}}\"\nid_types:\n  - name: user_id\n    extends: user_id\n    filters:\n      - type: exclude\n        value: 123456\nid_types:\n  - name: customer_id\n    extends: user_id\n    filters:\n      - type: include\n        regex: sample\n```\n\n*   **id\\_types**: Enlists the type of data to be used for creating ID stitcher / EntityVar / InputVar. For example, anonymous IDs that do not include the value `undefined` or email addresses in proper format.\n    *   **extends**: Name of the ID type that you wish to extend.\n    *   **name**: The type of data that will be fetched, like email, user ID, etc. It is different from whatever is present in the table column, like int or varchar.\n    *   **filters**: Filter(s) that the type should go through before being included. Filters are processed in order. Current filters enable one to include and exclude specific values or regular expressions.\n\n#### Custom list of ID types\n\nTo have custom list of ID types other than the provisions in the default package, you can remove and add your list as follows:\n\n```\nentities:\n  - name: user\n    id_types:\n      - user_id\n      - anonymous_id\n      - email\n\nid_types:\n  - name: user_id\n  - name: anonymous_id\n    filters:\n      - type: exclude\n        value: \"\"\n      - type: exclude\n        value: \"unknown\"\n      - type: exclude\n        value: \"NaN\"\n  - name: email\n    filters:\n    - type: include\n      regex: \"[A-Za-z0-9+_.-]+@(.+)\"\n```\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> Make sure that the ID types are also defined in the entity definition.\n\n## Model Contracts\n\nWith model contracts, you can declare constraints that the model adheres to. A model having a dependency on another model would also need to declare a contract specifying what columns and entities the input model must have. For contract validation, these columns should be present in the referenced model.\n\nFor an input of a project like a library project, the model contract is used to enforce constraints on tables/views that get wired to it downstream.\n\n```\n# inputs.yaml\n  - name: rsIdentifies\n    contract:\n      is_optional: false\n      is_event_stream: true\n      with_entity_ids:\n        - user\n      with_columns:\n        - name: timestamp\n        - name: user_id\n        - name: anonymous_id\n        - name: email\n```\n\nIn SQL model, the contract would contain all the columns from IDs and features. Each internal model also publishes the contract it promises to adhere to. Suppose `rsSessionTable` has an input `shopify_session_features`. Model contracts enable `rsSessionTable` to specify the constraints that `shopify_session_features` must adhere to.\n\n```\nmodels:\n- name: rsSessionTable\n  model_type: sql_template\n  model_spec:\n    ... # model specifications\n    single_sql: |\n      {% set contract = BuildContract('{\"with_columns\":[{\"name\":\"user_id\"}, {\"name\":\"anonymous_id\"}]}') %}\n      {% with SessionFeature = this.DeRef(\"models/shopify_session_features\",contract)%}\n          select user_id as id1, anonymous_id as id2 from {{SessionFeature}}\n  \tcontract:\n    with_entity_ids:\n      - user\n    with_columns:\n      - name: user_id\n        type: string\n        is_optional: true # false being default\n      - name: anonymous_id\n        type: string\n        is_optional: true # false being default\n```\n\nHere, `rsSessionTable` declares that its input `shopify_session_features` must have columns `user_id` and `anonymous_id`. This helps in improving data quality and error handling. Internally, this requested contract is validated against `shopify_session_features`’s actual contract. For validation to pass, `input_shopify_session_features_contract` must be a subset of `shopify_session_features`’s published contract.\n\nThis enables more comprehensive static and dynamic validations of our projects.\n\n## Partial feature tables\n\nPartial feature tables are created when only a few input sources are available.\n\nFor example, lets say that you import a library package and some of the input models assumed in the package are not present in your warehouse.\n\nWhen you remap some of these input models to nil, those inputs and the features directly or indirectly dependent upon those inputs are disabled. In such cases, a partial feature table is created from the rest of the available inputs. Similarly, ID stitcher also runs even if few of the edge sources are not present in the warehouse or remapped to nil.\n\n## Pre and post hooks\n\nA pre hook enables you to execute an SQL before running a model, for example, if you want to change DB access, create a DB object, etc. Likewise, a post hook enables you to execute an SQL after running a model. The SQL can also be templatized. Here’s an example code snippet:\n\n```\nmodels:\n  - name: test_id_stitcher\n    model_type: id_stitcher\n    hooks:\n      pre_run: \"CREATE OR REPLACE VIEW {{warehouse.ObjRef('V1')}} AS (SELECT * from {{warehouse.ObjRef('Temp_tbl_a')}});\"\n      post_run: 'CREATE OR REPLACE VIEW {{warehouse.ObjRef(\"V2\")}} AS (SELECT * from {{warehouse.ObjRef(\"Temp_tbl_a\")}});'\n    model_spec:\n      - # rest of model specs go here\n```\n\n## Use Amazon S3 bucket as input\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> This is an experimental feature.\n\nIf you store data in your Amazon S3 bucket in a CSV file format, you can use it as an input for the Profiles models. The S3 URI path must be specified in the `app_defaults.s3`:\n\n```\nname: s3_table\ncontract:\n  is_optional: false\n  is_event_stream: true\n  with_entity_ids:\n    - user\n  with_columns:\n    - name: insert_ts\n      datatype: timestamp\n    - name: num_a\n      datatype: integer\napp_defaults:\n  s3: \"s3://bucket-name/prefix/example.csv\"\n  occurred_at_col: insert_ts\n  ids:\n    - select: \"id1\"\n      type: test_id\n      entity: user\n    - select: \"id2\"\n      type: test_id\n      entity: user\n```\n\nEnsure that the CSV file follows the standard format with the first row as the header containing column names, for example:\n\n```\nID1,ID2,ID3,INSERT_TS,NUM_A\na,b,ex,2000-01-01T00:00:01Z,1\nD,e,ex,2000-01-01T00:00:01Z,3\nb,c,ex,2000-01-01T00:00:01Z,2\nNULL,d,ex,2000-01-01T00:00:01Z,4\n```\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> Note that:\n> \n> *   To escape comma (`,`) from any cell of the CSV file, enclose that cell with double quotes `\" \"` .\n> *   Double quotes (`\" \"`) enclosing a cell are ignored.\n\nFollow the below steps to grant PB the required permissions to access the file in S3 Bucket:\n\n### Private S3 bucket\n\nAdd `region`, [`access key id`](#generate-access-key-id-and-secret-access-key), [`secret access key`](#generate-access-key-id-and-secret-access-key), and [`session token`](#generate-session-token) in your `siteconfig` file so that PB can access the private bucket. By default, the region is set to `us-east-1` unless specified otherwise.\n\n```\naws_credential:\n    region: us-east-1\n    access_key: **********\n    secret_access_key: **********\n    session_token: **********\n```\n\n#### Generate `access key id` and `secret access key`\n\n1.  Open the AWS IAM console in your AWS account.\n2.  Click **Policies**.\n3.  Click **Create policy**.\n4.  In the Policy editor section, click the JSON option.\n5.  Replace the existing JSON policy with the following policy and replace the <bucket\\_name> with your actual bucket name:\n\n```\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:GetObjectVersion\"\n            ],\n            \"Resource\": \"arn:aws:s3:::<bucket_name>/*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:ListBucket\",\n                \"s3:GetBucketLocation\"\n            ],\n            \"Resource\": \"arn:aws:s3:::<bucket_name>\",\n            \"Condition\": {\n                \"StringLike\": {\n                    \"s3:prefix\": [\n                        \"*\"\n                    ]\n                }\n            }\n        }\n    ]\n}\n```\n\n6.  Click **Review policy**.\n7.  Enter the policy name. Then, click **Create policy** to create the policy.\n\nFurther, create an IAM user by following the below steps:\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> An IAM user requires the following permissions on an S3 bucket and folder to access files in the folder (and sub-folders):\n> \n> *   s3:GetBucketLocation\n> *   s3:GetObject\n> *   s3:GetObjectVersion\n> *   s3:ListBucket\n\n1.  In AWS IAM console, Click **Users**.\n2.  Click **Create user**.\n3.  Enter a name for the user.\n4.  Select Programmatic access as the access type, then click **Next: Permissions**.\n5.  Click **Attach existing policies directly**, and select the policy you created earlier. Then click **Next**.\n6.  Review the user details, then click **Create user**.\n7.  Copy the access key ID and secret access key values.\n\n#### Generate `session token`\n\n1.  Use the AWS CLI to create a named profile with the AWS credentials that you copied in the previous step.\n2.  To get the session token, run the following command:\n\n```\n $ aws sts get-session-token --profile <named-profile>\n```\n\nSee [Snowflake](https://docs.snowflake.com/en/user-guide/data-load-s3-config-aws-iam-user), [Redshift](https://docs.aws.amazon.com/redshift/latest/dg/copy-usage_notes-access-permissions.html), and [Databricks](https://docs.databricks.com/en/ingestion/copy-into/generate-temporary-credentials.html) for more information.\n\n### Public S3 Bucket\n\nYou **must** have the following permissions on the S3 bucket and folder to access files in the folder (and sub-folders):\n\n*   s3:GetBucketLocation\n*   s3:GetObject\n*   s3:GetObjectVersion\n*   s3:ListBucket\n\nYou can use the following policy in your bucket to grant the above permissions:\n\n1.  Go to the **Permissions** tab of your S3 bucket.\n2.  Edit bucket policy in **Permissions** tab and add the following policy. Replace the <bucket\\_name> with your actual bucket name:\n\n```\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": \"*\",\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:GetObjectVersion\"\n            ],\n            \"Resource\": \"arn:aws:s3:::<bucket_name>/*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": \"*\",\n            \"Action\": [\n                \"s3:ListBucket\",\n                \"s3:GetBucketLocation\"\n            ],\n            \"Resource\": \"arn:aws:s3:::<bucket_name>\"\n        }\n    ]\n}\n```\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> In Redshift, you additionally need to set an IAM role as **default** for your cluster, unless access keys are provided. It is necessary because more than one IAM role can be associated with the cluster, and Redshift needs explicit permission granted through an IAM role to access the S3 bucket (Public or Private).\n> \n> Follow [Redshift Documentation](https://docs.aws.amazon.com/redshift/latest/mgmt/default-iam-role.html#set-default-iam) for setting an IAM role as default.\n\n## Use CSV file as input\n\nAn input file (`models/inputs.yaml`) contains details of input sources such as tables, views, or CSV files along with column name and SQL expression for retrieving values.\n\nYou can read data from a CSV file by using `csv: <path_to_filename>` under `app_defaults` in the input specs. CSV data is loaded internally as a single SQL select query, making it useful for seeding tests.\n\nA sample code is as shown:\n\n```\n    app_defaults:\n      csv: \"../common.xtra/Temp_tbl_a.csv\"\n      # remaining syntax is same for all input sources\n```\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> RudderStack does not support CSV files with more than a few hundred rows.\n\n## Filter data\n\nYou can filter out any data by using the `filters` field in your projects file:\n\nFor example, if you want to exclude all the blacklisted email addresses, you can create an input model (for example, `csv_email_blacklist`) with CSV file as a source, that contains all such email addresses:\n\n```\nid_types:\n  - name: email\n    filters:\n      - type: exclude\n        sql:\n          select: email\n          from: inputs/csv_email_blacklist\n```\n\nAnother example, if you want to exclude all the user\\_ids, you can create an SQL model (for example, `sql_exclusion_model`) that contains a specific logic to enlist all such IDs:\n\n```\nid_types:\n  - name: user_id\n    filters:\n      - type: exclude\n        sql:\n          select: user_id\n          from: inputs/models/sql_exclusion_model\n```\n\nIt is recommended to use git-tags instead of the latest commit on main branch of your library projects. You can use a specific tag, for example: `https://github.com/org-name/lib-name/tag/schema_<n>`. If you want Profile Builder to figure out the best schema version for every run, you can use the placeholder {{best\\_schema\\_version}}, for example, `https://github.com/org-name/lib-name/tag/schema_{{best_schema_version}}`. The selection of compatible git tags is done by PB, that is it will figure out the best compatible version for the lib package.\n\nA sample project file:\n\n```\npackages:\n  - name: shopify_features\n    url: https://github.com/org-name/lib-names/tag/schema_{{best_schema_version}}\n    inputsMap:\n      rsCartUpdate: inputs/rsCartUpdate\n      rsIdentifies: inputs/rsIdentifies\n```\n\nUsing this will make Profiles use the best compatible version of the library project in case of any schema updates.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> You don’t have to replace the placeholder `{{best_schema_version}}`. For instance, if `https://github.com/org-name/lib-names/tags/` has a tag for schema\\_44, then `https://github.com/org-name/lib-names/tag/schema_44` will be automatically used. In any case, if you replace the placeholder with actual tag name, the project will work without any issues.\n\n## Use private Git repos via CLI\n\nFollow these steps:\n\n1.  [Generate the SSH Key](https://git-scm.com/book/en/v2/Git-on-the-Server-Generating-Your-SSH-Public-Key).\n2.  Associate the SSH Key to your Git Project. Check the section on [Profile Builder CLI](https://www.rudderstack.com/docs/archive/profiles/0.11/get-started/quickstart-ui/).\n3.  Add private keys as credentials in the `siteconfig.yaml` file:\n\n```\ngitcreds:\n  - reporegex: git@<provider-host>:<org-name>/*\n    key: |\n    -----BEGIN OPENSSH PRIVATE KEY-----\n    b3BlbnNzaC1rZXktdjEAAAAABG5vbmUAAAAEb..........\n    -----END OPENSSH PRIVATE KEY-----    \n```\n\n## Supported Git URLs\n\nProfiles supports Git URLs for packages and scheduling via UI. You can host the repos at:\n\n*   GitHub\n*   GitLab\n*   BitBucket\n\n[Contact the RudderStack team](mailto:support@rudderstack.com) if your preferred host isn’t included.\n\nFor private repos, RudderStack only supports SSH Git URLs. You need to add credentials to the `siteconfig.yaml` and public ssh key manually to the platforms. See [Use private Git repos via CLI](#use-private-git-repos-via-cli).\n\nThe URL scheme doesn’t depend on individual Git provider host. You can use the below-mentioned Git URLs:\n\n**1\\. URL for the default branch of a repository**\n\n*   **Syntax:**\n    \n    `https://<provider-host>/<org-name>/<repo-name>/path/to/project`\n    \n*   **Example:**\n    \n    `https://github.com/rudderlabs/librs360-shopify-features/shopify-features` `https://gitlab.com/rudderlabs/librs360-shopify-features/shopify-features` `https://bitbucket.org/rudderlabs/librs360-shopify-features/shopify-features`\n    \n\n**For private repos, RudderStack support SSH URLs:**\n\n*   **Syntax:**\n    \n    `git@<provider-host>:<org-name>/<repo-name>/path/to/project`\n    \n*   **Example:**\n    \n    `git@github.com:rudderlabs/librs360-shopify-features/shopify-features` `git@gitlab.com:rudderlabs/librs360-shopify-features/shopify-features` `git@gbitbucket.org:rudderlabs/librs360-shopify-features/shopify-features`\n    \n\n**2\\. URL for a specific branch of a repository**\n\n*   **Syntax:**\n    \n    `https://<provider-host>/<org-name>/<repo-name>/tree/<branch-name>/path/to/project`\n    \n*   **Example:**\n    \n    `https://github.com/rudderlabs/librs360-shopify-features/tree/main/shopify-features` `https://gitlab.com/rudderlabs/librs360-shopify-features/tree/main/shopify-features` `https://bitbucket.org/rudderlabs/librs360-shopify-features/tree/main/shopify-features`\n    \n\n**3\\. URL for a specific tag within the repository**\n\n*   **Syntax:**\n    \n    `https://<provider-host>/<org-name>/<repo-name>/tag/<tag-name>/path/to/project`\n    \n*   **Example:**\n    \n    `https://github.com/rudderlabs/librs360-shopify-features/tag/wht_test/shopify-features` `https://gitlab.com/rudderlabs/librs360-shopify-features/tag/wht_test/shopify-features` `https://bitbucket.org/rudderlabs/librs360-shopify-features/tag/wht_test/shopify-features`\n    \n\n**4\\. URL for a specific commit within the repository**\n\n*   **Syntax:**\n    \n    `https://<provider-host>/<org-name>/<repo-name>/commit/<commit-hash>/path/to/project`\n    \n*   **Example:**\n    \n    `https://github.com/rudderlabs/librs360-shopify-features/commit/b8d49/shopify-features` `https://gitlab.com/rudderlabs/librs360-shopify-features/commit/b8d49/shopify-features` `https://bitbucket.org/rudderlabs/librs360-shopify-features/commit/b8d49/shopify-features`\n    \n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> RudderStack supports the git SSH URL with following pattern only in the dashboard:\n> \n> *   `git@<provider-host>:<org-name>/<repo-name>/tree/<branch-name>`\n> *   `git@<provider-host>:<org-name>/<repo-name>`\n> *   `git@<provider-host>:<org-name>/<repo-name>/tree/main/path/to/project`\n> \n> RudderStack supports any subfolder in git project without .git extension.\n\n## View model dependencies\n\nYou can create a DAG to see all the model dependencies, that is, how a model is dependent on other models by using any one of the following commands:\n\n`pb show dataflow`  \nOR  \n`pb show dependencies`\n\nFurther, you can use the `pb show models` command to view information about the models in your project. See [show](https://www.rudderstack.com/docs/archive/profiles/0.11/developer-guides/commands/#show) command for more information.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Additional Concepts | RudderStack Docs",
    "description": "Additional concepts related to Profiles like packages, best practices, partial feature tables, etc.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.11/get-started/quickstart-ui/",
    "markdown": "# Profiles UI | RudderStack Docs\n\nCreate your Profiles project from the RudderStack dashboard.\n\n* * *\n\n*     3 minute read  \n    \n\nThis guide lists the detailed steps to create a Profiles project in the RudderStack dashboard.\n\n## Create Profiles project\n\nRudderStack provides you with the following options to create a Profiles project:\n\n### Basic entity setup\n\nTo create a Profiles project from scratch:\n\n1.  Log in to the [RudderStack dashboard](https://app.rudderstack.com/) and go to **Unify** > **Profiles** option in the left sidebar.\n2.  Click **Create project**.\n\n[![Choose predefined template](https://www.rudderstack.com/docs/images/profiles/create-project.webp)](https://www.rudderstack.com/docs/images/profiles/create-project.webp)\n\n3.  Select **Basic Entity Setup** on the next screen.\n4.  Enter a unique name and description for your Profiles project.\n5.  Select the data warehouse from the dropdown and the source(s) connected to the warehouse.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> RudderStack currently supports the [Snowflake](https://www.rudderstack.com/docs/sources/reverse-etl/snowflake/#configuring-the-connection-credentials), [Redshift](https://www.rudderstack.com/docs/sources/reverse-etl/amazon-redshift/#configuring-the-connection-credentials), [Databricks](https://www.rudderstack.com/docs/sources/reverse-etl/databricks/#configuring-the-connection-credentials), and [BigQuery](https://www.rudderstack.com/docs/sources/reverse-etl/google-bigquery/#configuring-the-connection-credentials) warehouses for creating a Profiles project. You need to set up the warehouse source in the first place to populate it here in the dropdown.\n\n[![Choose predefined template](https://www.rudderstack.com/docs/images/profiles/source.webp)](https://www.rudderstack.com/docs/images/profiles/source.webp)\n\n6.  Map the identifiers from above-selected source by selecting the source, event, property, and ID type by clicking **Add mapping**:\n\n[![Choose predefined template](https://www.rudderstack.com/docs/images/profiles/map-id.webp)](https://www.rudderstack.com/docs/images/profiles/map-id.webp)\n\n7.  Define features either by adding a custom feature or selecting a pre-defined feature as shown:\n\n[![Choose predefined template](https://www.rudderstack.com/docs/images/profiles/define-features.webp)](https://www.rudderstack.com/docs/images/profiles/define-features.webp)\n\n*   To add a custom feature, click **Add a custom feature** and enter the relevant feature details:\n\n[![Choose predefined template](https://www.rudderstack.com/docs/images/profiles/custom-feature.webp)](https://www.rudderstack.com/docs/images/profiles/custom-feature.webp)\n\n*   To use a pre-defined feature, select the required feature from the **Template features library**.\n\n8.  Select the [schedule type](https://www.rudderstack.com/docs/sources/reverse-etl/sync-schedule-settings/).\n9.  Enter the warehouse details where you want to store this Profiles project.\n10.  Finally, review all the provided details and click **Create Profiles project**.\n\n### Import project from Git Repo\n\nTo create a Profiles project by importing it from a Git repository:\n\n1.  Log in to the [RudderStack dashboard](https://app.rudderstack.com/) and go to **Unify** > **Profiles** option in the left sidebar.\n2.  Click **Create project**.\n\n[![Choose predefined template](https://www.rudderstack.com/docs/images/profiles/create-project.webp)](https://www.rudderstack.com/docs/images/profiles/create-project.webp)\n\n3.  Select **Import project from Git Repo** on the next screen.\n4.  Enter a unique name and description for your Profiles project.\n5.  Enter the [SSH URL of your Git repository](https://docs.github.com/en/authentication/connecting-to-github-with-ssh) to be used for your Profiles project.\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> Make sure the SSH URL you provide contains the Profiles project, otherwise it can result in an error.\n\n6.  Copy the SSH public key and click **Add deploy key** which will take you to your git repository’s **Deploy keys** section. See [Set up deploy keys](https://docs.github.com/en/authentication/connecting-to-github-with-ssh/managing-deploy-keys#set-up-deploy-keys) for more information.\n7.  Select the warehouse and enter its credentials to store this Profiles project.\n8.  Finally, click **Create Profiles project**.\n\n## Download project\n\nTo download the Profiles project, click **View** corresponding to your Profiles project and click **Download this project**:\n\n[![Download Profiles project](https://www.rudderstack.com/docs/images/profiles/download-profiles-project.webp)](https://www.rudderstack.com/docs/images/profiles/download-profiles-project.webp)\n\n## Profile details\n\nTo view the profile details, click **View** corresponding to your Profiles project:\n\n| Option | Description |\n| --- | --- |\n| **Overview** | Lists the features of your Profiles project. |\n| **History** | Displays the history of Profile runs. |\n| **Explorer** | Displays the preview of first 50 rows of the output tables (features and ID stitching) only after the successful run of the project. You can also search for all the records, by typing in a unique identifier such as user\\_id, email, etc. |\n| **Settings** | Displays your profile settings and lets you delete your Profiles project. You can edit the project by clicking the edit icon next to each section. |\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> Note that:\n> \n> *   It can take up to thirty minutes for the data preview to show up in your profile’s **History** tab.\n> *   If you keep getting a blank screen, it may be because you do not have sufficient access. Make sure you have a [Connections Admin](https://www.rudderstack.com/docs/dashboard-guides/user-management/#resource-roles) resource role with [access to PII](https://www.rudderstack.com/docs/dashboard-guides/data-management/#limiting-access-to-pii-related-features). In case the problem persists, contact [RudderStack support](mailto:support@rudderstack.com).\n\n## FAQ\n\n**When trying to fetch data for a lib project, then data/columns are shown as blank. What should I do?**\n\nYou’ll need to sync data from a source to a destination. If data is synced from the source you are using and not from some pre-existing tables in the destination, the missing column/data issues should not occur.\n\n**I am not able to see Unify tab on the web app though I have admin privileges. What should I do?**\n\nDisable any adblockers on your web browser.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Profiles UI | RudderStack Docs",
    "description": "Create your Profiles project from the RudderStack dashboard.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.10/get-started/quickstart-ui/",
    "markdown": "# Profiles UI | RudderStack Docs\n\nCreate your first Profiles project using RudderStack dashboard.\n\n* * *\n\n*     3 minute read  \n    \n\nYou can create a Profiles project via RudderStack dashboard. It generates outputs based on the pre-defined library projects. However, you can augment these outputs by downloading the Profiles project and updating it manually.\n\nThis guide lists the detailed steps to create a Profiles project in the RudderStack dashboard.\n\n## 1\\. Create Profile\n\n1.  Log in to the [RudderStack dashboard](https://app.rudderstack.com/).\n2.  From the left navigation bar, navigate to **Unify** > **Profiles** and click **Create profile**.\n3.  Enter the following details:\n\n### 2\\. Name the profile\n\n| Setting | Description |\n| --- | --- |\n| **Profile name** | Enter a unique name for your profile to help you identify it later. |\n| **Description** | Enter a description for your profile. |\n\nClick **Done** to proceed.\n\n### 3\\. Select a warehouse\n\nSelect the data warehouse you want to use and enter the relevant connection settings.\n\nClick **Verify Credentials** > **Next** to move to the next step.\n\n### 4\\. Configure project\n\nChoose one of the RudderStack’s predefined library projects to generate your Profiles project, or add your own project using a GitHub URL.\n\n[![Choose predefined template](https://www.rudderstack.com/docs/images/profiles/choose-profiles-template.webp)](https://www.rudderstack.com/docs/images/profiles/choose-profiles-template.webp)\n\nClick **Configure** > **Done** to move to the next step.\n\n### 5\\. Set Schedule\n\nSelect the [schedule type](https://www.rudderstack.com/docs/sources/reverse-etl/common-settings/sync-schedule-settings/) and click **Done**.\n\n### 6\\. Review and create profile\n\nHere, you will get a preview of what your data table looks like. Review and ensure its correctness and click **Create** to create the profile.\n\n### 7\\. Add SSH Key to GIT Project (for non-library projects)\n\nIf you have added the project URL instead of selecting from a predefined list of library projects, then follow these steps to add a public SSH key to your GIT project:\n\n1.  Open your profile’s GIT repository (on github.com) in your web browser and click the **Settings** tab.\n2.  In the left side bar, select **Deploy Keys**, then click **Add Deploy Key**.\n3.  Assign a name to it, for example, `Sample Profiles Key`, and paste the key generated by the RudderStack webapp. You need not check **Allow write access**.\n4.  Click on **Add Key**.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> Using git-tags instead of the latest commit on main branch of projects is recommended. For example, `https://github.com/org-name/lib-name/tag/schema_{{best_schema_version}}`.\n> \n> The selection of compatible git tags is done by PB - it figures the best compatible version for the package. The tool uses the best compatible version of the project in case of any schema updates.\n\n## Download project\n\nOnce created, you can download the Profiles project locally by clicking **Download this project**.\n\n[![Download Profiles project](https://www.rudderstack.com/docs/images/profiles/download-profiles-project.webp)](https://www.rudderstack.com/docs/images/profiles/download-profiles-project.webp)\n\nThe downloaded zipped project contains the `pb_project.yaml` and `models/inputs.yaml` files.\n\n## Profile details\n\nTo view the profile details, click **View** in the Profiles homepage:\n\n| Option | Description |\n| --- | --- |\n| **Overview** | Lists the features of your Profiles project. |\n| **History** | Displays the history of Profile runs. |\n| **Settings** | Displays your profile settings. You can edit them by clicking the edit icon next to each section. |\n| **Delete** | Deletes your Profiles project. |\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> Note that:\n> \n> *   It can take up to thirty minutes for the data preview to show up in your profile’s **History** tab.\n> *   If you keep getting a blank screen, it may be because you do not have sufficient access. Make sure you have a [Connections Admin](https://www.rudderstack.com/docs/dashboard-guides/user-management/#resource-roles) resource role with [access to PII](https://www.rudderstack.com/docs/dashboard-guides/data-management/#limiting-access-to-pii-related-features). In case the problem persists, contact [RudderStack support](mailto:support@rudderstack.com).\n\n## FAQ\n\n**When trying to fetch data for a lib project, then data/columns are shown as blank. What should I do?**\n\nYou’ll need to sync data from a source to a destination. If data is synced from the source you are using and not from some pre-existing tables in the destination, the missing column/data issues should not occur.\n\n**I am not able to see Unify tab on the web app though I have admin privileges. What should I do?**\n\nDisable any adblockers on your web browser.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Profiles UI | RudderStack Docs",
    "description": "Create your first Profiles project using RudderStack dashboard.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.11/get-started/profile-builder/",
    "markdown": "# Profile Builder CLI | RudderStack Docs\n\nCreate a Profiles project using the Profile Builder (PB) tool.\n\n* * *\n\n*     7 minute read  \n    \n\n**Profile Builder (PB)** is a command-line interface (CLI) tool that simplifies data transformation within your warehouse. It generates customer profiles by stitching data together from multiple sources.\n\nThis guide lists the detailed steps to install and use the Profile Builder (PB) tool to create, configure, and run a new project.\n\n## Prerequisites\n\nYou must have:\n\n*   [Python 3](https://www.python.org/downloads/) installed on your machine.\n*   Admin privileges on your machine.\n\n## Steps\n\nTo set up a project using the PB tool, follow these steps:\n\n### 1: Install PB\n\nInstall the Profile Builder tool by running the following command:\n\n```\npip3 install profiles-rudderstack\n```\n\nIf you have already installed PB, use the following command to update its version:\n\n```\npip3 install profiles-rudderstack -U\n```\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> RudderStack recommends using a Python virtual environment to maintain an isolated and clean environment.\n> \n> ```\n> pipx install profiles-rudderstack\n> ```\n\nValidate Profile Builder’s version after install using:\n\nSee also: [Setup and installation FAQ](https://www.rudderstack.com/docs/archive/profiles/0.11/faq/#setup-and-installation)\n\n### 2: Create warehouse connection\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> RudderStack supports **Snowflake**, **Redshift**, **BigQuery**, and **Databricks** warehouses for Profiles. You must grant certain [warehouse permissions](https://www.rudderstack.com/docs/archive/profiles/0.11/developer-guides/permissions/) to let RudderStack read from schema having the source tables (for example, `tracks` and `identifies` tables generated via Event Stream sources), and write data in a new schema created for Profiles.\n\nCreate a warehouse connection to allow PB to access your data:\n\nThen, follow the prompts to enter details about your warehouse connection.\n\nA sample connection for a Snowflake account is as follows:\n\n```\nEnter Connection Name: test\nEnter target:  (default:dev):  # Press enter, leaving it to default\nEnter account: ina13147.us-east-1\nEnter warehouse: rudder_warehouse\nEnter dbname: your_rudderstack_db \nEnter schema: rs_profiles # A segregated schema for storing tables/views created by Profiles\nEnter user: profiles_test_user\nEnter password: <password>\nEnter role: profiles_role\nAppend to /Users/<user_name>/.pb/siteconfig.yaml? [y/N]\ny\n```\n\nA sample connection for a Redshift account is as follows:\n\n**Using warehouse credentials**\n\n```\nHow would you like to connect with warehouse? [1] Using the warehouse credentials[Username, Password]\n[2] Using access key security credentials values [AWS Access Key ID and Secret Access Key].\n1\nPlease enter the following details for creating a new connection, that will enable pb to connect to your redshift account.\nThe created connections details will be stored in the file /Users/rudderstack/.pb/siteconfig.yaml\nEnter Connection Name: test\nEnter target:  (default:dev):  # Press enter, leaving it to default\nEnter host: warehouseabc.us-west-1.redshift.amazonaws.com\nEnter port: 5439\nEnter dbname: your_rudderstack_db \nEnter schema: rs_profiles # A segregated schema for storing tables/views created by Profiles\nEnter user: profiles_test_user\nEnter password: <password>\nEnter sslmode: options - [disable require]: # Enter \"require\" in case your Redshift connection mandates sslmode. \nAppend to /Users/rudderstack/.pb/siteconfig.yaml? [y/N]\ny\n```\n\n**Using access key security credentials**\n\n```\nHow would you like to connect with warehouse? [1] Using the warehouse credentials[Username, Password]\n[2] Using access key security credentials values [AWS Access Key ID and Secret Access Key].\n2\nPlease enter the following details for creating a new connection, that will enable pb to connect to your redshift account.\nThe created connections details will be stored in the file /Users/rudderstack/.pb/siteconfig.yaml\nEnter Connection Name: test\nEnter target:  (default:dev):  # Press enter, leaving it to default\nEnter host: warehouseabc.us-west-1.redshift.amazonaws.com\nEnter dbname: your_rudderstack_db \nEnter schema: rs_profiles # A segregated schema for storing tables/views created by Profiles\nEnter user: profiles_test_user\nEnter shared_profile: as\nAppend to /Users/rudderstack/.pb/siteconfig.yaml? [y/N]\ny\n```\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> To know more about host, log in to your AWS Console and go to **Clusters**.\n\nA sample connection for a Databricks account is as follows:\n\n```\nEnter Connection Name: test\nEnter target:  (default:dev):  # Press enter, leaving it to default\nEnter host: a1.8.azuredatabricks.net # The hostname or URL of your Databricks cluster\nEnter port: 443 # The port number used for establishing the connection. Usually it is 443 for https connections.\nEnter http_endpoint: /sql/1.0/warehouses/919uasdn92h # The path or specific endpoint you wish to connect to.\nEnter access_token: <password> # The access token created for authenticating the instance.\nEnter user: profiles_test_user # Username of your Databricks account.\nEnter schema: rs_profiles # A segregated schema for storing tables/views created by Profiles\nEnter catalog: your_rudderstack_db # The database or catalog having data that you’ll be accessing.\nAppend to /Users/<user_name>/.pb/siteconfig.yaml? [y/N]\ny\n```\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> RudderStack currently supports Databricks on Azure. To get the Databricks connection details:\n> \n> 1.  Log in to your Azure’s Databricks Web UI.\n> 2.  Click on **SQL Warehouses** on the left.\n> 3.  Select the warehouse to connect to.\n> 4.  Select the **Connection Details** tab.\n\nA sample connection for a BigQuery account is as follows:\n\n```\nEnter Connection Name: test\nEnter target:  (default:dev):  # Press enter, leaving it to default\nEnter credentials: json file path: # File path of your BQ JSON file, for example, /Users/alexm/Downloads/big.json. Entering an incorrect path will exit the program.\nEnter project_id: profiles121\nEnter schema: rs_profiles\nAppend to /Users/<user_name>/.pb/siteconfig.yaml? [y/N]\ny\n```\n\nThis creates a [site configuration file](https://www.rudderstack.com/docs/archive/profiles/0.11/developer-guides/site-configuration/) inside your home directory: `~/.pb/siteconfig.yaml`. If you don’t see the file, enable the **View hidden files** option.\n\n### 3: Create project\n\nRun the following command to create a sample project:\n\n```\npb init pb-project -o MyProfilesProject\n```\n\nThe above command creates a new project in the **MyProfilesProject** folder with the following structure:\n\n[![Project structure](https://www.rudderstack.com/docs/images/profiles/project-structure.webp)](https://www.rudderstack.com/docs/images/profiles/project-structure.webp)\n\nSee [Project structure](https://www.rudderstack.com/docs/archive/profiles/0.11/developer-guides/structure/) for more information on the PB project files.\n\nNavigate to the `pb_project.yaml` file and set the value of `connection:` to the connection name as defined in the previous step.\n\n### 4: Change input sources\n\n*   Navigate to your project and open the `models/inputs.yaml` file. Here, you will see a list of tables/views along with their respective ID types.\n*   Replace the placeholder table names with the actual table names in the `table` field.\n\nSee [Project structure](https://www.rudderstack.com/docs/archive/profiles/0.11/developer-guides/structure/#inputs) for more information on setting these values.\n\n### 5: Validate project\n\nNavigate to your project and validate your warehouse connection and input sources:\n\nIf there are no errors, proceed to the next step. In case of errors, check if your warehouse schemas and tables have the [required permissions](https://www.rudderstack.com/docs/archive/profiles/0.11/developer-guides/permissions/).\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> Currently, this command is not supported for BigQuery warehouse.\n\n### 6: Generate SQL files\n\nCompile the project:\n\nThis generates SQL files in the `output/` folder that you can run directly on the warehouse.\n\n### 7: Generate output tables\n\nRun the project and generate [material tables](https://www.rudderstack.com/docs/archive/profiles/0.11/glossary/#material-tables):\n\nThis command generates and runs the SQL files in the warehouse, creating the material tables.\n\n### 8: View generated tables\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> The view `user_default_id_stitcher` will always point to the latest generated ID stitcher and `user_profile` to the latest feature table.\n\nYou can run the `pb show models` command to get the exact name and path of the generated ID stitcher/feature table. See [show](https://www.rudderstack.com/docs/archive/profiles/0.11/developer-guides/commands/#show) command for more information.\n\nThen, execute the below query to view the generated tables in the warehouse:\n\n```\nselect * from <table_name> limit 10;\n```\n\nHere’s what the columns imply:\n\n![ID Stitcher Table](https://www.rudderstack.com/docs/images/profiles/idstitcher-table.webp)\n\n*   **user\\_main\\_id**: Rudder ID generated by Profile Builder. Think of a 1-to-many relationship, with one Rudder ID connected to different IDs belonging to same user such as User ID, Anonymous ID, Email, Phone number, etc.\n*   **other\\_id**: ID in input source tables that is stitched to a Rudder ID.\n*   **other\\_id\\_type**: Type of the other ID to be stitched (User ID, Anonymous ID, Email, etc).\n*   **valid\\_at**: Date at which the corresponding ID value occurred in the source tables. For example, the date at which a customer was first browsing anonymously, or when they logged into the CRM with their email ID, etc.\n\n![Feature Table](https://www.rudderstack.com/docs/images/profiles/feature-table.webp)\n\n*   **user\\_main\\_id**: Rudder ID generated by Profile Builder.\n*   **valid\\_at**: Date when the feature table entry was created for this record.\n*   **first\\_seen, last\\_seen, country, first\\_name, etc.** - All features for which values are computed.\n\n## Migrate your existing project\n\nTo migrate an existing PB project to the [schema version](https://www.rudderstack.com/docs/archive/profiles/0.11/glossary/#schema-versions) supported by your PB binary, navigate to your project’s folder. Then, run the following command to replace the contents of the existing folder with the new one:\n\n```\npb migrate auto --inplace\n```\n\nA confirmation message appears on screen indicating that the migration is complete. A sample message for a user migrating their project from version 25 to 44:\n\n```\n2023-10-17T17:48:33.104+0530\tINFO\tmigrate/migrate.go:161\t\nProject migrated from version 25 to version 44\n```\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Profile Builder CLI | RudderStack Docs",
    "description": "Create a Profiles project using the Profile Builder (PB) tool.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.10/data-modeling/",
    "markdown": "# Data Modeling | RudderStack Docs\n\nRudderStack’s flexibility in modeling caters to all your data needs seamlessly.\n\n* * *\n\n*     3 minute read  \n    \n\nProfiles models your organizational data by analyzing all the data in your warehouse to create unified customer profiles and enrich them with features to help you scale your business efficiently and swiftly.\n\nWhen you run the Profiles project, it creates an identity graph and user entity traits 360 as outputs. You can augment the graph and create new user features by writing simple definitions in a configuration file or via SQL models.\n\n[![Profiles data modeling](https://www.rudderstack.com/docs/images/profiles/data-modeling.webp)](https://www.rudderstack.com/docs/images/profiles/data-modeling.webp)\n\n## Highlights\n\n*   Flexibility to use event stream, ETL, or any external tools as input sources.\n*   Support to define various entities like user, product, organization, etc.\n*   Intelligent merging of entities with different identifiers, like stitching Salesforce IDs.\n*   Ease of creating features/traits for any entity and using them to deliver personalization.\n*   Deal with advanced use-case scenarios using custom SQL queries.\n\n## Use varied input sources\n\nRudderStack Profiles gives you the flexibility of using a variety of input sources. These sources are essentially the tables or views which you can create using:\n\n*   [Event Stream](https://www.rudderstack.com/docs/sources/event-streams/) (loaded from event data)\n*   ETL extract (loaded from [Cloud Extract](https://www.rudderstack.com/docs/sources/extract/) (loaded from event data))\n*   Existing tables in the warehouse (generated by external tools like DBT).\n\n## Define entities\n\nEntities refer to an object for which you can create a profile. RudderStack allows you to use the desired object as an entity. For example, user, customer, product, or any other object that requires profiling.\n\nYou can define the entities in `pb_project.yaml` file and use them declaratively while describing the columns of your input sources.\n\n## Unify entities\n\nOnce you define the entities, you can resolve different identities for an entity using the process of [identity stitching](https://www.rudderstack.com/docs/archive/profiles/0.10/identity-stitching/). It matches the different identifiers across multiple devices, digital touchpoints, and other data (like offline point-of-sale interactions) to build a comprehensive identity graph. The identity graph includes nodes (identifiers) and their relationships (edges), and it is generated as a transparent table in the warehouse.\n\nFor example, you can stitch Salesforce IDs or other ID types.\n\n## Enrich with features\n\nOnce you map all the available identifiers to an individual user or entity, it is easier to collect their traits and compute the user features you want in your customer 360 table.\n\nUsing the identity graph as a map, the Profiles **entity traits 360** model lets you define or perform calculations over the customer data in your warehouse. An entity traits 360 model creates a feature using entityVars. Each column in a entity traits 360 model is a trait or user feature for a user or entity. In a feature table, the specified entityVars are unified into a view.\n\nYou don’t need any other tool or deep technical/SQL expertise to create these features. Trait defintion is in a single unified framework and there is no need to move data across silos.\n\nTo implement advanced use cases, you can use [custom SQL queries](https://www.rudderstack.com/docs/archive/profiles/0.10/feature-development/sql-models/) to define user features.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Data Modeling | RudderStack Docs",
    "description": "RudderStack's flexibility in modeling caters to all your data needs seamlessly.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.11/identity-stitching/",
    "markdown": "# Identity Stitching | RudderStack Docs\n\nStitch multiple identifiers together to create a unified and comprehensive profile.\n\n* * *\n\n*     4 minute read  \n    \n\n## How to perform identity stitching?\n\nYou can use the RudderStack’s [identity stitching model](https://www.rudderstack.com/docs/archive/profiles/0.11/example/id-stitcher/) to define the identifiers you want to combine together. You can also define the input sources, like [Event Stream](https://www.rudderstack.com/docs/sources/event-streams/), [Cloud Extract](https://www.rudderstack.com/docs/sources/extract/) sources, which automatically produce an identity graph because all the schemas and unique identifiers are known.\n\n## Problem of multiple identities\n\nCompanies gather user data across digital touchpoints like websites, mobile apps, enterprise systems like CRMs, marketing platforms, etc. During this process, a single user is identified with multiple identifiers across their product journey, like their email ID, phone number, device ID, anonymous ID, usernames, and more. Also, the user information is spread across dozens of devices, accounts, or products as they often change their devices and use work and personal emails together.\n\nThe user data stored in the warehouse contains unstructured objects that represent one or more user (or entity) identities. Competitive businesses seeking to keep an edge with user data need to clarify this mess of data points into an accurate model of customer behavior and build personalized relationships.\n\nTo create a unified user profile, it is essential to correlate all of the different user identifiers into one canonical identifier so that all the data related to a particular user or entity can be associated with that user or entity.\n\nThis unification step, called **Identity Stitching**, ties all the user data from these tables into a unified view, giving you a 360-degree view of the user.\n\n## Unify identities across all platforms and devices\n\nIdentity stitching combines unique identifiers across your digital touchpoints to identify users and create a unified, omnichannel view of your customers.\n\nIt matches the different identifiers across multiple devices, digital touchpoints, and other data (like offline point-of-sale interactions) to build a comprehensive identity graph. This identity graph includes nodes (identifiers) and their relationships (edges), and it is generated as a transparent table in the warehouse.\n\nRudderStack performs identity stitching by mapping all the unique identifiers into a single canonical ID (for example, `rudder_id`), then uses that ID to make user feature development easier (for example, summing the payment events against a single `rudder_id`).\n\n[![single identity created from different identities](https://www.rudderstack.com/docs/images/profiles/id-stitching.webp)](https://www.rudderstack.com/docs/images/profiles/id-stitching.webp)\n\n## Identity graph\n\nIdentity stitching starts with the creation of an identity graph. The identity graph is a database housing the entity identifiers where you can identify and connect details related to your customer journeys. Further, it stitches them together in one customer profile representing their whole identity.\n\nThe most fundamental data in an identity graph is the ID tag associated with a device, account, network, session, transaction, or any other anonymous identifier that can engage with your company. Once you’ve collected this data and associated it with a single customer identity (wherever possible), your customer data becomes more reliable, and you can move on to achieve higher goals.\n\nAn identity graph incorporates models that help it in ingesting new information. As you add a new data point with whatever connections are immediately known, the graph database determines if it fits into any existing customer identifier. If there is a clear link - such as a matching device ID or conclusive biographical data like a credit card number - the graph incorporates the data into a relevant user node.\n\n[![identity graph](https://www.rudderstack.com/docs/images/profiles/identity-graph.webp)](https://www.rudderstack.com/docs/images/profiles/identity-graph.webp)\n\n## Notable features\n\n*   Use different input sources like RudderStack’s [Event Stream](https://www.rudderstack.com/docs/sources/event-streams/), [Cloud Extract](https://www.rudderstack.com/docs/sources/extract/) sources, or any existing tables in the warehouse.\n*   Merge identities for entities like a user, customer, product, business account, etc.\n*   Stitch identities from all the required channels like web, mobile, marketing platforms, etc.\n*   Use identity stitching results to develop user features and deliver personalized campaigns.\n\n## Why identity stitching?\n\n*   **Understand user behavior**: Consolidate and connect customer data from various sources to better understand customers’ preferences, behaviors, and interactions across multiple touchpoints.\n*   **Provide personalized support**: Deliver personalized marketing messages and experiences to your customers. Ensures that the right message reaches the right person at the right time, increasing the effectiveness of marketing campaigns.\n*   **Enrich user profile with features**: Enhance user profiles with additional data points and features. These features can include demographic information, preferences, purchase history, browsing behavior, or any other static or computed data points.\n\n#### See also\n\n*   [Sample identity stitching project](https://www.rudderstack.com/docs/archive/profiles/0.11/example/id-stitcher/)\n*   [Problem of Identity resolution](https://www.rudderstack.com/blog/the-tale-of-identity-graph-and-identity-resolution/)\n*   [How to achieve ID mapping in a data warehouse](https://www.rudderstack.com/blog/identity-graph-and-identity-resolution-in-sql/)\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Identity Stitching | RudderStack Docs",
    "description": "Stitch multiple identifiers together to create a unified and comprehensive profile.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.11/data-modeling/",
    "markdown": "# Data Modeling | RudderStack Docs\n\nRudderStack’s flexibility in modeling caters to all your data needs seamlessly.\n\n* * *\n\n*     3 minute read  \n    \n\nProfiles models your organizational data by analyzing all the data in your warehouse to create unified customer profiles and enrich them with features to help you scale your business efficiently and swiftly.\n\nWhen you run the Profiles project, it creates an identity graph and user entity traits 360 as outputs. You can augment the graph and create new user features by writing simple definitions in a configuration file or via SQL models.\n\n[![Profiles data modeling](https://www.rudderstack.com/docs/images/profiles/data-modeling.webp)](https://www.rudderstack.com/docs/images/profiles/data-modeling.webp)\n\n## Highlights\n\n*   Flexibility to use event stream, ETL, or any external tools as input sources.\n*   Support to define various entities like user, product, organization, etc.\n*   Intelligent merging of entities with different identifiers, like stitching Salesforce IDs.\n*   Ease of creating features/traits for any entity and using them to deliver personalization.\n*   Deal with advanced use-case scenarios using custom SQL queries.\n\n## Use varied input sources\n\nRudderStack Profiles gives you the flexibility of using a variety of input sources. These sources are essentially the tables or views which you can create using:\n\n*   [Event Stream](https://www.rudderstack.com/docs/sources/event-streams/) (loaded from event data)\n*   ETL extract (loaded from [Cloud Extract](https://www.rudderstack.com/docs/sources/extract/) (loaded from event data))\n*   Existing tables in the warehouse (generated by external tools like DBT).\n\n## Define entities\n\nEntities refer to an object for which you can create a profile. RudderStack allows you to use the desired object as an entity. For example, user, customer, product, or any other object that requires profiling.\n\nYou can define the entities in `pb_project.yaml` file and use them declaratively while describing the columns of your input sources.\n\n## Unify entities\n\nOnce you define the entities, you can resolve different identities for an entity using the process of [identity stitching](https://www.rudderstack.com/docs/archive/profiles/0.11/identity-stitching/). It matches the different identifiers across multiple devices, digital touchpoints, and other data (like offline point-of-sale interactions) to build a comprehensive identity graph. The identity graph includes nodes (identifiers) and their relationships (edges), and it is generated as a transparent table in the warehouse.\n\nFor example, you can stitch Salesforce IDs or other ID types.\n\n## Enrich with features\n\nOnce you map all the available identifiers to an individual user or entity, it is easier to collect their traits and compute the user features you want in your customer 360 table.\n\nUsing the identity graph as a map, the Profiles **entity traits 360** model lets you define or perform calculations over the customer data in your warehouse. An entity traits 360 model creates a feature using entityVars. Each column in a entity traits 360 model is a trait or user feature for a user or entity. In a feature table, the specified entityVars are unified into a view.\n\nYou don’t need any other tool or deep technical/SQL expertise to create these features. Trait defintion is in a single unified framework and there is no need to move data across silos.\n\nTo implement advanced use cases, you can use [custom SQL queries](https://www.rudderstack.com/docs/archive/profiles/0.11/feature-development/sql-models/) to define user features.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Data Modeling | RudderStack Docs",
    "description": "RudderStack's flexibility in modeling caters to all your data needs seamlessly.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.11/feature-development/",
    "markdown": "# Feature Development | RudderStack Docs\n\nPrivacy Overview\n\nThis site uses cookies to improve your experience while you navigate through the website. Out of these cookies, the cookies that are categorized as necessary are stored on your browser as they are as essential for the working of basic functionalities of the website. We also use third-party cookies that help us analyze and understand how you use this website. These cookies will be stored in your browser only with your consent. You also have the option to opt-out of these cookies. But opting out of some of these cookies may have an effect on your browsing experience.\n\nNecessary cookies are absolutely essential for the website to function properly. This category only includes cookies that ensures basic functionalities and security features of the website. These cookies do not store any personal information.",
    "title": "Feature Development | RudderStack Docs",
    "description": "Use unified profiles and enrich them with the required features/traits to drive targeted campaigns.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.10/identity-stitching/",
    "markdown": "# Identity Stitching | RudderStack Docs\n\nStitch multiple identifiers together to create a unified and comprehensive profile.\n\n* * *\n\n*     4 minute read  \n    \n\n## How to perform identity stitching?\n\nYou can use the RudderStack’s [identity stitching model](https://www.rudderstack.com/docs/archive/profiles/0.10/example/id-stitcher/) to define the identifiers you want to combine together. You can also define the input sources, like [Event Stream](https://www.rudderstack.com/docs/sources/event-streams/), [Cloud Extract](https://www.rudderstack.com/docs/sources/extract/) sources, which automatically produce an identity graph because all the schemas and unique identifiers are known.\n\n## Problem of multiple identities\n\nCompanies gather user data across digital touchpoints like websites, mobile apps, enterprise systems like CRMs, marketing platforms, etc. During this process, a single user is identified with multiple identifiers across their product journey, like their email ID, phone number, device ID, anonymous ID, usernames, and more. Also, the user information is spread across dozens of devices, accounts, or products as they often change their devices and use work and personal emails together.\n\nThe user data stored in the warehouse contains unstructured objects that represent one or more user (or entity) identities. Competitive businesses seeking to keep an edge with user data need to clarify this mess of data points into an accurate model of customer behavior and build personalized relationships.\n\nTo create a unified user profile, it is essential to correlate all of the different user identifiers into one canonical identifier so that all the data related to a particular user or entity can be associated with that user or entity.\n\nThis unification step, called **Identity Stitching**, ties all the user data from these tables into a unified view, giving you a 360-degree view of the user.\n\n## Unify identities across all platforms and devices\n\nIdentity stitching combines unique identifiers across your digital touchpoints to identify users and create a unified, omnichannel view of your customers.\n\nIt matches the different identifiers across multiple devices, digital touchpoints, and other data (like offline point-of-sale interactions) to build a comprehensive identity graph. This identity graph includes nodes (identifiers) and their relationships (edges), and it is generated as a transparent table in the warehouse.\n\nRudderStack performs identity stitching by mapping all the unique identifiers into a single canonical ID (for example, `rudder_id`), then uses that ID to make user feature development easier (for example, summing the payment events against a single `rudder_id`).\n\n[![single identity created from different identities](https://www.rudderstack.com/docs/images/profiles/id-stitching.webp)](https://www.rudderstack.com/docs/images/profiles/id-stitching.webp)\n\n## Identity graph\n\nIdentity stitching starts with the creation of an identity graph. The identity graph is a database housing the entity identifiers where you can identify and connect details related to your customer journeys. Further, it stitches them together in one customer profile representing their whole identity.\n\nThe most fundamental data in an identity graph is the ID tag associated with a device, account, network, session, transaction, or any other anonymous identifier that can engage with your company. Once you’ve collected this data and associated it with a single customer identity (wherever possible), your customer data becomes more reliable, and you can move on to achieve higher goals.\n\nAn identity graph incorporates models that help it in ingesting new information. As you add a new data point with whatever connections are immediately known, the graph database determines if it fits into any existing customer identifier. If there is a clear link - such as a matching device ID or conclusive biographical data like a credit card number - the graph incorporates the data into a relevant user node.\n\n[![identity graph](https://www.rudderstack.com/docs/images/profiles/identity-graph.webp)](https://www.rudderstack.com/docs/images/profiles/identity-graph.webp)\n\n## Notable features\n\n*   Use different input sources like RudderStack’s [Event Stream](https://www.rudderstack.com/docs/sources/event-streams/), [Cloud Extract](https://www.rudderstack.com/docs/sources/extract/) sources, or any existing tables in the warehouse.\n*   Merge identities for entities like a user, customer, product, business account, etc.\n*   Stitch identities from all the required channels like web, mobile, marketing platforms, etc.\n*   Use identity stitching results to develop user features and deliver personalized campaigns.\n\n## Why identity stitching?\n\n*   **Understand user behavior**: Consolidate and connect customer data from various sources to better understand customers’ preferences, behaviors, and interactions across multiple touchpoints.\n*   **Provide personalized support**: Deliver personalized marketing messages and experiences to your customers. Ensures that the right message reaches the right person at the right time, increasing the effectiveness of marketing campaigns.\n*   **Enrich user profile with features**: Enhance user profiles with additional data points and features. These features can include demographic information, preferences, purchase history, browsing behavior, or any other static or computed data points.\n\n#### See also\n\n*   [Sample identity stitching project](https://www.rudderstack.com/docs/archive/profiles/0.10/example/id-stitcher/)\n*   [Problem of Identity resolution](https://www.rudderstack.com/blog/the-tale-of-identity-graph-and-identity-resolution/)\n*   [How to achieve ID mapping in a data warehouse](https://www.rudderstack.com/blog/identity-graph-and-identity-resolution-in-sql/)\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Identity Stitching | RudderStack Docs",
    "description": "Stitch multiple identifiers together to create a unified and comprehensive profile.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.10/feature-development/sql-models/",
    "markdown": "# SQL Models | RudderStack Docs\n\nPrivacy Overview\n\nThis site uses cookies to improve your experience while you navigate through the website. Out of these cookies, the cookies that are categorized as necessary are stored on your browser as they are as essential for the working of basic functionalities of the website. We also use third-party cookies that help us analyze and understand how you use this website. These cookies will be stored in your browser only with your consent. You also have the option to opt-out of these cookies. But opting out of some of these cookies may have an effect on your browsing experience.\n\nNecessary cookies are absolutely essential for the website to function properly. This category only includes cookies that ensures basic functionalities and security features of the website. These cookies do not store any personal information.",
    "title": "SQL Models | RudderStack Docs",
    "description": "Use custom SQL queries to enrich unified profiles for some advanced use-cases.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.11/feature-development/sql-models/",
    "markdown": "# SQL Models | RudderStack Docs\n\nPrivacy Overview\n\nThis site uses cookies to improve your experience while you navigate through the website. Out of these cookies, the cookies that are categorized as necessary are stored on your browser as they are as essential for the working of basic functionalities of the website. We also use third-party cookies that help us analyze and understand how you use this website. These cookies will be stored in your browser only with your consent. You also have the option to opt-out of these cookies. But opting out of some of these cookies may have an effect on your browsing experience.\n\nNecessary cookies are absolutely essential for the website to function properly. This category only includes cookies that ensures basic functionalities and security features of the website. These cookies do not store any personal information.",
    "title": "SQL Models | RudderStack Docs",
    "description": "Use custom SQL queries to enrich unified profiles for some advanced use-cases.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.10/predictions/",
    "markdown": "# Predictions (Early Access) | RudderStack Docs\n\nUse Profiles’ predictive features to train machine learning models.\n\n* * *\n\n*     7 minute read  \n    \n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> Predictions is part of our [Early Access Program](https://www.rudderstack.com/docs/resources/early-access-program/), where we work with early users and customers to test new features and get feedback before making them generally available. These features are functional but can change as we improve them. We recommend connecting with our team before running them in production.\n> \n> [Contact us](https://www.rudderstack.com/contact/) to get access to this feature.\n\nPredictions extends Profiles’ standard [feature development](https://www.rudderstack.com/docs/archive/profiles/0.10/feature-development/) functionality and lets you easily create predictive features in your warehouse. You can predict features like:\n\n*   Is a customer likely to churn in the next 30 days?\n*   Will a user make a purchase in the next 7 days?\n*   Is a lead going to convert?\n*   How much is a user likely to spend in the next 90 days?\n\nFinally, you can add the predicted feature to user profiles in your warehouse automatically and deliver ML-based segments and audiences to your marketing, product, and customer success teams.\n\n## Use cases\n\nThis section covers some common Predictions use cases.\n\n### Churn prediction\n\nPredicting churn is one of the crucial initiatives across businesses. Without a predicted churn score, your actions are reactive, whereas you can act proactively with a user trait like `is_likely_to_churn`. Once you have such features, you can activate them with the appropriate outreach programs to prevent user churn.\n\n### Customer LTV prediction\n\nPredictions helps you understand your customers’ purchasing behavior over time. You can predict how much amount a particular customer is likely to spend within the prediction time range.\n\n## Prerequisites\n\n*   You must be using a [Snowflake](https://www.rudderstack.com/docs/destinations/warehouse-destinations/snowflake/) or [Redshift](https://www.rudderstack.com/docs/destinations/warehouse-destinations/redshift/) warehouse.\n*   You must set up a standard Profiles project with a [feature table model](https://www.rudderstack.com/docs/archive/profiles/0.10/example/feature-table/).\n*   **Optional**: If you are using Snowflake, you might need to create a [Snowpark](https://www.snowflake.com/en/data-cloud/snowpark/)\\-optimized warehouse if your dataset is significantly large.\n\n## Project setup\n\nThis section highlights the project setup steps for the churn prediction and LTV models.\n\n### Churn prediction\n\nSetting up Predictions for predicting churn involves four easy-to-follow steps:\n\n1.  [Set up a feature table with labels](#1-set-up-a-feature_table_model)\n2.  [Configure training parameters](#2-training) to generate the predictive features.\n3.  [Configure prediction parameters](#3-prediction) to generate the predictive features.\n4.  [Schedule periodic Predictions](#4-scheduling) to generate the predictive features.\n\n#### 1\\. Set up a `feature_table_model`\n\nFollow the [Feature table](https://www.rudderstack.com/docs/archive/profiles/0.10/example/feature-table/) guide to start with a basic Profiles project. The feature you want to predict should be a part of the feature table in the project.\n\nFor example, to predict 30-day inactive churn in advance, you should define it in the feature table so that the model knows how to compute this for historic users.\n\n```\nentity_var:\n  name: churn_30_days\n  select: case when days_since_last_seen >= 30 then 1 else 0 end\n```\n\n#### 2\\. Training\n\nRudderStack simplifies your training configuration to a set of parameters. Start with a [`python_model`](https://www.rudderstack.com/docs/archive/profiles/0.10/predictions/python-models/) type and mention the following parameters:\n\n```\ntrain:\n    file_extension: .json\n    file_validity: 168h\n    inputs: &inputs\n      - packages/feature_table/models/shopify_user_features\n    config:\n      data: &model_data_input_configs\n        label_column: churn_30_days\n        label_value: 1\n        prediction_horizon_days: 30\n        output_profiles_ml_model: *model_name\n        eligible_users: ''\n        inputs: *inputs\n        entity_column: user_main_id\n        recall_to_precision_importance: 1.0\n      preprocessing: \n        ignore_features: [name, gender, device_type]\n```\n\n| Parameter | Description |\n| --- | --- |\n| `file_extension`  <br>Required | The file extension. This is a static value and does not need to be modified. |\n| `file_validity`  <br>Required | If the last trained model is older than this duration, then the model is trained again. |\n| `inputs`  <br>Required | Path to the base features project. |\n| `label_column`  <br>Required | Column for which we want the Predictions. |\n| `prediction_horizon_days`  <br>Required | Number of days in advance when the prediction should be made.<br><br>See [Prediction horizon days](https://www.rudderstack.com/docs/profiles/glossary/#prediction-horizon-days) for more information. |\n| `output_profiles_ml_model`  <br>Required | Name of the model. |\n| `eligible_users` | Definition of the feature that needs to be defined only for a segment of users.<br><br>For example, `country='US' and is_payer=true` |\n| `config.data.inputs` | Path to the referenced project. The `inputs` key above should have `&inputs` added to it. |\n| `entity_column` | If the value of`id_column_name` in the ID stitcher is changed, it should be referenced here too. This field is optional otherwise. |\n| `recall_to_precision_importance` | Also referred to as **beta** in f-beta score, this field is used in classification models to fine-tune the model threshold and give more weight to recall against precision.<br><br>**Note**: This is an optional parameter. If not specified, it defaults to `1.0`, giving equal weight to precision and recall. |\n| `ignore_features` | List of columns from the feature table which the model ignores for training. |\n\n#### 3\\. Prediction\n\nIn your `python_model`, mention the following parameters:\n\n```\npredict:\n    inputs:\n      - packages/feature_table/models/shopify_user_features\n    config:\n      data: *model_data_input_configs\n      outputs:\n        column_names:\n          percentile: &percentile_name percentile_churn_score_7_days\n          score: churn_score_7_days\n        feature_meta_data: &feature_meta_data\n          features:\n            - name: *percentile_name\n              description: 'Percentile of churn score. Higher the percentile, higher the probability of churn'\n```\n\n| Parameter | Description |\n| --- | --- |\n| `inputs`  <br>Required | Path to the base features project. |\n| `percentile`  <br>Required | Column in the output table having the percentile score. |\n| `score`  <br>Required | Column in the output table having the probabilistic score. |\n| `description`  <br>Required | Custom description to give for the feature. |\n\n#### 4\\. Scheduling\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> [Contact us](https://www.rudderstack.com/contact/) to enable this feature for your account.\n\n1.  Upload your project to a GitHub repository.\n2.  Create a Profiles project in the [RudderStack dashboard](https://app.rudderstack.com/). Use the GitHub repository to set up the project.\n3.  Schedule your project with the required cadence. Note that this schedule is for prediction.\n\nTrainings are scheduled as per your configuration of the `file_validity` parameter in the `training` section of your project.\n\n### LTV models\n\nWhile the default labels in the churn prediction model are Boolean, Profiles also lets you predict a continuous variable like revenue or LTV. The configuration is almost similar to churn prediction with some minor adjustments.\n\n#### Set up a `feature_table_model`\n\nThe steps are similar to the setup for [churn prediction](#1-set-up-a-feature_table_model).\n\n#### Training\n\nStart with a [`python_model`](https://github.com/rudderlabs/rudderstack-profiles-base-features-git-flow/blob/feature/prml-319-add-ltv-specific-predictive-feature-in-base-features/models/profiles-ml.yaml) type and specify the following parameters:\n\n```\ntrain:\n    file_extension: .json\n    file_validity: 168h\n    inputs: &inputs\n      - packages/feature_table/models/shopify_user_features\n    config:\n      data: &model_data_input_configs\n        label_column: amount_spent_past_7_days\n        task: regression\n        prediction_horizon_days: 7\n        output_profiles_ml_model: *model_name\n        eligible_users: ''\n        inputs: *inputs\n        entity_column: user_main_id\n      preprocessing: \n        ignore_features: [name, gender, device_type]\n```\n\n| Parameter | Description |\n| --- | --- |\n| `file_extension`  <br>Required | The file extension. This is a static value and does not need to be modified. |\n| `file_validity`  <br>Required | If the last trained model is older than this duration, then the model is trained again. |\n| `inputs`  <br>Required | Path to the base features project. |\n| `label_column`  <br>Required | Column for which we want the Predictions. |\n| `task` | Set to `regression`. Unless specified explicitly, Profiles sets it to `classification` by default. |\n| `prediction_horizon_days`  <br>Required | Number of days in advance when the prediction should be made.<br><br>See [Prediction horizon days](https://www.rudderstack.com/docs/profiles/glossary/#prediction-horizon-days) for more information. |\n| `output_profiles_ml_model`  <br>Required | Name of the model. |\n| `eligible_users` | Definition of the feature that needs to be defined only for a segment of users.<br><br>For example, `country='US' and is_payer=true` |\n| `config.data.inputs` | Path to the referenced project. The `inputs` key above should have `&inputs` added to it. |\n| `entity_column` | If the value of`id_column_name` in the ID stitcher is changed, it should be referenced here too. This field is optional otherwise. |\n| `ignore_features` | List of columns from the feature table which the model ignores for training. |\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> Note that:\n> \n> *   Unlike churn prediction, you should not specify the `label_value` and `recall_to_precision_importance` labels.\n> *   The LTV model introduces a new parameter called `task` which you must set to `regression`. Profiles assumes a classification model by default, unless explicitly specified otherwise.\n\n#### Prediction\n\nThe `python_model` for the LTV use case remains almost the same as churn prediction, except some minor changes:\n\n```\npredict:\n    inputs:\n      - packages/feature_table/models/shopify_user_features\n    config:\n      data: *model_data_configs\n      preprocessing: *model_prep_configs\n      outputs:\n        column_names:\n          percentile: &percentile_name percentile_predicted_amount_spent\n          score: predicted_amount_spent\n        feature_meta_data: &feature_meta_data\n          features:\n            - name: *percentile_name\n              description: 'Percentile of predicted future LTV. Higher the percentile, higher the expected LTV.'\n```\n\n| Parameter | Description |\n| --- | --- |\n| `inputs`  <br>Required | Path to the base features project. |\n| `percentile`  <br>Required | Column in the output table having the percentile score. |\n| `score`  <br>Required | Column in the output table having the probabilistic score. |\n| `description`  <br>Required | Custom description to give for the feature. |\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> The label column is missing in the above `predict` block as the `score` parameter captures the actual prediction value and a Boolean flag is not meaningful in regression use cases.\n\n#### Scheduling\n\nThe steps are similar to the setup for [churn prediction](#4-scheduling).\n\n## Results\n\nFinal output or the predicted features are pushed to your customer360 table. Use the **Explorer** tab to check the predicted value for any given user along with the historical values up to last 5 runs.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> To check the predicted value for a given user:\n> \n> 1.  In the **Preview** section, go to the **Predictive features** tab.\n> 2.  Check the user profile for which the predictive feature has a value.\n> 3.  Search the **USER\\_MAIN\\_ID** of the profile in **Profile viewer**.\n\n[![Predictions predictive feature value](https://www.rudderstack.com/docs/images/profiles/profilesml-predictivefeaturevalue.webp)](https://www.rudderstack.com/docs/images/profiles/profilesml-predictivefeaturevalue.webp)\n\nThe value of the predictive feature is a probability. You can consider it as `true` or `false` based on your threshold.\n\nAll your predictive features are listed separately in the **Overview** tab of your Profiles project. You can check the logs of each run in the `artifacts` directory (available in the **History** tab of your Profiles project).\n\n[![ProfilesML artifacts](https://www.rudderstack.com/docs/images/profiles/profilesml-artifacts.webp)](https://www.rudderstack.com/docs/images/profiles/profilesml-artifacts.webp)\n\nPredictions is a part of RudderStack’s [Early Access Program](https://www.rudderstack.com/docs/resources/early-access-program/). [Contact us](https://www.rudderstack.com/contact/) to get access to this feature.\n\n## FAQ\n\n#### **Is there a project to understand Predictions further?**\n\nYou can check the [Shopify churn model](https://github.com/rudderlabs/rudderstack-profiles-shopify-churn/) that builds a churn prediction score on top of the Shopify library project.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Predictions (Early Access) | RudderStack Docs",
    "description": "Use Profiles' predictive features to train machine learning models.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.11/predictions/",
    "markdown": "# Predictions (Early Access) | RudderStack Docs\n\nUse Profiles’ predictive features to train machine learning models.\n\n* * *\n\n*     7 minute read  \n    \n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> Predictions is part of our [Early Access Program](https://www.rudderstack.com/docs/resources/early-access-program/), where we work with early users and customers to test new features and get feedback before making them generally available. These features are functional but can change as we improve them. We recommend connecting with our team before running them in production.\n> \n> [Contact us](https://www.rudderstack.com/contact/) to get access to this feature.\n\nPredictions extends Profiles’ standard [feature development](https://www.rudderstack.com/docs/archive/profiles/0.11/feature-development/) functionality and lets you easily create predictive features in your warehouse. You can predict features like:\n\n*   Is a customer likely to churn in the next 30 days?\n*   Will a user make a purchase in the next 7 days?\n*   Is a lead going to convert?\n*   How much is a user likely to spend in the next 90 days?\n\nFinally, you can add the predicted feature to user profiles in your warehouse automatically and deliver ML-based segments and audiences to your marketing, product, and customer success teams.\n\nThe following self-guided tour shows you how to build the predictive traits. You can follow the guide and build the project yourself, including sample data, in our [Predictions sample project](https://www.rudderstack.com/docs/archive/profiles/0.11/predictions/predictive-features-snowflake/).\n\n## Use cases\n\nThis section covers some common Predictions use cases.\n\n### Churn prediction\n\nPredicting churn is one of the crucial initiatives across businesses. Without a predicted churn score, your actions are reactive, whereas you can act proactively with a user trait like `is_likely_to_churn`. Once you have such features, you can activate them with the appropriate outreach programs to prevent user churn.\n\n### Customer LTV prediction\n\nPredictions helps you understand your customers’ purchasing behavior over time. You can predict how much amount a particular customer is likely to spend within the prediction time range.\n\n## Prerequisites\n\n*   You must be using a [Snowflake](https://www.rudderstack.com/docs/destinations/warehouse-destinations/snowflake/) or [Redshift](https://www.rudderstack.com/docs/destinations/warehouse-destinations/redshift/) warehouse.\n*   You must set up a standard Profiles project with a [feature table model](https://www.rudderstack.com/docs/archive/profiles/0.11/example/feature-table/).\n*   **Optional**: If you are using Snowflake, you might need to create a [Snowpark](https://www.snowflake.com/en/data-cloud/snowpark/)\\-optimized warehouse if your dataset is significantly large.\n\n## Project setup\n\nThis section highlights the project setup steps for the churn prediction and LTV models.\n\n### Churn prediction\n\nSetting up Predictions for predicting churn involves four easy-to-follow steps:\n\n1.  [Set up a feature table with labels](#1-set-up-a-feature_table_model)\n2.  [Configure training parameters](#2-training) to generate the predictive features.\n3.  [Configure prediction parameters](#3-prediction) to generate the predictive features.\n4.  [Schedule periodic Predictions](#4-scheduling) to generate the predictive features.\n\n### 1\\. Set up a `feature_table_model`\n\nFollow the [Feature table](https://www.rudderstack.com/docs/archive/profiles/0.11/example/feature-table/) guide to start with a basic Profiles project. The feature you want to predict should be a part of the feature table in the project.\n\nFor example, to predict 30-day inactive churn in advance, you should define it in the feature table so that the model knows how to compute this for historic users.\n\n```\nentity_var:\n  name: churn_30_days\n  select: case when days_since_last_seen >= 30 then 1 else 0 end\n```\n\n### 2\\. Training\n\nRudderStack simplifies your training configuration to a set of parameters. Start with a [`python_model`](https://www.rudderstack.com/docs/archive/profiles/0.11/predictions/python-models/) type and mention the following parameters:\n\n```\ntrain:\n    file_extension: .json\n    file_validity: 168h\n    inputs: &inputs\n      - packages/feature_table/models/shopify_user_features\n    config:\n      data: &model_data_input_configs\n        label_column: churn_30_days\n        label_value: 1\n        prediction_horizon_days: 30\n        output_profiles_ml_model: *model_name\n        eligible_users: ''\n        inputs: *inputs\n        entity_column: user_main_id\n        recall_to_precision_importance: 1.0\n      preprocessing: \n        ignore_features: [name, gender, device_type]\n```\n\n| Parameter | Description |\n| --- | --- |\n| `file_extension`  <br>Required | The file extension. This is a static value and does not need to be modified. |\n| `file_validity`  <br>Required | If the last trained model is older than this duration, then the model is trained again. |\n| `inputs`  <br>Required | Path to the base features project. |\n| `label_column`  <br>Required | Column for which we want the Predictions. |\n| `prediction_horizon_days`  <br>Required | Number of days in advance when the prediction should be made.<br><br>See [Prediction horizon days](https://www.rudderstack.com/docs/profiles/glossary/#prediction-horizon-days) for more information. |\n| `output_profiles_ml_model`  <br>Required | Name of the model. |\n| `eligible_users` | Definition of the feature that needs to be defined only for a segment of users.<br><br>For example, `country='US' and is_payer=true` |\n| `config.data.inputs` | Path to the referenced project. The `inputs` key above should have `&inputs` added to it. |\n| `entity_column` | If the value of`id_column_name` in the ID stitcher is changed, it should be referenced here too. This field is optional otherwise. |\n| `recall_to_precision_importance` | Also referred to as **beta** in f-beta score, this field is used in classification models to fine-tune the model threshold and give more weight to recall against precision.<br><br>**Note**: This is an optional parameter. If not specified, it defaults to `1.0`, giving equal weight to precision and recall. |\n| `ignore_features` | List of columns from the feature table which the model ignores for training. |\n\n#### 3\\. Prediction\n\nIn your `python_model`, mention the following parameters:\n\n```\npredict:\n    inputs:\n      - packages/feature_table/models/shopify_user_features\n    config:\n      data: *model_data_input_configs\n      outputs:\n        column_names:\n          percentile: &percentile_name percentile_churn_score_7_days\n          score: churn_score_7_days\n        feature_meta_data: &feature_meta_data\n          features:\n            - name: *percentile_name\n              description: 'Percentile of churn score. Higher the percentile, higher the probability of churn'\n```\n\n| Parameter | Description |\n| --- | --- |\n| `inputs`  <br>Required | Path to the base features project. |\n| `percentile`  <br>Required | Column in the output table having the percentile score. |\n| `score`  <br>Required | Column in the output table having the probabilistic score. |\n| `description`  <br>Required | Custom description to give for the feature. |\n\n#### 4\\. Scheduling\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> [Contact us](https://www.rudderstack.com/contact/) to enable this feature for your account.\n\n1.  Upload your project to a GitHub repository.\n2.  Create a Profiles project in the [RudderStack dashboard](https://app.rudderstack.com/). Use the GitHub repository to set up the project.\n3.  Schedule your project with the required cadence. Note that this schedule is for prediction.\n\nTrainings are scheduled as per your configuration of the `file_validity` parameter in the `training` section of your project.\n\n### LTV models\n\nWhile the default labels in the churn prediction model are Boolean, Profiles also lets you predict a continuous variable like revenue or LTV. The configuration is almost similar to churn prediction with some minor adjustments.\n\n#### Set up a `feature_table_model`\n\nThe steps are similar to the setup for [churn prediction](#1-set-up-a-feature_table_model).\n\n#### Training\n\nStart with a [`python_model`](https://github.com/rudderlabs/rudderstack-profiles-base-features-git-flow/blob/feature/prml-319-add-ltv-specific-predictive-feature-in-base-features/models/profiles-ml.yaml) type and specify the following parameters:\n\n```\ntrain:\n    file_extension: .json\n    file_validity: 168h\n    inputs: &inputs\n      - packages/feature_table/models/shopify_user_features\n    config:\n      data: &model_data_input_configs\n        label_column: amount_spent_past_7_days\n        task: regression\n        prediction_horizon_days: 7\n        output_profiles_ml_model: *model_name\n        eligible_users: ''\n        inputs: *inputs\n        entity_column: user_main_id\n      preprocessing: \n        ignore_features: [name, gender, device_type]\n```\n\n| Parameter | Description |\n| --- | --- |\n| `file_extension`  <br>Required | The file extension. This is a static value and does not need to be modified. |\n| `file_validity`  <br>Required | If the last trained model is older than this duration, then the model is trained again. |\n| `inputs`  <br>Required | Path to the base features project. |\n| `label_column`  <br>Required | Column for which we want the Predictions. |\n| `task` | Set to `regression`. Unless specified explicitly, Profiles sets it to `classification` by default. |\n| `prediction_horizon_days`  <br>Required | Number of days in advance when the prediction should be made.<br><br>See [Prediction horizon days](https://www.rudderstack.com/docs/profiles/glossary/#prediction-horizon-days) for more information. |\n| `output_profiles_ml_model`  <br>Required | Name of the model. |\n| `eligible_users` | Definition of the feature that needs to be defined only for a segment of users.<br><br>For example, `country='US' and is_payer=true` |\n| `config.data.inputs` | Path to the referenced project. The `inputs` key above should have `&inputs` added to it. |\n| `entity_column` | If the value of`id_column_name` in the ID stitcher is changed, it should be referenced here too. This field is optional otherwise. |\n| `ignore_features` | List of columns from the feature table which the model ignores for training. |\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> Note that:\n> \n> *   Unlike churn prediction, you should not specify the `label_value` and `recall_to_precision_importance` labels.\n> *   The LTV model introduces a new parameter called `task` which you must set to `regression`. Profiles assumes a classification model by default, unless explicitly specified otherwise.\n\n#### Prediction\n\nThe `python_model` for the LTV use case remains almost the same as churn prediction, except some minor changes:\n\n```\npredict:\n    inputs:\n      - packages/feature_table/models/shopify_user_features\n    config:\n      data: *model_data_configs\n      preprocessing: *model_prep_configs\n      outputs:\n        column_names:\n          percentile: &percentile_name percentile_predicted_amount_spent\n          score: predicted_amount_spent\n        feature_meta_data: &feature_meta_data\n          features:\n            - name: *percentile_name\n              description: 'Percentile of predicted future LTV. Higher the percentile, higher the expected LTV.'\n```\n\n| Parameter | Description |\n| --- | --- |\n| `inputs`  <br>Required | Path to the base features project. |\n| `percentile`  <br>Required | Column in the output table having the percentile score. |\n| `score`  <br>Required | Column in the output table having the probabilistic score. |\n| `description`  <br>Required | Custom description to give for the feature. |\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> The label column is missing in the above `predict` block as the `score` parameter captures the actual prediction value and a Boolean flag is not meaningful in regression use cases.\n\n#### Scheduling\n\nThe steps are similar to the setup for [churn prediction](#4-scheduling).\n\n## Results\n\nFinal output or the predicted features are pushed to your customer360 table. Use the **Explorer** tab to check the predicted value for any given user along with the historical values up to last 5 runs.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> To check the predicted value for a given user:\n> \n> 1.  In the **Preview** section, go to the **Predictive features** tab.\n> 2.  Check the user profile for which the predictive feature has a value.\n> 3.  Search the **USER\\_MAIN\\_ID** of the profile in **Profile viewer**.\n\n[![Predictions predictive feature value](https://www.rudderstack.com/docs/images/profiles/profilesml-predictivefeaturevalue.webp)](https://www.rudderstack.com/docs/images/profiles/profilesml-predictivefeaturevalue.webp)\n\nThe value of the predictive feature is a probability. You can consider it as `true` or `false` based on your threshold.\n\nAll your predictive features are listed separately in the **Overview** tab of your Profiles project. You can check the logs of each run in the `artifacts` directory (available in the **History** tab of your Profiles project).\n\n[![ProfilesML artifacts](https://www.rudderstack.com/docs/images/profiles/profilesml-artifacts.webp)](https://www.rudderstack.com/docs/images/profiles/profilesml-artifacts.webp)\n\nPredictions is a part of RudderStack’s [Early Access Program](https://www.rudderstack.com/docs/resources/early-access-program/). [Contact us](https://www.rudderstack.com/contact/) to get access to this feature.\n\n## FAQ\n\n#### **Is there a project to understand Predictions further?**\n\nYou can check the [Shopify churn model](https://github.com/rudderlabs/rudderstack-profiles-shopify-churn/) that builds a churn prediction score on top of the Shopify library project.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Predictions (Early Access) | RudderStack Docs",
    "description": "Use Profiles' predictive features to train machine learning models.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.11/predictions/python-models/",
    "markdown": "# Python Models | RudderStack Docs\n\nProfiles model for generating predictive features.\n\n* * *\n\n*     2 minute read  \n    \n\nPredictive features are generated using a new type of Profiles models called `python_model`.\n\n## Using a Python model\n\nThere are two key steps involved in using a Python model - **train** and **predict**.\n\nTo use a Python model, you need to modify the `train` and `predict` blocks in your `profiles.yaml` file. The following snippet highlights these blocks:\n\n```\n# This is a sample file, for detailed reference see: https://rudderlabs.github.io/pywht/\nmodels:\n  - name: shopify_churn\n    model_type: python_model\n    model_spec:\n      occurred_at_col: insert_ts\n      entity_key: user\n      validity_time: 24h # 1 day\n      py_repo_url: https://github.com/rudderlabs/rudderstack-profiles-classifier.git\n      predict:\n        inputs:\n          - packages/feature_table/models/shopify_user_features\n        config:\n          outputs:\n            column_names:\n              percentile: &percentile_name percentile_churn_score_7_days\n              score: churn_score_7_days\n            feature_meta_data: &feature_meta_data\n              features:\n                - name: *percentile_name\n                  description: 'Percentile of churn score. Higher the percentile, higher the probability of churn'\n      train:\n        file_extension: .json\n        file_validity: 60m\n        inputs:\n          - packages/feature_table/models/shopify_user_features\n        config:\n          data:\n            label_column: is_churned_7_days\n            label_value: 1\n            prediction_horizon_days: 7\n            model_name: 'shopify_user_features'\n            \n      <<: *feature_meta_data\n```\n\nIn a Python model, the actual logic resides in a remote location defined by the key `py_repo_url`. This need not be modified for setting up a predictive feature.\n\nIn the `train` block, you can define the label columns by pointing to the `entity_var` defined in the feature table model. You also need to define the following:\n\n*   Expected label value for users who performed the event.\n*   Horizon days, that is, number of days in advance when the predictions need to be made.\n*   Feature table model name defined for your predictive features project. See [Set up a feature table model](https://www.rudderstack.com/docs/archive/profiles/0.11/predictions/#set-up-a-feature-table-model) for more information.\n*   Criteria for eligible users so the model need not be used to predict for all users. You can set this criteria by defining a SQL statement referring to the different `entity_vars`. For example:\n\n```\neligible_users: lower(country) = 'us' and amount_spent_overall > 0\n```\n\nThe above example ensures that the model is trained only on the paying users from the US. Also, the model makes predictions only on this set of users.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> Note that:\n> \n> *   The `eligible_users` key should be added as one more parameter in the data configuration.\n> *   To build a model based on all available users, you can leave the `eligible_users` parameter blank.\n\n## Optional: Run Python model locally\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> If you are using the RudderStack dashboard for running the models, you can skip this step. However, note that RudderStack runs the models locally a few time to get the correct setup.\n\nTo run Python models locally, you need to set up a Python environment with the required packages and add the Python path to the [`siteconfig.yaml`](https://www.rudderstack.com/docs/archive/profiles/0.11/developer-guides/site-configuration/) file.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Python Models | RudderStack Docs",
    "description": "Profiles model for generating predictive features.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.11/predictions/predictive-features-snowflake/",
    "markdown": "# Step by Step Predictions Guide\n\nCreate predictive features with RudderStack and Snowflake without using MLOps\n\n* * *\n\n*     2 minute read  \n    \n\nPredictive features like future LTV and churn propensity can be game changing for a business. If your marketing, customer success, and other teams want to use them, though, your company often faces a binary choice: use a one-size-fits-all solution within an existing SaaS platform (i.e., marketing automation tool), or build out ML and MLOps capabilities internally.\n\nBoth options have significant drawbacks. First, templated SaaS-based solutions can’t leverage all of your customer data and aren’t configurable, which results in low accuracy and impact. On the other hand, hiring data scientists and setting up MLOps is expensive and complex.\n\nModern data teams need an option in the middle: the ability to deploy model templates on all of their customer data, but without additional tooling, processes and headcount.\n\nWith RudderStack Predictions and Snowflake, you can create predictive features directly in your warehouse, without the need to set up MLOps processes and infrastructure. Predictions leverages the full power of Snowpark to run ML models within your existing data engineering workflow.\n\nIn this section, you will learn about two ways to build predictive features in RudderStack Predictions:\n\n1.  [Set up automated features in the RudderStack UI](https://www.rudderstack.com/docs/archive/profiles/0.11/predictions/predictive-features-snowflake/setup-automated-features/) - You can setup and run the jobs within the RudderStack UI. This process makes it easy for less technical users to implement basic predictive features.\n2.  [Code your own custom predictions](https://www.rudderstack.com/docs/archive/profiles/0.11/predictions/predictive-features-snowflake/custom-code/) - Predictions also supports a code-based approach that gives technical users full control to define custom predictive features that match their unique business logic.\n\nIt’s important to note that Predictions runs on top of RudderStack [Profiles](https://www.rudderstack.com/docs/profiles/overview/), a product that automates identity resolution and user feature development in Snowflake.\n\nPredictions leverages the Profiles identity graph to train and run ML models. Because Predictions is part of Profiles, project outputs include an identity graph, standard user featuers (i.e., `last_seen`) and predictive user features (i.e., `percentile_churn_score_30_days`). Both types of features are built using RudderStack data sources and standardized feature definitions.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Step by Step Predictions Guide | RudderStack Docs",
    "description": "Create predictive features with RudderStack and Snowflake without using MLOps",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.10/predictions/python-models/",
    "markdown": "# Python Models | RudderStack Docs\n\nProfiles model for generating predictive features.\n\n* * *\n\n*     2 minute read  \n    \n\nPredictive features are generated using a new type of Profiles models called `python_model`.\n\n## Using a Python model\n\nThere are two key steps involved in using a Python model - **train** and **predict**.\n\nTo use a Python model, you need to modify the `train` and `predict` blocks in your `profiles.yaml` file. The following snippet highlights these blocks:\n\n```\n# This is a sample file, for detailed reference see: https://rudderlabs.github.io/pywht/\nmodels:\n  - name: shopify_churn\n    model_type: python_model\n    model_spec:\n      occurred_at_col: insert_ts\n      entity_key: user\n      validity_time: 24h # 1 day\n      py_repo_url: https://github.com/rudderlabs/rudderstack-profiles-classifier.git\n      predict:\n        inputs:\n          - packages/feature_table/models/shopify_user_features\n        config:\n          outputs:\n            column_names:\n              percentile: &percentile_name percentile_churn_score_7_days\n              score: churn_score_7_days\n            feature_meta_data: &feature_meta_data\n              features:\n                - name: *percentile_name\n                  description: 'Percentile of churn score. Higher the percentile, higher the probability of churn'\n      train:\n        file_extension: .json\n        file_validity: 60m\n        inputs:\n          - packages/feature_table/models/shopify_user_features\n        config:\n          data:\n            label_column: is_churned_7_days\n            label_value: 1\n            prediction_horizon_days: 7\n            model_name: 'shopify_user_features'\n            \n      <<: *feature_meta_data\n```\n\nIn a Python model, the actual logic resides in a remote location defined by the key `py_repo_url`. This need not be modified for setting up a predictive feature.\n\nIn the `train` block, you can define the label columns by pointing to the `entity_var` defined in the feature table model. You also need to define the following:\n\n*   Expected label value for users who performed the event.\n*   Horizon days, that is, number of days in advance when the predictions need to be made.\n*   Feature table model name defined for your predictive features project. See [Set up a feature table model](https://www.rudderstack.com/docs/archive/profiles/0.10/predictions/#set-up-a-feature-table-model) for more information.\n*   Criteria for eligible users so the model need not be used to predict for all users. You can set this criteria by defining a SQL statement referring to the different `entity_vars`. For example:\n\n```\neligible_users: lower(country) = 'us' and amount_spent_overall > 0\n```\n\nThe above example ensures that the model is trained only on the paying users from the US. Also, the model makes predictions only on this set of users.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> Note that:\n> \n> *   The `eligible_users` key should be added as one more parameter in the data configuration.\n> *   To build a model based on all available users, you can leave the `eligible_users` parameter blank.\n\n## Optional: Run Python model locally\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> If you are using the RudderStack dashboard for running the models, you can skip this step. However, note that RudderStack runs the models locally a few time to get the correct setup.\n\nTo run Python models locally, you need to set up a Python environment with the required packages and add the Python path to the [`siteconfig.yaml`](https://www.rudderstack.com/docs/archive/profiles/0.10/developer-guides/site-configuration/) file.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Python Models | RudderStack Docs",
    "description": "Profiles model for generating predictive features.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.10/activation-api/",
    "markdown": "# Activation API (Early Access) | RudderStack Docs\n\nExpose user profiles stored in your Redis instance over an API.\n\n* * *\n\n*     7 minute read  \n    \n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> The Activation API is part of our [Early Access Program](https://www.rudderstack.com/docs/resources/early-access-program/), where we work with users and customers to test new features and get feedback before making them generally available. These features are fully functional but can change as we improve them. We recommend connecting with our team before running them in production.\n> \n> [Contact us](https://www.rudderstack.com/contact/) if you would like access to this feature.\n\nWith RudderStack’s Activation API, you can fetch enriched user traits stored in your Redis instance and use them for near real-time personalization for your target audience.\n\n[![Activation API](https://www.rudderstack.com/docs/images/profiles/activation-api.webp)](https://www.rudderstack.com/docs/images/profiles/activation-api.webp)\n\n## Overview\n\nA brief summary of how the Activation API works:\n\n1.  Sync all your customer 360 data from your Profiles project to your Redis store.\n2.  The Activation API sits on top of this Redis instance and provides endpoints for retrieving and using the enriched user data for personalization.\n\n## How to use the Activation API\n\n1.  Use your Profiles project to create a 360-degree view of the features stored in your data warehouse.\n2.  Set up a [Reverse ETL connection](https://www.rudderstack.com/docs/sources/reverse-etl/) from the warehouse containing the above customer 360 table to the [Redis destination](https://www.rudderstack.com/docs/destinations/streaming-destinations/redis/) in RudderStack. See [Redis destination configuration](#redis-destination-configuration) for more details.\n3.  In your Profiles project settings, turn on the **Enable sync to Redis** toggle.\n\n[![Enable Redis sync for using Activation API](https://www.rudderstack.com/docs/images/profiles/enable-redis-sync.webp)](https://www.rudderstack.com/docs/images/profiles/enable-redis-sync.webp)\n\n4.  [Generate a personal access token](#faq) with **Admin** role in your RudderStack dashboard. This token is required to authenticate and use the Activation API.\n5.  Note the Redis destination ID. See [FAQ](#redis-destination-id) for more information on obtaining this ID.\n6.  [Use the Activation API endpoint](#get-user-profiles) to access your Redis instance and get user data.\n\n> ![success](https://www.rudderstack.com/docs/images/tick.svg)\n> \n> See [Use case](#use-case) for more information on how you can use this data.\n\nThis API uses [Bearer Authentication](https://swagger.io/docs/specification/authentication/bearer-authentication/) for authenticating all requests.\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> Set the [personal access token](#faq) as the bearer token for authentication.\n\n## Base URL\n\n```\nhttps://profiles.rudderstack.com/v1/\n```\n\n## Get user profiles\n\n#### Request body\n\nString\n\nRedis destination ID.\n\nObject\n\nID containing `type` and `value`\n\n```\n{\n  \"entity\": <entity_type>,  // User, project, account, etc.\n  \"destinationId\": <redis_destination_id> , // Redis destination ID\n  \"id\": {\n    \"type\": <id_type>,\n    \"value\": <id_value>\n  }\n}\n```\n\n#### Example request\n\n```\nPOST /v1/activation HTTP/1.1\nHost: profiles.rudderstack.com\nContent-Type: application/json\nAuthorization: Bearer <personal_access_token>\nContent-Length: 90\n\n{\n \"entity\": <entity_type>,\n \"destinationId\": <redis_destination_id>, // Redis destination ID\n \"id\": {\n   \"type\": <id_type>,\n   \"value\": <id_value>\n }\n}\n```\n\n```\ncurl --location 'https://profiles.rudderstack.com/v1/activation' \\\n--header 'Content-Type: application/json' \\\n--header 'Authorization: Bearer <personal_access_token>' \\\n--data '{\n \"entity\": <entity_type>,\n \"destinationId\": <redis_destination_id>, // Redis destination ID\n \"id\": {\n   \"type\": <id_type>,\n   \"value\": <id_value>\n }\n}'\n```\n\n```\nconst axios = require('axios');\nlet data = JSON.stringify({\n  \"destinationId\": <redis_destination_id>,\n  \"entity\": <entity_type>,\n  \"id\": {\n    \"type\": <id_type>,\n    \"value\": <id_value>\n  }\n});\n\nlet config = {\n  method: 'post',\n  maxBodyLength: Infinity,\n  url: 'https://profiles.rudderstack.com/v1/activation',\n  headers: {\n    'Content-Type': 'application/json',\n    'authorization': 'Bearer <personal_access_token>'\n  },\n  data: data\n};\n\naxios.request(config)\n  .then((response) => {\n    console.log(JSON.stringify(response.data));\n  })\n  .catch((error) => {\n    console.log(error);\n  });\n```\n\n#### Responses\n\n*   If the personal access token is absent or trying to access a destination to which it does not have access:\n\n```\nstatusCode: 401\nResponse: {\n  \"error\": \"Unauthorized request. Please check your access token\"\n}\n```\n\n*   If the destination is not Redis or the destination ID is absent/blank:\n\n```\nstatusCode: 404\nResponse: {\n  \"error\": \"Invalid Destination. Please verify you are passing the right destination ID\"\n}\n```\n\n*   If ID is present:\n\n```\nstatusCode: 200\nResponse:\n{\n  \"entity\": <entity_type>,\n  \"id\": {\n    \"type\": <id_type>,\n    \"value\": <id_value>\n  },\n  \"data\": {\n    <traits_from_Redis>\n  }\n}\n```\n\n*   If ID is not present in Redis:\n\n```\nstatusCode: 200\nResponse:\n{\n  \"entity\": <entity_type>,\n  \"id\": {\n    \"type\": <id_type>,\n    \"value\": <id_value>\n  },\n  \"data\": {}\n}\n```\n\n## Use case\n\nYou can use the Activation API for real-time personalization. Once you fetch the user traits from your Redis instance via the API, you can pull them into your client application to alter the application behavior in real-time based on user interactions.\n\nYou can respond immediately with triggered, user-focused messaging based on actions like page views or app clicks and provide a better customer experience.\n\n[![Real time personalization use case](https://www.rudderstack.com/docs/images/profiles/activation-api-use-case.webp)](https://www.rudderstack.com/docs/images/profiles/activation-api-use-case.webp)\n\n## Redis configuration\n\nThis section describes the settings required to set up a [Reverse ETL connection](https://www.rudderstack.com/docs/sources/reverse-etl/) from your warehouse (containing the project’s output table) to the [Redis destination](https://www.rudderstack.com/docs/destinations/streaming-destinations/redis/) in RudderStack.\n\n[![Redis connection settings](https://www.rudderstack.com/docs/images/profiles/redis-connection-settings.webp)](https://www.rudderstack.com/docs/images/profiles/redis-connection-settings.webp)\n\n*   **Address**: Enter the public endpoint of your Redis database. If you are using [Redis Cloud](https://app.redislabs.com/#/), you can find this endpoint by going to your Redis database and navigating to **Configuration** tab > **General**.\n\n[![Redis database public endpoint](https://www.rudderstack.com/docs/images/profiles/redis-public-endpoint.webp)](https://www.rudderstack.com/docs/images/profiles/redis-public-endpoint.webp)\n\n*   **Password**: Enter the database password. You can find the password in the **Security** section of the **Configuration** tab:\n\n[![Redis database password](https://www.rudderstack.com/docs/images/profiles/redis-database-password.webp)](https://www.rudderstack.com/docs/images/profiles/redis-database-password.webp)\n\n*   **Database**: Enter the database name.\n*   **Cluster Mode**: Disable this setting if you haven’t set up Redis in a cluster.\n\n## Data mapping\n\nRudderStack creates multiple Reverse ETL sources automatically based on your Profiles project. You will see separate sources for different `id_served` connected to the same Redis destination.\n\nThe following `pb_project.yaml` snippet shows the sources to be created. RudderStack syncs the customer 360 features to different tables in Redis indexed by `id_served`.\n\n```\nentities:\n  - name: user\n    serve_traits:\n      - id_served: user_id\n      - id_served: anonymous_id\n      - id_served: email\n      - id_served: cart_token\n      - id_served: rudder_id\n```\n\n## FAQ\n\n#### How do I generate a personal access token to use the Activation API?\n\n1.  Log in to your [RudderStack dashboard](https://app.rudderstack.com/).\n2.  Go to **Settings** > **Your Profile** > **Account** tab and scroll down to **Personal access tokens**. Then, click **Generate new token**:\n\n[![New personal access token in RudderStack dashboard](https://www.rudderstack.com/docs/images/rudderstack-api/personal-access-token-1.webp)](https://www.rudderstack.com/docs/images/rudderstack-api/personal-access-token-1.webp)\n\n3.  Enter the **Token name**. Set **Role** to **Admin** and click **Generate**.\n\n[![Personal access token name and role](https://www.rudderstack.com/docs/images/rudderstack-api/personal-access-token-2.webp)](https://www.rudderstack.com/docs/images/rudderstack-api/personal-access-token-2.webp)\n\n4.  Use the personal access token to authenticate to the Activation API.\n\n[![Personal access token details](https://www.rudderstack.com/docs/images/rudderstack-api/personal-access-token-3.webp)](https://www.rudderstack.com/docs/images/rudderstack-api/personal-access-token-3.webp)\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> Save the generated token securely as it will not be visible again once you close this window.\n\n#### Where can I find the Redis destination ID?\n\n1.  Set up a [Redis destination](https://www.rudderstack.com/docs/destinations/streaming-destinations/redis/) in RudderStack.\n2.  Go to the **Settings** tab of your destination to see the **Destination ID**:\n\n[![Redis destination ID](https://www.rudderstack.com/docs/images/rudderstack-api/redis-destination-id.webp)](https://www.rudderstack.com/docs/images/rudderstack-api/redis-destination-id.webp)\n\n#### How can I make Profiles work with the Activation API?\n\nTo use the Activation API with your Profiles project, you need a successful run of your Profiles project that is not past the retention period.\n\nTo enable the Activation API for your Profiles project, turn on the **Enable sync to Redis** setting. A Profile run will then sync automatically.\n\n[![Toggle API in Settings](https://www.rudderstack.com/docs/images/rudderstack-api/activation-api-toggle-settings.png)](https://www.rudderstack.com/docs/images/rudderstack-api/activation-api-toggle-settings.png)\n\n#### Why am I getting an error trying to enable API in my instance for a custom project hosted on GitHub?\n\nFor GitHub projects, you need to explicitly add the IDs of the custom project that need to be served.\n\nIn your `pb_project.yaml` file, you can specify them as shown:\n\n```\nentities:\n  - name: user\n    serve_traits:\n      - id_served: user_id\n      - id_served: anonymous_id\n      - id_served: email\n      - id_served: cart_token\n      - id_served: user_main_id\n```\n\n#### If I force a full resync, stop it, and then start a new sync, does RudderStack always perform a full sync the next time?\n\nIt depends on the state of the task when it was canceled.\n\n*   If the sync is cancelled while RudderStack is preparing a snapshot, then next run depends on the state of the previous successful run and any mapping changes.\n*   If it is cancelled after the sync data is prepared, the next run is incremental.\n\nGenerally if a sync is cancelled manually, it is recommended to trigger a full sync if the previous cancelled task was a full sync. If the previously cancelled sync was incremental, triggering an incremental sync is recommended.\n\n#### Does RudderStack perform a full sync if I add a new column?\n\nRudderStack does not change the sync mode if you make any column additions. It triggers a full sync only if you change/update the data mappings, for example, if the newly added column is sent to the destination via the [Visual Data Mapper](https://www.rudderstack.com/docs/sources/reverse-etl/visual-data-mapper/).\n\nFor Profiles activation syncs, RudderStack updates the mappings and automatically sends all columns from the customer 360 view by triggering a full sync.\n\n#### Suppose I’m running a full sync and the Profiles job is running in parallel and finishes eventually. What happens to the scheduled sync? Does it get queued?\n\nRudderStack first creates a temporary snapshot copy of any sync when it starts. So its syncing the created copy. Even if a Profiles job is running in parallel, the sync - if started - is not impacted by it.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Activation API (Early Access) | RudderStack Docs",
    "description": "Expose user profiles stored in your Redis instance over an API.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.11/predictions/predictive-features-snowflake/sample-data/",
    "markdown": "# Snowflake Sample Data | RudderStack Docs\n\nSample data for Snowflake\n\n* * *\n\n*     2 minute read  \n    \n\nRudderStack provides a sample data set for users walking through the Snowflake guide to implement the Predictions feature. This dataset is available in the [Snowflake marketplace](https://app.snowflake.com/marketplace/listing/GZT0Z856CMJ/rudderstack-inc-rudderstack-event-data-for-quickstart).\n\nThe tables provide all of the needed data to run Profiles and Predictions jobs through the automated UI process as well as the code-based YAML workflow.\n\n## What tables are included in the data set?\n\nThe data set includes 4 tables of RudderStack event data:\n\n*   `PAGES` - page view events from anonymous and known users\n*   `TRACKS` - summarized tracked user actions (like login, signup, order\\_completed, etc.)\n*   `IDENTIFIES` - identify calls run when a user provides a unique identifier (i.e., upon singup)\n*   `ORDER_COMPLETED` - detailed payloads from tracked order\\_completed events\n\nAs of January 2023, here are the approximate number of rows in each table:\n\n*   `PAGES`: ~43k\n*   `TRACKS`: ~14k\n*   `IDENTIFIES`: ~4.8k\n*   `ORDER_COMPLETED`: ~2.2k\n\nThese volumes follow the pattern of a normal eCommerce conversion funnel (pageview, signup, order). Specifically, here’s a rough breakdown of the user journey by volume:\n\n*   30% - never sign in\n*   10% - sign in but never add an item to cart\n*   40% - add to cart and abandon\n*   20% - make purchases\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> Note that this data includes _future_ data until Apr 2024, and starts in June 2023. This is to ensure that future users can still run the project with ‘current’ data. Our team will refresh the data periodically throughout the year.\n\n## What properties are included in the data set?\n\nThis data set includes a _subset_ of the standard properties found in our [Warehouse Schema Spec](https://www.rudderstack.com/docs/destinations/warehouse-destinations/warehouse-schema/) for each table.\n\nThe required columns for running Profiles and Predictions projects are present.\n\nNote that we intentionally limited the number of columns to make the data set easily understandable.\n\n### What user information is included in the data set?\n\nThis data set contains a total of ~10k unique users by `anonymousId`. About half of those unique users (~4.8k) are known users (with an associated `identify` call).\n\nUser data includes a subset of our standard properties for `identify` calls. (We intentionally limited the number of columns to make the data set easily understandable. )\n\nAll of the email addresses were randomly generated and no PII was used in the generation of this data set.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Snowflake Sample Data | RudderStack Docs",
    "description": "Sample data for Snowflake",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.11/predictions/predictive-features-snowflake/setup-automated-features/",
    "markdown": "# Setup Automated Features | RudderStack Docs\n\nSet up RudderStack to build automated features in the RudderStack UI\n\n* * *\n\n*     6 minute read  \n    \n\nSetting up automated features in the RudderStack UI is a straight-forward process. Predictive features are configured within a Profiles project and automatically added to the feature table output when the project is run.\n\n## Project setup\n\nFollow the steps below to set up a project and build predictive features:\n\n### Log into RudderStack\n\nYou can log-in [here](https://app.rudderstack.com/login).\n\n### Navigate to Profiles screen\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/Navigation.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/Navigation.webp)\n\n### Enter a name and description\n\nEnter a unique name and description for the Profiles Project where you want to build the predictive features.\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/Profiles-Name.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/Profiles-Name.webp)\n\n### Select sources\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/Sources.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/Sources.webp)\n\nSelect your Snowflake warehouse. If you have not configured the Snowflake warehouse, set up an event stream connection to Snowflake in RudderStack ([see details here](https://www.rudderstack.com/docs/destinations/warehouse-destinations/snowflake/)) and refer to the setup steps above.\n\nOnce you select the warehouse, you will be able to choose from RudderStack event sources that are connected to Snowflake. In this example, the JavaScript source created above is used to write to the same schema as the sample data. Profiles will use the `PAGES`, `TRACKS`, `IDENTIFIES`, and `ORDER_COMPLETED` tables from that schema to build automated and predictive features.\n\n## Map ID fields\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/Map-ID.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/Map-ID.webp)\n\nMap the fields from the source table(s) to the correct type of ID. The standard ID types are:\n\n*   `user_id`\n*   `anonymous_id`\n*   `email`\n\n**Note that for RudderStack event sources, standard ID column names will be mapped for you automatically**. If you have included additional identifiers in your payloads, you can map those custom column names to standard identifiers by clicking **Add mapping** at the bottom of the table.\n\n### Map `Order_Completed` table\n\nClick on **Add mapping** and map the `USER_ID` and `ANONYMOUS_ID` columns to standard identifiers to include the `ORDER_COMPLETED` table as a source for the identity graph and user features.\n\n| Source | Event | Property | ID Type |\n| --- | --- | --- | --- |\n| QuickStart Test Site | ORDER\\_COMPLETED | USER\\_ID | user\\_id |\n| QuickStart Test Site | ORDER\\_COMPLETED | ANONYMOUS\\_ID | anonymous\\_id |\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/map-id-orders.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/map-id-orders.webp)\n\n## Create default features in the UI\n\nThere are two types of automated features you can define in the UI:\n\n*   Default features\n*   Custom features\n\nThis guide focuses on the default features that are automatically generated.\n\n### Set up default features\n\nDefault features are features commonly used in Profiles projects. RudderStack provides a template library for these features to make them easy to add to your project. Templated features give you access to over 40 different standard and predictive features, which are generated in Snowflake automatically.\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/feature-categories.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/feature-categories.webp)\n\nDefault features are divided into 4 categories:\n\n*   **Attribution** - campaign, source, and churn features\n*   **Demographics** - user trait features\n*   **Engagement** - user activity features\n*   **Predictive ML Features** - predictive features\n\nYou can open the drop down menu for each category and select as many as you would like for your project.\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/feature-attribution.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/feature-attribution.webp)\n\nFor this guide, select:\n\n*   Attribution\n    *   `first_source_name`\n    *   `is_churned_30_days`\n    *   `is_churned_90_days`\n*   Demographics\n    *   `first_name`\n    *   `last_name`\n    *   `state`\n*   Engagement\n    *   `first_date_seen`\n    *   `last_date_seen`\n    *   `total_sessions_90_days`\n    *   `total_sessions_last_week`\n*   Predictive ML Features\n    *   `percentile_churn_score_30_days`\n\nIt is important to remember that RudderStack runs all of the feature-generation code transparently in Snowflake. For any of the default features, other than Predictive ML Features, you can click on **Preview Code** and get a yaml code snippet defining that feature (the yaml definition is used to generate SQL). This is helpful for technical users who want a deeper understanding of feature logic (and a running start for coding their own features).\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/churn-code-snippet.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/churn-code-snippet.webp)\n\n#### Churn definition\n\nRudderStack Predictions automatically generates a binary churn value for every user based on inactivity over a 7, 30, or 90-day period.\n\nFor example, to calculate the `is_churned_7_days` value, RudderStack looks for any activity timestamp for a particular user in the `TRACKS` and `PAGES` tables over the previous 7 day period. Practically, this means that RudderStack executes a ‘max timestamp’ query against those tables to see if users have viewed a page or performed other tracked actions (like clicks, form submits, add to carts, etc.) and then calculates the difference from today. If the query returns 7 or more, that means they haven’t performed any activity over the last 7 days and their `is_churned_7_days` trait is set to `1`.\n\n#### How Predictions models percentile churn scores\n\nUsing the standard definition (no activity over a defined period), RudderStack Predictions automatically runs a python-based churn model in Snowpark that predicts whether users will become inactive (churn) over the next 7, 30, or 90-day period. This model is trained on existing user data, using the Profiles identity graph, so it is recommended that you have a minimum of 5,000-10,000 unique users to achieve accurate output for business use cases.\n\n**How Predictions automates ML with Snowpark**\n\nPredictions streamlines integration with Snowpark by using the authentication from your existing Snowflake integration in RudderStack.\n\nIn order to run models in Snowpark, there is one additional set of permissions required. To run Predictions jobs, you must have permission to create stages within your schema. For more information see the **CREATE STAGE** [documentation](https://docs.snowflake.com/en/sql-reference/sql/create-stage#access-control-requirements).\n\nOnce permissions are granted, you will be able to run jobs that produce predictive features. **If you have followed the steps in [Prerequisite](https://www.rudderstack.com/docs/archive/profiles/0.11/predictions/predictive-features-snowflake/prerequisite/) guide, that permission has already been granted.**\n\n## Create custom features in the UI\n\nIf a needed feature is not in the template library, you can define a custom feature in the UI. Custom features can be standard or predictive features.\n\n### Add Custom Features\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/custom-feature.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/custom-feature.webp)\n\nClick on **Add a custom feature** at the top of the page and build an `average_order` feature with the following values:\n\n| Field | Value |\n| --- | --- |\n| **Name** | average\\_order |\n| **Description** | Average Order Size including shipping, taxes, and discounts |\n| **Function Type** | AGGREGATE |\n| **Function** | AVG |\n| **Event** | EVENTS.ORDER\\_COMPLETED |\n| **Property or Trait** | TOTAL |\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/custom-feature-define.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/custom-feature-define.webp)\n\nOnce complete click **Save**. The custom feature will be added to the top of the page.\n\n## Set Schedule\n\nThere are three options to set a schedule for how often the feature generation job runs:\n\n*   Basic\n*   Cron\n*   Manual\n\n### Basic\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/schedule-basic.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/schedule-basic.webp)\n\nSchedule on a predetermined interval.\n\nThe frequency can be every:\n\n*   30 minutes\n*   1 hour\n*   3 hours\n*   6 hours\n*   12 hours\n*   24 hours\n\nThen select a starting time for the initial sync.\n\n### Cron\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/schedule-cron.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/schedule-cron.webp)\n\nSchedule using cron expressions for more specific scheduling (i.e. Daily on Tuesdays and Thursdays).\n\nIf you are not familiar with cron expressions, you can use the builder in the UI.\n\n### Manual\n\nOnly runs when manually triggered within the UI. For this guide, select **Manual**.\n\n## Save, review, and create project\n\n### Save project\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/save-project.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/save-project.webp)\n\nFill in the **Schema** field with `PROFILES` (to match what we created earlier). This is where the feature table will be written to in Snowflake.\n\n### Review and create project\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/review-create.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/review-create.webp)\n\nFinally, review all the settings and when ready, click `Create user 360`.\n\n## Review created features\n\nOnce the initial project run is initiated, it may take up to 25-30 minutes to complete. Once the job is done, you are able to explore the data in RudderStack’s UI, including model fit charts for predictive features and individual user records with all features.\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/ui-predictive.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/ui-predictive.webp)\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/ui-predictive-graphs.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/ui-predictive-graphs.webp)\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/ui-explorer.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/ui-explorer.webp)\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Setup Automated Features | RudderStack Docs",
    "description": "Set up RudderStack to build automated features in the RudderStack UI",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.11/predictions/predictive-features-snowflake/prerequisite/",
    "markdown": "# Prerequisites | RudderStack Docs\n\nPrerequisites to generate predictive features in Snowflake using RudderStack Predictions\n\n* * *\n\n*     8 minute read  \n    \n\nTo follow this guide, you will need access to both RudderStack and Snowflake. If you do not have access, follow these links to create a free [RudderStack account](https://app.rudderstack.com/signup?type=freetrial) and [Snowflake account](https://signup.snowflake.com/).\n\nOnce you set up your RudderStack account, [reach out to our support team](mailto:support@rudderstack.com?subject=I%20would%20like%20access%20to%20your%20Predictions%20feature) to request access to our Predictions feature.\n\n## Set up Snowflake for Event Stream data\n\nBecause Predictions is designed to run in a production environment, you need to perform some basic set up in Snowflake (and later, your RudderStack workspace) to simulate the pipelines you would run when collecting user event data.\n\n### Create a new role and user in Snowflake\n\nIn your Snowflake console, run the following commands to create the role `QUICKSTART`.\n\nVerify the role `QUICKSTART` was successfully created.\n\nCreate a new user QUICKSTART\\_USER with a password `<strong_unique_password>`.\n\n```\nCREATE USER QUICKSTART_USER PASSWORD = '<strong_unique_password>' DEFAULT_ROLE = 'QUICKSTART';\n```\n\nVerify the user `QUICKSTART_USER` was successfully created.\n\n### Create RudderStack schema and grant permissions to role\n\nCreate a dedicated schema `_RUDDERSTACK` in your database.\n\n**Replace `<YOUR_DATABASE>` in all queries with your actual database name.**\n\n```\nCREATE SCHEMA \"<YOUR_DATABASE>\".\"_RUDDERSTACK\";\n```\n\nGrant full access to the schema \\_RUDDERSTACK for the previously created role `QUICKSTART`.\n\n```\nGRANT ALL PRIVILEGES ON SCHEMA \"<YOUR_DATABASE>\".\"_RUDDERSTACK\" TO ROLE QUICKSTART;\n```\n\n### Grant permissions on the warehouse, database, schema, and table\n\nEnable the user `QUICKSTART_USER` to perform all operations allowed for the role `QUICKSTART` (via the privileges granted to it).\n\n```\nGRANT ROLE QUICKSTART TO USER QUICKSTART_USER;\n```\n\nRun the following commands to allow the role `QUICKSTART` to look up the objects within your warehouse, database, schema, and the specific table or view:\n\n```\nGRANT USAGE ON WAREHOUSE \"<YOUR_WAREHOUSE>\" TO ROLE QUICKSTART;\nGRANT USAGE ON DATABASE \"<YOUR_DATABASE>\" TO ROLE QUICKSTART;\nGRANT USAGE ON SCHEMA \"<YOUR_DATABASE>\".\"_RUDDERSTACK\" TO ROLE QUICKSTART;\nGRANT SELECT ON ALL TABLES IN SCHEMA \"<YOUR_DATABASE>\".\"_RUDDERSTACK\" TO ROLE  QUICKSTART;\nGRANT SELECT ON FUTURE TABLES IN SCHEMA \"<YOUR_DATABASE>\".\"_RUDDERSTACK\" TO ROLE QUICKSTART;\nGRANT SELECT ON ALL VIEWS IN SCHEMA \"<YOUR_DATABASE>\".\"_RUDDERSTACK\" TO ROLE QUICKSTART;\nGRANT SELECT ON FUTURE VIEWS IN SCHEMA \"<YOUR_DATABASE>\".\"_RUDDERSTACK\" TO ROLE QUICKSTART;\n```\n\n**Replace `<YOUR_DATABASE>` with the exact Snowflake database name.**\n\n## Import RudderStack event data from the Snowflake marketplace\n\nTo set up automated features, you will need the RudderStack event data in your Snowflake warehouse. If you already use RudderStack and have the following tables and fields (see below), skip to the [Profiles Schema and Permissions](#profiles-schema-and-permissions) section. For this guide, using the provided sample data is recommended.\n\n*   `TRACKS`\n*   `IDENTIFIES`\n    *   `user_id`\n    *   `anonymous_id`\n    *   `email`\n*   `PAGES`\n*   `ORDER_COMPLETED`\n\n**NOTE:** You must have all the three identity types in your `INDENTIFIES` table. If you are using your own data and don’t normally track email, you can send the following `identify` call to add the column:\n\n```\nrudderanalytics.identify('userId', {\n    email:'email@address.com',\n    name:'name'\n})\n```\n\n### Get sample data\n\nIf you are setting up RudderStack for the first time go to the [Snowflake Marketplace](https://app.snowflake.com/marketplace/listing/GZT0Z856CMJ/rudderstack-inc-rudderstack-event-data-for-quickstart) and add RudderStack Event Data for Quickstart to your Snowflake account for free. This will add a database with the needed tables to your Snowflake warehouse with no additional storage cost for you.\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/Snowflake-marketplace.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/Snowflake-marketplace.webp)\n\nAt the next screen, open **Options** and add role `QUICKSTART` to have access to this database.\n\n### Create schema for sample data\n\nThe database with the sample data is read-only so you will need to copy it to a new schema to be able to create a valid event stream pipeline (and run a Predictions job on the data).\n\nCreate a new schema in the database you already set up. Name the schema “EVENTS”.\n\n```\nCREATE SCHEMA \"<YOUR_DATABASE>\".\"EVENTS\";\n```\n\nGive permission to the `QUICKSTART` role to create new tables in the above schema.\n\n```\nGRANT ALL PRIVILEGES ON SCHEMA \"<YOUR_DATABASE>\".\"EVENTS\" FOR ROLE QUICKSTART;\n```\n\nCopy the sample data into the newly create schema.\n\n```\nCREATE TABLE \"<YOUR_DATABASE>\".\"EVENTS\".\"TRACKS\" AS SELECT * FROM \"SNOWFLAKE_QUICKSTART\".\"PUBLIC\".\"TRACKS\";\nCREATE TABLE \"<YOUR_DATABASE>\".\"EVENTS\".\"IDENTIFIES\" AS SELECT * FROM \"SNOWFLAKE_QUICKSTART\".\"PUBLIC\".\"IDENTIFIES\";\nCREATE TABLE \"<YOUR_DATABASE>\".\"EVENTS\".\"PAGES\" AS SELECT * FROM \"SNOWFLAKE_QUICKSTART\".\"PUBLIC\".\"PAGES\";\nCREATE TABLE \"<YOUR_DATABASE>\".\"EVENTS\".\"ORDER_COMPLETED\" AS SELECT * FROM \"SNOWFLAKE_QUICKSTART\".\"PUBLIC\".\"ORDER_COMPLETED\";\n```\n\nNow you are ready to create a pipeline connection in RudderStack.\n\n## Create JavaScript source\n\nRudderStack’s Profiles and Predictions products require a warehouse destination with an active sync from a source (a data pipeline). Therefore we will create a JavaScript source that can send a test event to Snowflake.\n\nAfter logging into RudderStack, navigate to the **Directory** from the sidebar on the left, then select the JavaScript source from the list of sources.\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/select-js-source.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/select-js-source.webp)\n\nEnter “QuickStart Test Site” for the source name and click `Continue`. You have successfully added a source!\n\nNote at the bottom of the JavaScript Source page is a `Write Key`. You will need this for sending a test event after connecting the Snowflake destination.\n\n## Create Snowflake destination\n\nNavigate to the **Overview** tab in the JavaScript source view and click on **Add Destination**.\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/add-destination.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/add-destination.webp)\n\nSelect the Snowflake destination from the list, then on the next page give it the name “Snowflake QuickStart” and click **Continue**.\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/select-snowflake.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/select-snowflake.webp)\n\nAdd in your Snowflake connection credentials:\n\n*   **Account**: Your account name.\n*   **Database**: Your database name that you used in the previous steps for `QUICKSTART`.\n*   **Warehouse**: Your warehouse that you granted usage to `QUICKSTART`.\n*   **User**: `QUICKSTART_USER`\n*   **Role**: `QUICKSTART`\n*   **Password**: Password for `QUICKSTART_USER`.\n*   **Namespace**: `EVENTS`\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/snowflake-config.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/snowflake-config.webp)\n\nAt the bottom under **Object Storage Configuration** toggle **Use RudderStack managed object storage** ON.\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/object-storage-toggle.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/object-storage-toggle.webp)\n\nLeave the defaults for all other settings and click **Continue**. RudderStack will verify credentials and that it has the needed permissions.\n\nYou have now created a pipeline connection in RudderStack!\n\n## Send test event\n\nYou can use a test site to send a `connection_setup` event. This will not effect the sample data tables. But first, get the following configuration data from RudderStack:\n\n*   RudderStack Data Plane URL\n*   JavaScript Source Write Key\n\n### Data Plane URL\n\nGo to the **Connections** page in the RudderStack app and copy the **Data Plane** URL from the top of the page.\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/data-plane-url.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/data-plane-url.webp)\n\n### Write key\n\nGo to your JavaScript source in RudderStack and in the **Setup** tab scroll down and copy the **Write key**.\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/write-key.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/write-key.webp)\n\n### Test event\n\nGo to RudderStack’s [test website](https://ryanmccrary.github.io/rudderstackdemo/) and copy your Data Plane URL and Write Key into the top fields and press **Submit**.\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/test-site-setup.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/test-site-setup.webp)\n\nEnter `connection_setup` into the `event_name` field next to **Send Custom Event** and then click on **Send Custom Event**.\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/test-site-event.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/test-site-event.webp)\n\nYou can check the event using RudderStack’s [**Live events**](https://www.rudderstack.com/docs/dashboard-guides/live-events/) view or check the **Syncs** tab in the Snowflake destination.\n\n**Note that the test event needs to be delivered to Snowflake to validate the pipeline.** If needed, you can run a manual sync by clicking **Sync now** in the **Syncs** tab of the Snowflake destination view in RudderStack.\n\n## Profiles schema and permissions\n\nRemember that Predictions automatically runs a Profiles job to create an identity graph. In this step, create a new schema where the identity graph and the related tables and views will be generated.\n\n```\nCREATE SCHEMA \"<YOUR_DATABASE>\".\"PROFILES\";\n```\n\nNow we need to grant permissions to the `QUICKSTART` role.\n\nProfiles will need the following permissions to run:\n\n*   Read access to all input tables to the model (already complete if you followed the previous setup steps)\n*   Write access to the schemas and common tables that the Profiles project creates.\n\nFor the write access run the following statements:\n\n```\nGRANT ALL PRIVILEGES ON SCHEMA PROFILES_QUICKSTART.PROFILES TO ROLE QUICKSTART;\nGRANT SELECT ON ALL TABLES IN SCHEMA PROFILES_QUICKSTART.PROFILES TO ROLE QUICKSTART;\nGRANT SELECT ON FUTURE TABLES IN SCHEMA PROFILES_QUICKSTART.PROFILES TO ROLE QUICKSTART;\nGRANT SELECT ON ALL VIEWS IN SCHEMA PROFILES_QUICKSTART.PROFILES TO ROLE QUICKSTART;\nGRANT SELECT ON FUTURE VIEWS IN SCHEMA PROFILES_QUICKSTART.PROFILES TO ROLE QUICKSTART;\n```\n\nYou are now ready to run Profiles and Predictions projects in the RudderStack UI!\n\n## Profiles CLI setup\n\nBefore you start building automated features, you need to perform some additional setup steps so that you can transition seamlessly from the UI-based workflow to the code-based workflow in the [code your own custom predictions](https://www.rudderstack.com/docs/archive/profiles/0.11/predictions/predictive-features-snowflake/custom-code/) section.\n\nTo build custom features with code, you will need Python3 and the RudderStack Profiles CLI tool (`PB`, for Profiles Builder) installed on your machine. If you do not have `PB` installed, follow the instructions below. This includes authentication for your Snowflake environment. **Use the warehouse, database, and schema setup in the previous steps.** This authentication will be used for accessing your Snowflake warehouse and running Snowpark. For more information about Profiles CLI tool, see [documentation](https://www.rudderstack.com/docs/profiles/get-started/profile-builder/).\n\n### Install Profile Builder tool\n\nOpen a console window and install the Profile Builder `PB` tool.\n\n```\npip3 install profiles-rudderstack\n```\n\nCheck the version to make sure it is at least `0.10.5`\n\n### Install ML dependency\n\nIn order to run ML models you will need to install the python package `profiles-multieventstream-features`. Run the following command to install it.\n\n```\npip install git+https://github.com/rudderlabs/profiles-pycorelib\n```\n\nEnsure you have the following python packages installed. These are required to use the `rudderstack-profiles-classifier` package to train classification models for predictive features.\n\n```\ncachetools>=4.2.2\nhyperopt>=0.2.7\njoblib>=1.2.0\nmatplotlib>=3.7.1\nseaborn>=0.12.0\nnumpy>=1.23.1\npandas>=1.4.3\nPyYAML>=6.0.1\nsnowflake_connector_python>=3.1.0\nsnowflake-snowpark-python[pandas]>=0.10.0\nscikit_learn>=1.1.1\nscikit_plot>=0.3.7\nshap>=0.41.0\nplatformdirs>=3.8.1\nxgboost>=1.5.0\nredshift-connector\n```\n\n### Create warehouse connection\n\nInitiate a warehouse connection:\n\nFollow the prompts and enter the details for your Snowflake warehouse/database/schema/user.\n\n```\nEnter Connection Name: quickstart\nEnter target:  (default:dev)  # Press enter, leaving it to default\nEnter account: <YOUR_ACCOUNT>\nEnter warehouse: <YOUR_WAREHOUSE>\nEnter dbname: <YOUR_DATABASE>\nEnter schema: PROFILES\nEnter user: QUICKSTART_USER\nEnter password: <password>\nEnter role: QUICKSTART\nAppend to /Users/<user_name>/.pb/siteconfig.yaml? [y/N]\ny\n```\n\n### Enable ML models\n\nFinally, enable ML models within `siteconfig.yaml`.\n\nOpen the file `/Users/<user_name>/.pb/siteconfig.yaml` in a text editor.\n\nAt the bottom of the file there is a `py_models` section. Update it to look like this:\n\n```\npy_models:\n    enabled: true\n    python_path: $(which python3)\n    credentials_presets: null\n    allowed_git_urls_regex: \"\"\n```\n\n## Snowpark\n\nPredictive features utilizes Snowpark within your Snowflake environment. It uses the same authentication as Snowflake and is able to run jobs within Snowflake.\n\nThis will run python code in a virtual warehouse in Snowflake and will incur compute costs. These costs vary depending on the type of model and the quantity of data used in training and prediction. For more general information on Snowflake compute costs, see [Understanding Compute Costs](https://docs.snowflake.com/en/user-guide/cost-understanding-compute).\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Prerequisites | RudderStack Docs",
    "description": "Prerequisites to generate predictive features in Snowflake using RudderStack Predictions",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.10/feature-development/",
    "markdown": "# Feature Development | RudderStack Docs\n\nUse unified profiles and enrich them with the required features/traits to drive targeted campaigns.\n\n* * *\n\n*     3 minute read  \n    \n\nOnce you have [performed identity stitching](https://www.rudderstack.com/docs/archive/profiles/0.10/identity-stitching/#how-to-perform-identity-stitching) to map the individual entities to known identifiers, you can enhance the unified profiles with additional data points and features.\n\nTo develop user features, you can use a **entity traits 360** to define the features/traits in your warehouse tables. You can perform calculations over this data to devise meaningful outcomes, which can help marketing teams to run effective campaigns.\n\n## How to define features?\n\nYou can use RudderStack’s [Entity Traits 360 model](https://www.rudderstack.com/docs/archive/profiles/0.10/example/feature-table/) to write feature definitions. The Profiles project generates and runs SQL in the background and automatically adds the resulting features to a entity traits 360.\n\nYou can produce a entity traits 360 or a feature table for specific projects like personalization, recommendations, or analytics.\n\n> ![success](https://www.rudderstack.com/docs/images/tick.svg)\n> \n> You can combine the features to create even more features. You can also use [custom SQL queries](https://www.rudderstack.com/docs/archive/profiles/0.10/feature-development/sql-models/) to enrich unified user profiles for advanced use cases.\n\nA sample configuration file to create a entity traits 360 model:\n\n```\nvar_groups:\n  - name: vars\n    entity_key: session\n    vars:\n        - entity_var:\n            name: campaign_source_first_touch\n            select: first_value(context_campaign_source) # Window functions are supported in entity_vars, as long as the value is unique for a given user id\n            window:\n            # All the window functions in entity_vars are partitioned by main_id by default. It can take only order_by as a parameter.\n                order_by:\n                # In this example, we take the oldest occurence of the campaign source, so we sort the rows by timestamp in ascending order\n                    - timestamp asc\n            from: inputs/rsIdentifies\n            where: context_campaign_source is not null and context_campaign_source != ''\n        - entity_var:\n            name: first_seen_tracks\n            select: min(timestamp::date) # Get oldest timestamp from the tracks table\n            from: inputs/rsTracks\n            description: First seen timestamp from tracks table.\n            is_feature: false\n        - entity_var:\n            name: first_seen_identifies\n            select: min(timestamp::date)\n            from: inputs/rsIdentifies\n            description: First seen timestamp from identifies table\n            is_feature: false\n        - entity_var:\n            # Once min timestamps from both tracks and identifies are defined, we pick the earliest timestamp of both here.\n            # The prev two are temp features used to derive this feature.\n            name: first_seen_date\n            select: to_date(least(coalesce('{{session.Var(\"first_seen_tracks\")}}','{{session.Var(\"first_seen_identifies\")}}'), coalesce('{{session.Var(\"first_seen_identifies\")}}','{{session.Var(\"first_seen_tracks\")}}')\n        - entity_var:\n            name: orders_completed_in_past_365_days\n            select: count( * ) # SQL function to count all rows from tracks table where event_type = 'order_completed'\n            from: inputs/rsTracks\n            where: event = 'order_completed' and datediff(day, date(timestamp), current_date()) <= 365\n            description: Number of orders completed in the past 365 days.\n        - entity_var:\n            name: n_active_days_total\n            select: count(distinct(timestamp::date)) # SQL function to first convert timestamp to date, then count the distinct dates\n            from: inputs/rsTracks # Refering to the tracks table defined in the inputs.yaml file\n            description: Number of days since the user first visited the app.\n```\n\n## Benefits\n\n*   You can use the output of the identity graph to define or compute features across multiple entity traits 360s.\n*   As the number of features/traits increases, Profiles makes the maintenance process much easier by using a configuration file (as opposed to large and complex SQL queries).\n*   Profiles generates highly performant SQL to build entity traits 360s, which helps mitigate computing costs and engineering resources when the data sets become large, dependencies become complex, and features require data from multiple sources.\n\n## Use-cases\n\n*   Create analytics queries like demographic views, user activity views, etc.\n*   Send data using a [Reverse ETL](https://www.rudderstack.com/docs/sources/reverse-etl/) pipeline to various cloud destinations.\n*   Use RudderStack Audiences to send customer profiles to marketing tools (available for beta customers).\n\n#### See also\n\n*   [Sample entity traits 360 project](https://www.rudderstack.com/docs/archive/profiles/0.10/example/feature-table/)\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Feature Development | RudderStack Docs",
    "description": "Use unified profiles and enrich them with the required features/traits to drive targeted campaigns.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.11/activation-api/",
    "markdown": "# Activation API (Early Access) | RudderStack Docs\n\nExpose user profiles stored in your Redis instance over an API.\n\n* * *\n\n*     7 minute read  \n    \n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> The Activation API is part of our [Early Access Program](https://www.rudderstack.com/docs/resources/early-access-program/), where we work with users and customers to test new features and get feedback before making them generally available. These features are fully functional but can change as we improve them. We recommend connecting with our team before running them in production.\n> \n> [Contact us](https://www.rudderstack.com/contact/) if you would like access to this feature.\n\nWith RudderStack’s Activation API, you can fetch enriched user traits stored in your Redis instance and use them for near real-time personalization for your target audience.\n\n[![Activation API](https://www.rudderstack.com/docs/images/profiles/activation-api.webp)](https://www.rudderstack.com/docs/images/profiles/activation-api.webp)\n\n## Overview\n\nA brief summary of how the Activation API works:\n\n1.  Sync all your customer 360 data from your Profiles project to your Redis store.\n2.  The Activation API sits on top of this Redis instance and provides endpoints for retrieving and using the enriched user data for personalization.\n\n## How to use the Activation API\n\n1.  Use your Profiles project to create a 360-degree view of the features stored in your data warehouse.\n2.  In your Profiles project settings, scroll down to **Activation API** and turn on the **Enable sync to Redis** toggle.\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> Before you enable the Activation API toggle, make sure that:\n> \n> *   You have at least one successful Profiles run.\n> *   Your `pb_project.yaml` > `entities` defines a `serve_traits` property.\n> \n> Also, note that the first time you toggle on this API, RudderStack requests for your Redis credentials of the Profiles Redis store.\n\n[![Enable Redis sync for using Activation API](https://www.rudderstack.com/docs/images/profiles/enable-redis-sync.webp)](https://www.rudderstack.com/docs/images/profiles/enable-redis-sync.webp)\n\n3.  [Generate a personal access token](#faq) with **Admin** role in your RudderStack dashboard. This token is required to authenticate and use the Activation API.\n4.  Note the Redis destination ID. See [FAQ](#redis-destination-id) for more information on obtaining this ID.\n5.  [Use the Activation API endpoint](#get-user-profiles) to access your Redis instance and get user data.\n\n> ![success](https://www.rudderstack.com/docs/images/tick.svg)\n> \n> See [Use case](#use-case) for more information on how you can use this data.\n\nThis API uses [Bearer Authentication](https://swagger.io/docs/specification/authentication/bearer-authentication/) for authenticating all requests.\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> Set the [personal access token](#faq) as the bearer token for authentication.\n\n## Base URL\n\n```\nhttps://profiles.rudderstack.com/v1/\n```\n\n## Get user profiles\n\n#### Request body\n\nString\n\nRedis destination ID.\n\nObject\n\nID containing `type` and `value`\n\n```\n{\n  \"entity\": <entity_type>,  // User, project, account, etc.\n  \"destinationId\": <redis_destination_id> , // Redis destination ID\n  \"id\": {\n    \"type\": <id_type>,\n    \"value\": <id_value>\n  }\n}\n```\n\n#### Example request\n\n```\nPOST /v1/activation HTTP/1.1\nHost: profiles.rudderstack.com\nContent-Type: application/json\nAuthorization: Bearer <personal_access_token>\nContent-Length: 90\n\n{\n \"entity\": <entity_type>,\n \"destinationId\": <redis_destination_id>, // Redis destination ID\n \"id\": {\n   \"type\": <id_type>,\n   \"value\": <id_value>\n }\n}\n```\n\n```\ncurl --location 'https://profiles.rudderstack.com/v1/activation' \\\n--header 'Content-Type: application/json' \\\n--header 'Authorization: Bearer <personal_access_token>' \\\n--data '{\n \"entity\": <entity_type>,\n \"destinationId\": <redis_destination_id>, // Redis destination ID\n \"id\": {\n   \"type\": <id_type>,\n   \"value\": <id_value>\n }\n}'\n```\n\n```\nconst axios = require('axios');\nlet data = JSON.stringify({\n  \"destinationId\": <redis_destination_id>,\n  \"entity\": <entity_type>,\n  \"id\": {\n    \"type\": <id_type>,\n    \"value\": <id_value>\n  }\n});\n\nlet config = {\n  method: 'post',\n  maxBodyLength: Infinity,\n  url: 'https://profiles.rudderstack.com/v1/activation',\n  headers: {\n    'Content-Type': 'application/json',\n    'authorization': 'Bearer <personal_access_token>'\n  },\n  data: data\n};\n\naxios.request(config)\n  .then((response) => {\n    console.log(JSON.stringify(response.data));\n  })\n  .catch((error) => {\n    console.log(error);\n  });\n```\n\n#### Responses\n\n*   If the personal access token is absent or trying to access a destination to which it does not have access:\n\n```\nstatusCode: 401\nResponse: {\n  \"error\": \"Unauthorized request. Please check your access token\"\n}\n```\n\n*   If the destination is not Redis or the destination ID is absent/blank:\n\n```\nstatusCode: 404\nResponse: {\n  \"error\": \"Invalid Destination. Please verify you are passing the right destination ID\"\n}\n```\n\n*   If ID is present:\n\n```\nstatusCode: 200\nResponse:\n{\n  \"entity\": <entity_type>,\n  \"id\": {\n    \"type\": <id_type>,\n    \"value\": <id_value>\n  },\n  \"data\": {\n    <traits_from_Redis>\n  }\n}\n```\n\n*   If ID is not present in Redis:\n\n```\nstatusCode: 200\nResponse:\n{\n  \"entity\": <entity_type>,\n  \"id\": {\n    \"type\": <id_type>,\n    \"value\": <id_value>\n  },\n  \"data\": {}\n}\n```\n\n## Use case\n\nYou can use the Activation API for real-time personalization. Once you fetch the user traits from your Redis instance via the API, you can pull them into your client application to alter the application behavior in real-time based on user interactions.\n\nYou can respond immediately with triggered, user-focused messaging based on actions like page views or app clicks and provide a better customer experience.\n\n[![Real time personalization use case](https://www.rudderstack.com/docs/images/profiles/activation-api-use-case.webp)](https://www.rudderstack.com/docs/images/profiles/activation-api-use-case.webp)\n\n## Redis configuration\n\nThis section describes the settings required to set up a [Reverse ETL connection](https://www.rudderstack.com/docs/sources/reverse-etl/) from your warehouse (containing the project’s output table) to the [Redis destination](https://www.rudderstack.com/docs/destinations/streaming-destinations/redis/) in RudderStack.\n\n[![Redis connection settings](https://www.rudderstack.com/docs/images/profiles/redis-connection-settings.webp)](https://www.rudderstack.com/docs/images/profiles/redis-connection-settings.webp)\n\n*   **Address**: Enter the public endpoint of your Redis database. If you are using [Redis Cloud](https://app.redislabs.com/#/), you can find this endpoint by going to your Redis database and navigating to **Configuration** tab > **General**.\n\n[![Redis database public endpoint](https://www.rudderstack.com/docs/images/profiles/redis-public-endpoint.webp)](https://www.rudderstack.com/docs/images/profiles/redis-public-endpoint.webp)\n\n*   **Password**: Enter the database password. You can find the password in the **Security** section of the **Configuration** tab:\n\n[![Redis database password](https://www.rudderstack.com/docs/images/profiles/redis-database-password.webp)](https://www.rudderstack.com/docs/images/profiles/redis-database-password.webp)\n\n*   **Database**: Enter the database name.\n*   **Cluster Mode**: Disable this setting if you haven’t set up Redis in a cluster.\n\n## Data mapping\n\nRudderStack creates multiple Reverse ETL sources automatically based on your Profiles project. You will see separate sources for different `id_served` connected to the same Redis destination.\n\nThe following `pb_project.yaml` snippet shows the sources to be created. RudderStack syncs the customer 360 features to different tables in Redis indexed by `id_served`.\n\n```\nentities:\n  - name: user\n    serve_traits:\n      - id_served: user_id\n      - id_served: anonymous_id\n      - id_served: email\n      - id_served: cart_token\n      - id_served: rudder_id\n```\n\n## FAQ\n\n#### How do I generate a personal access token to use the Activation API?\n\n1.  Log in to your [RudderStack dashboard](https://app.rudderstack.com/).\n2.  Go to **Settings** > **Your Profile** > **Account** tab and scroll down to **Personal access tokens**. Then, click **Generate new token**:\n\n[![New personal access token in RudderStack dashboard](https://www.rudderstack.com/docs/images/rudderstack-api/personal-access-token-1.webp)](https://www.rudderstack.com/docs/images/rudderstack-api/personal-access-token-1.webp)\n\n3.  Enter the **Token name**. Set **Role** to **Admin** and click **Generate**.\n\n[![Personal access token name and role](https://www.rudderstack.com/docs/images/rudderstack-api/personal-access-token-2.webp)](https://www.rudderstack.com/docs/images/rudderstack-api/personal-access-token-2.webp)\n\n4.  Use the personal access token to authenticate to the Activation API.\n\n[![Personal access token details](https://www.rudderstack.com/docs/images/rudderstack-api/personal-access-token-3.webp)](https://www.rudderstack.com/docs/images/rudderstack-api/personal-access-token-3.webp)\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> Save the generated token securely as it will not be visible again once you close this window.\n\n#### Where can I find the Redis destination ID?\n\n1.  Set up a [Redis destination](https://www.rudderstack.com/docs/destinations/streaming-destinations/redis/) in RudderStack.\n2.  Go to the **Settings** tab of your destination to see the **Destination ID**:\n\n[![Redis destination ID](https://www.rudderstack.com/docs/images/rudderstack-api/redis-destination-id.webp)](https://www.rudderstack.com/docs/images/rudderstack-api/redis-destination-id.webp)\n\n#### How can I make Profiles work with the Activation API?\n\nTo use the Activation API with your Profiles project, you need a successful run of your Profiles project that is not past the retention period.\n\nTo enable the Activation API for your Profiles project, turn on the **Enable sync to Redis** setting. A Profile run will then sync automatically.\n\n[![Toggle API in Settings](https://www.rudderstack.com/docs/images/rudderstack-api/activation-api-toggle-settings.png)](https://www.rudderstack.com/docs/images/rudderstack-api/activation-api-toggle-settings.png)\n\n#### Why am I getting an error trying to enable API in my instance for a custom project hosted on GitHub?\n\nFor GitHub projects, you need to explicitly add the IDs of the custom project that need to be served.\n\nIn your `pb_project.yaml` file, you can specify them as shown:\n\n```\nentities:\n  - name: user\n    serve_traits:\n      - id_served: user_id\n      - id_served: anonymous_id\n      - id_served: email\n      - id_served: cart_token\n      - id_served: user_main_id\n```\n\n#### If I force a full resync, stop it, and then start a new sync, does RudderStack always perform a full sync the next time?\n\nIt depends on the state of the task when it was canceled.\n\n*   If the sync is cancelled while RudderStack is preparing a snapshot, then next run depends on the state of the previous successful run and any mapping changes.\n*   If it is cancelled after the sync data is prepared, the next run is incremental.\n\nGenerally if a sync is cancelled manually, it is recommended to trigger a full sync if the previous cancelled task was a full sync. If the previously cancelled sync was incremental, triggering an incremental sync is recommended.\n\n#### Does RudderStack perform a full sync if I add a new column?\n\nRudderStack does not change the sync mode if you make any column additions. It triggers a full sync only if you change/update the data mappings, for example, if the newly added column is sent to the destination via the [Visual Data Mapper](https://www.rudderstack.com/docs/sources/reverse-etl/visual-data-mapper/).\n\nFor Profiles activation syncs, RudderStack updates the mappings and automatically sends all columns from the customer 360 view by triggering a full sync.\n\n#### Suppose I’m running a full sync and the Profiles job is running in parallel and finishes eventually. What happens to the scheduled sync? Does it get queued?\n\nRudderStack first creates a temporary snapshot copy of any sync when it starts. So its syncing the created copy. Even if a Profiles job is running in parallel, the sync - if started - is not impacted by it.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Activation API (Early Access) | RudderStack Docs",
    "description": "Expose user profiles stored in your Redis instance over an API.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.11/predictions/predictive-features-snowflake/custom-code/",
    "markdown": "# Custom Predictive Features | RudderStack Docs\n\nCode your own custom predictive features\n\n* * *\n\n*     8 minute read  \n    \n\nWhile automated features are incredibly useful for quickly deploying activity-based churn scores, data teams inevitably want to go deeper and define custom predictions that match their unique business logic and KPIs.\n\nBasic customization is possible in the UI as we covered above, but Predictions also supports a code-based workflow that gives technical users full control and complete customizability, as well as the ability to integrate the process into their existing development workflow.\n\nFor example, if you are an eCommerce company, it can be helpful to predict whether or not a user will make a purchase over a certain dollar amount, over the next `n` days.\n\nRudderStack makes it easy to migrate from the UI-based workflow to the code-based workflow to build these more complex use cases.\n\n## Download project files\n\nOn the Profiles screen, find your project and click the **Download this Project** button on the top right side. This will download all the files for that Profiles project in a compressed (zip) file including the modeling files.\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/download-project.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/download-project.webp)\n\nInside the Profiles folder you will find `pb_project.yaml` and a `models` folder with `resources.yaml`.\n\n```\nRudderStack QuickStart\n├── pb_project.yaml\n├── models\n│   ├── resources.yaml\n```\n\n### pb\\_project.yaml\n\n`pb_project.yaml` is the main configuration file for the Profiles project. The top section defines the `name`, `schema_version`, `connection`, and `model_folder` (where the files that define the details of the Profiles project can be found).\n\nUpdate the following values:\n\n*   `name` to `Profile-Quickstart` to match the name in the UI.\n*   `connection` to `QUICKSTART` to match the database connection we made in the Prerequisites section.\n\n```\nname: Profile-QuickStart\nschema_version: 49 # Or most recent version\nconnection: QUICKSTART\nmodel_folders:\n    - models\n```\n\nBelow there is an `entities` section that defines the entities and the kinds of ID’s make up that entity. An entity is a business concept or unit that will be used to build the identity graph and features. Projects can contain multiple entities like user, household, and organization.\n\nFor this project, there is one entity called `user` with 5 different types of IDs. An ID type maps which ID fields can be joined together. For example, if you have two tables with `user_id` columns called `id` and `userid`, by giving each the type `user_id` Profiles knows to join those tables on those columns.\n\nThe ID fields are already mapped to these types in the UI. Nothing needs to be updated in this section.\n\n```\nentities:\n    - name: user\n      serve_traits:\n        - id_served: user_id\n        - id_served: anonymous_id\n        - id_served: email\n        - id_served: cart_token\n        - id_served: user_main_id\n```\n\nFinally there is a `packages` section. This section allows you to import a Profiles project from GitHub and use the feature definitions from that project in this one. The imported project provides the definitions for the standard features selected in the UI. Nothing needs to be updated in this section.\n\n```\npackages:\n    - name: base_features\n      url: https://github.com/rudderlabs/rudderstack-profiles-multieventstream-features\n      inputsMap: # These are the tables automatically mapping in the UI (TRACKS, PAGES, IDENTIFIES)\n        rsIdentifies_1: inputs/rsIdentifies_1\n        rsIdentifies_2: nil\n        rsIdentifies_3: nil\n        rsPages_1: inputs/rsPages_1\n        rsPages_2: nil\n        rsPages_3: nil\n        rsTracks_1: inputs/rsTracks_1\n        rsTracks_2: nil\n        rsTracks_3: nil\n      overrides: # By default all features are imported from the project, therefore the features we did not select need to be disabled\n        - requested_enable_status: disabled\n          models:\n            - entity/user/active_days_in_past_365_days\n            - entity/user/active_days_in_past_7_days\n            - entity/user/avg_session_length_in_sec_365_days\n            - entity/user/avg_session_length_in_sec_last_week\n            - entity/user/avg_session_length_in_sec_overall\n            - entity/user/campaign_sources\n            - entity/user/campaigns_list\n            - models/churn_7_days_model\n            - models/churn_90_days_model\n            - entity/user/country\n            - entity/user/currency\n            - entity/user/days_since_account_creation\n            - entity/user/days_since_last_seen\n            - entity/user/first_campaign_name\n            - entity/user/is_churned_7_days\n            - entity/user/last_campaign_name\n            - entity/user/last_source_name\n            - entity/user/max_timestamp_bw_tracks_pages\n            - entity/user/mediums_list\n            - entity/user/sources_list\n            - entity/user/total_sessions_365_days\n            - entity/user/total_sessions_till_date\n```\n\n### resource.yaml\n\n`resources.yaml` contains two main sections: `inputs` and `var_groups`.\n\nThe `inputs` section defines what ID’s are in each table and their mapping. Currently these are all the tables and mappings that were defined in the UI. These tables are used for creating an identity graph and all features related to it.\n\nIf you want to add another table in the future, the table and ID mappings would be added here. Below is an example of the `ORDER_COMPLETED` table we manually mapped in the UI. It consists of the following fields:\n\n| Field | Description |\n| --- | --- |\n| name | alias for the table; the primary reference in the rest of the yaml files |\n| table | `<SCHEMA>.<TABLE_NAME>` |\n| select | column with ID |\n| type | kind of ID |\n| entity | what entity the ID should be mapped to |\n| to\\_default\\_stitcher | `true` unless you decide to use a different ID stitcher |\n| remapping | leave as `null` |\n\n```\n- name: rs_EVENTS_ORDER_COMPLETED\n  app_defaults:\n    table: EVENTS.ORDER_COMPLETED\n    ids:\n        - select: USER_ID\n          type: user_id\n          entity: user\n          to_default_stitcher: true\n        - select: ANONYMOUS_ID\n          type: anonymous_id\n          entity: user\n          to_default_stitcher: true\n  remapping: null\n```\n\nThe `var_groups` section is where custom features are defined, both custom features created in the UI and those added via code in this file. Custom features are organized into groups by entity (in our case only `user`). The entity is like the `group by` variable in a SQL query.\n\nBelow that custom features are defined in the `vars` subsection. Here is the `average_order` feature we created in the UI.\n\n```\n- entity_var:\n    is_feature: true\n    name: average_order\n    description: Average Order Size including shipping, taxes, and discounts\n    select: AVG(TOTAL)\n    from: inputs/rs_EVENTS_ORDER_COMPLETED\n```\n\nA name and description are required for the custom feature and then it is defined using declarative SQL syntax. This allows you to define the custom feature the same way you would if creating a new table with SQL.\n\n## Create a custom predictive feature\n\nJust like in the UI workflow, you must already have defined the feature you want to predict. Therefore we are going to add a new custom feature for large purchases in the last 90 days. **NOTE: Currently predictive features can only be binary (i.e. 1/0)**\n\nA large order is defined here as any order with a `TOTAL` of > $100.\n\nAt the bottom of the `resources.yaml`, add the name and definition for `large_purchase_last_90`.\n\n```\n- entity_var:\n  name: large_purchase_last_90\n  description: Customer that made a purchase of >$100 in the last 90 days.\n  select: CASE WHEN MAX(TOTAL) > 100 THEN 1 ELSE 0 END\n  from: inputs/re_EVENTS_ORDER_COMPLETED\n  where: DATEDIFF(days, TIMESTAMP, CURRENT_DATE) <= 90\n```\n\nYou can use SQL functions and keywords in the definition. FOr example, a `CASE` statement in the SELECT statement and add a `where` statement and use the `DATEDIFF` function. You can also use the alias for the `ORDER_COMPLETED` table in the `from` statement.\n\nFor more details on Profiles and project file structure, you can review the Profiles [documentation](https://www.rudderstack.com/docs/profiles/overview/).\n\n## Organize the project in two files (**OPTIONAL**)\n\nProfiles does not need specific yaml files in the `models` folder in order to run. That allows you to organize your code as you feel is best. You can keep it all in one file or can split it over multiple files.\n\nYou can split the `resources.yaml` file into `inputs.yaml` and `profiles.yaml` by creating the two yaml files. Then copy everything from the `inputs` section into `inputs.yaml` and `var_groups` into `profiles.yaml`.\n\nOnce done, you can delete the `resources.yaml`.\n\n## Add a custom predictive feature\n\nThis section explains how to create 2 new custom predictive features from `large_purchase_last_90` called `likelihood_large_purchase_90` (raw score) and `percentile_large_purchase_90`(percentile score).\n\n#### Add Python ML requirement\n\nIn order to add custom predictive features, add the `profiles-pycorelib` package to the project requirements. At the bottom of `pb_project.yaml` add the following code to `pb_project.yaml`.\n\n```\npython_requirements:\n  - profiles-pycorelib==0.2.1\n```\n\n#### Create ml\\_models.yaml\n\nNow, create a new file and name it `ml_models.yaml`. This file is where you can define 2 new custom predictive features and how to train the ML model. The code for these new predictive features is discussed below.\n\nThis file is organized by the predictive model created for predictive features, not the individual features. The top level consists of:\n\n| Field/Section | Description |\n| --- | --- |\n| `name` | Name of the model (not feature) |\n| `model_type` | `python_model` |\n| `model_spec` | All of the model specifications |\n\n* * *\n\n`model_spec` section:\n\n| Section | Description |\n| --- | --- |\n| `train` | Training configuration |\n| `predict` | Scoring configuration |\n\n```\nmodels:\n    - name: &model_name large_purchase_90_model\n      model_type: python_model\n      model_spec:\n        occurred_at_col: insert_ts\n        entity_key: user\n        validity_time: 24h\n        py_repo_url: git@github.com:rudderlabs/rudderstack-profiles-classifier.git # Model training and scoring repo\n\n        train:\n          file_extension: .json\n          file_validity: 2160h # 90 days; when the model will be retrained\n          inputs: &inputs\n            - packages/base_features/models/rudder_user_base_features # location of the base features created in the UI\n            - packages/large_purchase_last_90 # custom feature created in var_groups\n            - models/average_order # custom feature we created in the UI\n          config:\n            data: &model_data_config\n              package_name: feature_table\n              label_column: large_purchase_last_90 # target feature\n              label_value: 1 # target feature value predicting\n              prediction_horizon_days: 90 # how far into the future\n              features_profiles_model:  'rudder_user_base_features' #taken from inputs\n              output_profiles_ml_model: *model_name\n              eligible_users: 'large_purchase_last_90 is not null' # limit training data to those with non-null values\n              inputs: *inputs\n            preprocessing: &model_prep_config\n              ignore_features: [first_name, last_name, state] # features we do not used in a model\n\n        predict:\n          inputs: *inputs # copy from train\n          config:\n            data: *model_data_config # copy from train\n            preprocessing: *model_prep_config # copy from train\n            outputs:\n              column_names:\n                percentile: &percentile percentile_large_purchase_90 # name of percentile feature\n                score: &raw_score likelihood_large_purchase_90 # name of raw likelihood feature\n              feature_meta_data: &feature_meta_data\n                features:\n                  - name: *percentile\n                    description: 'Percentile of likelihood score. Higher the score the more likely to make a larger purchase'\n                  - name: *raw_score\n                    description: 'Raw likelihood score. Higher the score the more likely to make a larger purchase'\n\n        <<: *feature_meta_data\n```\n\n## Compile and run\n\nSave all files. Now compile the project, this will make sure all SQL and python files are able to be created.\n\nFinally, run the project. This will generate the same files as `compile` and then execute them in Snowflake. The first run can take at least 30 minutes because of training ML models.\n\n## Final table\n\nThe final predictive features can be found in your Snowflake environment together in the same table. The table will provide you with the unified user ID, created by RudderStack, when the features are valid as of (i.e. when the model was last run to create these features), and model ID, and your predictive features.\n\n[![Predictive features in Snowflake](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/final-table.webp)](https://www.rudderstack.com/docs/images/profiles/predictive-features-snowflake/final-table.webp)\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Custom Predictive Features | RudderStack Docs",
    "description": "Code your own custom predictive features",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.11/glossary/",
    "markdown": "# Glossary | RudderStack Docs\n\nFamiliarize yourself with the commonly used terms across RudderStack Profiles.\n\n* * *\n\n*     6 minute read  \n    \n\n## Custom Models (Python)\n\nOne can build custom Python models for ML by downloading pre-defined Python templates. The results are usually saved as attributes of related entities (for example, `churnProbability`).\n\n## Edge sources\n\nThe `edge_sources` field provides the input sources for an identity stitching model. You can specify it in the `models/profiles.yaml` file to list the input sources defined in the `inputs.yaml` file.\n\n## Entity\n\nEntity refers to an object for which you can create a profile. An entity can be a user, customer, product, or any other object that requires profiling.\n\n## Entity var\n\nThese are various attributes related to the entity whose profile you are trying to create (`name`, `city`, `LastVisitTimestamp`, etc). Together, all the attributes can create a complete picture of the entity. Attribute is a single value derived by performing calculation or aggregation on a set of values. By default, every entity var gets stored as a feature, such as `days_active`, `last_seen`, etc.\n\n## Entity Traits 360\n\nIf the features/traits of an entity are spread across multiple entity vars and ML models, you can use Entity Traits 360 to get them together into a single view. These models are usually defined in `pb_project.yaml` file by creating entries under `serve_traits` key with corresponding entity.\n\n## Features\n\nFeatures are inputs for the machine learning model. In a general sense, features are pieces of user information we already know. For example, number of days they opened the app in the past week, items they left in the cart, etc.\n\n## Feature tables\n\nFeature tables are the outputs based on events, user attributes, and other defined criteria across any data set in your data warehouse. You can define models that can create feature tables reflecting a complete 360 data for users with ID stitching, ML notebooks and external sources, etc.\n\n## ID Stitcher\n\nData usually comes from different sources and these sources may assign different IDs. To track a user’s journey (or any other entity) uniquely across all these data sources, we need to stitch together all these IDs. ID stitching helps map different IDs of the same user (or any other entity) to a single canonical ID. It does this by doing connected component analysis over the Id-Id edge graph specified in its configuration.\n\n## ID Collator\n\nID Collator is similar to ID Stitcher. It is used when entity has only a single ID type associated (for example, session IDs). In these cases, connected component analysis is not required and we use a simpler model type called ID Collator. It consolidates all entity IDs from multiple input tables into a single collated list.\n\n## Inputs\n\nInputs refers to the input data sources used to create the material (output) tables in the warehouse. The inputs file (`models/inputs.yaml`) lists the tables/views you use as input sources, including the column name and SQL expression for retrieving the values.\n\nYou can use data from various input sources such as event stream (loaded from event data), ETL extract (loaded from Cloud Extract), and any existing tables in the warehouse (generated by external tools).\n\n## Input var\n\nInstead of a single value per entity ID, it represents a single value per row of an input model. Think of it as representing addition of an additional column to an input model. It can be used to define entity features. However, it is not itself an entity feature because it does not represent a single value per entity ID.\n\n## Label\n\nLabel is the output of the machine learning model and is the metric we want to predict. In our case, it is the unknown user trait we want to know in advance.\n\n## Machine learning model\n\nA machine learning model can be thought of as a function that takes in some input parameters and returns an output.\n\nUnlike regular programming, this function is not explicitly defined. Instead, a high level architecture is defined and several pieces are filled by looking at the data. This whole process of filling the gaps is driven by different optimisation algorithms as they try to learn complex patterns in the input data that explain the output.\n\n## Materialization\n\nMaterialization refers to the process of creating output tables/views in a warehouse using PB models. You can define the `run_type` to create the output table:\n\n*   `run_type`: Possible values are:\n    *   `discrete`: Calculates the model result from it’s inputs whenever run (default mode). A SQL model supports only `discrete` run type.\n    *   `incremental`: The model reads updates from input sources and results from the previous run. It only updates or inserts data and does not delete anything. The `incremental` mode is more efficient. However, only identity stitching model supports it.\n\n## Material tables\n\nWhen you run the PB models, they produce materials - that is, tables/views in the database that contain the results of that model run. These output tables are known as material tables.\n\n## Predictions\n\nThe model’s output is called a prediction. A good model makes predictions that are close to the actual label. You generally need predictions where the labels are not available. In our case, most often the labels come a few days later.\n\n## Prediction horizon days\n\nThis refer to the number of days in advance when we make the prediction.\n\nFor example, statements like “A user is likely to sign-up in the next 30 days, 90 days, etc.” are often time-bound, that is, the predictions are meaningless without the time period.\n\n## Profile Builder (PB)\n\nProfile Builder (PB) is a command-line interface (CLI) tool that simplifies data transformation within your warehouse. It generates customer profiles by stitching data together from multiple sources. It takes existing tables or the output of other transformations as input to generate output tables or views based on your specifications.\n\n## PB project\n\nA PB project is a collection of interdependent warehouse transformations. These transformations are run over the warehouse to query the data for sample outputs, discover features in the warehouse, and more.\n\n## PB model\n\nAny transformation that can be applied to the warehouse data is called a PB model. RudderStack supports various types of models like ID stitching, feature tables, Python models, etc.\n\n## Schema versions\n\nEvery PB release supports a specific set of project schemas. A project schema determines the correct layout of a PB project, including the exact keys and their values in all project files.\n\n## SQL template models\n\nSometimes the standard model types provided by Profiles are insufficient to capture complex use cases. In such cases, RudderStack supports the use of SQL template models to explicitly templatize SQL.\n\nSQL template models can be used as an input to an entity-var/ input-var or as an edge-source in id-stitcher.\n\n## Training\n\nTraining refers to the process of a machine learning model looking at the available data and trying to learn a function that explains the [labels](#label).\n\nOnce you train a model on historic users, you can use it to make predictions for new users. You need to keep retraining the model as you get new data so that the model continues to learn any emerging patterns in the new users.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Glossary | RudderStack Docs",
    "description": "Familiarize yourself with the commonly used terms across RudderStack Profiles.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.10/developer-guides/",
    "markdown": "# Developer Guides | RudderStack Docs\n\nPrivacy Overview\n\nThis site uses cookies to improve your experience while you navigate through the website. Out of these cookies, the cookies that are categorized as necessary are stored on your browser as they are as essential for the working of basic functionalities of the website. We also use third-party cookies that help us analyze and understand how you use this website. These cookies will be stored in your browser only with your consent. You also have the option to opt-out of these cookies. But opting out of some of these cookies may have an effect on your browsing experience.\n\nNecessary cookies are absolutely essential for the website to function properly. This category only includes cookies that ensures basic functionalities and security features of the website. These cookies do not store any personal information.",
    "title": "Developer Guides | RudderStack Docs",
    "description": "Learn about the Profiles project structure, warehouse permissions, site configuration file, and more.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.11/developer-guides/",
    "markdown": "# Developer Guides | RudderStack Docs\n\nPrivacy Overview\n\nThis site uses cookies to improve your experience while you navigate through the website. Out of these cookies, the cookies that are categorized as necessary are stored on your browser as they are as essential for the working of basic functionalities of the website. We also use third-party cookies that help us analyze and understand how you use this website. These cookies will be stored in your browser only with your consent. You also have the option to opt-out of these cookies. But opting out of some of these cookies may have an effect on your browsing experience.\n\nNecessary cookies are absolutely essential for the website to function properly. This category only includes cookies that ensures basic functionalities and security features of the website. These cookies do not store any personal information.",
    "title": "Developer Guides | RudderStack Docs",
    "description": "Learn about the Profiles project structure, warehouse permissions, site configuration file, and more.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.10/glossary/",
    "markdown": "# Glossary | RudderStack Docs\n\nFamiliarize yourself with the commonly used terms across RudderStack Profiles.\n\n* * *\n\n*     6 minute read  \n    \n\n## Custom Models (Python)\n\nOne can build custom Python models for ML by downloading pre-defined Python templates. The results are usually saved as attributes of related entities (for example, `churnProbability`).\n\n## Edge sources\n\nThe `edge_sources` field provides the input sources for an identity stitching model. You can specify it in the `models/profiles.yaml` file to list the input sources defined in the `inputs.yaml` file.\n\n## Entity\n\nEntity refers to an object for which you can create a profile. An entity can be a user, customer, product, or any other object that requires profiling.\n\n## Entity var\n\nThese are various attributes related to the entity whose profile you are trying to create (`name`, `city`, `LastVisitTimestamp`, etc). Together, all the attributes can create a complete picture of the entity. Attribute is a single value derived by performing calculation or aggregation on a set of values. By default, every entity var gets stored as a feature, such as `days_active`, `last_seen`, etc.\n\n## Entity Traits 360\n\nIf the features/traits of an entity are spread across multiple entity vars and ML models, you can use Entity Traits 360 to get them together into a single view. These models are usually defined in `pb_project.yaml` file by creating entries under `serve_traits` key with corresponding entity.\n\n## Features\n\nFeatures are inputs for the machine learning model. In a general sense, features are pieces of user information we already know. For example, number of days they opened the app in the past week, items they left in the cart, etc.\n\n## ID Stitcher\n\nData usually comes from different sources and these sources may assign different IDs. To track a user’s journey (or any other entity) uniquely across all these data sources, we need to stitch together all these IDs. ID stitching helps map different IDs of the same user (or any other entity) to a single canonical ID. It does this by doing connected component analysis over the Id-Id edge graph specified in its configuration.\n\n## ID Collator\n\nID Collator is similar to ID Stitcher. It is used when entity has only a single ID type associated (for example, session IDs). In these cases, connected component analysis is not required and we use a simpler model type called ID Collator. It consolidates all entity IDs from multiple input tables into a single collated list.\n\n## Inputs\n\nInputs refers to the input data sources used to create the material (output) tables in the warehouse. The inputs file (`models/inputs.yaml`) lists the tables/views you use as input sources, including the column name and SQL expression for retrieving the values.\n\nYou can use data from various input sources such as event stream (loaded from event data), ETL extract (loaded from Cloud Extract), and any existing tables in the warehouse (generated by external tools).\n\n## Input var\n\nInstead of a single value per entity ID, it represents a single value per row of an input model. Think of it as representing addition of an additional column to an input model. It can be used to define entity features. However, it is not itself an entity feature because it does not represent a single value per entity ID.\n\n## Label\n\nLabel is the output of the machine learning model and is the metric we want to predict. In our case, it is the unknown user trait we want to know in advance.\n\n## Machine learning model\n\nA machine learning model can be thought of as a function that takes in some input parameters and returns an output.\n\nUnlike regular programming, this function is not explicitly defined. Instead, a high level architecture is defined and several pieces are filled by looking at the data. This whole process of filling the gaps is driven by different optimisation algorithms as they try to learn complex patterns in the input data that explain the output.\n\n## Materialization\n\nMaterialization refers to the process of creating output tables/views in a warehouse using PB models. You can define the `run_type` to create the output table:\n\n*   `run_type`: Possible values are:\n    *   `discrete`: Calculates the model result from it’s inputs whenever run (default mode). A SQL model supports only `discrete` run type.\n    *   `incremental`: The model reads updates from input sources and results from the previous run. It only updates or inserts data and does not delete anything. The `incremental` mode is more efficient. However, only identity stitching model supports it.\n\n## Material tables\n\nWhen you run the PB models, they produce materials - that is, tables/views in the database that contain the results of that model run. These output tables are known as material tables.\n\n## Predictions\n\nThe model’s output is called a prediction. A good model makes predictions that are close to the actual label. You generally need predictions where the labels are not available. In our case, most often the labels come a few days later.\n\n## Prediction horizon days\n\nThis refer to the number of days in advance when we make the prediction.\n\nFor example, statements like “A user is likely to sign-up in the next 30 days, 90 days, etc.” are often time-bound, that is, the predictions are meaningless without the time period.\n\n## Profile Builder (PB)\n\nProfile Builder (PB) is a command-line interface (CLI) tool that simplifies data transformation within your warehouse. It generates customer profiles by stitching data together from multiple sources. It takes existing tables or the output of other transformations as input to generate output tables or views based on your specifications.\n\n## PB project\n\nA PB project is a collection of interdependent warehouse transformations. These transformations are run over the warehouse to query the data for sample outputs, discover features in the warehouse, and more.\n\n## PB model\n\nEach transformation that can be applied to the warehouse data is called a PB model.\n\n## Schema versions\n\nEvery PB release supports a specific set of project schemas. A project schema determines the correct layout of a PB project, including the exact keys and their values in all project files.\n\n## SQL template models\n\nSometimes the standard model types provided by Profiles are insufficient to capture complex use cases. In such cases, RudderStack supports the use of SQL template models to explicitly templatize SQL.\n\nSQL template models can be used as an input to an entity-var/ input-var or as an edge-source in id-stitcher.\n\n## Training\n\nTraining refers to the process of a machine learning model looking at the available data and trying to learn a function that explains the [labels](#label).\n\nOnce you train a model on historic users, you can use it to make predictions for new users. You need to keep retraining the model as you get new data so that the model continues to learn any emerging patterns in the new users.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Glossary | RudderStack Docs",
    "description": "Familiarize yourself with the commonly used terms across RudderStack Profiles.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.10/developer-guides/permissions/",
    "markdown": "# Warehouse Permissions | RudderStack Docs\n\nGrant RudderStack the required permissions on your data warehouse.\n\n* * *\n\n*     4 minute read  \n    \n\nRudderStack supports **Snowflake**, **Redshift**, and **Databricks** for creating unified user profiles.\n\nTo read and write data to the warehouse, RudderStack requires specific warehouse permissions as explained in the following sections.\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> Keeping separate schemas for projects running via CLI and web is recommended. This way projects run from the CLI will never risk overwriting your production data.\n\n## Snowflake\n\nSnowflake uses a combination of DAC and RBAC models for [access control](https://docs.snowflake.com/en/user-guide/security-access-control-overview.html). However, RudderStack chooses an RBAC-based access control mechanism as multiple users can launch the [Profile Builder CLI](https://www.rudderstack.com/docs/archive/profiles/0.10/get-started/profile-builder/).\n\nAlso, it is not ideal to tie the result of an individual user run with that user. Hence, it is recommended to create a generic role (for example, `PROFILES_ROLE`) with the following privileges:\n\n*   Read access to all the inputs to the model (can be shared in case of multiple schemas/tables).\n*   Write access to the schemas and common tables as the PB project creates material (output) tables.\n\nIf you want to access any material created from the project run, the role (`PROFILES_ROLE`) must also have read access to all of those schemas.\n\nBelow are some sample commands which grant the required privileges to the role (`PROFILES_ROLE`) in a Snowflake warehouse:\n\n```\n-- Create role\nCREATE ROLE PROFILES_ROLE;\nSHOW ROLES; -- To validate\n```\n\n```\n-- Create user\nCREATE USER PROFILES_TEST_USER PASSWORD='<StrongPassword>' DEFAULT_ROLE='PROFILES_ROLE';\nSHOW USERS; -- To validate\n```\n\n```\n-- Grant role to user and database\nGRANT ROLE PROFILES_ROLE TO USER PROFILES_TEST_USER;\nGRANT USAGE ON DATABASE YOUR_RUDDERSTACK_DB TO ROLE PROFILES_ROLE;\n```\n\n```\n-- Create separate schema for Profiles and grant privileges to role\nCREATE SCHEMA YOUR_RUDDERSTACK_DB.RS_PROFILES;\nGRANT ALL PRIVILEGES ON SCHEMA YOUR_RUDDERSTACK_DB.RS_PROFILES TO ROLE PROFILES_ROLE;\nGRANT USAGE ON WAREHOUSE RUDDER_WAREHOUSE TO ROLE PROFILES_ROLE;\nGRANT USAGE ON SCHEMA YOUR_RUDDERSTACK_DB.EVENTSSCHEMA TO ROLE PROFILES_ROLE;\nGRANT SELECT ON ALL TABLES IN SCHEMA YOUR_RUDDERSTACK_DB.RS_PROFILES TO PROFILES_ROLE;\nGRANT SELECT ON FUTURE TABLES IN SCHEMA YOUR_RUDDERSTACK_DB.RS_PROFILES TO PROFILES_ROLE;\nGRANT SELECT ON ALL VIEWS IN SCHEMA YOUR_RUDDERSTACK_DB.RS_PROFILES TO PROFILES_ROLE;\nGRANT SELECT ON FUTURE VIEWS IN SCHEMA YOUR_RUDDERSTACK_DB.RS_PROFILES TO PROFILES_ROLE;\n```\n\nFor accessing input sources, you can individually grant select on tables/views, or give blanket grant to all in a schema.\n\n```\n-- Assuming we want read access to tables/views in schema EVENTSSCHEMA\nGRANT SELECT ON ALL TABLES IN SCHEMA YOUR_RUDDERSTACK_DB.EVENTSSCHEMA TO PROFILES_ROLE;\nGRANT SELECT ON FUTURE TABLES IN SCHEMA YOUR_RUDDERSTACK_DB.EVENTSSCHEMA TO PROFILES_ROLE;\nGRANT SELECT ON ALL VIEWS IN SCHEMA YOUR_RUDDERSTACK_DB.EVENTSSCHEMA TO PROFILES_ROLE;\nGRANT SELECT ON FUTURE VIEWS IN SCHEMA YOUR_RUDDERSTACK_DB.EVENTSSCHEMA TO PROFILES_ROLE;\n```\n\n```\n-- Assuming we want read access to tracks and identifies tables in schema EVENTSSCHEMA\nGRANT SELECT ON TABLE YOUR_RUDDERSTACK_DB.EVENTSSCHEMA.TRACKS TO PROFILES_ROLE;\nGRANT SELECT ON TABLE YOUR_RUDDERSTACK_DB.EVENTSSCHEMA.IDENTIFIES TO PROFILES_ROLE;\n```\n\n## Redshift\n\nWhen working with Redshift, the required privileges are different compared to Snowflake.\n\nSuppose the inputs/edge sources are in a single schema `website_eventstream` and the name of the newly created PB user is `rudderstack_admin`. In this case, the requirements are as follows:\n\n*   A separate schema `rs_profiles` (to store all the common and output tables).\n*   The `rudderstack_admin` user should have all the privileges on the above schema and the associated tables.\n*   The `rudderstack_admin` user should have `USAGE` privilege on schemas that have the edge sources and input tables (`website_eventstream`) and read (`SELECT`) privileges on specific tables as well. This privilege can extend to the migration schema and other schemas from where data from warehouses comes in.\n*   The `rudderstack_admin` user should have privileges to use `plpythonu` to create some UDFs.\n\nThe sample commands are as follows:\n\n```\nCREATE USER rudderstack_admin WITH PASSWORD '<strong_unique_password>';\nCREATE SCHEMA rs_profiles;\nGRANT ALL ON SCHEMA \"rs_profiles\" TO rudderstack_admin;\nGRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA \"rs_profiles\" TO rudderstack_admin;\nGRANT USAGE ON SCHEMA \"website_eventstream\" TO rudderstack_admin;\nGRANT USAGE ON LANGUAGE plpythonu TO rudderstack_admin;\n```\n\n```\nGRANT SELECT ON ALL TABLES IN SCHEMA \"website_eventstream\" TO rudderstack_admin;\n```\n\nTo give access to only specific input tables/views referred in your Profiles project, use the below command:\n\n```\nGRANT SELECT ON TABLE \"<YOUR_SCHEMA>\".\"<YOUR_TABLE>\" TO rudderstack_admin;\n```\n\n## Databricks\n\nThe steps are as follows:\n\n1.  Open the Databricks UI.\n2.  Create a new user.\n3.  Reuse an existing catalog or create a new one by clicking **Create Catalog**.\n4.  Grant `USE SCHEMA` privilege on the catalog.\n5.  Create a separate schema to write objects created by RudderStack Profiles.\n6.  Grant all privileges on this schema.\n7.  Grant privileges to access relevant schemas for the input tables. For example, if an input schema is in a schema named `website_eventstream`, then you can run the following commands to assign a blanket grant to all schemas or only specific tables/views referred in your Profiles project:\n\n```\nCREATE USER rudderstack_admin WITH PASSWORD <strong_unique_password>;\nGRANT USE SCHEMA ON CATALOG <catalog name> TO rudderstack_admin;\nCREATE SCHEMA RS_PROFILES;\nGRANT ALL PRIVILEGES ON SCHEMA RS_PROFILES TO rudderstack_admin;\nGRANT SELECT ON SCHEMA website_eventstream TO rudderstack_admin;\n```\n\n```\nGRANT SELECT ON ALL TABLES IN SCHEMA \"website_eventstream\" TO rudderstack_admin;\n```\n\nTo give access to only specific input tables/views referred in your Profiles project, use the below command:\n\n```\nGRANT SELECT ON TABLE public.input_table TO rudderstack_admin; \n```\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Warehouse Permissions | RudderStack Docs",
    "description": "Grant RudderStack the required permissions on your data warehouse.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.10/developer-guides/site-configuration/",
    "markdown": "# Site Configuration | RudderStack Docs\n\nPrivacy Overview\n\nThis site uses cookies to improve your experience while you navigate through the website. Out of these cookies, the cookies that are categorized as necessary are stored on your browser as they are as essential for the working of basic functionalities of the website. We also use third-party cookies that help us analyze and understand how you use this website. These cookies will be stored in your browser only with your consent. You also have the option to opt-out of these cookies. But opting out of some of these cookies may have an effect on your browsing experience.\n\nNecessary cookies are absolutely essential for the website to function properly. This category only includes cookies that ensures basic functionalities and security features of the website. These cookies do not store any personal information.",
    "title": "Site Configuration | RudderStack Docs",
    "description": "Know the detailed specifications mentioned in a site configuration file.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.10/developer-guides/structure/",
    "markdown": "# Project Structure | RudderStack Docs\n\nKnow the detailed PB project structure, configuration files and their parameters.\n\n* * *\n\n*     6 minute read  \n    \n\nOnce you complete the [Profile Builder CLI](https://www.rudderstack.com/docs/archive/profiles/0.10/get-started/profile-builder/) steps, you will be able to see the Profile Builder project on your machine.\n\nThis guide explains the configuration files structure along with their fields and description:\n\n[![Project structure](https://www.rudderstack.com/docs/images/profiles/project-structure.webp)](https://www.rudderstack.com/docs/images/profiles/project-structure.webp)\n\n## `pb_project.yaml`\n\nThe `pb_project.yaml` file contains the project details like the name, schema version, warehouse connection, [entityEntity refers to a digital representation of a class of real world distinct objects for which you can create a profile.](https://www.rudderstack.com/docs/resources/glossary/#entity) names along with ID types, etc.\n\nA sample `pb_project.yaml` file with entity type as `user`:\n\n```\nname: sample_attribution\nschema_version: 49\nconnection: test\ninclude_untimed: true\nmodel_folders:\n  - models\nentities:\n  - name: user\n    id_types:\n      - main_id\n      - user_id\n      - anonymous_id\n      - email\npackages:\n  - name: corelib\n    url: \"https://github.com/rudderlabs/rudderstack-profiles-corelib/tag/schema_{{best_schema_version}}\"\n\n# Profiles can also use certain model types defined in Python.\n# Examples include ML models. Those dependencies are specified here.\npython_requirements:\n  - profiles-pycorelib==0.1.0\n```\n\nThe following table explains the fields used in the above file:\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `name` | String | Name of the project. |\n| `schema_version` | Integer | Project’s YAML version. Each new schema version comes with improvements and added functionalities. |\n| `connection` | String | Connection name from [`siteconfig.yaml`](https://www.rudderstack.com/docs/archive/profiles/0.10/developer-guides/site-configuration/) used for connecting to the warehouse. |\n| `include_untimed` | Boolean | Determines if inputs having no timestamps should be allowed. If true, data without timestamps is included when running the models. |\n| `model_folders` | String | Names of folders where model files are stored. |\n| [`entities`](#entities) | List | Lists all the entities used in the project for which you can define models. Each entry for an entity here is a JSON object specifying entity’s name and attributes. |\n| `packages` | List | List of packages with their name and URL. Optionally, you can also extend ID types filters for including or excluding certain values from this list. |\n\n##### `entities`\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `name` | String | Name of the entity used in the project. |\n| [`id_types`](#id_types) | List | List of all identifier types associated with the current entity. |\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> The identifiers listed in `id_types` may have a many-to-one relationship with an entity but each ID must belong to a single entity.\n> \n> For example, a `user` entity might have `id_types` as the `salesforce_id`, `anonymous_id`, `email`, and `session_id` (a user may have many session IDs over time). However, it should not include something like `ip_address`, as a single IP can be used by different users at different times and it is not considered as a user identifier.\n\n##### `packages`\n\nYou can import library packages in a project signifying where the project inherits its properties from.\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `name` | String | Specify a name. |\n| `url` | String | HTTPS URL of the lib package, with a tag for the best schema version. |\n\n## `inputs.yaml`\n\nThe `inputs.yaml` file lists all the input sources (tables/views) which should be used to obtain values for [models](#models) and eventually create output tables.\n\nIt also specifies the table/view along with column name and SQL expression for retrieving values. The input specification may also include metadata, and the constraints on those columns.\n\nA sample `inputs.yaml` file:\n\n```\ninputs:\n  - name: salesforceTasks\n    contract:\n      is_optional: false\n      is_event_stream: true\n      with_entity_ids:\n        - user\n      with_columns:\n        - name: activitydate\n        - name: whoid\n    app_defaults:\n      table: salesforce.task\n      occurred_at_col: activitydate\n      ids:\n        # column name or sql expression\n        - select: \"whoid\" \n          type: salesforce_id\n          entity: user\n          to_default_stitcher: true\n  - name: salesforceContact\n    contract:\n      is_optional: false\n      is_event_stream: true\n      with_entity_ids:\n        - user\n      with_columns:\n        - name: createddate\n        - name: id\n        - name: email\n    app_defaults:\n      table: salesforce.contact\n      occurred_at_col: createddate\n      ids:\n        - select: \"id\"\n          type: salesforce_id\n          entity: user\n          to_default_stitcher: true\n        - select: \"case when lower(email) like any ('%gmail%', '%yahoo%') then lower(email)  else split_part(lower(email),'@',2) end\"\n          type: email\n          entity: user\n          to_default_stitcher: true\n  - name: websitePageVisits\n    contract:\n      is_optional: false\n      is_event_stream: true\n      with_entity_ids:\n        - user\n      with_columns:\n        - name: timestamp\n        - name: anonymous_id\n        - name: context_traits_email\n        - name: user_id\n    app_defaults:\n      table: autotrack.pages\n      occurred_at_col: timestamp\n      ids:\n        - select: \"anonymous_id\"\n          type: rudder_anon_id\n          entity: user\n          to_default_stitcher: true\n        # below sql expression check the email type, if it is gmail and yahoo return email otherwise spilt email return domain of email.  \n        - select: \"case when lower(coalesce(context_traits_email, user_id)) like any ('%gmail%', '%yahoo%') then lower(coalesce(context_traits_email, user_id))  \\\n              else split_part(lower(coalesce(context_traits_email, user_id)),'@',2) end\"\n          type: email\n          entity: user\n          to_default_stitcher: true\n```\n\nThe following table explains the fields used in the above file:\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `name` | String | Name of the input model. |\n| `contract` | Dictionary | A model contract provides essential information about the model like the necessary columns and entity IDs that it should contain. This is crucial for other models that depend on it, as it helps find errors early and closer to the point of their origin. |\n| `app_defaults` | Dictionary | Values that input defaults to when you run the project directly. For library projects, you can remap the inputs and override the app defaults while importing the library projects. |\n\n##### `contract`\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `is_optional` | Boolean | Whether the model’s existence in the warehouse is mandatory. |\n| `is_event_stream` | Boolean | Whether the table/view is a series/stream of events. A model that has a `timestamp` column is an event stream model. |\n| `with_entity_ids` | List | List of all entities with which the model is related. A model M1 is considered related to model M2 if there is an ID of model M2 in M1’s output columns. |\n| `with_columns` | List | List of all ID columns that this contract is applicable for. |\n\n##### `app_defaults`\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `table`/`view` | String | Name of the warehouse table/view containing the data. You can prefix the table/view with an external schema or database in the same warehouse, if applicable. Note that you can specify either a table or view but not both. |\n| `occurred_at_col` | String | Name of the column in table/view containing the timestamp. |\n| [`ids`](#ids) | List | Specifies the list of all IDs present in the source table along with their column names (or column SQL expressions).<br><br>**Note**: Some input columns may contain IDs of associated entities. By their presence, such ID columns associate the row with the entity of the ID. The ID Stitcher may use these declarations to automatically discover ID-to-ID edges. |\n\n##### `ids`\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `select` | String | Specifies the column name to be used as the identifier. You can also specify a SQL expression if some transformation is required.<br><br>**Note**: You can also refer table from another Database/Schema in the same data warehouse. For example, `table: <database_name>.<schema_name>.<table_name>`. |\n| `type` | String | Type of identifier. All the ID types of a project are declared in [`pb_project.yaml`](#project-details). You can specify additional filters on the column expression.<br><br>**Note**: Each ID type is linked only with a single entity. |\n| `entity` | String | Entity name defined in the [`pb_project.yaml`](#project-details) file to which the ID belongs. |\n| `to_default_stitcher` | Boolean | Set this **optional** field to `false` for the ID to be excluded from the default ID stitcher. |\n\n## `profiles.yaml`\n\nThe `profiles.yaml` file lists entity\\_vars / input\\_vars used to create the output tables under `var_groups:`.\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `name` | String | A unique name for the var\\_group. |\n| `entity_key` | String | The entity to which the var\\_group belongs to. |\n| `vars` | Object | This section is used to specify variables, with the help of `entity_var` and `input_var`. Aggregation on stitched ID type is done by default and is implicit. |\n\nOptionally, you can create models using the above vars. The following fields are common for all the model types:\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `name` | String | Name of the model. Note that a table with the same name is created in the data warehouse. For example, if you define the name as `user_table`, the output table will be named something like `Material_user_table_<rest-of-generated-hash>_<timestamp-number>`. |\n| `model_type` | String | Defines the type of model. Possible values are: `id_stitcher`, `feature_table_model`, and `sql_template`. |\n| `model_spec` | Object | Creates a detailed configuration specification for the target model. Different schema is applicable for different model types as explained in each section below. |\n\nRudderStack supports the following model types:\n\n*   [Entity Traits 360 / Feature Table (legacy)](https://www.rudderstack.com/docs/archive/profiles/0.10/example/feature-table/)\n*   [SQL Template](https://www.rudderstack.com/docs/archive/profiles/0.10/example/sql-model/)\n*   [ID Stitcher](https://www.rudderstack.com/docs/archive/profiles/0.10/example/id-stitcher/)\n*   [ID Collator](https://www.rudderstack.com/docs/archive/profiles/0.10/example/id-collator/)\n*   [Python Models](https://www.rudderstack.com/docs/archive/profiles/0.10/predictions/python-models/)\n*   [Packages](https://www.rudderstack.com/docs/archive/profiles/0.10/example/packages/)\n\n## `README.md`\n\nThe `README.md` file provides a quick overview on how to use PB along with SQL queries for data analysis.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Project Structure | RudderStack Docs",
    "description": "Know the detailed PB project structure, configuration files and their parameters.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.10/example/id-stitcher/",
    "markdown": "# Identity Stitching | RudderStack Docs\n\nStep-by-step tutorial on how to stitch together different user identities.\n\n* * *\n\n*     6 minute read  \n    \n\nThis guide provides a detailed walkthrough on how to use a PB project and create output tables in a warehouse for a custom identity stitching model.\n\n## Prerequisites\n\n*   Familiarize yourself with:\n    \n    *   A basic Profile Builder project by following the [Profile Builder CLI](https://www.rudderstack.com/docs/archive/profiles/0.10/get-started/profile-builder/) steps.\n    *   [Structure](https://www.rudderstack.com/docs/archive/profiles/0.10/developer-guides/structure/) of a Profile Builder project and the parameters used in different files.\n\n## Output\n\nAfter [running the project](https://www.rudderstack.com/docs/archive/profiles/0.10/get-started/profile-builder/#7-generate-output-tables), you can view the generated material tables:\n\n1.  Log in to your Snowflake console.\n2.  Click **Worksheets** from the top navigation bar.\n3.  In the left sidebar, click **Database** and the corresponding **Schema** to view the list of all tables. You can hover over a table to see the full table name along with its creation date and time.\n4.  Write a SQL query like `select * from <table_name> limit 10;` and execute it to see the results:\n\n1.  Open [Postico2](https://eggerapps.at/postico2/). If required, create a new connection by entering the relevant details. Click **Test Connection** followed by **Connect**.\n2.  Click the **+** icon next to **Queries** in the left sidebar.\n3.  You can click **Database** and the corresponding schema to view the list of all tables/views.\n4.  Double click on the appropriate view name to paste the name on an empty worksheet.\n5.  You can prefix `SELECT *` from the view name pasted previously and suffix `LIMIT 10;` at the end.\n6.  Press Cmd+Enter keys, or click the **Run** button to execute the query.\n\n1.  Enter your Databricks workspace URL in the web browser and log in with your username and password.\n2.  Click the **Catalog** icon in left sidebar.\n3.  Choose the appropriate catalog from the list and click on it to view the contents.\n4.  You will see list of tables/views. Click on the appropriate table/view name to paste the name on the worksheet.\n5.  Then, you can prefix `SELECT * FROM` before the pasted view name and suffix `LIMIT 10;` at the end.\n6.  Select the query text. Press Cmd+Enter or click the **Run** button to execute the query.\n\nA sample output containing the results in Snowflake:\n\n![Generated tables (Snowflake)](https://www.rudderstack.com/docs/images/profiles/snowflake-console.webp)\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> Profiles creates a default ID stitcher even if you do not define any specs for creating one. It takes `default ID stitcher` as the input and all the sources and ID types defined in the file `inputs.yaml`. When you define the specs, it creates a custom ID stitcher.\n\n## Sample project for Custom ID Stitcher\n\nThis sample project considers multiple user identifiers in different warehouse tables to ties them together to create a unified user profile. The following sections describe how to define your PB project files:\n\n### Project detail\n\nThe [`pb_project.yaml`](https://www.rudderstack.com/docs/archive/profiles/0.10/developer-guides/structure/#project-details) file defines the project details such as name, schema version, connection name and the entities which represent different identifiers.\n\nYou can define all the identifiers from different input sources you want to stitch together as a `rudder_id` (`main_id` in this example):\n\n```\n# Project name\nname: sample_id_stitching\n# Project's yaml schema version\nschema_version: 49\n# Warehouse connection\nconnection: test\n# Allow inputs without timestamps\ninclude_untimed: true\n# Folder containing models\nmodel_folders:\n  - models\n# Entities in this project and their ids.\nentities:\n  - name: user\n    id_stitcher: models/user_id_stitcher # modelRef of custom ID stitcher model\n    id_types:\n      - main_id # You need to add ``main_id`` to the list only if you have defined ``main_id_type: main_id`` in the id stitcher buildspec.\n      - user_id # one of the identifier from your data source.\n      - email\n# lib packages can be imported in project signifying that this project inherits its properties from there\npackages:\n  - name: corelib\n    url: \"https://github.com/rudderlabs/profiles-corelib/tag/schema_{{best_schema_version}}\"\n    # if required then you can extend the package definition such as for ID types.\n```\n\n### Input\n\nThe [input file](https://www.rudderstack.com/docs/archive/profiles/0.10/developer-guides/structure/#inputs) (`models/inputs.yaml`) file includes the input table references and corresponding SQL for the above-mentioned entities:\n\n```\ninputs:\n- name: rsIdentifies\n  contract: # constraints that a model adheres to\n    is_optional: false\n    is_event_stream: true\n    with_entity_ids:\n      - user\n    with_columns:\n      - name: timestamp\n      - name: user_id\n      - name: anonymous_id\n      - name: email\n  app_defaults:\n    table: rudder_events_production.web.identifies # one of the WH table RudderStack generates when processing identify or track events.\n    occurred_at_col: timestamp\n    ids:\n      - select: \"user_id\" # kind of identity sql to pick this column from above table.\n        type: user_id\n        entity: user # as defined in project file\n        to_default_stitcher: true\n      - select: \"anonymous_id\"\n        type: anonymous_id\n        entity: user\n        to_default_stitcher: true\n      - select: \"lower(email)\" # can use sql.\n        type: email\n        entity: user\n        to_default_stitcher: true\n- name: rsTracks\n  contract:\n    is_optional: false\n    is_event_stream: true\n    with_entity_ids:\n      - user\n    with_columns:\n      - name: timestamp\n      - name: user_id\n      - name: anonymous_id\n  app_defaults:\n    table: rudder_events_production.web.tracks # another table in WH maintained by RudderStack processing track events.\n    occurred_at_col: timestamp\n    ids:\n      - select: \"user_id\"\n        type: user_id\n        entity: user\n        to_default_stitcher: true\n      - select: \"anonymous_id\"\n        type: anonymous_id\n        entity: user\n        to_default_stitcher: true\n```\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> As seen in the above file, you can use SQL to achieve some complex scenario as well.\n\n### Model\n\nProfiles **Identity stitching** model maps and unifies all the specified identifiers (in `pb_project.yaml` file) across different platforms. It tracks the user journey uniquely across all the data sources and stitches them together to a `rudder_id`.\n\nA sample `profiles.yaml` file specifying an identity stitching model (`user_id_stitcher`) with relevant inputs:\n\n```\nmodels:\n  - name: user_id_stitcher\n    model_type: id_stitcher\n    model_spec:\n      validity_time: 24h\n      entity_key: user\n      materialization:\n        run_type: incremental\n      incremental_timedelta: 12h\n      main_id_type: main_id\n      edge_sources:\n        - from: inputs/rsIdentifies\n        - from: inputs/rsTracks\n```\n\n##### Model specification fields\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `validity_time` | Time | Specifies the validity of the model with respect to its timestamp. For example, a model run as part of a scheduled nightly job for 2009-10-23 00:00:00 UTC with `validity_time`: `24h` would still be considered potentially valid and usable for any run requests, which do not require precise timestamps between 2009-10-23 00:00:00 UTC and 2009-10-24 00:00:00 UTC. This specifies the validity of generated feature table. Once the validity is expired, scheduling takes care of generating new tables. For example: 24h for 24 hours, 30m for 30 minutes, 3d for 3 days |\n| `entity_key` | String | Specifies the relevant entity from your `input.yaml` file. For example, here it should be set to `user`. |\n| `materialization` | List | Adds the key `run_type`: `incremental` to run the project in incremental mode. This mode considers row inserts and updates from the `edge_sources` input. These are inferred by checking the timestamp column for the next run. One can provide buffer time to consider any lag in data in the warehouse for the next incremental run like if new rows are added during the time of its run. If you do not specify this key then it’ll default to `run_type`: `discrete`. |\n| `incremental_timedelta` | List | (Optional )If materialization key is set to `run_type`: `incremental`, then this field sets how far back data should be fetched prior to the previous material for a model (to handle data lag, for example). The default value is 4 days. |\n| `main_id_type` | ProjectRef | (Optional) ID type reserved for the output of the identity stitching model, often set to `main_id`. It must not be used in any of the inputs and must be listed as an id type for the entity being stitched. If you do not set it, it defaults to `rudder_id`. Do not add this key unless it’s explicitly required, like if you want your identity stitcher table’s `main_id` column to be called `main_id`. |\n| `edge_sources` | List | Specifies inputs for the identity stitching model as mentioned in the `inputs.yaml` file. |\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Identity Stitching | RudderStack Docs",
    "description": "Step-by-step tutorial on how to stitch together different user identities.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.10/developer-guides/yaml-refresher/",
    "markdown": "# YAML 101 | RudderStack Docs\n\nA quick overview on YAML and its basics for use in Profile Builder.\n\n* * *\n\n*     4 minute read  \n    \n\nYAML is the preferred choice for writing Profile Builder files due to its simplicity and ease of use.\n\nThis guide explains the base concepts, syntax, and best practices for writing code in YAML.\n\n## What is YAML?\n\n[YAML](https://yaml.org/), short for **YAML Ain’t Markup Language** or **Yet Another Markup Language**, is a data serialization format often used in config files and exchange of data. A YAML file uses indentation, specific characters, and line breaks for representing various data structures.\n\n## Sample YAML file\n\nBelow is how a sample YAML document looks like. It contains key-value pairs where the keys are on the left, followed by a colon (`:`), and the associated values are on the right. The hierarchy and data structure is defined using indentation. The next section explains this in more detail.\n\n```\n# This is a comment\nperson:\n  name: Ruddy Buddy # Note the spacing used for indentation\n  age: 42\n  is_employed: true\n  address: # An object called address\n    street: Jefferson Davis Highway\n    city: Ruther Glen\n    state: Vermont\n    phone: 555-90-210\n  favorite_sports: # A list\n    - soccer\n    - baseball\n```\n\nThe above code has details of an object called `person` with properties like `name`, `age`, `gender`, `is_student`, `address` and `favorite sports`.\n\nHere’s how the same YAML file looks in the JSON format:\n\n```\n{\n  \"person\": {\n    \"name\": \"Ruddy Buddy\",\n    \"age\": 42,\n    \"is_employed\": true,\n    \"address\": {\n      \"street\": \"Jefferson Davis Highway\",\n      \"city\": \"Ruther Glen\",\n      \"state\": \"Vermont\",\n      \"phone\": \"555-90-210\"\n    },\n    \"favorite_sports\": [\n      \"soccer\",\n      \"baseball\"\n    ]\n  }\n}\n```\n\n## Indentation\n\nIn YAML, the indentation is done using spaces - to define the structure of data. Throughout the YAML file, the number of spacing should be consistent. Typically, we use two spaces for indentation. YAML is whitespace-sensitive, so do not mix spaces and tabs.\n\n```\n# Example of correct indentation\nperson:\n  name: Ruddy Buddy # We used 2 spaces\n  age: 42\n\n# Example of incorrect indentation\nperson:\n  name: Ruddy Buddy \n    age: 42 # We mixed spacing and tabs\n```\n\nAs shown above, YAML has single-line comments that start with hash (`#`) symbol, for providing additional explanation or context in the code. Comments are used to improve readability and they do not affect the code’s functionality.\n\n```\n# YAML comment\nperson:\n  name: Ruddy Buddy # Name of the person\n  age: 42 # Age of the person\n```\n\n## Data types in YAML\n\nYAML supports several data types:\n\n*   **Scalars**: Represent strings, numbers, and boolean values.\n*   **Sequences**: Represent lists and are denoted using a hyphen (`-`).\n*   **Mappings**: Key-value pairs used to define objects or dictionaries using colon (`:`).\n\n```\n# Example of data types in YAML\nperson:\n  name: Ruddy Buddy # Scalar (string)\n  age: 42 # Scalar (number)\n  is_employed: true # Scalar (boolean)\n  address: # Mapping (object)\n    street: Jefferson Davis Highway\n    city: Ruther Glen\n    state: Vermont\n    phone: 555-90-210\n  favorite_sports: # Sequence (list)\n    - soccer\n    - baseball\n```\n\n## Chomp modifiers\n\nYAML provides two chomp modifiers for handling line breaks in scalar values.\n\n*   `>`: Removes all newlines and replaces them with spaces.\n\n```\ndescription: >\n  Here is an example of long description\n  which has multiple lines. Later, it\n  will be converted into a single line.  \n```\n\n*   `|`: Preserves line breaks and spaces.\n\n```\ndescription: |\n  Here is another long description, however\n  it will preserve newlines and so the original\n  format shall be as-it-is.  \n```\n\n## Special characters\n\nYou can use escape symbols for special characters in YAML. For example, writing an apostrophe in description can cause the YAML parser to fail. In this case, you can use the escape character.\n\n## Best practices for writing YAML\n\nFollow these best practices for writing clean YAML code in your Profiles projects:\n\n*   Always keep consistent indentation (preferably spaces over tabs).\n*   Give meaningful names to your keys.\n*   Avoid excessive nesting.\n*   YAML is case sensitive, so be mindful of that.\n*   Add comments wherever required.\n*   Use blank lines to separate sections like ID stitcher, feature table, etc.\n*   If your strings contain special characters, then use escape symbols.\n*   Make sure you end the quotes in strings to avoid errors.\n*   Use chomp modifiers for multi-line SQL.\n\n## Conclusion\n\nThe above guidelines constitute some best practices to write effective [Builder](https://www.rudderstack.com/docs/profiles/get-started/profile-builder/) code in Profiles. You can also see the following references:\n\n*   [YAML for VS Code](https://marketplace.visualstudio.com/items?itemName=redhat.vscode-yaml): Extension for comprehensive YAML support in Visual Studio Code.\n*   [YAML Lint](https://www.yamllint.com/) for linting.\n\nFor more information or in any case of any issues, [contact](mailto:support@rudderstack.com) the RudderStack team.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "YAML 101 | RudderStack Docs",
    "description": "A quick overview on YAML and its basics for use in Profile Builder.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.10/example/",
    "markdown": "# Examples | RudderStack Docs\n\nPrivacy Overview\n\nThis site uses cookies to improve your experience while you navigate through the website. Out of these cookies, the cookies that are categorized as necessary are stored on your browser as they are as essential for the working of basic functionalities of the website. We also use third-party cookies that help us analyze and understand how you use this website. These cookies will be stored in your browser only with your consent. You also have the option to opt-out of these cookies. But opting out of some of these cookies may have an effect on your browsing experience.\n\nNecessary cookies are absolutely essential for the website to function properly. This category only includes cookies that ensures basic functionalities and security features of the website. These cookies do not store any personal information.",
    "title": "Examples | RudderStack Docs",
    "description": "Detailed tutorials on executing different models in warehouse to generate material (output) tables.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.10/developer-guides/tutorials/",
    "markdown": "# Profiles Tutorials - Additional Concepts\n\nPrivacy Overview\n\nThis site uses cookies to improve your experience while you navigate through the website. Out of these cookies, the cookies that are categorized as necessary are stored on your browser as they are as essential for the working of basic functionalities of the website. We also use third-party cookies that help us analyze and understand how you use this website. These cookies will be stored in your browser only with your consent. You also have the option to opt-out of these cookies. But opting out of some of these cookies may have an effect on your browsing experience.\n\nNecessary cookies are absolutely essential for the website to function properly. This category only includes cookies that ensures basic functionalities and security features of the website. These cookies do not store any personal information.",
    "title": "Profiles Tutorials - Additional Concepts | RudderStack Docs",
    "description": "Additional concepts related to Profiles.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.10/example/id-collator/",
    "markdown": "# ID Collator | RudderStack Docs\n\nStep-by-step tutorial stitching different user identities together.\n\n* * *\n\n*     3 minute read  \n    \n\nID Stitching is one of the most important features of Profiles. Being able to perform ID stitching to determine the accounts belonging to the same customer/user is very important to get a 360-degree view of that user.\n\nHowever many a times, we may not require ID stitching for a particular entity, especially if there are no edges in the ID graph of an entity. To build a feature table on such an entity, you will still need to perform ID stitching. Although this approach is not wrong, it is computationally redundant.\n\nProfiles provides the ID Collator is to get all IDs of that particular entity from various input tables and create one collated list of IDs.\n\n## Sample project\n\nLet’s take a case where we have defined two entities in our project - one is `user` and the other is `session`.\n\nIf `user` entity has multiple IDs defined, there are basically edges which make the use of an ID stitcher logical. On the other hand, `session` may have only one ID, `ssn_id`, there won’t be any possibility of edges. In such a case, all we need is a complete list of `ssn_id`.\n\nHere is the corresponding inputs and entities definition.\n\n```\nentities:\n  - name: user\n    id_column_name: user_rud_id\n    id_types:\n      - user_id\n      - anonymous_id\n  - name: session\n    id_column_name: session_id\n    id_types:\n      - ssn_id\n```\n\nProject file:\n\n```\ninputs:\n  - name: user_accounts\n    table: tbl_user_accounts\n    occurred_at_col: insert_ts\n    ids:\n      - select: \"user_id\"\n        type: user_id\n        entity: user\n  - name: sign_in\n    table: tbl_sign_in\n    occurred_at_col: insert_ts\n    ids:\n      - select: \"user_id\"\n        type: user_id\n        entity: user\n      - select: \"ssn_id\"\n        type: ssn_id\n        entity: session\n      - select: \"anonymous_id\"\n        type: anonymous_id\n        entity: user\n  - name: sign_up\n    table: tbl_sign_up\n    occurred_at_col: insert_ts\n    ids:\n      - select: \"user_id\"\n        type: user_id\n        entity: user\n      - select: \"ssn_id\"\n        type: ssn_id\n        entity: session\n      - select: \"anonymous_id\"\n        type: anonymous_id\n        entity: user\n```\n\nHere, the `entity: session` has only one ID type. Creating an ID stitcher for such an entity is possible but unnecessary.\n\nUsing all the models having `ssn_id`, we can just make a union of all `ssn_id` and get all distinct values of it and obtain the final list of sessions.\n\nThe underlying SQL will look as follows:\n\n```\nSELECT ssn_id as session_id from sign_in\n        UNION\n    SELECT ssn_id as session_id from sign_up\n;\n```\n\n## YAML Changes\n\nThe YAML writer cannot define a custom ID collator the way they define a custom ID stitcher. If an entity has no edges, the PB project will automatically figure out if an ID collator is needed. To exclude certain inputs (having the required ID) from being used in the collation, we can just set `to_id_stitcher: false` in the input.\n\n```\nentities:\n  - name: session\n    id_column_name: session_id\n    id_types:\n      - ssn_id\n```\n\nThe `id_column_name` is a new field added in the entity definition which will be the name of the ID column and it applies to both ID stitcher and ID collator.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> In the ID collator, you won’t generate a UUID like in ID stitcher.\n\n## Comparing ID Collator and ID Stitcher\n\n| ID Stitcher | ID Collator |\n| --- | --- |\n| Uses edges to converge the ID graph. | Collates all distinct IDs as there is only one ID Type and no edges are present. |\n| Higher cost of computation. | Lower cost of computation. |\n| A UUID is generated and used as the unique identifier for the entity. | Collates the existing IDs only. |\n| The generated ID is always of the type: `rudder_id` | The ID column of the generated ID collator table/view will be of the ID type of the corresponding ID. |\n| User may override the default ID stitcher with custom one. | You cannot override the default ID collator, though you can define a custom ID stitcher to override default ID collator. |\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "ID Collator | RudderStack Docs",
    "description": "Step-by-step tutorial stitching different user identities together.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.10/example/sql-model/",
    "markdown": "# SQL Models | RudderStack Docs\n\nStep-by-step tutorial on how to create a SQL Template model.\n\n* * *\n\n*     6 minute read  \n    \n\nThis guide provides a detailed walkthrough on how to use a PB project and create SQL Template models using custom SQL queries.\n\n## Prerequisites\n\n*   Familiarize yourself with:\n    \n    *   A basic Profile Builder project by following the [Profile Builder CLI](https://www.rudderstack.com/docs/archive/profiles/0.10/get-started/profile-builder/) steps.\n    *   [Structure](https://www.rudderstack.com/docs/archive/profiles/0.10/developer-guides/structure/) of a Profile Builder project and the parameters used in different files.\n\n## Sample project\n\nThe following sections describe how to define your PB project files:\n\n### Project detail\n\nThe [`pb_project.yaml`](https://www.rudderstack.com/docs/archive/profiles/0.10/developer-guides/structure/#project-details) file defines the project details such as name, schema version, connection name and the entities which represent different identifiers.\n\nYou can define all the identifiers from different input sources you want to stitch together as a single ID (`main_id` in this example):\n\n```\nname: sample_test\nschema_version: 49\nconnection: test\nmodel_folders:\n  - models\nentities:\n  - name: user\n    id_stitcher: models/test_id__\n    id_types:\n      - test_id\n      - exclude_id\ninclude_untimed: true\nid_types:\n  - name: test_id\n    filters:\n      - type: include\n        regex: \"([0-9a-z])*\"\n      - type: exclude\n        value: \"\"\n  - name: exclude_id\n```\n\n### Input\n\nThe [input file](https://www.rudderstack.com/docs/archive/profiles/0.10/developer-guides/structure/#inputs) (`models/inputs.yaml`) file includes the input table references and corresponding SQL for the above-mentioned entities:\n\n```\ninputs:\n  - name: tbl_a\n    app_defaults:\n      table: Temp_tbl_a\n    occurred_at_col: insert_ts\n    ids:\n      - select: TRIM(COALESCE(NULL, id1))\n        type: test_id\n        entity: user\n        to_default_stitcher: true\n      - select: \"id2\"\n        type: test_id\n        entity: user\n        to_default_stitcher: true\n      - select: \"id3\"\n        type: exclude_id\n        entity: user\n        to_default_stitcher: true\n  - name: tbl_b\n    app_defaults:\n      view: Temp_view_b\n    occurred_at_col: timestamp\n    ids:\n      - select: \"id1\"\n        type: test_id\n        entity: user\n        to_default_stitcher: true\n      - select: \"id2\"\n        type: test_id\n        entity: user\n        to_default_stitcher: true\n      - select: \"id3\"\n        type: test_id\n        entity: user\n        to_default_stitcher: true\n  - name: tbl_c\n    app_defaults:\n      table: Temp_tbl_c\n    ids:\n      - select: \"id1\"\n        type: test_id\n        entity: user\n        to_default_stitcher: true\n      - select: \"id2\"\n        type: test_id\n        entity: user\n        to_default_stitcher: true\n```\n\n### Model\n\nProfiles **SQL model** lets you write custom SQL queries to achieve advanced use-cases to create desired output tables.\n\nA sample `profiles.yaml` file specifying a SQL model (`test_sql`):\n\n```\nmodels:\n- name: test_sql\n  model_type: sql_template\n  model_spec:\n    validity_time: 24h# 1 day\n    materialization:                 // optional\n      run_type: discrete             // optional [discrete, incremental]\n    single_sql: |\n        {%- with input1 = this.DeRef(\"inputs/tbl_a\") -%}\n          select id1 as new_id1, id2 as new_id2, {{input1}}.*\n            from {{input1}}\n        {%- endwith -%}        \n    occurred_at_col: insert_ts        // optional\n    ids:\n      - select: \"new_id1\"\n        type: test_id\n        entity: user\n        to_default_stitcher: true\n      - select: \"new_id2\"\n        type: test_id\n        entity: user\n        to_default_stitcher: true\n      - select: \"id3\"\n        type: test_id\n        entity: user\n        to_default_stitcher: true\n```\n\n##### Model specification fields\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `validity_time` | Time | Time Specifies the validity of the model with respect to its timestamp. For example, a model run as part of a scheduled nightly job for 2009-10-23 00:00:00 UTC with `validity_time`: `24h` would still be considered potentially valid and usable for any run requests, which do not require precise timestamps between 2009-10-23 00:00:00 UTC and 2009-10-24 00:00:00 UTC. This specifies the validity of generated feature table. Once the validity is expired, scheduling takes care of generating new tables. For example: 24h for 24 hours, 30m for 30 minutes, 3d for 3 days. |\n| `materialization` | List | Adds the key `run_type`: `incremental` to run the project in incremental mode. This mode considers row inserts and updates from the edge\\_sources input. These are inferred by checking the timestamp column for the next run. One can provide buffer time to consider any lag in data in the warehouse for the next incremental run like if new rows are added during the time of its run. If you do not specify this key then it’ll default to `run_type`: `discrete`. |\n| `single_sql` | List | Specifies the SQL template which must evaluate to a single SELECT SQL statement. After execution, it should produce a dataset which will materialize based on the provided materialization. |\n| `multi-sql` | List | Specifies the SQL template which can evaluate to multiple SQL statements. One of these SQL statements (typically the last one) must be a CREATE statement which shall be responsible for materializing the model into a table.<br><br>**Note**: You should set only one of `single_sql` or `multi_sql`. |\n| `occurred_at_col` | List | Name of the column which contains the timestamp value in the output of sql template. |\n| `ids` | List | Specifies the list of all IDs present in the source table along with their column names (or column SQL expressions). It is required in case you want to use SQL models as an input to the `input_var` or `entity_var` fields. |\n\n## SQL template\n\nYou can pass custom SQL queries to the `single_sql` or `multi_sql` fields, which is also known as a **SQL template**. It provides the flexibility to write custom SQL by refering to any of the input sources listed in the `inputs.yaml` or any model listed in `models/profiles.yaml`.\n\nThe SQL templates follow a set query syntax which serves the purpose of creating a model. Follow the below rules to write SQL templates:\n\n*   Write SQL templates in the [pongo2 template engine](https://pkg.go.dev/github.com/flosch/pongo2#readme-first-impression-of-a-template) syntax.\n    \n*   Avoid circular referencing while referencing the models. For example, `sql_model_a` references `sql_model_b` and `sql_model_b` references `sql_model_a`.\n    \n*   Use `timestamp` variable (refers to the start time of the current run) to filter new events.\n    \n*   `this` refers to the current model’s material. You can use the following methods to access the material properties available for `this`:\n    \n    *   `DeRef(\"path/to/model\")`: Use this syntax `{{ this.DeRef(\"path/to/model\") }}` to refer to any model and return a database object corresponding to that model. The database object, in return, gives the actual name of the table/view in the warehouse. Then, generate the output, for example:\n    \n    ```\n    {% with input_table = this.DeRef(\"inputs/tbl_a\") %}\n        select a as new_a, b as new_b, {{input_table}}.*\n          from {{input_table}}\n    {% endwith %}\n    ```\n    \n    *   `GetMaterialization()`: Returns a structure with two fields: `MaterializationSpec{OutputType, RunType}`.\n        *   `OutputType`: You must use `OutputType` with `ToSQL()` method:  \n            For example, `CREATE OR REPLACE {{this.GetMaterialization().OutputType.ToSQL()}} {{this.GetSelectTargetSQL()}} AS ...`\n        *   `RunType`: For example, `this.GetMaterialization().RunType`\n\n### Model Contracts\n\nWith model contracts, you can declare constraints that the model adheres to. A model having a dependency on another model would also need to declare a contract specifying what columns and entities the input model must have. For contract validation, these columns should be present in the referenced model.\n\nFor an input of a project like a library project, the model contract is used to enforce constraints on tables/views that get wired to it downstream.\n\n```\n# inputs.yaml\n  - name: rsIdentifies\n    contract:\n      is_optional: false\n      is_event_stream: true\n      with_entity_ids:\n        - user\n      with_columns:\n        - name: timestamp\n        - name: user_id\n        - name: anonymous_id\n        - name: email\n```\n\nIn SQL model, the contract would contain all the columns from IDs and features. Each internal model also publishes the contract it promises to adhere to. Suppose `rsSessionTable` has an input `shopify_session_features`. Model contracts enable `rsSessionTable` to specify the constraints that `shopify_session_features` must adhere to.\n\n```\nmodels:\n- name: rsSessionTable\n  model_type: sql_template\n  model_spec:\n    ... # model specifications\n    single_sql: |\n      {% set contract = BuildContract('{\"with_columns\":[{\"name\":\"user_id\"}, {\"name\":\"anonymous_id\"}]}') %}\n      {% with SessionFeature = this.DeRef(\"models/shopify_session_features\",contract)%}\n          select user_id as id1, anonymous_id as id2 from {{SessionFeature}}\n  \tcontract:\n    with_entity_ids:\n      - user\n    with_columns:\n      - name: user_id\n        type: string\n        is_optional: true # false being default\n      - name: anonymous_id\n        type: string\n        is_optional: true # false being default\n```\n\nHere, `rsSessionTable` declares that its input `shopify_session_features` must have columns `user_id` and `anonymous_id`. This helps in improving data quality and error handling. Internally, this requested contract is validated against `shopify_session_features`’s actual contract. For validation to pass, `input_shopify_session_features_contract` must be a subset of `shopify_session_features`’s published contract.\n\nThis enables more comprehensive static and dynamic validations of our projects.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "SQL Models | RudderStack Docs",
    "description": "Step-by-step tutorial on how to create a SQL Template model.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.10/example/feature-table/",
    "markdown": "# Entity Traits 360 / Feature Table\n\nStep-by-step tutorial on creating an entity traits 360 / feature table model.\n\n* * *\n\n*     12 minute read  \n    \n\nOnce you have done [identity stitching](https://www.rudderstack.com/docs/archive/profiles/0.10/identity-stitching/) to unify the identity of your users across all the cross-platforms, you can evaluate and maintain the required features/traits for each identified user in a entity traits 360.\n\nThis guide provides a detailed walkthrough on how to use a PB project and create output tables in a warehouse for an entity traits 360 model.\n\n## Prerequisites\n\nFamiliarize yourself with:\n\n*   A basic Profile Builder project by following the [Profile Builder CLI](https://www.rudderstack.com/docs/archive/profiles/0.10/get-started/profile-builder/) steps.\n*   [Structure](https://www.rudderstack.com/docs/archive/profiles/0.10/developer-guides/structure/) of a Profile Builder project and the parameters used in different files.\n*   [Identity Stitching](https://www.rudderstack.com/docs/archive/profiles/0.10/example/id-stitcher/) model as Entity Traits 360 reuses its output to extract the required features/traits.\n\n## Entity Traits 360 Models\n\nOnce traits are defined on an entity, we need the means to use them.\n\nA primary application to send them to the downstream destinations. The destination could either be the [Activation API](https://www.rudderstack.com/docs/archive/profiles/0.10/activation-api/) or via any of the [rETL destinations](https://www.rudderstack.com/docs/sources/reverse-etl/) that RudderStack supports. Each such destination requires data in the form of a table with an ID column and 1 or more feature columns. This is possible using Entity Traits 360.\n\nEntity Traits 360 provides a way to access entity features based on any of the given ID types, including the entity main id as the identifier column. It creates a view which will have all (or a specified set of) features on that entity from across the project.\n\nTo configure the creation of a specific set of traits 360 models, add `serve_traits` section in the entity. You need to provide a list of ID types as `id_served`. Optionally, you can also give a name which specifies the name of the generated model. If you don’t specify a name, it will create the model with a default name. By default, it will add all available features on the entity into the view.\n\nIf you want finer control, you can also include or exclude any features from any models by defining a custom entity traits 360 model and add the reference to the `serve_traits` section like `model_served: models/name_of_custom_traits_360_model`. See below on more examples of this.\n\n### Default entity traits 360 model\n\n`pb_project.yaml`:\n\n```\n...\nentities:\n  - name: user\n    id_types:\n      - user_id\n    serve_traits:\n      - id_served: user_id\n        name: user_id_stitched_features\n        # This will add an entity-traits-360 model with user_id as the identifier with model name user_id_stitched_features.\n        # It will contain all the available features.\n```\n\n**Custom entity traits 360 model:**\n\nThis is an example of custom entity traits 360 model. Here we are including / excluding features from models of choice.\n\n`pb_project.yaml`:\n\n```\n...\nentities:\n  - name: user\n    id_types:\n      - user_id\n    serve_traits:\n      - id_served: user_id\n        model_served: models/cart_entity_traits_360\n```\n\n`models/profiles.yaml`:\n\n```\nmodels:\n  - name: cart_entity_traits_360\n    model_type: entity_traits_360\n    model_spec:\n      validity_time: 24h # 1 day\n      entity_key: user\n      id_served: user_id\n      feature_list:\n        - from: packages/pkg/models/cart_table # a table created by package\n          include: [\"*\"] # will include all the traits\n        - from: models/user_var_table\n          include: [\"*\"]\n          exclude: [cart_quantity, purchase_status] # except two, all the other traits will be included\n        - from: models/sql_model\n          include: [lifetime_value] # will include only one trait\n```\n\n## Sample project\n\nThis sample project uses the output of an identity stitching model as an input to create a entity traits 360. The following sections describe how to define your PB project files:\n\n### Project detail\n\nThe [`pb_project.yaml`](https://www.rudderstack.com/docs/archive/profiles/0.10/developer-guides/structure/#project-details) file defines the project details such as name, schema version, connection name and the entities which represent different identifiers.\n\nYou can define all the identifiers from different input sources you want to stitch together as a `user_main_id`:\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> You need to add `main_id` to the list only if you have defined `main_id_type: main_id` in the ID stitcher buildspec.\n\n```\n# Project name\nname: sample_id_stitching\n# Project's yaml schema version\nschema_version: 49\n# Warehouse connection\nconnection: test\n# Allow inputs without timestamps\ninclude_untimed: true\n# Folder containing models\nmodel_folders:\n  - models\n# Entities in this project and their ids.\nentities:\n  - name: user\n    id_types:\n      - main_id # You need to add ``main_id`` to the list only if you have defined ``main_id_type: main_id`` in the id stitcher buildspec.\n      - user_id # one of the identifier from your data source.\n      - email\n# lib packages can be imported in project signifying that this project inherits its properties from there\npackages:\n  - name: corelib\n    url: \"https://github.com/rudderlabs/profiles-corelib/tag/schema_{{best_schema_version}}\"\n    # if required then you can extend the package definition such as for ID types.\n```\n\n### Input\n\nThe [input file](https://www.rudderstack.com/docs/archive/profiles/0.10/developer-guides/structure/#inputs) file includes the input table references and corresponding SQL for the above-mentioned entities:\n\n```\ninputs:\n- name: rsIdentifies\n  contract: # constraints that a model adheres to\n    is_optional: false\n    is_event_stream: true\n    with_entity_ids:\n      - user\n    with_columns:\n      - name: timestamp\n      - name: user_id\n      - name: anonymous_id\n      - name: email\n  app_defaults:\n    table: rudder_events_production.web.identifies # one of the WH table RudderStack generates when processing identify or track events.\n    occurred_at_col: timestamp\n    ids:\n      - select: \"user_id\" # kind of identity sql to pick this column from above table.\n        type: user_id\n        entity: user # as defined in project file\n        to_default_stitcher: true\n      - select: \"anonymous_id\"\n        type: anonymous_id\n        entity: user\n        to_default_stitcher: true\n      - select: \"lower(email)\" # can use sql.\n        type: email\n        entity: user\n        to_default_stitcher: true\n- name: rsTracks\n  contract:\n    is_optional: false\n    is_event_stream: true\n    with_entity_ids:\n      - user\n    with_columns:\n      - name: timestamp\n      - name: user_id\n      - name: anonymous_id\n  app_defaults:\n    table: rudder_events_production.web.tracks # another table in WH maintained by RudderStack processing track events.\n    occurred_at_col: timestamp\n    ids:\n      - select: \"user_id\"\n        type: user_id\n        entity: user\n        to_default_stitcher: true\n      - select: \"anonymous_id\"\n        type: anonymous_id\n        entity: user\n        to_default_stitcher: true\n```\n\n### Model\n\nProfiles **entity traits 360** model lets you define the specific features/traits you want to evaluate from the huge spread of scattered data in your warehouse tables.\n\nA sample `profiles.yaml` file specifying a entity traits 360 model (`user_profile`):\n\n```\nvar_groups:\n  - name: first_group\n    entity_key: user\n    vars:\n      - entity_var:\n          name: first_seen\n          select: min(timestamp::date)\n          from: inputs/rsTracks\n          where: properties_country is not null and properties_country != ''\n      - entity_var:\n          name: last_seen\n          select: max(timestamp::date)\n          from: inputs/rsTracks\n      - entity_var:\n          name: user_lifespan\n          select: '{{user.Var(\"last_seen\")}} - {{user.Var(\"first_seen\")}}'\n          description: Life Time Value of a customer\n      - entity_var:\n          name: days_active\n          select: count(distinct timestamp::date)\n          from: inputs/rsTracks\n          description: No. of days a customer was active\n      - entity_var:\n          name: campaign_source\n          default: \"'organic'\"\n      - entity_var:\n          name: user_rank\n          default: -1\n      - entity_var:\n          name: campaign_source_first_touch\n          select: first_value(context_campaign_source)\n          window:\n              order_by:\n                  - timestamp asc\n              partition_by:\n                  - main_id\n          from: inputs/rsIdentifies\n          where: context_campaign_source is not null and context_campaign_source != ''\n      - input_var:\n          name: num_c_rank_num_b_partition\n          select: rank()\n          from: inputs/tbl_c\n          default: -1\n          window:\n            partition_by:\n              - '{{tbl_c}}.num_b'\n            order_by:\n              - '{{tbl_c}}.num_c asc'\n          where: '{{tbl_c}}.num_b >= 10'\n      - entity_var:\n          name: min_num_c_rank_num_b_partition\n          select: min(num_c_rank_num_b_partition)\n          from: inputs/tbl_c\n```\n\n**`entity_var`**\n\nThe `entity_var` field provides inputs for the entity traits 360 model. This variable stores the data temporarily, however, you can choose to store its data permanently by specifying the `name` in it as a feature in the `features` key.\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `name` | String | Name of the `entity_var` to identify it uniquely. |\n| `select` | String | Column name/value you want to select from the table. This defines the actual value that will be stored in the variable. You can use simple SQL expressions or select an `entity_var` as `{{entityName.Var(\\\"entity_var\\\")}}`. It has to be an aggregate operation that ensures the output is a unique value for a given `main_id`. For example: min(timestamp), count(\\*), sum(amount) etc. This holds true even when a window function (optional) is used. For example:: first\\_value(), last\\_value() etc are valid while rank(), row\\_number(), etc. are not valid and give unpredictable results. |\n| `from` | List | Reference to the source table from where data is to be fetched. You can either refer to another model from the same YAML or some other table specified in input YAML. |\n| `where` | String | Any filters you want to apply on the input table before selecting a value. This must be SQL compatible and should consider the data type of the table. |\n| `default` | String | Default value in case no data matches the filter. When defining default values, make sure you enclose the string values in single quotes followed by double quotes to avoid SQL failure. However, you can use the non-string values without any quotes. |\n| `description` | String | Textual description of the `entity_var`. |\n| `window` | Object | Specifies the window function. Window functions in SQL usually have both `partition_by` and `order_by` properties. But for `entity_var`, `partition_by` is added with `main_id` as default; so, adding `partition_by` manually is not supported. If you need partitioning on other columns too, check out `input_var` where `partition_by` on arbitrary and multiple columns is supported. |\n\n**`input_var`**\n\nThe syntax of `input_var` is similar to `entity_var`, with the only difference that instead of each value being associated to a row of the entity traits 360, it’s associated with a row of the specified input. While you can think of an `entity_var` as adding a helper column to the entity traits 360, you can consider an `input_var` as adding a helper column to the input.\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `name` | String | Name to store the retrieved data. |\n| `select` | String | Data to be stored in the name. |\n| `from` | List | Reference to the source table from where data is to be fetched. |\n| `where` | String | (Optional) Applies conditions for fetching data. |\n| `default` | String | (Optional) Default value for any entity for which the calculated value would otherwise be NULL. |\n| `description` | String | (Optional) Textual description. |\n| `window` | Object | (Optional) Specifies a window over which the value should be calculated. |\n\n**`window`**\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `partition_by` | String | (Optional) List of SQL expressions to use in partitioning the data. |\n| `order_by` | String | (Optional) List of SQL expressions to use in ordering the data. |\n\nIn window option, `main_id` is not added by default, it can be any arbitrary list of columns from the input table. So if a feature should be partitioned by `main_id`, you must add it in the `partition_by` key.\n\n### Output\n\nAfter [running the project](https://www.rudderstack.com/docs/archive/profiles/0.10/get-started/profile-builder/#7-generate-output-tables), you can view the generated material tables.\n\n1.  Log in to your Snowflake console.\n2.  Click **Worksheets** from the top navigation bar.\n3.  In the left sidebar, click **Database** and the corresponding **Schema** to view the list of all tables. You can hover over a table to see the full table name along with its creation date and time.\n4.  Write a SQL query like `select * from <table_name>` and execute it to see the results:\n\n1.  Open [Postico2](https://eggerapps.at/postico2/). If required, create a new connection by entering the relevant details. Click **Test Connection** followed by **Connect**.\n2.  Click the **+** icon next to **Queries** in the left sidebar.\n3.  You can click **Database** and the corresponding schema to view the list of all tables/views.\n4.  Double click on the appropriate view name to paste the name on an empty worksheet.\n5.  You can prefix `SELECT *` from the view name pasted previously and suffix `LIMIT 10;` at the end.\n6.  Press Cmd+Enter keys, or click the **Run** button to execute the query.\n\n1.  Enter your Databricks workspace URL in the web browser and log in with your username and password.\n2.  Click the **Catalog** icon in left sidebar.\n3.  Choose the appropriate catalog from the list and click on it to view contents.\n4.  You will see list of tables/views. Click the appropriate table/view name to paste the name on worksheet.\n5.  You can prefix `SELECT * FROM` before the pasted view name and suffix `LIMIT 10;` at the end.\n6.  Select the query text. Press Cmd+Enter, or click the **Run** button to execute the query.\n\nA sample output containing the results in Snowflake:\n\n![Generated table (Snowflake)](https://www.rudderstack.com/docs/images/profiles/profiles-feature-table.webp)\n\n## Window functions\n\nA window function operates on a window (group) of related rows. It performs calculation on a subset of table rows that are connected to the current row in some way. The window function has the ability to access more than just the current row in the query result.\n\nThe window function returns one output row for each input row. The values returned are calculated by using values from the sets of rows in that window. A window is defined using a window specification, and is based on three main concepts:\n\n*   Window partitioning, which forms the groups of rows (`PARTITION BY` clause)\n*   Window ordering, which defines an order or sequence of rows within each partition (`ORDER BY` clause)\n*   Window frames, which are defined relative to each row to further restrict the set of rows (`ROWS` specification). It is also known as the frame clause.\n\n**Snowflake** does not enforces users to define the cumulative or sliding frames, and considers `ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING` as the default cumulative window frame. However, you can override this by defining the frame manually.\n\nOn the **Redshift** aggregate window function list given below, specify the `frame_clause` while using any function from the list:\n\n*   `AVG`\n*   `COUNT`\n*   `CUME_DIST`\n*   `DENSE_RANK`\n*   `FIRST_VALUE`\n*   `LAG`\n*   `LAST_VALUE`\n*   `LEAD`\n*   `LISTAGG`\n*   `MAX`\n*   `MEDIAN`\n*   `MIN`\n*   `NTH_VALUE`\n*   `PERCENTILE_CONT`\n*   `PERCENTILE_DISC`\n*   `RATIO_TO_REPORT`\n*   `STDDEV_POP`\n*   `STDDEV_SAMP` (synonym for `STDDEV`)\n*   `SUM`\n*   `VAR_POP`\n*   `VAR_SAMP` (synonym for `VARIANCE`)\n\nOn the Redshift ranking window functions given below, **do not** specify the `frame_clause` while using any function from the list:\n\n*   `DENSE_RANK`\n*   `NTILE`\n*   `PERCENT_RANK`\n*   `RANK`\n*   `ROW_NUMBER`\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> When using a window function, use `frame_clause` carefully. While It is not very critical for Snowflake, using it incorrectly in Redshift can lead to errors.\n\nExample of using `frame_clause`:\n\n```\n- entity_var:\n    name: first_num_b_order_num_b\n    select: first_value(tbl_c.num_b) # Specify frame clause as aggregate window function is used\n    from: inputs/tbl_c\n    default: -1\n    where: tbl_c.num_b >= 10\n    window:\n        order_by:\n        - tbl_c.num_b desc\n        frame_clause: ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING\n- entity_var:\n    name: first_num_b_order_num_b_rank\n    select: rank() # DO NOT specify frame clause as ranking window function is used\n    window:\n        partition_by:\n        - first_num_b_order_num_b > 0\n        order_by:\n        - first_num_b_order_num_b asc\n```\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> Note how `frame_clause` is specified in first `entity_var` and not in the second one.\n\n## Feature Table Model (legacy)\n\nProfiles **Feature table** model lets you define the specific features/traits you want to evaluate from the huge spread of scattered data in your warehouse tables.\n\nA sample `profiles.yaml` file specifying a feature table model (`user_profile`):\n\n```\nmodels:\n  - name: user_profile\n    model_type: feature_table_model\n    model_spec:\n      validity_time: 24h # 1 day\n      entity_key: user\n      features:\n        - user_lifespan\n        - days_active\n        - min_num_c_rank_num_b_partition\n```\n\n##### Model specification fields\n\n| Field | Data type | Description |\n| --- | --- | --- |\n| `validity_time` | Time | Specifies the validity of the model with respect to its timestamp. For example, a model run as part of a scheduled nightly job for 2009-10-23 00:00:00 UTC with `validity_time`: `24h` would still be considered potentially valid and usable for any run requests, which do not require precise timestamps between 2009-10-23 00:00:00 UTC and 2009-10-24 00:00:00 UTC. This specifies the validity of generated entity traits 360. Once the validity is expired, scheduling takes care of generating new tables. For example: `24h` for 24 hours, `30m` for 30 minutes, `3d` for 3 days, and so on. |\n| `entity_key` | String | Specifies the relevant entity from your `input.yaml` file. |\n| `features` | String | Specifies the list of `name` in `entity_var`, that must act as a feature. |\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Entity Traits 360 / Feature Table | RudderStack Docs",
    "description": "Step-by-step tutorial on creating an entity traits 360 / feature table model.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.10/example/packages/",
    "markdown": "# Packages | RudderStack Docs\n\nUtilize models from existing libary projects.\n\n* * *\n\n*     4 minute read  \n    \n\nProfiles gives you the flexibility to utilize models from existing library projects while defining your own models and inputs within the PB project. This approach allows for a seamless integration of library of pre-existing features, which are readily available and can be applied directly to data streamed into your warehouse.\n\nIn the absence of any explicitly defined models, the PB project is capable of compiling and running models from the library package given that inputs are present in the warehouse as assumed in the lib package.\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> Packages currently work only on Snowflake.\n\nGenerally, there will be some deviations in terms of the database name and schema name of input models - however, you can easily handle this by remapping inputs.\n\nA sample `pb_project.yaml` file may look as follows:\n\n```\nname: app_project\nschema_version: 49\nprofile: test\npackages:\n  - name: test_ft\n    gitUrl: \"https://github.com/rudderlabs/librs360-shopify-features/tree/main\"\n```\n\nIn this case, the PB project imports a single package. It does not require a separate `models` folder or entities as the input and output models will be sourced from the imported packages.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> Note that:\n> \n> *   If non-mandatory inputs required by the model are not present in the warehouse, you can still run the model.\n> *   If there is any deviation in the table/view name for input models, that is, if the inputs assumed in library package are present under some other name, make sure to do the remapping.\n> *   If some of the assumed inputs are not present at all, they should be remapped to `nil`. This way you can create and run imported packages with minimal set of inputs present.\n\nFor example, to import a library package with the name of `shopify_features`:\n\n```\npackages: \n  - name: shopify_features\n    url: https://github.com/rudderlabs/librs360-shopify-features/tree/main\n    inputsMap: \n      rsCartCreate: inputs/rsWarehouseCartCreate\n      rsCartUpdate: inputs/rsCartUpdate\n      rsIdentifies: inputs/rsIdentifies\n      rsOrderCancelled: inputs/rsOrderCancelled\n      rsOrderCreated: inputs/rsOrderCreated\n      rsPages: nil\n      rsTracks: nil\n```\n\nIn `models`/`inputs.yaml`, these inputs need to be defined with table names present in the warehouse.\n\n```\ninputs:\n  - name: rsWarehouseCartCreate\n    table: YOUR_DB.YOUR_SCHEMA.CART_CREATE_TABLE_NAME_IN_YOUR_WH\n    occurred_at_col: timestamp\n    ids:\n      - select: \"anonymous_id\"\n        type: anonymous_id\n        entity: user\n    source_metadata:\n      role: shopify\n      category: webhook\n  - name: rsIdentifies\n    table: YOUR_DB.YOUR_SCHEMA.IDENTIFIES\n    occurred_at_col: timestamp\n    ids:\n      - select: \"user_id\"\n        type: user_id\n        entity: user\n      - select: \"anonymous_id\"\n        type: anonymous_id\n        entity: user\n      - select: \"lower(email)\"\n        type: email\n        entity: user\n    source_metadata:\n      role: shopify\n      category: webhook\n```\n\nNote that the name of the table/view is changed to the appropriate name in your warehouse. If tables are present with the same name (including database name and schema name) then no remapping is required.\n\n### Available packages\n\nThe following list of packages are currently available in Profiles:\n\n*   [profiles-corelib](https://github.com/rudderlabs/profiles-corelib)\n*   [profiles-base-features](https://github.com/rudderlabs/rudderstack-profiles-base-features)\n*   profiles-shopify-features\n*   profiles-ecommerce-features\n*   profiles-stripe-features\n*   profiles-multieventstream-features\n\n[Contact us](mailto:support@rudderstack.com) to access these packages.\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> Packages currently work only on Snowflake.\n\n## Modify ID types\n\n### Extend existing package\n\nTo add custom ID types to the default list or modify an existing one, then you may extend the package to include your specifications.\n\nFor the corresponding `id_type`, add the key `extends:` followed by name of the same/different `id_type` that you wish to extend and the `filters` with `include`/`exclude` values.\n\n```\n---pb_project.yaml---\npackages:\n  - name: corelib\n    url: \"https://github.com/rudderlabs/rudderstack-profiles-corelib/tag/schema_{{best_schema_version}}\"\nid_types:\n  - name: user_id\n    extends: user_id\n    filters:\n      - type: exclude\n        value: 123456\nid_types:\n  - name: customer_id\n    extends: user_id\n    filters:\n      - type: include\n        regex: sample\n```\n\n*   **id\\_types**: Enlists the type of data to be used for creating ID stitcher / EntityVar / InputVar. For example, anonymous IDs that do not include the value `undefined` or email addresses in proper format.\n    *   **extends**: Name of the ID type that you wish to extend.\n    *   **name**: The type of data that will be fetched, like email, user ID, etc. It is different from whatever is present in the table column, like int or varchar.\n    *   **filters**: Filter(s) that the type should go through before being included. Filters are processed in order. Current filters enable one to include and exclude specific values or regular expressions.\n\n### Custom list of ID types\n\nTo have custom list of ID types other than the provisions in the default package, you can remove and add your list as follows:\n\n```\nentities:\n  - name: user\n    id_types:\n      - user_id\n      - anonymous_id\n      - email\n\nid_types:\n  - name: user_id\n  - name: anonymous_id\n    filters:\n      - type: exclude\n        value: \"\"\n      - type: exclude\n        value: \"unknown\"\n      - type: exclude\n        value: \"NaN\"\n  - name: email\n    filters:\n    - type: include\n      regex: \"[A-Za-z0-9+_.-]+@(.+)\"\n```\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> Make sure that the ID types are also defined in the entity definition.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Packages | RudderStack Docs",
    "description": "Utilize models from existing libary projects.",
    "languageCode": "en"
  },
  {
    "url": "https://www.rudderstack.com/docs/archive/profiles/0.10/faq/",
    "markdown": "# Profiles FAQ | RudderStack Docs\n\nCommonly asked questions on RudderStack Profiles.\n\n* * *\n\n*     19 minute read  \n    \n\nThis guide contains solutions for some of the commonly asked questions on Profiles.\n\n## Setup and installation\n\n**I have installed Python3, yet when I install and execute `pb` it doesn’t return anything on screen.**\n\nTry restarting your Terminal/Shell/PowerShell and try again.\n\nYou can also try to find the location of your Python executable. PB would be installed where the executables embedded in other Python packages are installed.\n\n**I am an existing user who updated to the new version and now I am unable to use the PB tool. On Windows, I get the error:** `'pb' is not recognized as an internal or external command, operable program or batch file.`\n\nExecute the following commands to do a fresh install:\n\n1.  `pip3 uninstall profiles-rudderstack-bin`\n2.  `pip3 uninstall profiles-rudderstack`\n3.  `pip3 install profiles-rudderstack --no-cache-dir`\n\n**I am unable to download, getting** `ERROR: Package 'profiles-rudderstack' requires a different Python: 3.7.10 not in '>=3.8, <=3.10'`\n\nUpdate your Python 3 to a version greater than or equal to 3.8 and less than or equal to 3.10.\n\n**I am unable to download profile builder by running `pip3 install profiles-rudderstack` even though I have Python installed.**\n\nFirstly, make sure that Python3 is correctly installed. You can also try to substitute `pip3` with `pip` and execute the install command.\n\nIf that doesn’t work, it’s high likely that Python3 is accessible from a local directory.\n\n1.  Navigate to that directory and try the install command again.\n2.  After installation, PB should be accessible from anywhere.\n3.  Validate that you’re able to access the path using `which pb`.\n4.  You may also execute `echo $PATH` to view current path settings.\n5.  In case it doesn’t show the path then you can find out where |ProductName| is installed using :substitution-code:`pip3 show profiles-rudderstack`. This command will display a list of the files associated with the application, including the location in which it was installed, navigate to that directory.\n6.  Navigate to `/bin` subdirectory and execute command `ls` to confirm that `pb` is present there.\n7.  To add the path of the location where PB is installed via pip3, execute: `export PATH=$PATH:<path_to_application>`. This will add the path to your system’s PATH variable, making it accessible from any directory. It is important to note that the path should be complete and not relative to the current working directory.\n\nIf you still face issues, then you can try to install it manually. [Contact us](mailto:support@rudderstack.com) for the executable file and download it on your machine. Follow the below steps afterwards:\n\n1.  Create `rudderstack` directory: `sudo mkdir /usr/local/rudderstack`.\n2.  Move the downloaded file to that directory: `sudo mv <name_of_downloaded_file> /usr/local/rudderstack/pb`.\n3.  Grant executable permission to the file: `chmod +x /usr/local/rudderstack/pb`.\n4.  Navigate to directory `/usr/local/rudderstack` from your file explorer. Ctrl+Click on pb and select **Open** to run it from Terminal.\n5.  Symlink to a filename pb in `/usr/local/bin` so that command can locate it from env PATH. Create file if it does not exist: `sudo touch /usr/local/bin/pb`. Then execute`sudo ln -sf /usr/local/rudderstack/pb /usr/local/bin/pb`.\n6.  Verify the installation by running `pb` in Terminal. In case you get error `command not found: pb` then check if `/usr/local/bin` is defined in PATH by executing command: `echo $PATH`. If not, then add `/usr/local/bin` to PATH.\n\n1.  If the Windows firewall prompts you after downloading, proceed with `Run Anyway`.\n2.  Rename the executable as `pb`.\n3.  Move the file to a safe directory such as `C:\\\\Program Files\\\\Rudderstack`, create the directory if not present.\n4.  Set the path of `pb.exe` file in environment variables.\n5.  Verify the installation by running `pb` in command prompt.\n\n**When I try to install Profile Builder tool using pip3 I get error message saying: `Requirement already satisfied`**\n\nTry the following steps:\n\n1.  Uninstall PB using `pip3 uninstall profiles-rudderstack`.\n2.  Install again using `pip3 install profiles-rudderstack`.\n\nNote that this won’t remove your existing data such as models and siteconfig files.\n\n**I have multiple models in my project. Can I run only a single model?**\n\nYes, you can. In your spec YAML file for the model you don’t want to run, set `materialization` to `disabled`:\n\n```\nmaterialization:\n    enable_status: disabled\n```\n\nA sample `profiles.yaml` file highlighted a disabled model:\n\n```\nmodels:\n- name: test_sql\n  model_type: sql_template\n  model_spec:\n    validity_time: 24h# 1 day\n    materialization:                \n      run_type: discrete\n      enable_status: disabled  // Disables running the model.\n    single_sql: |\n        {%- with input1 = this.DeRef(\"inputs/tbl_a\") -%}\n          select id1 as new_id1, {{input1}}.*\n            from {{input1}}\n        {%- endwith -%}        \n    occurred_at_col: insert_ts\n    ids:\n      - select: \"new_id1\"\n        type: test_id\n        entity: user\n```\n\n* * *\n\n## Warehouse permissions\n\n**I have two separate roles to read from input tables and write to output tables? How should I define the roles?**\n\nYou need to create an additional role as a union of those two roles. PB project needs to read the input tables and write the results back to the warehouse schema.\n\nFurthermore, each run is executed using a single role as specified in the `siteconfig.yaml` file. Hence, it is best in terms of security to create a new role which has read as well as write access for all the relevant inputs and the output schema.\n\n**How do I test if the role I am using has sufficient privileges to access the objects in the warehouse?**\n\nYou can use the `pb validate access` command to validate the access privileges on all the input/output objects.\n\n* * *\n\n## Compile command\n\n**I am trying to execute the `compile` command by fetching a repo via GIT URL but getting this error: `making git new public keys: ssh: no key found`**\n\nYou need to add the OpenSSH private key to your `siteconfig.yaml` file. If you get the error `could not find expected` afterwards, try correcting the spacing in your `siteconfig.yaml` file.\n\n**While trying to segregate identity stitching and feature table in separate model files, I am getting this error: `mapping values are not allowed in this context`**\n\nThis is due to the spacing issue in `siteconfig.yaml` file. You may create a new project to compare the spacing. Also, make sure you haven’t missed any keys.\n\n* * *\n\n## Command progress & lifecycle\n\n**I executed a command and it is taking too long. Is there a way to kill a process on data warehouse?**\n\nIt could be due to the other queries running simultaneously on your warehouse. To clear them up, open the **Queries** tab in your warehouse and manually kill the long running processes.\n\n**Due to the huge data, I am experiencing long execution times. My screen is getting locked, thereby preventing the process from getting completed. What can I do?**\n\nYou can use the `screen` command on UNIX/MacOS to detach your screen and allow the process to run in the background. You can use your terminal for other tasks, thus avoiding screen lockouts and allowing the query to complete successfully.\n\nHere are some examples:\n\n*   To start a new screen session and execute a process in detached mode: `screen -L -dmS profiles_rn_1 pb run`. Here:\n    *   `-L` flag enables logging.\n    *   `-dmS` starts as a daemon process in detached mode.\n    *   `profiles_rn_1` is the process name.\n*   To list all the active screen sessions: `screen -ls`.\n*   To reattach to a detached screen session: `screen -r [PID or screen name]`.\n\n**The CLI was running earlier but it is unable to access the tables now. Does it delete the view and create again?**\n\nYes, every time you run the project, Profiles creates a new materials table and replaces the view.\n\nHence, you need to grant a select on future views/tables in the respective schema and not just the existing views/tables.\n\n**Does the CLI support downloading a git repo using siteconfig before executing** `pb run` **? Or do I have to manually clone the repo first?**\n\nYou can pass the Git URL as a parameter instead of project’s path, as shown:\n\n**When executing** `run` **command, I get a message:** `Please use sequence number ... to resume this project in future runs` **. Does it mean that a user can exit using Ctrl+C and later if they give this seq\\_no then it’ll continue from where it was cancelled earlier?**\n\nThe `pb run --seq_no <>` flag allows for the provision of a sequence number to run the project. This flag can either resume an existing project or use the same context to run it again.\n\nWith the introduction of time-grain models, multiple sequence numbers can be assigned and used for a single project run.\n\n**What flag should I set to force a run for same end time, even if a previous run exists?**\n\nYou can execute `pb run --force --model_refs models/my_id_stitcher,entity/user/user_var_1,entity/user/user_var_2,...`\n\n**Can the hash change even if schema version did not change?**\n\nYes, as the hash versions depends on project’s implementation while the schema versions are for the project’s YAML layout.\n\n* * *\n\n## Identity stitching\n\n**There are many large size connected components in my warehouse. To increase the accuracy of stitched data, I want to increase the number of iterations. Is it possible?**\n\nThe default value of the largest diameter, that is, the longest path length in connected components, is 30.\n\nYou can increase it by defining a `max_iterations` key under `model_spec` of your ID stitcher model in `models/profiles.yaml`, and specifying its value as the max diameter of connected components.\n\n> ![warning](https://www.rudderstack.com/docs/images/warning.svg)\n> \n> Note that the algorithm can give incorrect results in case of large number of iterations.\n\n**Do I need to write different query each time for viewing the data of created tables?**\n\nNo, you can instead use a view name, which always points to the latest created material table. For example, if you’ve defined **user\\_stitching** in your `models/profiles.yaml` file, then execute `SELECT * FROM MY_WAREHOUSE.MY_SCHEMA.user_stitching`.\n\n**In my model, I have set the key `validity_time: 24h`. What happens when the validity of generated tables expire? Will re-running the identity stitching model generate the same hash until the validity expires?**\n\nFirstly, hash does not depend on the timestamp, it depends on the yaml in the underlying code. That’s why the material name is `material_name_HASH_SEQNO`. The sequence number (SEQNO) depends on timestamp.\n\nSecondly, a material generated for a specific timestamp (aside for the timeless timestamp) is not regenerated unless you do a `pb run --force`. The CLI checks if the material you are requesting already exists in the database, and if it does, returns that. The `validity_time` is an extension of that.\n\nFor a model with `validity_time: 24h` and inputs having the `timestamp` columns, if you request a material for latest time, but one was generated for that model 5 minutes ago, the CLI will return that one instead. Using the CLI to run a model always generates a model for a certain timestamp, it’s just if you don’t specify a timestamp then it uses the current timestamp.\n\nSo, for a model with validity\\_time (vt), and having the `timestamp` columns, if you request a material for t1, but one already exists for t0 where t1-vt <= t0 <= t1, the CLI will return that one instead.\n\nIf multiple materials exist that satisfy the requirement, then it returns the one with the timestamp closest to t1.\n\n**I want to use `customer_id` instead of `main_id` as the ID type. So I changed the name in `pb_project.yaml`, however now I am getting this error: `Error: validating project sample_attribution: listing models for child source models/: error listing models: error building model domain_profile_id_stitcher: main id type main_id not in project id types`.**\n\nIn addition to making changes in the file `pb_project.yaml` file, you also need to set `main_id_type: customer_id` in the `models/profiles.yaml` file.\n\n**I ran identity stitching model but not able to see the output tables under the list of tables in Snowflake. What might be wrong?**\n\nIn Snowflake, you can check the **Databases** > **Views** dropdown from the left sidebar. For example, if your model name is `domain_profile_id_stitcher`, you should be able to see the table with this name. In case it is still not visible, try changing the role using dropdown menu from the top right section.\n\n**I am using a view as an input source but getting ann error that the view is not accessible, even though it exists in DB.**\n\nViews need to be refreshed from time-to-time. You can try recreating the view in your warehouse and also execute a `select *` on the same.\n\n**What might be the reason for following errors:**\n\n*   `processing no result iterator: pq: cannot change number of columns in view`. The output view name already exists in some other project. To fix this, try dropping the view or changing its name.\n    \n*   `creating Latest View of moldel 'model_name': processing no result iterator: pq: cannot change data type of view column \"valid_at\"` Drop the view `domain_profile` in your warehouse and execute the command again.\n    \n*   `processing no result iterator: pq: column \"rudder_id\" does not exist`. This occurs when you execute a PB project with a model name, having `main_id` in it, and then you run another project with the same model name but no `main_id`. To resolve this, try dropping the earlier materials using `cleanup materials` command.\n    \n\n**I have a source table in which `email` gets stored in the column for `user_id`, so the field has a mix of different ID types. I have to tie it to another table where email is a separate field. When doing so, I have two separate entries for email, as type `email` and `user_id`. What should I do?**\n\nYou can implement the following line in the inputs tables in question:\n\n```\n  - select: case when lower(user_id) like '%@%' THEN lower(user_id) else null end\n    type: email \n    entity: user\n    to_default_stitcher: true\n```\n\n**How do I validate the results of identity stitching model?**\n\nContact [RudderStack Support](mailto:support@rudderstack.com) if you need help in validating the clusters.\n\n**Which identifiers would you recommend that I include in the ID stitcher for an ecommerce Profiles project?**\n\nWe suggest including identifiers that are unique for every user and can be tracked across different platforms and devices. These identifiers might include but not limited to:\n\n*   Email ID\n*   Phone number\n*   Device ID\n*   Anonymous ID\n*   User names\n\nThese identifiers can be specified in the file `profiles.yaml` file in the identity stitching model.\n\nRemember, the goal of identity stitching is to create a unified user profile by correlating all of the different user identifiers into one canonical identifier, so that all the data related to a particular user or entity can be associated with that user or entity.\n\n* * *\n\n## Feature Table\n\n**How can I run a feature table without running its dependencies?**\n\nSuppose you want to re-run the user entity\\_var `days_active` and the `rsTracks` input\\_var `last_seen` for a previous run with `seq_no 18`.\n\nThen, execute the following command:\n\n```\npb run --force --model_refs entity/user/days_active,inputs/rsTracks/last_seen --seq_no 18\n```\n\n**Is it possible to run the feature table model independently, or does it require running alongside the ID stitcher model?**\n\nYou can provide a specific timestamp while running the project, instead of using the default latest time. PB recognizes if you have previously executed an identity stitching model for that time and reuses that table instead of generating it again.\n\nYou can execute a command similar to: `pb run --begin_time 2023-06-02T12:00:00.0Z --end_time 2023-06-03T12:00:00.0Z`. Note that:\n\n*   To reuse a specific identity stitching model, the timestamp value must match exactly to when it was run.\n*   If you have executed identity stitching model in the incremental mode and do not have an exact timestamp for reusing it, you can select any timestamp **greater** than a non-deleted run. This is because subsequent stitching takes less time.\n*   To perform another identity stitching using PB, pick a timestamp (for example, `1681542000`) and stick to it while running the feature table model. For example, the first time you execute `pb run --begin_time 2023-06-02T12:00:00.0Z --end_time 2023-06-03T12:00:00.0Z`, it will run the identity stitching model along with the feature models. However, in subsequent runs, it will reuse the identity stitching model and only run the feature table models.\n\n**While trying to add a feature table, I get an error at line 501, but I do not have these many lines in my YAML.**\n\nThe line number refers to the generated SQL file in the output folder. Check the console for the exact file name with the sequence number in the path.\n\n**While creating a feature table, I get this error:** `Material needs to be created but could not be: processing no result iterator: 001104 (42601): Uncaught exception of type 'STATEMENT ERROR': 'SYS _W. FIRSTNAME' in select clause is neither an aggregate nor in the group by clause.`\n\nThis error occurs when you use a window function `any_value` that requires a window frame clause. For example:\n\n```\n  - entity_var:\n      name: email\n      select: LAST_VALUE(email)\n      from: inputs/rsIdentifies\n      window:\n        order_by: \n        - timestamp desc\n```\n\n**Is it possible to create a feature out of an identifier? For example, I have a RS user\\_main\\_id with two of user\\_ids stitched to it. Only one of the user\\_ids has a purchase under it. Is it possible to show that user\\_id in the feature table for this particular user\\_main\\_id?**\n\nIf you know which input/warehouse table served as the source for that particular ID type, then you can create features from any input and also apply a `WHERE` clause within the entity\\_var.\n\nFor example, you can create an aggregate array of user\\_id’s from the purchase history table, where total\\_price > 0 (exclude refunds, for example). Or, if you have some LTV table with user\\_id’s, you could exclude LTV < 0.\n\n* * *\n\n## YAML\n\n**Are there any best practices I should follow when writing the PB project’s YAML files?**\n\n*   Use spaces instead of tabs.\n*   Always use proper casing. For example: id\\_stitching, and not id\\_Stitching.\n*   Make sure that the source table you are referring to, exists in data warehouse or data has been loaded into it.\n*   If you’re pasting table names from your Snowflake Console, remove the double quotes in the `inputs.yaml` file.\n*   Make sure your syntax is correct. You can compare with the sample files.\n*   Indentation is meaningful in YAML, make sure that the spaces have same level as given in sample files.\n\n**How do I debug my YAML file step-by-step?**\n\nYou can use the `--model_args` parameter of the `pb run` command to do so. It lets you run your YAML file till a specific feature/tablevar. For example:\n\n```\n$ pb run -p samples/attribution --model_args domain_profile:breakpoint:blacklistFlag\n```\n\nSee [run command](https://www.rudderstack.com/docs/archive/profiles/0.10/developer-guides/commands/#run) for more information.\n\n> ![info](https://www.rudderstack.com/docs/images/info.svg)\n> \n> This is only applicable to versions prior to v0.9.0.\n\n**Can I use double quotes when referencing another entity\\_var in a macro?**\n\nYou can use an escape character. For example:\n\n```\n  - entity_var:\n      name: days_since_last_seen\n      select: \"{{macro_datediff('{{user.Var(\\\"max_timestamp_bw_tracks_pages\\\")}}')}}\"\n```\n\nAlso, if have a case statement, then you can add something like the following:\n\n`select: CASE WHEN {{user.Var(\"max_timestamp_tracks\")}}>={{user.Var(\"max_timestamp_pages\")}} THEN {{user.Var(\"max_timestamp_tracks\")}} ELSE {{user.Var(\"max_timestamp_pages\")}} END`\n\n* * *\n\n## ML / Python Models\n\n**Despite deleting WhtGitCache folder and adding keys to siteconfig, I get this error:** `Error: loading project: populating dependencies for project:base_features, model: churn_30_days_model: getting creator recipe while trying to get ProjectFolder: fetching git folder for git@github.com:rudderlabs/rudderstack-profiles-classifier.git: running git plain clone: repository not found`\n\nIf your token is valid, then replace `git@github.com:rudderlabs/rudderstack-profiles-classifier.git` with `https://github.com/rudderlabs/rudderstack-profiles-classifier.git` in the `profile-ml` file.\n\n* * *\n\n## Miscellaneous\n\n**Why am I getting _Authentication FAILED_ error on my data warehouse while executing the run/compile commands?**\n\nSome possible reasons for this error might be:\n\n*   Incorrect warehouse credentials.\n*   Insufficient user permissions to read and write data. You can ask your administrator to change your role or grant these privileges.\n\n**Why am I getting _Object does not exist or not authorized_ error on running this SQL query: `SELECT * FROM \"MY_WAREHOUSE\".\"MY_SCHEMA\".\"Material_domain_profile_c0635987_6\"`?**\n\nYou must remove double quotes from your warehouse and schema names before running the query, that is `SELECT * FROM MY_WAREHOUSE.MY_SCHEMA.Material_domain_profile_c0635987_6`.\n\n**Is there a way to obtain the timestamp of any material table?**\n\nYes, you can use the `GetTimeFilteringColSQL()` method to get the timestamp column of any material. It filters out rows based on the timestamp. It returns the `occurred_at_col` in case of an event\\_stream table or `valid_at` in case the material has that column. In absense of both, it returns an empty string. For example:\n\n```\n  SELECT * FROM {<from_material>}\n    WHERE\n      <from_material>.GetTimeFilteringColSQL() > <some_timestamp>;\n```\n\n**What is the difference between setting up Profiles in the RudderStack dashboard and Profile Builder CLI tool?**\n\nYou can run Profiles in the RudderStack dashboard or via [Profile Builder CLI](https://www.rudderstack.com/docs/archive/profiles/0.10/get-started/profile-builder/).\n\nThe main difference is that the RudderStack dashboard only generates outputs based on the pre-defined templates. However, you can augment those outputs by downloading the config file and updating it manually.\n\nOn the other hand, the CLI tool lets you achieve the end-to-end flow via creating a Profile Builder project.\n\n**Does the Profiles tool have logging enabled by default for security and compliance purposes?**\n\nLogging is enabled by default for nearly all the commands executed by CLI (`init`, `validate access`, `compile`, `run`, `cleanup`, etc.). Logs for all the output shown on screen are stored in the file `logfile.log` in the **logs** directory of your project folder. This includes logs for both successful and failed runs. RudderStack appends new entries at the end of the file once a command is executed.\n\nSome exceptions where the logs are not stored are:\n\n*   `query`: The logger file stores the printing output and does not store the actual database output. However, you can access the SQL queries logs in your warehouse.\n*   `help`: For any command.\n\n**In the warehouse, I see lots of material\\_user\\_id\\_stitcher\\_ tables generated in the rs\\_profiles schema. How do I identify the latest ID stitched table?**\n\nThe view `user_id_stitcher` will always point to the latest generated ID stitcher. You may check its definition to see the exact table name it is referring to.\n\n**How can I remove the material tables that are no longer needed?**\n\nTo clean up all the materials older than a specific duration, for example 10 days, execute the following command:\n\n```\npb cleanup materials -r 10\n```\n\nThe minimum value you can set here is `1`. So if you have run the ID stitcher today, then you can remove all the older materials using `pb cleanup materials -r 1`.\n\n**Which tables and views are important in Profiles schema that should not be deleted?**\n\n*   `material_registry`\n*   `material_registry_<number>`\n*   `pb_ct_version`\n*   `ptr_to_latest_seqno_cache`\n*   `wht_seq_no`\n*   `wht_seq_no_<number>`\n*   Views whose names match your models in the YAML files.\n*   Material tables from the latest run (you may use the `pb cleanup materials` command to delete materials older than a specific duration).\n\n**I executed the auto migrate command and now I see a bunch of nested** `original_project_folder`. **Are we migrating through each different version of the tool?**\n\nThis is a symlink to the original project. Click on it in the Finder (Mac) to open the original project folder.\n\n\\*\\*I am getting a \\*\\* `ssh: handshake failed` **error when referring to a public project hosted on GitHub. It throws error for https:// path and works fine for ssh: path. I have set up token in GitHub and added to siteconfig.yaml file but I still get this error.**\n\nYou need to follow a different format for `gitcreds:` in siteconfig. See [SiteConfiguration](https://www.rudderstack.com/docs/archive/profiles/0.10/developer-guides/site-configuration/) for the format.\n\nAfter changing `siteconfig`, if you still get an error, then clear the `WhtGitCache` folder inside the directory having the `siteconfig` file.\n\n**If I add filters to** `id_types` **in the project file, then do all rows that include any of those values get filtered out of the analysis, or is it just the specific value of that id type that gets filered?**\n\nThe PB tool does not extract rows. Instead, it extracts pairs from rows.\n\nSo if you had a row with email, user\\_id, and anonymous\\_id and the anonymous\\_id is excluded, then the PB tool still extracts the email, user\\_id edge from the row.\n\n**In the material registry table, what does** `status: 2` **mean?**\n\n*   `status: 2` means that the material has successfully completed its run.\n*   `status: 1` means that the material did not complete its run.\n\n**I am using Windows and get the following error:** `Error: while trying to migrate project: applying migrations: symlink <path>: A required privilege is not held by the client`.\n\nYour user requires privileges to create a symlink. You may either grant extra privileges to the user or try with a user containing Admin privileges on PowerShell. In case that doesn’t help, try to install and use it via WSL (Widows subsystem for Linux).\n\n**Can I specify any git account like CommitCode or BitBucket while configuring a project in the web app?**\n\nProfiles UI only supports the repos hosted on GitHub currently.\n\nQuestions? Contact us by [email](mailto:docs@rudderstack.com) or on [Slack](https://rudderstack.com/join-rudderstack-slack-community)",
    "title": "Profiles FAQ | RudderStack Docs",
    "description": "Commonly asked questions on RudderStack Profiles.",
    "languageCode": "en"
  }
]